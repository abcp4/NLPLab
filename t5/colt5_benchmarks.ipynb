{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from colt5_attention import (\n",
    "    ConditionalRoutedFeedForward,\n",
    "    ConditionalRoutedAttention,\n",
    "    ConditionalRoutedTransformerBlock\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mock input, say it is 32768 length\n",
    "\n",
    "tokens = torch.randn(2, 32768, 512)\n",
    "mask = torch.ones(2, 32768).bool()  # can handle variable lengthed sequences\n",
    "\n",
    "# feedforward\n",
    "\n",
    "ff = ConditionalRoutedFeedForward(\n",
    "    dim = 512,\n",
    "    light_ff_mult = 0.5,      # hidden dimension ratio of light branch\n",
    "    heavy_ff_mult = 4,        # hidden dimension ratio of heavy branch\n",
    "    num_heavy_tokens = 1024   # heavy branch receives only 1024 routed tokens of 32768\n",
    ")\n",
    "\n",
    "ff_out = ff(tokens, mask = mask)  # (2, 32768, 512) - light and heavy branch summed\n",
    "\n",
    "# attention\n",
    "\n",
    "attn = ConditionalRoutedAttention(\n",
    "    dim = 512,\n",
    "    light_dim_head = 64,       # attention head dimension of light branch\n",
    "    light_heads = 8,           # number of attention heads for light branch\n",
    "    light_window_size = 128,   # local attention receptive field for light\n",
    "    heavy_dim_head = 64,       # attention head dimension of heavy branch\n",
    "    heavy_heads = 8,           # number of attention heads for heavy branch\n",
    "    num_heavy_tokens_q = 1024, # heavy branch receives only 1024 routed tokens of 32768\n",
    "    num_heavy_tokens_kv = 1024 # heavy branch receives only 1024 routed tokens of 32768\n",
    ")\n",
    "\n",
    "block = ConditionalRoutedTransformerBlock(\n",
    "    dim = 512,\n",
    "    light_dim_head = 64,\n",
    "    light_heads = 8,\n",
    "    light_window_size = 128,\n",
    "    heavy_dim_head = 64,\n",
    "    heavy_heads = 8,\n",
    "    light_ff_mult = 0.5,\n",
    "    heavy_ff_mult = 4,\n",
    "    num_heavy_ff_tokens = 1024,\n",
    "    num_heavy_attn_tokens_q = 1024,\n",
    "    num_heavy_attn_tokens_kv = 1024\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for 100 iterations:  117.16679191589355\n"
     ]
    }
   ],
   "source": [
    "#time the for loop\n",
    "import time\n",
    "start = time.time()\n",
    "for i in range(100):\n",
    "    block_out = block(tokens, mask = mask) # (2, 32768, 512)\n",
    "end = time.time()\n",
    "\n",
    "print(\"Time taken for 100 iterations: \", end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "input 10x smaller, loop 10x more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mock input, say it is 32768 length\n",
    "\n",
    "tokens = torch.randn(2, 3276, 512)\n",
    "mask = torch.ones(2, 3276).bool()  # can handle variable lengthed sequences\n",
    "\n",
    "# feedforward\n",
    "\n",
    "ff = ConditionalRoutedFeedForward(\n",
    "    dim = 512,\n",
    "    light_ff_mult = 0.5,      # hidden dimension ratio of light branch\n",
    "    heavy_ff_mult = 4,        # hidden dimension ratio of heavy branch\n",
    "    num_heavy_tokens = 1024   # heavy branch receives only 1024 routed tokens of 32768\n",
    ")\n",
    "\n",
    "ff_out = ff(tokens, mask = mask)  # (2, 32768, 512) - light and heavy branch summed\n",
    "\n",
    "# attention\n",
    "\n",
    "attn = ConditionalRoutedAttention(\n",
    "    dim = 512,\n",
    "    light_dim_head = 64,       # attention head dimension of light branch\n",
    "    light_heads = 8,           # number of attention heads for light branch\n",
    "    light_window_size = 128,   # local attention receptive field for light\n",
    "    heavy_dim_head = 64,       # attention head dimension of heavy branch\n",
    "    heavy_heads = 8,           # number of attention heads for heavy branch\n",
    "    num_heavy_tokens_q = 102, # heavy branch receives only 1024 routed tokens of 32768\n",
    "    num_heavy_tokens_kv = 102 # heavy branch receives only 1024 routed tokens of 32768\n",
    ")\n",
    "\n",
    "block = ConditionalRoutedTransformerBlock(\n",
    "    dim = 512,\n",
    "    light_dim_head = 64,\n",
    "    light_heads = 8,\n",
    "    light_window_size = 128,\n",
    "    heavy_dim_head = 64,\n",
    "    heavy_heads = 8,\n",
    "    light_ff_mult = 0.5,\n",
    "    heavy_ff_mult = 4,\n",
    "    num_heavy_ff_tokens = 102,\n",
    "    num_heavy_attn_tokens_q = 102,\n",
    "    num_heavy_attn_tokens_kv = 102\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for 1000 iterations:  116.82632446289062\n"
     ]
    }
   ],
   "source": [
    "#time the for loop\n",
    "import time\n",
    "start = time.time()\n",
    "for i in range(1000):   \n",
    "    block_out = block(tokens, mask = mask) # (2, 3276, 512)\n",
    "end = time.time()\n",
    "\n",
    "print(\"Time taken for 1000 iterations: \", end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mock input, say it is 32768 length\n",
    "\n",
    "tokens = torch.randn(2, 327, 512)\n",
    "mask = torch.ones(2, 327).bool()  # can handle variable lengthed sequences\n",
    "\n",
    "# feedforward\n",
    "\n",
    "ff = ConditionalRoutedFeedForward(\n",
    "    dim = 512,\n",
    "    light_ff_mult = 0.5,      # hidden dimension ratio of light branch\n",
    "    heavy_ff_mult = 4,        # hidden dimension ratio of heavy branch\n",
    "    num_heavy_tokens = 32   # heavy branch receives only 1024 routed tokens of 32768\n",
    ")\n",
    "\n",
    "ff_out = ff(tokens, mask = mask)  # (2, 32768, 512) - light and heavy branch summed\n",
    "\n",
    "# attention\n",
    "\n",
    "attn = ConditionalRoutedAttention(\n",
    "    dim = 512,\n",
    "    light_dim_head = 64,       # attention head dimension of light branch\n",
    "    light_heads = 8,           # number of attention heads for light branch\n",
    "    light_window_size = 128,   # local attention receptive field for light\n",
    "    heavy_dim_head = 64,       # attention head dimension of heavy branch\n",
    "    heavy_heads = 8,           # number of attention heads for heavy branch\n",
    "    num_heavy_tokens_q = 32, # heavy branch receives only 1024 routed tokens of 32768\n",
    "    num_heavy_tokens_kv = 32 # heavy branch receives only 1024 routed tokens of 32768\n",
    ")\n",
    "\n",
    "block = ConditionalRoutedTransformerBlock(\n",
    "    dim = 512,\n",
    "    light_dim_head = 64,\n",
    "    light_heads = 8,\n",
    "    light_window_size = 128,\n",
    "    heavy_dim_head = 64,\n",
    "    heavy_heads = 8,\n",
    "    light_ff_mult = 0.5,\n",
    "    heavy_ff_mult = 4,\n",
    "    num_heavy_ff_tokens = 32,\n",
    "    num_heavy_attn_tokens_q = 32,\n",
    "    num_heavy_attn_tokens_kv = 32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for 1000 iterations:  132.3905553817749\n"
     ]
    }
   ],
   "source": [
    "#time the for loop\n",
    "import time\n",
    "start = time.time()\n",
    "for i in range(10000):   \n",
    "    block_out = block(tokens, mask = mask) # (2, 327, 512)\n",
    "end = time.time()\n",
    "\n",
    "print(\"Time taken for 1000 iterations: \", end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mock input, say it is 32768 length\n",
    "\n",
    "tokens = torch.randn(2, 109, 512)\n",
    "mask = torch.ones(2, 109).bool()  # can handle variable lengthed sequences\n",
    "\n",
    "# feedforward\n",
    "\n",
    "ff = ConditionalRoutedFeedForward(\n",
    "    dim = 512,\n",
    "    light_ff_mult = 0.5,      # hidden dimension ratio of light branch\n",
    "    heavy_ff_mult = 4,        # hidden dimension ratio of heavy branch\n",
    "    num_heavy_tokens = 10   # heavy branch receives only 1024 routed tokens of 32768\n",
    ")\n",
    "\n",
    "ff_out = ff(tokens, mask = mask)  # (2, 32768, 512) - light and heavy branch summed\n",
    "\n",
    "# attention\n",
    "\n",
    "attn = ConditionalRoutedAttention(\n",
    "    dim = 512,\n",
    "    light_dim_head = 64,       # attention head dimension of light branch\n",
    "    light_heads = 8,           # number of attention heads for light branch\n",
    "    light_window_size = 128,   # local attention receptive field for light\n",
    "    heavy_dim_head = 64,       # attention head dimension of heavy branch\n",
    "    heavy_heads = 8,           # number of attention heads for heavy branch\n",
    "    num_heavy_tokens_q = 10, # heavy branch receives only 1024 routed tokens of 32768\n",
    "    num_heavy_tokens_kv = 10 # heavy branch receives only 1024 routed tokens of 32768\n",
    ")\n",
    "\n",
    "block = ConditionalRoutedTransformerBlock(\n",
    "    dim = 512,\n",
    "    light_dim_head = 64,\n",
    "    light_heads = 8,\n",
    "    light_window_size = 128,\n",
    "    heavy_dim_head = 64,\n",
    "    heavy_heads = 8,\n",
    "    light_ff_mult = 0.5,\n",
    "    heavy_ff_mult = 4,\n",
    "    num_heavy_ff_tokens = 10,\n",
    "    num_heavy_attn_tokens_q = 10,\n",
    "    num_heavy_attn_tokens_kv = 10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for 1000 iterations:  247.19424271583557\n"
     ]
    }
   ],
   "source": [
    "#time the for loop\n",
    "import time\n",
    "start = time.time()\n",
    "for i in range(30000):   \n",
    "    block_out = block(tokens, mask = mask) # (2, 327, 512)\n",
    "end = time.time()\n",
    "\n",
    "print(\"Time taken for 1000 iterations: \", end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mock input, say it is 32768 length\n",
    "\n",
    "tokens = torch.randn(20, 3276, 512)\n",
    "mask = torch.ones(20, 3276).bool()  # can handle variable lengthed sequences\n",
    "\n",
    "# feedforward\n",
    "\n",
    "ff = ConditionalRoutedFeedForward(\n",
    "    dim = 512,\n",
    "    light_ff_mult = 0.5,      # hidden dimension ratio of light branch\n",
    "    heavy_ff_mult = 4,        # hidden dimension ratio of heavy branch\n",
    "    num_heavy_tokens = 102   # heavy branch receives only 1024 routed tokens of 32768\n",
    ")\n",
    "\n",
    "ff_out = ff(tokens, mask = mask)  # (2, 32768, 512) - light and heavy branch summed\n",
    "\n",
    "# attention\n",
    "\n",
    "attn = ConditionalRoutedAttention(\n",
    "    dim = 512,\n",
    "    light_dim_head = 64,       # attention head dimension of light branch\n",
    "    light_heads = 8,           # number of attention heads for light branch\n",
    "    light_window_size = 128,   # local attention receptive field for light\n",
    "    heavy_dim_head = 64,       # attention head dimension of heavy branch\n",
    "    heavy_heads = 8,           # number of attention heads for heavy branch\n",
    "    num_heavy_tokens_q = 102, # heavy branch receives only 1024 routed tokens of 32768\n",
    "    num_heavy_tokens_kv = 102 # heavy branch receives only 1024 routed tokens of 32768\n",
    ")\n",
    "\n",
    "block = ConditionalRoutedTransformerBlock(\n",
    "    dim = 512,\n",
    "    light_dim_head = 64,\n",
    "    light_heads = 8,\n",
    "    light_window_size = 128,\n",
    "    heavy_dim_head = 64,\n",
    "    heavy_heads = 8,\n",
    "    light_ff_mult = 0.5,\n",
    "    heavy_ff_mult = 4,\n",
    "    num_heavy_ff_tokens = 102,\n",
    "    num_heavy_attn_tokens_q = 102,\n",
    "    num_heavy_attn_tokens_kv = 102\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for 100 iterations:  12.182886838912964\n"
     ]
    }
   ],
   "source": [
    "#time the for loop\n",
    "import time\n",
    "start = time.time()\n",
    "for i in range(10):\n",
    "    block_out = block(tokens, mask = mask) # (2, 32768, 512)\n",
    "end = time.time()\n",
    "\n",
    "print(\"Time taken for 100 iterations: \", end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mock input, say it is 32768 length\n",
    "\n",
    "tokens = torch.randn(200, 327, 512)\n",
    "mask = torch.ones(200, 327).bool()  # can handle variable lengthed sequences\n",
    "\n",
    "# feedforward\n",
    "\n",
    "ff = ConditionalRoutedFeedForward(\n",
    "    dim = 512,\n",
    "    light_ff_mult = 0.5,      # hidden dimension ratio of light branch\n",
    "    heavy_ff_mult = 4,        # hidden dimension ratio of heavy branch\n",
    "    num_heavy_tokens = 10   # heavy branch receives only 1024 routed tokens of 32768\n",
    ")\n",
    "\n",
    "ff_out = ff(tokens, mask = mask)  # (2, 32768, 512) - light and heavy branch summed\n",
    "\n",
    "# attention\n",
    "\n",
    "attn = ConditionalRoutedAttention(\n",
    "    dim = 512,\n",
    "    light_dim_head = 64,       # attention head dimension of light branch\n",
    "    light_heads = 8,           # number of attention heads for light branch\n",
    "    light_window_size = 128,   # local attention receptive field for light\n",
    "    heavy_dim_head = 64,       # attention head dimension of heavy branch\n",
    "    heavy_heads = 8,           # number of attention heads for heavy branch\n",
    "    num_heavy_tokens_q = 10, # heavy branch receives only 1024 routed tokens of 32768\n",
    "    num_heavy_tokens_kv = 10 # heavy branch receives only 1024 routed tokens of 32768\n",
    ")\n",
    "\n",
    "block = ConditionalRoutedTransformerBlock(\n",
    "    dim = 512,\n",
    "    light_dim_head = 64,\n",
    "    light_heads = 8,\n",
    "    light_window_size = 128,\n",
    "    heavy_dim_head = 64,\n",
    "    heavy_heads = 8,\n",
    "    light_ff_mult = 0.5,\n",
    "    heavy_ff_mult = 4,\n",
    "    num_heavy_ff_tokens = 10,\n",
    "    num_heavy_attn_tokens_q = 10,\n",
    "    num_heavy_attn_tokens_kv = 10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for 100 iterations:  13.222659587860107\n"
     ]
    }
   ],
   "source": [
    "#time the for loop\n",
    "import time\n",
    "start = time.time()\n",
    "for i in range(10):\n",
    "    block_out = block(tokens, mask = mask) # (2, 32768, 512)\n",
    "end = time.time()\n",
    "\n",
    "print(\"Time taken for 100 iterations: \", end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mock input, say it is 32768 length\n",
    "\n",
    "tokens = torch.randn(2000, 32, 512)\n",
    "mask = torch.ones(2000, 32).bool()  # can handle variable lengthed sequences\n",
    "\n",
    "# feedforward\n",
    "\n",
    "ff = ConditionalRoutedFeedForward(\n",
    "    dim = 512,\n",
    "    light_ff_mult = 0.5,      # hidden dimension ratio of light branch\n",
    "    heavy_ff_mult = 4,        # hidden dimension ratio of heavy branch\n",
    "    num_heavy_tokens = 3  # heavy branch receives only 1024 routed tokens of 32768\n",
    ")\n",
    "\n",
    "ff_out = ff(tokens, mask = mask)  # (2, 32768, 512) - light and heavy branch summed\n",
    "\n",
    "# attention\n",
    "\n",
    "attn = ConditionalRoutedAttention(\n",
    "    dim = 512,\n",
    "    light_dim_head = 64,       # attention head dimension of light branch\n",
    "    light_heads = 8,           # number of attention heads for light branch\n",
    "    light_window_size = 128,   # local attention receptive field for light\n",
    "    heavy_dim_head = 64,       # attention head dimension of heavy branch\n",
    "    heavy_heads = 8,           # number of attention heads for heavy branch\n",
    "    num_heavy_tokens_q = 3, # heavy branch receives only 1024 routed tokens of 32768\n",
    "    num_heavy_tokens_kv = 3 # heavy branch receives only 1024 routed tokens of 32768\n",
    ")\n",
    "\n",
    "block = ConditionalRoutedTransformerBlock(\n",
    "    dim = 512,\n",
    "    light_dim_head = 64,\n",
    "    light_heads = 8,\n",
    "    light_window_size = 128,\n",
    "    heavy_dim_head = 64,\n",
    "    heavy_heads = 8,\n",
    "    light_ff_mult = 0.5,\n",
    "    heavy_ff_mult = 4,\n",
    "    num_heavy_ff_tokens = 3,\n",
    "    num_heavy_attn_tokens_q = 3,\n",
    "    num_heavy_attn_tokens_kv = 3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for 100 iterations:  18.901021242141724\n"
     ]
    }
   ],
   "source": [
    "#time the for loop\n",
    "import time\n",
    "start = time.time()\n",
    "for i in range(10):\n",
    "    block_out = block(tokens, mask = mask) # (2, 32768, 512)\n",
    "end = time.time()\n",
    "\n",
    "print(\"Time taken for 100 iterations: \", end - start)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
