{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#roda com ambiente densephrases\n",
    "#!git clone -b v1.1.0 https://github.com/princeton-nlp/DensePhrases.git"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Question and Phrase Encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -U pydantic spacy==3.4.4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install git+https://github.com/huggingface/transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/miu/miniconda3/envs/proconvqa/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "from arguments import parse_args\n",
    "import json\n",
    "import logging\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "import datasets\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import torch\n",
    "from accelerate import Accelerator\n",
    "from accelerate.logging import get_logger\n",
    "from accelerate.utils import set_seed\n",
    "from datasets import load_dataset\n",
    "from huggingface_hub import Repository, create_repo\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "# from utils_qa import postprocess_qa_predictions\n",
    "\n",
    "import transformers\n",
    "from transformers import (\n",
    "    CONFIG_MAPPING,\n",
    "    MODEL_MAPPING,\n",
    "    AutoConfig,\n",
    "    AutoModelForQuestionAnswering,\n",
    "    AutoTokenizer,\n",
    "    get_scheduler,\n",
    ")\n",
    "#from transformers.utils import check_min_version, get_full_repo_name, send_example_telemetry\n",
    "from transformers.utils.versions import require_version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Will error if the minimal version of Transformers is not installed. Remove at your own risks.\n",
    "#check_min_version(\"4.27.0.dev0\")\n",
    "\n",
    "require_version(\"datasets>=1.8.0\", \"To fix: pip install -r examples/pytorch/question-answering/requirements.txt\")\n",
    "\n",
    "logger = get_logger(__name__)\n",
    "# You should update this to your particular problem to have better documentation of `model_type`\n",
    "MODEL_CONFIG_CLASSES = list(MODEL_MAPPING.keys())\n",
    "MODEL_TYPES = tuple(conf.model_type for conf in MODEL_CONFIG_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/29/2023 11:55:43 - INFO - __main__ - Distributed environment: NO\n",
      "Num processes: 1\n",
      "Process index: 0\n",
      "Local process index: 0\n",
      "Device: cuda\n",
      "\n",
      "Mixed precision type: no\n",
      "\n"
     ]
    }
   ],
   "source": [
    "args = parse_args()\n",
    "\n",
    "# Sending telemetry. Tracking the example usage helps us better allocate resources to maintain them. The\n",
    "# information sent is the one passed as arguments along with your Python/PyTorch versions.\n",
    "#send_example_telemetry(\"run_qa_no_trainer\", args)\n",
    "\n",
    "# Initialize the accelerator. We will let the accelerator handle device placement for us in this example.\n",
    "# If we're using tracking, we also need to initialize it here and it will by default pick up all supported trackers\n",
    "# in the environment\n",
    "accelerator_log_kwargs = {}\n",
    "\n",
    "if args.with_tracking:\n",
    "    accelerator_log_kwargs[\"log_with\"] = args.report_to\n",
    "    accelerator_log_kwargs[\"logging_dir\"] = args.output_dir\n",
    "\n",
    "accelerator = Accelerator(gradient_accumulation_steps=args.gradient_accumulation_steps, **accelerator_log_kwargs)\n",
    "\n",
    "# Make one log on every process with the configuration for debugging.\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
    "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "    level=logging.INFO,\n",
    ")\n",
    "logger.info(accelerator.state, main_process_only=False)\n",
    "if accelerator.is_local_main_process:\n",
    "    datasets.utils.logging.set_verbosity_warning()\n",
    "    transformers.utils.logging.set_verbosity_info()\n",
    "else:\n",
    "    datasets.utils.logging.set_verbosity_error()\n",
    "    transformers.utils.logging.set_verbosity_error()\n",
    "\n",
    "# If passed along, set the training seed now.\n",
    "if args.seed is not None:\n",
    "    set_seed(args.seed)\n",
    "\n",
    "# Handle the repository creation\n",
    "if accelerator.is_main_process:\n",
    "    if args.push_to_hub:\n",
    "        if args.hub_model_id is None:\n",
    "            repo_name = get_full_repo_name(Path(args.output_dir).name, token=args.hub_token)\n",
    "        else:\n",
    "            repo_name = args.hub_model_id\n",
    "        create_repo(repo_name, exist_ok=True, token=args.hub_token)\n",
    "        repo = Repository(args.output_dir, clone_from=repo_name, token=args.hub_token)\n",
    "\n",
    "        with open(os.path.join(args.output_dir, \".gitignore\"), \"w+\") as gitignore:\n",
    "            if \"step_*\" not in gitignore:\n",
    "                gitignore.write(\"step_*\\n\")\n",
    "            if \"epoch_*\" not in gitignore:\n",
    "                gitignore.write(\"epoch_*\\n\")\n",
    "    elif args.output_dir is not None:\n",
    "        os.makedirs(args.output_dir, exist_ok=True)\n",
    "accelerator.wait_for_everyone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "https://huggingface.co/bert-base-uncased/resolve/main/config.json not found in cache or force_download set to True, downloading to /home/miu/.var/app/com.visualstudio.code/cache/huggingface/transformers/tmp3dovd_hy\n",
      "Downloading: 100%|██████████| 570/570 [00:00<00:00, 189kB/s]\n",
      "storing https://huggingface.co/bert-base-uncased/resolve/main/config.json in cache at /home/miu/.var/app/com.visualstudio.code/cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "creating metadata file for /home/miu/.var/app/com.visualstudio.code/cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/miu/.var/app/com.visualstudio.code/cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.13.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json not found in cache or force_download set to True, downloading to /home/miu/.var/app/com.visualstudio.code/cache/huggingface/transformers/tmpowcxww6d\n",
      "Downloading: 100%|██████████| 28.0/28.0 [00:00<00:00, 39.4kB/s]\n",
      "storing https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json in cache at /home/miu/.var/app/com.visualstudio.code/cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
      "creating metadata file for /home/miu/.var/app/com.visualstudio.code/cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/miu/.var/app/com.visualstudio.code/cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.13.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt not found in cache or force_download set to True, downloading to /home/miu/.var/app/com.visualstudio.code/cache/huggingface/transformers/tmp45bt9b_7\n",
      "Downloading: 100%|██████████| 226k/226k [00:00<00:00, 703kB/s] \n",
      "storing https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt in cache at /home/miu/.var/app/com.visualstudio.code/cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "creating metadata file for /home/miu/.var/app/com.visualstudio.code/cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json not found in cache or force_download set to True, downloading to /home/miu/.var/app/com.visualstudio.code/cache/huggingface/transformers/tmpv3qsrvr1\n",
      "Downloading: 100%|██████████| 455k/455k [00:00<00:00, 997kB/s] \n",
      "storing https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json in cache at /home/miu/.var/app/com.visualstudio.code/cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
      "creating metadata file for /home/miu/.var/app/com.visualstudio.code/cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/miu/.var/app/com.visualstudio.code/cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/miu/.var/app/com.visualstudio.code/cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/miu/.var/app/com.visualstudio.code/cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/miu/.var/app/com.visualstudio.code/cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.13.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# args.model_name_or_path='xlnet-base-cased'\n",
    "#args.model_name_or_path='google/electra-small-discriminator'\n",
    "# args.model_name_or_path='SpanBERT/spanbert-base-cased'\n",
    "#distilbert\n",
    "# args.model_name_or_path='distilbert-base-uncased'\n",
    "args.model_name_or_path='bert-base-uncased'\n",
    "\n",
    "if args.config_name:\n",
    "    config = AutoConfig.from_pretrained(args.config_name)\n",
    "elif args.model_name_or_path:\n",
    "    config = AutoConfig.from_pretrained(args.model_name_or_path)\n",
    "else:\n",
    "    config = CONFIG_MAPPING[args.model_type]()\n",
    "    logger.warning(\"You are instantiating a new config instance from scratch.\")\n",
    "\n",
    "if args.tokenizer_name:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(args.tokenizer_name, use_fast=True)\n",
    "elif args.model_name_or_path:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(args.model_name_or_path, use_fast=True)\n",
    "else:\n",
    "    raise ValueError(\n",
    "        \"You are instantiating a new tokenizer from scratch. This is not supported by this script.\"\n",
    "        \"You can do it from another script, save it, and load it from here, using --tokenizer_name.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /home/miu/.var/app/com.visualstudio.code/cache/huggingface/transformers/tmp7tum2o5u\n",
      "Downloading: 100%|██████████| 420M/420M [00:14<00:00, 31.0MB/s] \n",
      "storing https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin in cache at /home/miu/.var/app/com.visualstudio.code/cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
      "creating metadata file for /home/miu/.var/app/com.visualstudio.code/cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
      "loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/miu/.var/app/com.visualstudio.code/cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n",
      "12/29/2023 11:56:07 - INFO - __main__ - DensePhrases encoder initialized with bert-base-uncased (<class 'transformers.models.bert.modeling_bert.BertModel'>)\n",
      "12/29/2023 11:56:07 - INFO - encoder - Initializing encoders with pre-trained LM\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "from encoder import Encoder\n",
    "from functools import partial\n",
    "from transformers import AutoModel\n",
    "\n",
    "# Prepare PLM if not load_dir\n",
    "pretrained = None\n",
    "phrase_only=False\n",
    "query_only=False\n",
    "\n",
    "load_encoder=False\n",
    "\n",
    "if not load_encoder:\n",
    "    pretrained = AutoModel.from_pretrained(\n",
    "        args.model_name_or_path,\n",
    "        config=config,\n",
    "        cache_dir=None,\n",
    "    )\n",
    "    load_class = Encoder\n",
    "    logger.info(f'DensePhrases encoder initialized with {args.model_name_or_path} ({pretrained.__class__})')\n",
    "else:\n",
    "    load_class = partial(\n",
    "        Encoder.from_pretrained,\n",
    "        pretrained_model_name_or_path=args.load_dir,\n",
    "        cache_dir=args.cache_dir if args.cache_dir else None,\n",
    "    )\n",
    "    logger.info(f'DensePhrases encoder loaded from {args.load_dir}')\n",
    "\n",
    "# DensePhrases encoder object\n",
    "model = load_class(\n",
    "    config=config,\n",
    "    tokenizer=tokenizer,\n",
    "    transformer_cls=MODEL_MAPPING[config.__class__],\n",
    "    pretrained=copy.deepcopy(pretrained) if pretrained is not None else None,\n",
    "    lambda_kl= 0.0,\n",
    "    lambda_neg=0.1,\n",
    "    lambda_flt=0.0,\n",
    "    pbn_size=0.0,\n",
    "    return_phrase=phrase_only,\n",
    "    return_query=query_only,\n",
    ")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/29/2023 01:25:25 - WARNING - datasets.builder - Found cached dataset squad (/home/miu/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453)\n",
      "100%|██████████| 2/2 [00:00<00:00, 835.10it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(DatasetDict({\n",
       "     train: Dataset({\n",
       "         features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "         num_rows: 87599\n",
       "     })\n",
       "     validation: Dataset({\n",
       "         features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "         num_rows: 10570\n",
       "     })\n",
       " }),\n",
       " {'question_column_name': 'question',\n",
       "  'column_names': ['id', 'title', 'context', 'question', 'answers'],\n",
       "  'context_column_name': 'context',\n",
       "  'answer_column_name': 'answers',\n",
       "  'pad_on_right': True,\n",
       "  'max_seq_length': 384})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from data_utils import prepare_train_features,prepare_validation_features\n",
    "from data_utils import get_datasets\n",
    "args.max_query_length=10\n",
    "raw_datasets,dataset_infos=get_datasets(args,tokenizer,logger)\n",
    "raw_datasets,dataset_infos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running tokenizer on train dataset:   0%|          | 0/88 [00:00<?, ?ba/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running tokenizer on train dataset:  99%|█████████▉| 87/88 [00:13<00:00,  6.65ba/s]\n",
      "Running tokenizer on validation dataset:  91%|█████████ | 10/11 [00:02<00:00,  3.96ba/s]\n"
     ]
    }
   ],
   "source": [
    "if \"train\" not in raw_datasets:\n",
    "    raise ValueError(\"--do_train requires a train dataset\")\n",
    "train_dataset = raw_datasets[\"train\"]\n",
    "if args.max_train_samples is not None:\n",
    "    # We will select sample from whole data if agument is specified\n",
    "    train_dataset = train_dataset.select(range(args.max_train_samples))\n",
    "\n",
    "# Create train feature from dataset\n",
    "with accelerator.main_process_first():\n",
    "    train_dataset = train_dataset.map(\n",
    "        # prepare_train_features,\n",
    "        lambda x: prepare_train_features(x, dataset_infos, tokenizer, args ),\n",
    "        batched=True,\n",
    "        num_proc=args.preprocessing_num_workers,\n",
    "        remove_columns=dataset_infos['column_names'],\n",
    "        load_from_cache_file=not args.overwrite_cache,\n",
    "        desc=\"Running tokenizer on train dataset\",\n",
    "    )\n",
    "    if args.max_train_samples is not None:\n",
    "        # Number of samples might increase during Feature Creation, We select only specified max samples\n",
    "        train_dataset = train_dataset.select(range(args.max_train_samples))\n",
    "\n",
    "if \"validation\" not in raw_datasets:\n",
    "    raise ValueError(\"--do_eval requires a validation dataset\")\n",
    "eval_examples = raw_datasets[\"validation\"]\n",
    "if args.max_eval_samples is not None:\n",
    "    # We will select sample from whole data\n",
    "    eval_examples = eval_examples.select(range(args.max_eval_samples))\n",
    "# Validation Feature Creation\n",
    "with accelerator.main_process_first():\n",
    "    eval_dataset = eval_examples.map(\n",
    "        # prepare_validation_features,\n",
    "        lambda x: prepare_validation_features(x, dataset_infos=dataset_infos, tokenizer=tokenizer, args=args),\n",
    "        batched=True,\n",
    "        num_proc=args.preprocessing_num_workers,\n",
    "        remove_columns=dataset_infos['column_names'],\n",
    "        load_from_cache_file=not args.overwrite_cache,\n",
    "        desc=\"Running tokenizer on validation dataset\",\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/28/2023 22:32:33 - INFO - __main__ - Sample 56952 of the training set: {'input_ids': [101, 2466, 1564, 1610, 27719, 1161, 1110, 1145, 1227, 1112, 1184, 136, 102, 5408, 1107, 1103, 3433, 6692, 1104, 2638, 1105, 2466, 1564, 1610, 27719, 1161, 117, 1137, 1103, 1822, 1583, 1105, 1146, 6754, 117, 4634, 1103, 1741, 117, 2670, 117, 1105, 1934, 1297, 1104, 1103, 1352, 1121, 1103, 4186, 1235, 1103, 3116, 1432, 119, 1103, 12600, 4669, 1107, 2638, 1564, 1610, 27719, 1161, 1108, 3035, 17118, 1118, 7162, 1121, 3738, 4035, 1403, 1931, 1105, 1103, 188, 11627, 2944, 27929, 119, 1103, 1146, 2528, 8355, 1616, 1104, 2466, 1564, 1610, 27719, 1161, 1108, 3035, 17118, 1118, 188, 18982, 1116, 118, 178, 4889, 1324, 117, 4035, 23655, 2737, 117, 1105, 176, 14170, 5641, 7418, 117, 1103, 1177, 118, 1270, 107, 1884, 19989, 107, 119, 7190, 1219, 1103, 2286, 118, 1106, 1523, 4186, 1432, 117, 1103, 188, 18982, 1116, 118, 178, 4889, 1324, 1121, 1184, 1110, 2052, 2350, 178, 9261, 5709, 1127, 1103, 2026, 1664, 118, 4035, 23655, 2737, 12338, 1372, 1196, 1103, 8011, 132, 4035, 23655, 2737, 1107, 11951, 10105, 8960, 1127, 10827, 1193, 1103, 2026, 12338, 1372, 1196, 1103, 8011, 119, 1219, 1103, 1821, 26237, 1389, 8953, 1594, 117, 1103, 4035, 23655, 2737, 1105, 1344, 1931, 188, 18982, 1116, 1104, 2638, 1564, 1610, 27719, 1161, 11097, 1106, 3118, 9125, 1106, 1103, 9304, 10721, 1324, 6371, 117, 1272, 1104, 1263, 17277, 1671, 1105, 2357, 6984, 1114, 1632, 9304, 5168, 1394, 119, 1103, 4035, 23655, 2737, 117, 1195, 3447, 1324, 117, 188, 18982, 1116, 118, 178, 4889, 1324, 117, 1105, 176, 14170, 7056, 1104, 2466, 1564, 1610, 27719, 1161, 11097, 1106, 5010, 1821, 26237, 1389, 4574, 1121, 9304, 5168, 1394, 119, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'input_ids_': [101, 2466, 1564, 1610, 27719, 1161, 1110, 1145, 1227, 102], 'token_type_ids_': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask_': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'start_positions': 32, 'end_positions': 33}.\n",
      "12/28/2023 22:32:33 - INFO - __main__ - Sample 5571 of the training set: {'input_ids': [101, 1184, 1674, 3084, 7272, 10559, 1776, 4649, 1329, 1241, 3009, 1105, 3084, 7272, 9597, 2233, 1112, 136, 102, 3084, 7272, 10559, 1776, 4649, 1110, 1103, 2025, 1104, 3084, 7272, 9597, 8708, 1105, 6854, 10148, 1118, 13766, 3009, 3002, 119, 1122, 1110, 1145, 1103, 2025, 1104, 1103, 1607, 1104, 1672, 5237, 2114, 1115, 1336, 1137, 1336, 1136, 4056, 2052, 119, 3084, 7272, 10559, 1776, 4649, 2745, 1241, 3009, 1105, 3084, 7272, 9597, 2233, 1112, 1157, 4686, 119, 1157, 3009, 4069, 1105, 3881, 1301, 2894, 1103, 2530, 1329, 1104, 4961, 1105, 11032, 119, 16681, 6239, 1103, 10345, 1104, 1216, 2674, 2578, 1112, 7415, 117, 1390, 117, 4694, 117, 6427, 117, 16106, 117, 9619, 3904, 117, 1751, 10016, 117, 8962, 3881, 117, 3480, 6286, 117, 21585, 10148, 117, 1846, 117, 1105, 1282, 2666, 119, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'input_ids_': [101, 1184, 1674, 3084, 7272, 10559, 1776, 4649, 1329, 102], 'token_type_ids_': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask_': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'start_positions': 74, 'end_positions': 75}.\n",
      "12/28/2023 22:32:33 - INFO - __main__ - Sample 64359 of the training set: {'input_ids': [101, 1184, 1669, 1408, 1213, 3862, 1477, 1550, 1201, 2403, 136, 102, 1290, 1126, 8967, 7540, 1132, 2991, 118, 25448, 117, 1147, 13104, 1132, 4054, 782, 2426, 19946, 1105, 1103, 10956, 2200, 11182, 1115, 1199, 1104, 1103, 1530, 3318, 1174, 119, 1780, 1199, 1523, 5048, 12571, 27177, 13104, 1336, 4248, 1126, 8967, 7540, 117, 1103, 3778, 1227, 11671, 1115, 1110, 3626, 1114, 6595, 2502, 1121, 1164, 4062, 1604, 1550, 1201, 2403, 1107, 1103, 1346, 11019, 12913, 5476, 1669, 119, 13104, 1104, 1211, 2030, 5093, 185, 23415, 7147, 16618, 2114, 1691, 1118, 1103, 1322, 1104, 1103, 6302, 25093, 117, 1164, 1853, 1580, 1550, 1201, 2403, 119, 185, 5971, 9828, 2430, 17246, 23423, 1164, 2480, 1199, 1404, 13104, 1121, 1103, 2286, 1137, 2572, 15901, 1811, 117, 1164, 3862, 1477, 1106, 3993, 1475, 1550, 1201, 2403, 117, 1132, 1103, 2606, 1104, 184, 2646, 2758, 7147, 16618, 1116, 117, 1105, 1103, 5041, 1107, 10396, 16156, 1895, 13104, 1104, 1103, 1372, 2845, 1107, 1103, 16371, 1669, 117, 1134, 1310, 2625, 1550, 1201, 2403, 119, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'input_ids_': [101, 1184, 1669, 1408, 1213, 3862, 1477, 1550, 1201, 102], 'token_type_ids_': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask_': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'start_positions': 117, 'end_positions': 122}.\n",
      "/home/miu/miniconda3/envs/proconvqa/lib/python3.8/site-packages/accelerate/accelerator.py:527: FutureWarning: The `use_fp16` property is deprecated and will be removed in version 1.0 of Accelerate use `Accelerator.mixed_precision == 'fp16'` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from data_utils import get_dataloaders\n",
    "args.per_device_train_batch_size=16\n",
    "train_dataloader,eval_dataloader,predict_dataloader=get_dataloaders(args,tokenizer,raw_datasets,train_dataset,eval_dataset,dataset_infos,accelerator,logger)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading builder script: 100%|██████████| 4.53k/4.53k [00:00<00:00, 1.60MB/s]\n",
      "Downloading extra modules: 100%|██████████| 3.32k/3.32k [00:00<00:00, 1.15MB/s]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import logging\n",
    "from model_utils import postprocess_qa_predictions,post_processing_function,create_and_fill_np_array\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "metric = evaluate.load(\"squad_v2\" if args.version_2_with_negative else \"squad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer\n",
    "# Split weights in two groups, one with weight decay and the other not.\n",
    "no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "optimizer_grouped_parameters = [\n",
    "    {\n",
    "        \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "        \"weight_decay\": args.weight_decay,\n",
    "    },\n",
    "    {\n",
    "        \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "        \"weight_decay\": 0.0,\n",
    "    },\n",
    "]\n",
    "optimizer = torch.optim.AdamW(optimizer_grouped_parameters, lr=args.learning_rate)\n",
    "\n",
    "# Scheduler and math around the number of training steps.\n",
    "overrode_max_train_steps = False\n",
    "num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)\n",
    "if args.max_train_steps is None:\n",
    "    args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch\n",
    "    overrode_max_train_steps = True\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=args.lr_scheduler_type,\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=args.num_warmup_steps * args.gradient_accumulation_steps,\n",
    "    num_training_steps=args.max_train_steps * args.gradient_accumulation_steps,\n",
    ")\n",
    "\n",
    "# Prepare everything with our `accelerator`.\n",
    "model, optimizer, train_dataloader, eval_dataloader, lr_scheduler = accelerator.prepare(\n",
    "    model, optimizer, train_dataloader, eval_dataloader, lr_scheduler\n",
    ")\n",
    "\n",
    "# We need to recalculate our total training steps as the size of the training dataloader may have changed.\n",
    "num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)\n",
    "if overrode_max_train_steps:\n",
    "    args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch\n",
    "# Afterwards we recalculate our number of training epochs\n",
    "args.num_train_epochs = math.ceil(args.max_train_steps / num_update_steps_per_epoch)\n",
    "\n",
    "# Figure out how many steps we should save the Accelerator states\n",
    "checkpointing_steps = args.checkpointing_steps\n",
    "if checkpointing_steps is not None and checkpointing_steps.isdigit():\n",
    "    checkpointing_steps = int(checkpointing_steps)\n",
    "\n",
    "# We need to initialize the trackers we use, and also store our configuration.\n",
    "# The trackers initializes automatically on the main process.\n",
    "if args.with_tracking:\n",
    "    experiment_config = vars(args)\n",
    "    # TensorBoard cannot log Enums, need the raw value\n",
    "    experiment_config[\"lr_scheduler_type\"] = experiment_config[\"lr_scheduler_type\"].value\n",
    "    accelerator.init_trackers(\"qa_no_trainer\", experiment_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/28/2023 22:32:49 - INFO - __main__ - ***** Running training *****\n",
      "12/28/2023 22:32:49 - INFO - __main__ -   Num examples = 89357\n",
      "12/28/2023 22:32:49 - INFO - __main__ -   Num Epochs = 3\n",
      "12/28/2023 22:32:49 - INFO - __main__ -   Instantaneous batch size per device = 16\n",
      "12/28/2023 22:32:49 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "12/28/2023 22:32:49 - INFO - __main__ -   Gradient Accumulation steps = 1\n",
      "12/28/2023 22:32:49 - INFO - __main__ -   Total optimization steps = 16755\n",
      "  0%|          | 0/16755 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "# Train!\n",
    "total_batch_size = args.per_device_train_batch_size * accelerator.num_processes * args.gradient_accumulation_steps\n",
    "\n",
    "logger.info(\"***** Running training *****\")\n",
    "logger.info(f\"  Num examples = {len(train_dataset)}\")\n",
    "logger.info(f\"  Num Epochs = {args.num_train_epochs}\")\n",
    "logger.info(f\"  Instantaneous batch size per device = {args.per_device_train_batch_size}\")\n",
    "logger.info(f\"  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}\")\n",
    "logger.info(f\"  Gradient Accumulation steps = {args.gradient_accumulation_steps}\")\n",
    "logger.info(f\"  Total optimization steps = {args.max_train_steps}\")\n",
    "\n",
    "# Only show the progress bar once on each machine.\n",
    "progress_bar = tqdm(range(args.max_train_steps), disable=not accelerator.is_local_main_process)\n",
    "completed_steps = 0\n",
    "starting_epoch = 0\n",
    "\n",
    "# Potentially load in the weights and states from a previous save\n",
    "if args.resume_from_checkpoint:\n",
    "    if args.resume_from_checkpoint is not None or args.resume_from_checkpoint != \"\":\n",
    "        accelerator.print(f\"Resumed from checkpoint: {args.resume_from_checkpoint}\")\n",
    "        accelerator.load_state(args.resume_from_checkpoint)\n",
    "        path = os.path.basename(args.resume_from_checkpoint)\n",
    "    else:\n",
    "        # Get the most recent checkpoint\n",
    "        dirs = [f.name for f in os.scandir(os.getcwd()) if f.is_dir()]\n",
    "        dirs.sort(key=os.path.getctime)\n",
    "        path = dirs[-1]  # Sorts folders by date modified, most recent checkpoint is the last\n",
    "    # Extract `epoch_{i}` or `step_{i}`\n",
    "    training_difference = os.path.splitext(path)[0]\n",
    "\n",
    "    if \"epoch\" in training_difference:\n",
    "        starting_epoch = int(training_difference.replace(\"epoch_\", \"\")) + 1\n",
    "        resume_step = None\n",
    "    else:\n",
    "        resume_step = int(training_difference.replace(\"step_\", \"\"))\n",
    "        starting_epoch = resume_step // len(train_dataloader)\n",
    "        resume_step -= starting_epoch * len(train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.num_train_epochs=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.output_dir='/home/miu/Projects/NLP/DensePhrasesMinimal/result_models'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▍      | 5804/16755 [18:15<28:35,  6.38it/s]   "
     ]
    }
   ],
   "source": [
    "for epoch in range(starting_epoch, args.num_train_epochs):\n",
    "    model.train()\n",
    "    if args.with_tracking:\n",
    "        total_loss = 0\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        # We need to skip steps until we reach the resumed step\n",
    "        if args.resume_from_checkpoint and epoch == starting_epoch:\n",
    "            if resume_step is not None and step < resume_step:\n",
    "                completed_steps += 1\n",
    "                continue\n",
    "\n",
    "        with accelerator.accumulate(model):\n",
    "            outputs = model(**batch)\n",
    "            loss=outputs[0]\n",
    "            # loss = outputs.loss\n",
    "            # We keep track of the loss at each epoch\n",
    "            if args.with_tracking:\n",
    "                total_loss += loss.detach().float()\n",
    "\n",
    "            accelerator.backward(loss)\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        # Checks if the accelerator has performed an optimization step behind the scenes\n",
    "        if accelerator.sync_gradients:\n",
    "            progress_bar.update(1)\n",
    "            completed_steps += 1\n",
    "\n",
    "        if isinstance(checkpointing_steps, int):\n",
    "            if completed_steps % checkpointing_steps == 0:\n",
    "                output_dir = f\"step_{completed_steps }\"\n",
    "                if args.output_dir is not None:\n",
    "                    output_dir = os.path.join(args.output_dir, output_dir)\n",
    "                accelerator.save_state(output_dir)\n",
    "\n",
    "        if completed_steps >= args.max_train_steps:\n",
    "            break\n",
    "\n",
    "    if args.checkpointing_steps == \"epoch\":\n",
    "        output_dir = f\"epoch_{epoch}\"\n",
    "        if args.output_dir is not None:\n",
    "            output_dir = os.path.join(args.output_dir, output_dir)\n",
    "        accelerator.save_state(output_dir)\n",
    "\n",
    "    if epoch < args.num_train_epochs - 1:\n",
    "        accelerator.wait_for_everyone()\n",
    "        unwrapped_model = accelerator.unwrap_model(model)\n",
    "        unwrapped_model.save_pretrained(\n",
    "            args.output_dir, is_main_process=accelerator.is_main_process, save_function=accelerator.save\n",
    "        )\n",
    "        if accelerator.is_main_process:\n",
    "            tokenizer.save_pretrained(args.output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([13, 341])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.output_dir='/home/miu/Projects/NLP/results/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/27/2023 16:43:46 - INFO - __main__ - ***** Running Evaluation *****\n",
      "12/27/2023 16:43:46 - INFO - __main__ -   Num examples = 10895\n",
      "12/27/2023 16:43:46 - INFO - __main__ -   Batch size = 8\n",
      "100%|██████████| 10570/10570 [00:08<00:00, 1270.13it/s]\n",
      "Configuration saved in /home/miu/Projects/NLP/PRO-ConvQA/results/config.json\n",
      "Model weights saved in /home/miu/Projects/NLP/PRO-ConvQA/results/pytorch_model.bin\n",
      "tokenizer config file saved in /home/miu/Projects/NLP/PRO-ConvQA/results/tokenizer_config.json\n",
      "Special tokens file saved in /home/miu/Projects/NLP/PRO-ConvQA/results/special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "# Evaluation\n",
    "logger.info(\"***** Running Evaluation *****\")\n",
    "logger.info(f\"  Num examples = {len(eval_dataset)}\")\n",
    "logger.info(f\"  Batch size = {args.per_device_eval_batch_size}\")\n",
    "\n",
    "all_start_logits = []\n",
    "all_end_logits = []\n",
    "\n",
    "model.eval()\n",
    "\n",
    "for step, batch in enumerate(eval_dataloader):\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**batch)\n",
    "        # start_logits = outputs.start_logits\n",
    "        # end_logits = outputs.end_logits\n",
    "\n",
    "        start_logits = outputs[0]\n",
    "        end_logits = outputs[-1]\n",
    "\n",
    "        if not args.pad_to_max_length:  # necessary to pad predictions and labels for being gathered\n",
    "            start_logits = accelerator.pad_across_processes(start_logits, dim=1, pad_index=-100)\n",
    "            end_logits = accelerator.pad_across_processes(end_logits, dim=1, pad_index=-100)\n",
    "\n",
    "        all_start_logits.append(accelerator.gather_for_metrics(start_logits).cpu().numpy())\n",
    "        all_end_logits.append(accelerator.gather_for_metrics(end_logits).cpu().numpy())\n",
    "\n",
    "max_len = max([x.shape[1] for x in all_start_logits])  # Get the max_length of the tensor\n",
    "\n",
    "# concatenate the numpy array\n",
    "start_logits_concat = create_and_fill_np_array(all_start_logits, eval_dataset, max_len)\n",
    "end_logits_concat = create_and_fill_np_array(all_end_logits, eval_dataset, max_len)\n",
    "\n",
    "# delete the list of numpy arrays\n",
    "del all_start_logits\n",
    "del all_end_logits\n",
    "\n",
    "outputs_numpy = (start_logits_concat, end_logits_concat)\n",
    "prediction = post_processing_function(eval_examples, eval_dataset, outputs_numpy)\n",
    "eval_metric = metric.compute(predictions=prediction.predictions, references=prediction.label_ids)\n",
    "logger.info(f\"Evaluation metrics: {eval_metric}\")\n",
    "\n",
    "# Prediction\n",
    "if args.do_predict:\n",
    "    logger.info(\"***** Running Prediction *****\")\n",
    "    logger.info(f\"  Num examples = {len(predict_dataset)}\")\n",
    "    logger.info(f\"  Batch size = {args.per_device_eval_batch_size}\")\n",
    "\n",
    "    all_start_logits = []\n",
    "    all_end_logits = []\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    for step, batch in enumerate(predict_dataloader):\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "            # start_logits = outputs.start_logits\n",
    "            # end_logits = outputs.end_logits\n",
    "\n",
    "            start_logits = outputs[0]\n",
    "            end_logits = outputs[-1]\n",
    "\n",
    "            if not args.pad_to_max_length:  # necessary to pad predictions and labels for being gathered\n",
    "                start_logits = accelerator.pad_across_processes(start_logits, dim=1, pad_index=-100)\n",
    "                end_logits = accelerator.pad_across_processes(end_logits, dim=1, pad_index=-100)\n",
    "\n",
    "            all_start_logits.append(accelerator.gather_for_metrics(start_logits).cpu().numpy())\n",
    "            all_end_logits.append(accelerator.gather_for_metrics(end_logits).cpu().numpy())\n",
    "\n",
    "    max_len = max([x.shape[1] for x in all_start_logits])  # Get the max_length of the tensor\n",
    "    # concatenate the numpy array\n",
    "    start_logits_concat = create_and_fill_np_array(all_start_logits, predict_dataset, max_len)\n",
    "    end_logits_concat = create_and_fill_np_array(all_end_logits, predict_dataset, max_len)\n",
    "\n",
    "    # delete the list of numpy arrays\n",
    "    del all_start_logits\n",
    "    del all_end_logits\n",
    "\n",
    "    outputs_numpy = (start_logits_concat, end_logits_concat)\n",
    "    prediction = post_processing_function(predict_examples, predict_dataset, outputs_numpy)\n",
    "    predict_metric = metric.compute(predictions=prediction.predictions, references=prediction.label_ids)\n",
    "    logger.info(f\"Predict metrics: {predict_metric}\")\n",
    "\n",
    "if args.with_tracking:\n",
    "    log = {\n",
    "        \"squad_v2\" if args.version_2_with_negative else \"squad\": eval_metric,\n",
    "        \"train_loss\": total_loss.item() / len(train_dataloader),\n",
    "        \"epoch\": epoch,\n",
    "        \"step\": completed_steps,\n",
    "    }\n",
    "if args.do_predict:\n",
    "    log[\"squad_v2_predict\" if args.version_2_with_negative else \"squad_predict\"] = predict_metric\n",
    "\n",
    "    accelerator.log(log, step=completed_steps)\n",
    "\n",
    "if args.output_dir is not None:\n",
    "    accelerator.wait_for_everyone()\n",
    "    unwrapped_model = accelerator.unwrap_model(model)\n",
    "    unwrapped_model.save_pretrained(\n",
    "        args.output_dir, is_main_process=accelerator.is_main_process, save_function=accelerator.save\n",
    "    )\n",
    "    if accelerator.is_main_process:\n",
    "        tokenizer.save_pretrained(args.output_dir)\n",
    "        if args.push_to_hub:\n",
    "            repo.push_to_hub(commit_message=\"End of training\", auto_lfs_prune=True)\n",
    "\n",
    "        logger.info(json.dumps(eval_metric, indent=4))\n",
    "        save_prefixed_metrics(eval_metric, args.output_dir)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load methods and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Finetuning the library models for question-answering on SQuAD (DistilBERT, Bert, XLM, XLNet).\"\"\"\n",
    "\n",
    "import timeit\n",
    "import h5py\n",
    "import argparse\n",
    "from arguments import parse_args\n",
    "import json\n",
    "import logging\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "import datasets\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import torch\n",
    "from accelerate import Accelerator\n",
    "from accelerate.logging import get_logger\n",
    "from accelerate.utils import set_seed\n",
    "from datasets import load_dataset\n",
    "from huggingface_hub import Repository, create_repo\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "# from utils_qa import postprocess_qa_predictions\n",
    "\n",
    "import transformers\n",
    "from transformers import (\n",
    "    CONFIG_MAPPING,\n",
    "    MODEL_MAPPING,\n",
    "    AutoConfig,\n",
    "    AutoModelForQuestionAnswering,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorWithPadding,\n",
    "    EvalPrediction,\n",
    "    SchedulerType,\n",
    "    default_data_collator,\n",
    "    get_scheduler,\n",
    ")\n",
    "#from transformers.utils import check_min_version, get_full_repo_name, send_example_telemetry\n",
    "from transformers.utils.versions import require_version\n",
    "\n",
    "\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = parse_args()\n",
    "\n",
    "# Sending telemetry. Tracking the example usage helps us better allocate resources to maintain them. The\n",
    "# information sent is the one passed as arguments along with your Python/PyTorch versions.\n",
    "#send_example_telemetry(\"run_qa_no_trainer\", args)\n",
    "\n",
    "# Initialize the accelerator. We will let the accelerator handle device placement for us in this example.\n",
    "# If we're using tracking, we also need to initialize it here and it will by default pick up all supported trackers\n",
    "# in the environment\n",
    "accelerator_log_kwargs = {}\n",
    "\n",
    "if args.with_tracking:\n",
    "    accelerator_log_kwargs[\"log_with\"] = args.report_to\n",
    "    accelerator_log_kwargs[\"logging_dir\"] = args.output_dir\n",
    "\n",
    "accelerator = Accelerator(gradient_accumulation_steps=args.gradient_accumulation_steps, **accelerator_log_kwargs)\n",
    "\n",
    "# Make one log on every process with the configuration for debugging.\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
    "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "    level=logging.INFO,\n",
    ")\n",
    "#logger.info(accelerator.state, main_process_only=False)\n",
    "if accelerator.is_local_main_process:\n",
    "    datasets.utils.logging.set_verbosity_warning()\n",
    "    transformers.utils.logging.set_verbosity_info()\n",
    "else:\n",
    "    datasets.utils.logging.set_verbosity_error()\n",
    "    transformers.utils.logging.set_verbosity_error()\n",
    "\n",
    "# If passed along, set the training seed now.\n",
    "if args.seed is not None:\n",
    "    set_seed(args.seed)\n",
    "\n",
    "# Handle the repository creation\n",
    "if accelerator.is_main_process:\n",
    "    if args.push_to_hub:\n",
    "        if args.hub_model_id is None:\n",
    "            repo_name = get_full_repo_name(Path(args.output_dir).name, token=args.hub_token)\n",
    "        else:\n",
    "            repo_name = args.hub_model_id\n",
    "        create_repo(repo_name, exist_ok=True, token=args.hub_token)\n",
    "        repo = Repository(args.output_dir, clone_from=repo_name, token=args.hub_token)\n",
    "\n",
    "        with open(os.path.join(args.output_dir, \".gitignore\"), \"w+\") as gitignore:\n",
    "            if \"step_*\" not in gitignore:\n",
    "                gitignore.write(\"step_*\\n\")\n",
    "            if \"epoch_*\" not in gitignore:\n",
    "                gitignore.write(\"epoch_*\\n\")\n",
    "    elif args.output_dir is not None:\n",
    "        os.makedirs(args.output_dir, exist_ok=True)\n",
    "accelerator.wait_for_everyone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from encoder import Encoder\n",
    "from functools import partial\n",
    "from transformers import AutoModel\n",
    "\n",
    "def load_encoder(device, args, phrase_only=False, query_only=False, freeze_embedding=True):\n",
    "    # Distributed training:\n",
    "    # The .from_pretrained methods guarantee that only one local process can concurrently\n",
    "    # download model & vocab.\n",
    "    config = AutoConfig.from_pretrained(\n",
    "        args.config_name if args.config_name else args.pretrained_name_or_path,\n",
    "        cache_dir=args.cache_dir if args.cache_dir else None,\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        args.tokenizer_name if args.tokenizer_name else args.pretrained_name_or_path,\n",
    "        cache_dir=args.cache_dir if args.cache_dir else None,\n",
    "        use_fast=True,\n",
    "    )\n",
    "\n",
    "    # Prepare PLM if not load_dir\n",
    "    pretrained = None\n",
    "    load_class = partial(\n",
    "        Encoder.from_pretrained,\n",
    "        pretrained_model_name_or_path=args.load_dir,\n",
    "        cache_dir=args.cache_dir if args.cache_dir else None,\n",
    "    )\n",
    "    logger.info(f'DensePhrases encoder loaded from {args.load_dir}')\n",
    "\n",
    "    # DensePhrases encoder object\n",
    "    model = load_class(\n",
    "        config=config,\n",
    "        tokenizer=tokenizer,\n",
    "        transformer_cls=MODEL_MAPPING[config.__class__],\n",
    "        pretrained=copy.deepcopy(pretrained) if pretrained is not None else None,\n",
    "        lambda_kl=getattr(args, 'lambda_kl', 0.0),\n",
    "        lambda_neg=getattr(args, 'lambda_neg', 0.0),\n",
    "        lambda_flt=getattr(args, 'lambda_flt', 0.0),\n",
    "        pbn_size=getattr(args, 'pbn_size', 0.0),\n",
    "        return_phrase=phrase_only,\n",
    "        return_query=query_only,\n",
    "    )\n",
    "    # Phrase only (for phrase embedding)\n",
    "    if phrase_only:\n",
    "        if hasattr(model, \"module\"):\n",
    "            del model.module.query_start_encoder\n",
    "            del model.module.query_end_encoder\n",
    "        else:\n",
    "            del model.query_start_encoder\n",
    "            del model.query_end_encoder\n",
    "        logger.info(\"Load only phrase encoders for embedding phrases\")\n",
    "    \n",
    "    \n",
    "    if freeze_embedding:\n",
    "        for name, param in model.named_parameters():\n",
    "            if name.endswith(\".embeddings.word_embeddings.weight\"):\n",
    "                param.requires_grad = False\n",
    "                logger.info(f'freezing {name}')\n",
    "\n",
    "    model.to(device)\n",
    "    logger.info('Number of model parameters: {:,}'.format(sum(p.numel() for p in model.parameters())))\n",
    "    return model, tokenizer, config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/SpanBERT/spanbert-base-cased/resolve/main/config.json from cache at /home/miu/.var/app/com.visualstudio.code/cache/huggingface/transformers/db038c967205352871fe5924ad8dd767343ca3c4869744e2e81eae6c199e8721.5b1ff81c06025be8bc48ae0cf34bb012d6cbc7f8918e8bcd354f80e6952b3ae7\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"SpanBERT/spanbert-base-cased\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.13.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "loading configuration file https://huggingface.co/SpanBERT/spanbert-base-cased/resolve/main/config.json from cache at /home/miu/.var/app/com.visualstudio.code/cache/huggingface/transformers/db038c967205352871fe5924ad8dd767343ca3c4869744e2e81eae6c199e8721.5b1ff81c06025be8bc48ae0cf34bb012d6cbc7f8918e8bcd354f80e6952b3ae7\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"SpanBERT/spanbert-base-cased\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.13.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/SpanBERT/spanbert-base-cased/resolve/main/vocab.txt from cache at /home/miu/.var/app/com.visualstudio.code/cache/huggingface/transformers/fe71705539a68d12ea01aee41383b9446270b7d15e508a9b82bb32bceb51def3.437aa611e89f6fc6675a049d2b5545390adbc617e7d655286421c191d2be2791\n",
      "loading file https://huggingface.co/SpanBERT/spanbert-base-cased/resolve/main/tokenizer.json from cache at None\n",
      "loading file https://huggingface.co/SpanBERT/spanbert-base-cased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/SpanBERT/spanbert-base-cased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/SpanBERT/spanbert-base-cased/resolve/main/tokenizer_config.json from cache at None\n",
      "loading configuration file https://huggingface.co/SpanBERT/spanbert-base-cased/resolve/main/config.json from cache at /home/miu/.var/app/com.visualstudio.code/cache/huggingface/transformers/db038c967205352871fe5924ad8dd767343ca3c4869744e2e81eae6c199e8721.5b1ff81c06025be8bc48ae0cf34bb012d6cbc7f8918e8bcd354f80e6952b3ae7\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"SpanBERT/spanbert-base-cased\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.13.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/SpanBERT/spanbert-base-cased/resolve/main/config.json from cache at /home/miu/.var/app/com.visualstudio.code/cache/huggingface/transformers/db038c967205352871fe5924ad8dd767343ca3c4869744e2e81eae6c199e8721.5b1ff81c06025be8bc48ae0cf34bb012d6cbc7f8918e8bcd354f80e6952b3ae7\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"SpanBERT/spanbert-base-cased\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.13.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "12/27/2023 19:27:26 - INFO - __main__ - DensePhrases encoder loaded from /home/miu/Projects/NLP/PRO-ConvQA/results\n",
      "loading weights file /home/miu/Projects/NLP/PRO-ConvQA/results/pytorch_model.bin\n",
      "12/27/2023 19:27:27 - INFO - encoder - Loading encoders from load_dir\n",
      "All model checkpoint weights were used when initializing Encoder.\n",
      "\n",
      "All the weights of Encoder were initialized from the model checkpoint at /home/miu/Projects/NLP/PRO-ConvQA/results.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use Encoder for predictions without further training.\n",
      "12/27/2023 19:27:27 - INFO - __main__ - freezing phrase_encoder.embeddings.word_embeddings.weight\n",
      "12/27/2023 19:27:27 - INFO - __main__ - freezing query_start_encoder.embeddings.word_embeddings.weight\n",
      "12/27/2023 19:27:27 - INFO - __main__ - freezing query_end_encoder.embeddings.word_embeddings.weight\n",
      "12/27/2023 19:27:28 - INFO - __main__ - Number of model parameters: 324,932,354\n"
     ]
    }
   ],
   "source": [
    "\n",
    "args.pretrained_name_or_path='results'\n",
    "args.load_dir='results'\n",
    "args.cache_dir=None\n",
    "args.tokenizer_name='SpanBERT/spanbert-base-cased'\n",
    "args.config_name='SpanBERT/spanbert-base-cased'\n",
    "\"\"\"\n",
    "args.pretrained_name_or_path='princeton-nlp/densephrases-multi'\n",
    "args.load_dir='princeton-nlp/densephrases-multi'\n",
    "args.cache_dir=None\n",
    "args.tokenizer_name='SpanBERT/spanbert-base-cased'\n",
    "args.config_name='SpanBERT/spanbert-base-cased'\n",
    "\"\"\"\n",
    "\n",
    "args.max_query_length=50\n",
    "\n",
    "model, tokenizer, _ = load_encoder(accelerator.device, args, phrase_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install faiss-gpu==1.6.5 blosc==1.10.6 spacy ujson transformers==4.13.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!mv /home/kiki/dados_servidor/Servidor/InformationRetrival/densephrases/DensePhrases/scripts ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/miu/miniconda3/envs/proconvqa/lib/python3.8/site-packages/thinc/compat.py:36: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  hasattr(torch, \"has_mps\")\n",
      "/home/miu/miniconda3/envs/proconvqa/lib/python3.8/site-packages/thinc/compat.py:37: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  and torch.has_mps  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from transformers.data.data_collator import default_data_collator, torch_default_data_collator\n",
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "from densephrases.utils.trainer_qa import QuestionAnsweringTrainer\n",
    "from transformers import (\n",
    "    DataCollatorWithPadding,\n",
    "    default_data_collator,\n",
    ")\n",
    "from densephrases.utils.single_utils import set_seed, load_encoder\n",
    "from densephrases import Options\n",
    "from scripts.preprocess.convert_squad_to_hf import convert_squad_to_hf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving 18829 (context, question, answer) triples.\n",
      "Writing to /home/miu/Projects/NLP/PRO-ConvQA/wikidump/wiki-dev/0000_hf.json\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_files = {}\n",
    "data_files[\"test\"] = convert_squad_to_hf('/home/miu/Projects/NLP/PRO-ConvQA/wikidump/wiki-dev/0000')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and tokenize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create phrase vectors\n",
    "args.draft = False\n",
    "args.pad_to_max_length=True\n",
    "args.append_title=False\n",
    "args.convert_squad_to_hf=True\n",
    "args.remove_unused_columns=True\n",
    "args.use_legacy_prediction_loop=False\n",
    "args.world_size=1\n",
    "args.eval_batch_size=8\n",
    "args.dataloader_drop_last=True\n",
    "args.dataloader_pin_memory=True\n",
    "args.output_dir='output_test/'\n",
    "args.test_file='/home/miu/Projects/NLP/PRO-ConvQA/wikidump/wiki-dev/0000'\n",
    "args.prediction_loss_only=False\n",
    "args.n_gpu=1\n",
    "args.past_index=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving 18829 (context, question, answer) triples.\n",
      "Writing to /home/miu/Projects/NLP/PRO-ConvQA/wikidump/wiki-dev/0000_hf.json\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/27/2023 19:27:50 - WARNING - datasets.builder - Using custom data configuration default-bdb26076fec24993\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset json/default to /home/miu/.var/app/com.visualstudio.code/cache/huggingface/datasets/json/default-bdb26076fec24993/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data files: 100%|██████████| 1/1 [00:00<00:00, 3819.95it/s]\n",
      "Extracting data files: 100%|██████████| 1/1 [00:00<00:00, 1536.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset json downloaded and prepared to /home/miu/.var/app/com.visualstudio.code/cache/huggingface/datasets/json/default-bdb26076fec24993/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 1288.18it/s]\n",
      "Running tokenizer on prediction dataset:  95%|█████████▍| 18/19 [00:03<00:00,  5.12ba/s]\n"
     ]
    }
   ],
   "source": [
    "filter_only=False\n",
    "\n",
    "output_path = 'dump/phrase' if not filter_only else 'dump/filter'\n",
    "if not os.path.exists(os.path.join(args.output_dir, output_path)):\n",
    "    os.makedirs(os.path.join(args.output_dir, output_path))\n",
    "\n",
    "start_time = timeit.default_timer()\n",
    "if ':' not in args.test_file:\n",
    "    test_files = [args.test_file]\n",
    "    offsets = [0]\n",
    "    output_dump_file = os.path.join(\n",
    "        args.output_dir, f\"{output_path}/{os.path.splitext(os.path.basename(args.test_file))[0]}.hdf5\"\n",
    "    )\n",
    "else:\n",
    "    dirname = os.path.dirname(args.test_file)\n",
    "    basename = os.path.basename(args.test_file)\n",
    "    start, end = list(map(int, basename.split(':')))\n",
    "    output_dump_file = os.path.join(\n",
    "        args.output_dir, f\"{output_path}/{start}-{end}.hdf5\"\n",
    "    )\n",
    "\n",
    "    # skip files if possible\n",
    "    if os.path.exists(output_dump_file):\n",
    "        with h5py.File(output_dump_file, 'r') as f:\n",
    "            dids = list(map(int, f.keys()))\n",
    "        start = int(max(dids) / 1000)\n",
    "        logger.info('%s exists; starting from %d' % (output_dump_file, start))\n",
    "\n",
    "    names = [str(i).zfill(4) for i in range(start, end)]\n",
    "    test_files = [os.path.join(dirname, name) for name in names]\n",
    "    offsets = [int(each) * 1000 for each in names]\n",
    "\n",
    "for offset, test_file in zip(offsets, test_files):\n",
    "    logger.info(f\"***** Pre-processing contexts from {test_file} *****\")\n",
    "\n",
    "    data_files = {}\n",
    "    if args.convert_squad_to_hf:\n",
    "        data_files[\"test\"] = convert_squad_to_hf(test_file)\n",
    "    else:\n",
    "        data_files[\"test\"] = test_file\n",
    "    extension = data_files[\"test\"].split(\".\")[-1]\n",
    "    raw_datasets = load_dataset(extension, data_files=data_files, field=\"data\", cache_dir=args.cache_dir)\n",
    "\n",
    "    column_names = raw_datasets[\"test\"].column_names\n",
    "    context_column_name = \"context\" if \"context\" in column_names else column_names[1]\n",
    "\n",
    "    # Padding side determines if we do (question|context) or (context|question).\n",
    "    pad_on_right = tokenizer.padding_side == \"right\"\n",
    "\n",
    "    if args.max_seq_length > tokenizer.model_max_length:\n",
    "        args.warning(\n",
    "            f\"The max_seq_length passed ({args.max_seq_length}) is larger than the maximum length for the\"\n",
    "            f\"model ({tokenizer.model_max_length}). Using max_seq_length={tokenizer.model_max_length}.\"\n",
    "        )\n",
    "    max_seq_length = min(args.max_seq_length, tokenizer.model_max_length)\n",
    "\n",
    "    # Validation preprocessing\n",
    "    def prepare_validation_features(examples, indexes):\n",
    "        # Tokenize our examples with truncation and maybe padding, but keep the overflows using a stride. This results\n",
    "        # in one example possible giving several features when a context is long, each of those features having a\n",
    "        # context that overlaps a bit the context of the previous feature.\n",
    "        if args.append_title:\n",
    "            tokenized_examples = tokenizer(\n",
    "                examples['title' if pad_on_right else context_column_name],\n",
    "                examples[context_column_name if pad_on_right else 'title'],\n",
    "                truncation=\"only_second\" if pad_on_right else \"only_first\",\n",
    "                max_length=max_seq_length,\n",
    "                stride=args.doc_stride,\n",
    "                return_overflowing_tokens=True,\n",
    "                return_offsets_mapping=True,\n",
    "                padding=\"max_length\" if args.pad_to_max_length else False,\n",
    "            )\n",
    "        else:\n",
    "            tokenized_examples = tokenizer(\n",
    "                examples[context_column_name],\n",
    "                truncation=\"only_first\",\n",
    "                max_length=max_seq_length,\n",
    "                stride=args.doc_stride,\n",
    "                return_overflowing_tokens=True,\n",
    "                return_offsets_mapping=True,\n",
    "                padding=\"max_length\" if args.pad_to_max_length else False,\n",
    "            )\n",
    "\n",
    "        # Since one example might give us several features if it has a long context, we need a map from a feature to\n",
    "        # its corresponding example. This key gives us just that.\n",
    "        sample_mapping = tokenized_examples[\"overflow_to_sample_mapping\"]\n",
    "        \n",
    "        # Inflate doc_idxs based on sample_mapping\n",
    "        tokenized_examples['doc_idx'] = [offset + examples['doc_idx'][i] for i in sample_mapping]\n",
    "\n",
    "        # This example_id indicates the index of an original paragraph (not question id)\n",
    "        tokenized_examples['example_id'] = [indexes[i] for i in sample_mapping]\n",
    "\n",
    "        for i in range(len(tokenized_examples[\"input_ids\"])):\n",
    "            # Grab the sequence corresponding to that example (to know what is the context and what is the question).\n",
    "            sequence_ids = tokenized_examples.sequence_ids(i)\n",
    "            context_index = 1 if pad_on_right and args.append_title else 0\n",
    "\n",
    "            # Set to None the offset_mapping that are not part of the context so it's easy to determine if a token\n",
    "            # position is part of the context or not.\n",
    "            tokenized_examples[\"offset_mapping\"][i] = [\n",
    "                (o if sequence_ids[k] == context_index else None)\n",
    "                for k, o in enumerate(tokenized_examples[\"offset_mapping\"][i])\n",
    "            ]\n",
    "\n",
    "        return tokenized_examples\n",
    "    \n",
    "    examples = raw_datasets[\"test\"]\n",
    "\n",
    "    # Predict Feature Creation\n",
    "    with accelerator.main_process_first():\n",
    "        dataset = examples.map(\n",
    "            prepare_validation_features,\n",
    "            batched=True,\n",
    "            num_proc=args.preprocessing_num_workers,\n",
    "            # num_proc=1,\n",
    "            with_indices=True,\n",
    "            remove_columns=column_names,\n",
    "            load_from_cache_file=not args.overwrite_cache,\n",
    "            desc=\"Running tokenizer on prediction dataset\",\n",
    "        )\n",
    "    \n",
    "    # Data collator\n",
    "    # We have already padded to max length if the corresponding flag is True, otherwise we need to pad in the data\n",
    "    # collator.\n",
    "    data_collator = (\n",
    "        default_data_collator\n",
    "        if args.pad_to_max_length\n",
    "        else DataCollatorWithPadding(tokenizer, pad_to_multiple_of=8 if args.fp16 else None)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.per_device_train_batch_size=8\n",
    "\n",
    "predict_dataset_for_model = dataset.remove_columns([ \"offset_mapping\",\"doc_idx\",\"overflow_to_sample_mapping\"])\n",
    "predict_dataloader = DataLoader(\n",
    "    predict_dataset_for_model, shuffle=True, collate_fn=data_collator, batch_size=args.per_device_train_batch_size,\n",
    ")\n",
    "\n",
    "model, predict_dataloader = accelerator.prepare(model, predict_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_numpy(tensor):\n",
    "    return tensor.detach().cpu().numpy()\n",
    "\n",
    "c=0\n",
    "feature_id=0\n",
    "all_predictions=[]\n",
    "model.return_phrase=True\n",
    "for step, inputs in enumerate(predict_dataloader):\n",
    "    outputs=model(**inputs)\n",
    "\n",
    "    for item_idx, (start, end, filter_start, filter_end) in enumerate(zip(*outputs)):\n",
    "        a= {'example_id': inputs['example_id'][item_idx].item(),\n",
    "            'feature_id': 0, # dataloader should not be shuffled\n",
    "            'start': to_numpy(start),\n",
    "            'end': to_numpy(end),\n",
    "            'filter_start': to_numpy(filter_start),\n",
    "            'filter_end': to_numpy(filter_end),\n",
    "        }\n",
    "        all_predictions.append(a)\n",
    "    feature_id+=1\n",
    "    c+=1\n",
    "    if c>10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([8, 384, 768]),\n",
       " torch.Size([8, 384, 768]),\n",
       " torch.Size([8, 384]),\n",
       " torch.Size([8, 384]))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[0].shape,outputs[1].shape,outputs[2].shape,outputs[3].shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 384])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] although the schedule for most of the personnel involved was grueling, the actors found the remote location for \" part iii \" relaxing, compared to shooting its predecessor. the role of clara clayton was written with mary steenburgen in mind. when she received the script, however, she was reluctant to commit to the film until her kids, who loved the original,\\'hounded\\'her. lloyd shared his first on - screen kiss with steenburgen in \" part iii \". the hill valley festival dance scene proved to be the most dangerous for lloyd and steenburgen ; overzealous dancing left steenburgen with a torn ligament in her foot. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(inputs['input_ids'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/27/2023 19:28:01 - WARNING - datasets.builder - Using custom data configuration default-a5a6aeab93cc2962\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset json/default to /home/miu/.var/app/com.visualstudio.code/cache/huggingface/datasets/json/default-a5a6aeab93cc2962/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data files: 100%|██████████| 1/1 [00:00<00:00, 4355.46it/s]\n",
      "Extracting data files: 100%|██████████| 1/1 [00:00<00:00, 2542.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset json downloaded and prepared to /home/miu/.var/app/com.visualstudio.code/cache/huggingface/datasets/json/default-a5a6aeab93cc2962/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 1269.08it/s]\n",
      "Running tokenizer on prediction dataset:   0%|          | 0/1 [00:00<?, ?ba/s]\n"
     ]
    }
   ],
   "source": [
    "df=[]\n",
    "query = 'tell me about the historical spending on defense'\n",
    "\n",
    "for i in range(1):\n",
    "    df.append({'doc_idx':0, 'par_idx':0, 'title':'', 'context':query})\n",
    "p='/home/miu/Projects/NLP/PRO-ConvQA/query.json'\n",
    "with open(p, 'w') as f:\n",
    "        json.dump({'data': df}, f)\n",
    "r_d = load_dataset(extension, data_files=p, field=\"data\", cache_dir=args.cache_dir)\n",
    "r_d=r_d['train']\n",
    "# query_inputs = prepare_features(r_d['train'])\n",
    "max_seq_length=10\n",
    "with accelerator.main_process_first():\n",
    "    query_dataset = r_d.map(\n",
    "        prepare_validation_features,\n",
    "        batched=True,\n",
    "        num_proc=1,\n",
    "        with_indices=True,\n",
    "        desc=\"Running tokenizer on prediction dataset\",\n",
    "    )\n",
    "query_dataset = query_dataset.remove_columns([\"par_idx\", \"offset_mapping\",\"doc_idx\",\"overflow_to_sample_mapping\"])\n",
    "query_dataloader = DataLoader(\n",
    "    query_dataset, shuffle=True, collate_fn=data_collator, batch_size=1,\n",
    ")\n",
    "query_dataloader = accelerator.prepare(query_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.return_query=True\n",
    "model.return_phrase=False\n",
    "\n",
    "for step, query_inputs in enumerate(query_dataloader):\n",
    "    #print(query_inputs)\n",
    "    query_outputs=model(input_ids_=query_inputs['input_ids'],attention_mask_=query_inputs['attention_mask'])\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((768,), (768,))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_start=query_outputs[0][0].cpu().detach().numpy()[0]\n",
    "q_end=query_outputs[1][0].cpu().detach().numpy()[0]\n",
    "q_start.shape,q_end.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 384, 768)\n",
      "(768,)\n"
     ]
    }
   ],
   "source": [
    "def search_mips(phrases_embs,query_emb):\n",
    "    print(phrases_embs.shape)\n",
    "    print(query_emb.shape)\n",
    "    scores=[]\n",
    "    for i in range(phrases_embs.shape[0]):\n",
    "        batch=[]\n",
    "        for j in range(phrases_embs.shape[1]):\n",
    "            if j%10==0:\n",
    "                #batch.append(phrases_embs[i][j])\n",
    "                s=np.dot(phrases_embs[i][j],query_emb)\n",
    "                scores.append((i,j,s))\n",
    "\n",
    "        #reduced_embs.append(batch)\n",
    "    scores=np.asarray(scores)\n",
    "    return scores\n",
    "\n",
    "scores=search_mips(outputs[0].cpu().detach().numpy(),q_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(39, array([1.        , 0.        , 1.79579234]))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_score = np.argmax(scores[:,2])\n",
    "best_score,scores[best_score]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] symbiotic relationship between joker and batman, and sees the villain shatter the trust between batman and his adopted family. capullo\\'s joker design replaced his traditional outfit with a utilitarian, messy, and disheveled appearance to convey that the character was on a mission ; his face ( surgically removed in 2011\\'s \" detective comics \" # 1 ) was reattached with belts, wires, and hooks, and he was outfitted with mechanics overalls. the joker\\'s face was restored in snyder\\'s and capullo\\'s \" \" ( 2014 ), the concluding chapter to \" death of the family \". [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i=int(scores[best_score][0])\n",
    "j=int(scores[best_score][1])\n",
    "tokenizer.decode(inputs['input_ids'][i][j:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test with question answer module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorboardX in /home/kiki/miniconda3/envs/densephrases/lib/python3.9/site-packages (2.6)\n",
      "Requirement already satisfied: packaging in /home/kiki/miniconda3/envs/densephrases/lib/python3.9/site-packages (from tensorboardX) (23.0)\n",
      "Requirement already satisfied: protobuf<4,>=3.8.0 in /home/kiki/miniconda3/envs/densephrases/lib/python3.9/site-packages (from tensorboardX) (3.20.3)\n",
      "Requirement already satisfied: numpy in /home/kiki/miniconda3/envs/densephrases/lib/python3.9/site-packages (from tensorboardX) (1.23.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorboardX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    }
   ],
   "source": [
    "args.fp16=False\n",
    "args.bf16=False\n",
    "data_collator = (\n",
    "            default_data_collator\n",
    "            if args.pad_to_max_length\n",
    "            else DataCollatorWithPadding(tokenizer, pad_to_multiple_of=8 if args.fp16 else None)\n",
    "        )\n",
    "args.seed=1000\n",
    "args.skip_memory_metrics=True\n",
    "#logger = logging.get_logger(__name__)\n",
    "#log_levels = logging.get_log_levels_dict().copy()\n",
    "#trainer_log_levels = dict(**log_levels, passive=-1)\n",
    "args.get_process_log_level=test\n",
    "args._setup_devices=False\n",
    "args.sharded_ddp=[]\n",
    "args.place_model_on_device=True\n",
    "args.deepspeed=False\n",
    "args.fp16_full_eval=False\n",
    "args.bf16_full_eval=False\n",
    "args.parallel_mode=False\n",
    "args.disable_tqdm=False\n",
    "args.should_save=False\n",
    "args.max_steps=1000\n",
    "args.half_precision_backend=False\n",
    "args.device=torch.device('cuda')\n",
    "args.report_to=['tensorboard']\n",
    "args.label_smoothing_factor=0\n",
    "args.label_names=None\n",
    "trainer = QuestionAnsweringTrainer(\n",
    "            model=model,\n",
    "            args=args,\n",
    "            tokenizer=tokenizer,\n",
    "            data_collator=data_collator,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.generate_phrase_vecs(dataset, examples, output_dump_file, offset, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dump_phrases(args, model, tokenizer, filter_only=False):\n",
    "    output_path = 'dump/phrase' if not filter_only else 'dump/filter'\n",
    "    if not os.path.exists(os.path.join(args.output_dir, output_path)):\n",
    "        os.makedirs(os.path.join(args.output_dir, output_path))\n",
    "\n",
    "    start_time = timeit.default_timer()\n",
    "    if ':' not in args.test_file:\n",
    "        test_files = [args.test_file]\n",
    "        offsets = [0]\n",
    "        output_dump_file = os.path.join(\n",
    "            args.output_dir, f\"{output_path}/{os.path.splitext(os.path.basename(args.test_file))[0]}.hdf5\"\n",
    "        )\n",
    "    else:\n",
    "        dirname = os.path.dirname(args.test_file)\n",
    "        basename = os.path.basename(args.test_file)\n",
    "        start, end = list(map(int, basename.split(':')))\n",
    "        output_dump_file = os.path.join(\n",
    "            args.output_dir, f\"{output_path}/{start}-{end}.hdf5\"\n",
    "        )\n",
    "\n",
    "        # skip files if possible\n",
    "        if os.path.exists(output_dump_file):\n",
    "            with h5py.File(output_dump_file, 'r') as f:\n",
    "                dids = list(map(int, f.keys()))\n",
    "            start = int(max(dids) / 1000)\n",
    "            logger.info('%s exists; starting from %d' % (output_dump_file, start))\n",
    "\n",
    "        names = [str(i).zfill(4) for i in range(start, end)]\n",
    "        test_files = [os.path.join(dirname, name) for name in names]\n",
    "        offsets = [int(each) * 1000 for each in names]\n",
    "\n",
    "    for offset, test_file in zip(offsets, test_files):\n",
    "        logger.info(f\"***** Pre-processing contexts from {test_file} *****\")\n",
    "\n",
    "        data_files = {}\n",
    "        if args.convert_squad_to_hf:\n",
    "            data_files[\"test\"] = convert_squad_to_hf(test_file)\n",
    "        else:\n",
    "            data_files[\"test\"] = test_file\n",
    "        extension = data_files[\"test\"].split(\".\")[-1]\n",
    "        raw_datasets = load_dataset(extension, data_files=data_files, field=\"data\", cache_dir=args.cache_dir)\n",
    "\n",
    "        column_names = raw_datasets[\"test\"].column_names\n",
    "        context_column_name = \"context\" if \"context\" in column_names else column_names[1]\n",
    "\n",
    "        # Padding side determines if we do (question|context) or (context|question).\n",
    "        pad_on_right = tokenizer.padding_side == \"right\"\n",
    "\n",
    "        if args.max_seq_length > tokenizer.model_max_length:\n",
    "            args.warning(\n",
    "                f\"The max_seq_length passed ({args.max_seq_length}) is larger than the maximum length for the\"\n",
    "                f\"model ({tokenizer.model_max_length}). Using max_seq_length={tokenizer.model_max_length}.\"\n",
    "            )\n",
    "        max_seq_length = min(args.max_seq_length, tokenizer.model_max_length)\n",
    "\n",
    "        # Validation preprocessing\n",
    "        def prepare_validation_features(examples, indexes):\n",
    "            # Tokenize our examples with truncation and maybe padding, but keep the overflows using a stride. This results\n",
    "            # in one example possible giving several features when a context is long, each of those features having a\n",
    "            # context that overlaps a bit the context of the previous feature.\n",
    "            if args.append_title:\n",
    "                tokenized_examples = tokenizer(\n",
    "                    examples['title' if pad_on_right else context_column_name],\n",
    "                    examples[context_column_name if pad_on_right else 'title'],\n",
    "                    truncation=\"only_second\" if pad_on_right else \"only_first\",\n",
    "                    max_length=max_seq_length,\n",
    "                    stride=args.doc_stride,\n",
    "                    return_overflowing_tokens=True,\n",
    "                    return_offsets_mapping=True,\n",
    "                    padding=\"max_length\" if args.pad_to_max_length else False,\n",
    "                )\n",
    "            else:\n",
    "                tokenized_examples = tokenizer(\n",
    "                    examples[context_column_name],\n",
    "                    truncation=\"only_first\",\n",
    "                    max_length=max_seq_length,\n",
    "                    stride=args.doc_stride,\n",
    "                    return_overflowing_tokens=True,\n",
    "                    return_offsets_mapping=True,\n",
    "                    padding=\"max_length\" if args.pad_to_max_length else False,\n",
    "                )\n",
    "\n",
    "            # Since one example might give us several features if it has a long context, we need a map from a feature to\n",
    "            # its corresponding example. This key gives us just that.\n",
    "            sample_mapping = tokenized_examples[\"overflow_to_sample_mapping\"]\n",
    "            \n",
    "            # Inflate doc_idxs based on sample_mapping\n",
    "            tokenized_examples['doc_idx'] = [offset + examples['doc_idx'][i] for i in sample_mapping]\n",
    "\n",
    "            # This example_id indicates the index of an original paragraph (not question id)\n",
    "            tokenized_examples['example_id'] = [indexes[i] for i in sample_mapping]\n",
    "\n",
    "            for i in range(len(tokenized_examples[\"input_ids\"])):\n",
    "                # Grab the sequence corresponding to that example (to know what is the context and what is the question).\n",
    "                sequence_ids = tokenized_examples.sequence_ids(i)\n",
    "                context_index = 1 if pad_on_right and args.append_title else 0\n",
    "\n",
    "                # Set to None the offset_mapping that are not part of the context so it's easy to determine if a token\n",
    "                # position is part of the context or not.\n",
    "                tokenized_examples[\"offset_mapping\"][i] = [\n",
    "                    (o if sequence_ids[k] == context_index else None)\n",
    "                    for k, o in enumerate(tokenized_examples[\"offset_mapping\"][i])\n",
    "                ]\n",
    "\n",
    "            return tokenized_examples\n",
    "        \n",
    "        examples = raw_datasets[\"test\"]\n",
    "\n",
    "        # Predict Feature Creation\n",
    "        with accelerator.main_process_first():\n",
    "            dataset = examples.map(\n",
    "                prepare_validation_features,\n",
    "                batched=True,\n",
    "                num_proc=args.preprocessing_num_workers,\n",
    "                # num_proc=1,\n",
    "                with_indices=True,\n",
    "                remove_columns=column_names,\n",
    "                load_from_cache_file=not args.overwrite_cache,\n",
    "                desc=\"Running tokenizer on prediction dataset\",\n",
    "            )\n",
    "        \n",
    "        # Data collator\n",
    "        # We have already padded to max length if the corresponding flag is True, otherwise we need to pad in the data\n",
    "        # collator.\n",
    "        data_collator = (\n",
    "            default_data_collator\n",
    "            if args.pad_to_max_length\n",
    "            else DataCollatorWithPadding(tokenizer, pad_to_multiple_of=8 if args.fp16 else None)\n",
    "        )\n",
    "\n",
    "        # Use trainer for predict\n",
    "        trainer = QuestionAnsweringTrainer(\n",
    "            model=model,\n",
    "            args=args,\n",
    "            tokenizer=tokenizer,\n",
    "            data_collator=data_collator,\n",
    "        )\n",
    "        trainer.generate_phrase_vecs(dataset, examples, output_dump_file, offset, args)\n",
    "\n",
    "        evalTime = timeit.default_timer() - start_time\n",
    "        logger.info(\"Evaluation done in total %f secs (%f sec per example)\", evalTime, evalTime / len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving 18829 (context, question, answer) triples.\n",
      "Writing to /home/kiki/dados_servidor/Servidor/InformationRetrival/densephrases/wikidump/wiki-dev/0000_hf.json\n",
      "\n",
      "Downloading and preparing dataset json/default to /home/kiki/dados_servidor/Servidor/InformationRetrival/densephrases/results/json/default-f6398ca9aaf53f37/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data files: 100%|██████████| 1/1 [00:00<00:00, 7423.55it/s]\n",
      "Extracting data files: 100%|██████████| 1/1 [00:00<00:00, 2137.77it/s]\n",
      "                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset json downloaded and prepared to /home/kiki/dados_servidor/Servidor/InformationRetrival/densephrases/results/json/default-f6398ca9aaf53f37/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 938.95it/s]\n",
      "max_steps is given, it will override any value given in num_train_epochs                               \n",
      "The following columns in the test set  don't have a corresponding argument in `Encoder.forward` and have been ignored: doc_idx, offset_mapping, overflow_to_sample_mapping.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-106:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/kiki/miniconda3/envs/densephrases/lib/python3.9/threading.py\", line 950, in _bootstrap_inner\n",
      "Exception in thread Thread-107:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/kiki/miniconda3/envs/densephrases/lib/python3.9/threading.py\", line 950, in _bootstrap_inner\n",
      "Exception in thread Thread-108:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/kiki/miniconda3/envs/densephrases/lib/python3.9/threading.py\", line 950, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/kiki/miniconda3/envs/densephrases/lib/python3.9/threading.py\", line 888, in run\n",
      "    self.run()\n",
      "  File \"/home/kiki/miniconda3/envs/densephrases/lib/python3.9/threading.py\", line 888, in run\n",
      "    self.run()\n",
      "  File \"/home/kiki/miniconda3/envs/densephrases/lib/python3.9/threading.py\", line 888, in run\n",
      "Exception in thread Thread-109:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/kiki/miniconda3/envs/densephrases/lib/python3.9/threading.py\", line 950, in _bootstrap_inner\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/kiki/dados_servidor/Servidor/InformationRetrival/densephrases/densephrases/utils/embed_utils.py\", line 198, in write\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/kiki/dados_servidor/Servidor/InformationRetrival/densephrases/densephrases/utils/embed_utils.py\", line 198, in write\n",
      "    self.run()\n",
      "  File \"/home/kiki/miniconda3/envs/densephrases/lib/python3.9/threading.py\", line 888, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/kiki/dados_servidor/Servidor/InformationRetrival/densephrases/densephrases/utils/embed_utils.py\", line 198, in write\n",
      "    with h5py.File(output_dump_file, 'a') as f:\n",
      "  File \"/home/kiki/miniconda3/envs/densephrases/lib/python3.9/site-packages/h5py/_hl/files.py\", line 567, in __init__\n",
      "Exception in thread Thread-110:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/kiki/miniconda3/envs/densephrases/lib/python3.9/threading.py\", line 950, in _bootstrap_inner\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/kiki/dados_servidor/Servidor/InformationRetrival/densephrases/densephrases/utils/embed_utils.py\", line 198, in write\n",
      "    fid = make_fid(name, mode, userblock_size, fapl, fcpl, swmr=swmr)\n",
      "  File \"/home/kiki/miniconda3/envs/densephrases/lib/python3.9/site-packages/h5py/_hl/files.py\", line 243, in make_fid\n",
      "Exception in thread Thread-111:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/kiki/miniconda3/envs/densephrases/lib/python3.9/threading.py\", line 950, in _bootstrap_inner\n",
      "    with h5py.File(output_dump_file, 'a') as f:\n",
      "  File \"/home/kiki/miniconda3/envs/densephrases/lib/python3.9/site-packages/h5py/_hl/files.py\", line 567, in __init__\n",
      "    fid = make_fid(name, mode, userblock_size, fapl, fcpl, swmr=swmr)\n",
      "  File \"/home/kiki/miniconda3/envs/densephrases/lib/python3.9/site-packages/h5py/_hl/files.py\", line 243, in make_fid\n",
      "Exception in thread Thread-112:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/kiki/miniconda3/envs/densephrases/lib/python3.9/threading.py\", line 950, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/kiki/miniconda3/envs/densephrases/lib/python3.9/threading.py\", line 888, in run\n",
      "    with h5py.File(output_dump_file, 'a') as f:\n",
      "  File \"/home/kiki/miniconda3/envs/densephrases/lib/python3.9/site-packages/h5py/_hl/files.py\", line 567, in __init__\n",
      "    with h5py.File(output_dump_file, 'a') as f:\n",
      "  File \"/home/kiki/miniconda3/envs/densephrases/lib/python3.9/site-packages/h5py/_hl/files.py\", line 567, in __init__\n",
      "    fid = h5f.open(name, h5f.ACC_RDWR, fapl=fapl)\n",
      "  File \"h5py/_objects.pyx\", line 54, in h5py._objects.with_phil.wrapper\n",
      "    fid = h5f.open(name, h5f.ACC_RDWR, fapl=fapl)\n",
      "  File \"h5py/_objects.pyx\", line 54, in h5py._objects.with_phil.wrapper\n",
      "  File \"h5py/_objects.pyx\", line 55, in h5py._objects.with_phil.wrapper\n",
      "  File \"h5py/h5f.pyx\", line 106, in h5py.h5f.open\n",
      "    self.run()\n",
      "  File \"/home/kiki/miniconda3/envs/densephrases/lib/python3.9/threading.py\", line 888, in run\n",
      "  File \"h5py/_objects.pyx\", line 55, in h5py._objects.with_phil.wrapper\n",
      "Exception in thread Thread-113:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/kiki/miniconda3/envs/densephrases/lib/python3.9/threading.py\", line 950, in _bootstrap_inner\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/kiki/dados_servidor/Servidor/InformationRetrival/densephrases/densephrases/utils/embed_utils.py\", line 198, in write\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/kiki/dados_servidor/Servidor/InformationRetrival/densephrases/densephrases/utils/embed_utils.py\", line 198, in write\n",
      "    self.run()\n",
      "  File \"/home/kiki/miniconda3/envs/densephrases/lib/python3.9/threading.py\", line 888, in run\n",
      "    with h5py.File(output_dump_file, 'a') as f:\n",
      "  File \"/home/kiki/miniconda3/envs/densephrases/lib/python3.9/site-packages/h5py/_hl/files.py\", line 567, in __init__\n",
      "  File \"h5py/h5f.pyx\", line 106, in h5py.h5f.open\n",
      "OSError: Unable to open file (truncated file: eof = 96, sblock->base_addr = 0, stored_eof = 2048)\n",
      "    fid = make_fid(name, mode, userblock_size, fapl, fcpl, swmr=swmr)\n",
      "  File \"/home/kiki/miniconda3/envs/densephrases/lib/python3.9/site-packages/h5py/_hl/files.py\", line 243, in make_fid\n",
      "    fid = make_fid(name, mode, userblock_size, fapl, fcpl, swmr=swmr)\n",
      "  File \"/home/kiki/miniconda3/envs/densephrases/lib/python3.9/site-packages/h5py/_hl/files.py\", line 243, in make_fid\n",
      "    fid = make_fid(name, mode, userblock_size, fapl, fcpl, swmr=swmr)\n",
      "  File \"/home/kiki/miniconda3/envs/densephrases/lib/python3.9/site-packages/h5py/_hl/files.py\", line 243, in make_fid\n",
      "OSError: Unable to open file (truncated file: eof = 96, sblock->base_addr = 0, stored_eof = 2048)\n",
      "    fid = h5f.open(name, h5f.ACC_RDWR, fapl=fapl)\n",
      "  File \"h5py/_objects.pyx\", line 54, in h5py._objects.with_phil.wrapper\n",
      "  File \"h5py/_objects.pyx\", line 55, in h5py._objects.with_phil.wrapper\n",
      "    fid = h5f.open(name, h5f.ACC_RDWR, fapl=fapl)\n",
      "  File \"h5py/_objects.pyx\", line 54, in h5py._objects.with_phil.wrapper\n",
      "  File \"h5py/_objects.pyx\", line 55, in h5py._objects.with_phil.wrapper\n",
      "  File \"h5py/h5f.pyx\", line 106, in h5py.h5f.open\n",
      "OSError: Unable to open file (truncated file: eof = 96, sblock->base_addr = 0, stored_eof = 2048)\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/kiki/dados_servidor/Servidor/InformationRetrival/densephrases/densephrases/utils/embed_utils.py\", line 198, in write\n",
      "Exception in thread Thread-114:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/kiki/miniconda3/envs/densephrases/lib/python3.9/threading.py\", line 950, in _bootstrap_inner\n",
      "    with h5py.File(output_dump_file, 'a') as f:\n",
      "  File \"/home/kiki/miniconda3/envs/densephrases/lib/python3.9/site-packages/h5py/_hl/files.py\", line 567, in __init__\n",
      "  File \"h5py/h5f.pyx\", line 106, in h5py.h5f.open\n",
      "OSError: Unable to open file (truncated file: eof = 96, sblock->base_addr = 0, stored_eof = 2048)\n",
      "    self.run()\n",
      "  File \"/home/kiki/miniconda3/envs/densephrases/lib/python3.9/threading.py\", line 888, in run\n",
      "    with h5py.File(output_dump_file, 'a') as f:\n",
      "  File \"/home/kiki/miniconda3/envs/densephrases/lib/python3.9/site-packages/h5py/_hl/files.py\", line 567, in __init__\n",
      "    fid = h5f.open(name, h5f.ACC_RDWR, fapl=fapl)\n",
      "  File \"h5py/_objects.pyx\", line 54, in h5py._objects.with_phil.wrapper\n",
      "    self.run()\n",
      "  File \"/home/kiki/miniconda3/envs/densephrases/lib/python3.9/threading.py\", line 888, in run\n",
      "  File \"h5py/_objects.pyx\", line 55, in h5py._objects.with_phil.wrapper\n",
      "  File \"h5py/h5f.pyx\", line 106, in h5py.h5f.open\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/kiki/dados_servidor/Servidor/InformationRetrival/densephrases/densephrases/utils/embed_utils.py\", line 198, in write\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/kiki/dados_servidor/Servidor/InformationRetrival/densephrases/densephrases/utils/embed_utils.py\", line 198, in write\n",
      "    fid = make_fid(name, mode, userblock_size, fapl, fcpl, swmr=swmr)\n",
      "  File \"/home/kiki/miniconda3/envs/densephrases/lib/python3.9/site-packages/h5py/_hl/files.py\", line 243, in make_fid\n",
      "    fid = make_fid(name, mode, userblock_size, fapl, fcpl, swmr=swmr)\n",
      "  File \"/home/kiki/miniconda3/envs/densephrases/lib/python3.9/site-packages/h5py/_hl/files.py\", line 243, in make_fid\n",
      "    with h5py.File(output_dump_file, 'a') as f:\n",
      "  File \"/home/kiki/miniconda3/envs/densephrases/lib/python3.9/site-packages/h5py/_hl/files.py\", line 567, in __init__\n",
      "    fid = h5f.open(name, h5f.ACC_RDWR, fapl=fapl)\n",
      "  File \"h5py/_objects.pyx\", line 54, in h5py._objects.with_phil.wrapper\n",
      "  File \"h5py/_objects.pyx\", line 55, in h5py._objects.with_phil.wrapper\n",
      "  File \"h5py/h5f.pyx\", line 106, in h5py.h5f.open\n",
      "OSError: Unable to open file (truncated file: eof = 96, sblock->base_addr = 0, stored_eof = 2048)\n",
      "Exception in thread Thread-115:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/kiki/miniconda3/envs/densephrases/lib/python3.9/threading.py\", line 950, in _bootstrap_inner\n",
      "    fid = make_fid(name, mode, userblock_size, fapl, fcpl, swmr=swmr)\n",
      "  File \"/home/kiki/miniconda3/envs/densephrases/lib/python3.9/site-packages/h5py/_hl/files.py\", line 243, in make_fid\n",
      "    fid = h5f.open(name, h5f.ACC_RDWR, fapl=fapl)\n",
      "  File \"h5py/_objects.pyx\", line 54, in h5py._objects.with_phil.wrapper\n",
      "    with h5py.File(output_dump_file, 'a') as f:\n",
      "  File \"/home/kiki/miniconda3/envs/densephrases/lib/python3.9/site-packages/h5py/_hl/files.py\", line 567, in __init__\n",
      "    fid = h5f.open(name, h5f.ACC_RDWR, fapl=fapl)\n",
      "  File \"h5py/_objects.pyx\", line 54, in h5py._objects.with_phil.wrapper\n",
      "OSError: Unable to open file (truncated file: eof = 96, sblock->base_addr = 0, stored_eof = 2048)\n",
      "    self.run()\n",
      "  File \"/home/kiki/miniconda3/envs/densephrases/lib/python3.9/threading.py\", line 888, in run\n",
      "    fid = make_fid(name, mode, userblock_size, fapl, fcpl, swmr=swmr)\n",
      "  File \"/home/kiki/miniconda3/envs/densephrases/lib/python3.9/site-packages/h5py/_hl/files.py\", line 243, in make_fid\n",
      "  File \"h5py/_objects.pyx\", line 55, in h5py._objects.with_phil.wrapper\n",
      "  File \"h5py/_objects.pyx\", line 55, in h5py._objects.with_phil.wrapper\n",
      "    fid = h5f.open(name, h5f.ACC_RDWR, fapl=fapl)\n",
      "  File \"h5py/_objects.pyx\", line 54, in h5py._objects.with_phil.wrapper\n",
      "  File \"h5py/h5f.pyx\", line 106, in h5py.h5f.open\n",
      "  File \"h5py/_objects.pyx\", line 55, in h5py._objects.with_phil.wrapper\n",
      "OSError: Unable to open file (truncated file: eof = 96, sblock->base_addr = 0, stored_eof = 2048)\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/kiki/dados_servidor/Servidor/InformationRetrival/densephrases/densephrases/utils/embed_utils.py\", line 198, in write\n",
      "  File \"h5py/h5f.pyx\", line 106, in h5py.h5f.open\n",
      "    OSError: Unable to open file (truncated file: eof = 96, sblock->base_addr = 0, stored_eof = 2048)\n",
      "  File \"h5py/h5f.pyx\", line 106, in h5py.h5f.open\n",
      "OSError: Unable to open file (truncated file: eof = 96, sblock->base_addr = 0, stored_eof = 2048)\n",
      "with h5py.File(output_dump_file, 'a') as f:\n",
      "  File \"/home/kiki/miniconda3/envs/densephrases/lib/python3.9/site-packages/h5py/_hl/files.py\", line 567, in __init__\n",
      "    fid = make_fid(name, mode, userblock_size, fapl, fcpl, swmr=swmr)\n",
      "  File \"/home/kiki/miniconda3/envs/densephrases/lib/python3.9/site-packages/h5py/_hl/files.py\", line 243, in make_fid\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    fid = h5f.open(name, h5f.ACC_RDWR, fapl=fapl)\n",
      "  File \"h5py/_objects.pyx\", line 54, in h5py._objects.with_phil.wrapper\n",
      "  File \"h5py/_objects.pyx\", line 55, in h5py._objects.with_phil.wrapper\n",
      "  File \"h5py/h5f.pyx\", line 106, in h5py.h5f.open\n",
      "OSError: Unable to open file (truncated file: eof = 96, sblock->base_addr = 0, stored_eof = 2048)\n",
      "Exception in thread Thread-116:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/kiki/miniconda3/envs/densephrases/lib/python3.9/threading.py\", line 950, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/kiki/miniconda3/envs/densephrases/lib/python3.9/threading.py\", line 888, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/kiki/dados_servidor/Servidor/InformationRetrival/densephrases/densephrases/utils/embed_utils.py\", line 198, in write\n",
      "    with h5py.File(output_dump_file, 'a') as f:\n",
      "  File \"/home/kiki/miniconda3/envs/densephrases/lib/python3.9/site-packages/h5py/_hl/files.py\", line 567, in __init__\n",
      "Exception in thread Thread-117:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/kiki/miniconda3/envs/densephrases/lib/python3.9/threading.py\", line 950, in _bootstrap_inner\n",
      "Exception in thread Thread-118:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/kiki/miniconda3/envs/densephrases/lib/python3.9/threading.py\", line 950, in _bootstrap_inner\n",
      "Exception in thread Thread-119:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/kiki/miniconda3/envs/densephrases/lib/python3.9/threading.py\", line 950, in _bootstrap_inner\n",
      "Exception in thread Thread-120:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/kiki/miniconda3/envs/densephrases/lib/python3.9/threading.py\", line 950, in _bootstrap_inner\n",
      "    fid = make_fid(name, mode, userblock_size, fapl, fcpl, swmr=swmr)\n",
      "  File \"/home/kiki/miniconda3/envs/densephrases/lib/python3.9/site-packages/h5py/_hl/files.py\", line 243, in make_fid\n",
      "Exception in thread Thread-121:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/kiki/miniconda3/envs/densephrases/lib/python3.9/threading.py\", line 950, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/kiki/miniconda3/envs/densephrases/lib/python3.9/threading.py\", line 888, in run\n",
      "    self.run()\n",
      "  File \"/home/kiki/miniconda3/envs/densephrases/lib/python3.9/threading.py\", line 888, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/kiki/dados_servidor/Servidor/InformationRetrival/densephrases/densephrases/utils/embed_utils.py\", line 198, in write\n",
      "    fid = h5f.open(name, h5f.ACC_RDWR, fapl=fapl)\n",
      "  File \"h5py/_objects.pyx\", line 54, in h5py._objects.with_phil.wrapper\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/kiki/dados_servidor/Servidor/InformationRetrival/densephrases/densephrases/utils/embed_utils.py\", line 198, in write\n",
      "Exception in thread Thread-122:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/kiki/miniconda3/envs/densephrases/lib/python3.9/threading.py\", line 950, in _bootstrap_inner\n",
      "    with h5py.File(output_dump_file, 'a') as f:\n",
      "  File \"/home/kiki/miniconda3/envs/densephrases/lib/python3.9/site-packages/h5py/_hl/files.py\", line 567, in __init__\n",
      "    fid = make_fid(name, mode, userblock_size, fapl, fcpl, swmr=swmr)\n",
      "  File \"/home/kiki/miniconda3/envs/densephrases/lib/python3.9/site-packages/h5py/_hl/files.py\", line 243, in make_fid\n",
      "    self.run()\n",
      "  File \"/home/kiki/miniconda3/envs/densephrases/lib/python3.9/threading.py\", line 888, in run\n",
      "    self.run()\n",
      "  File \"/home/kiki/miniconda3/envs/densephrases/lib/python3.9/threading.py\", line 888, in run\n",
      "    self.run()\n",
      "  File \"/home/kiki/miniconda3/envs/densephrases/lib/python3.9/threading.py\", line 888, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/kiki/dados_servidor/Servidor/InformationRetrival/densephrases/densephrases/utils/embed_utils.py\", line 198, in write\n",
      "  File \"h5py/_objects.pyx\", line 55, in h5py._objects.with_phil.wrapper\n",
      "    with h5py.File(output_dump_file, 'a') as f:\n",
      "  File \"/home/kiki/miniconda3/envs/densephrases/lib/python3.9/site-packages/h5py/_hl/files.py\", line 567, in __init__\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/kiki/dados_servidor/Servidor/InformationRetrival/densephrases/densephrases/utils/embed_utils.py\", line 198, in write\n",
      "    fid = h5f.open(name, h5f.ACC_RDWR, fapl=fapl)\n",
      "  File \"h5py/_objects.pyx\", line 54, in h5py._objects.with_phil.wrapper\n",
      "    self.run()\n",
      "  File \"/home/kiki/miniconda3/envs/densephrases/lib/python3.9/threading.py\", line 888, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/kiki/dados_servidor/Servidor/InformationRetrival/densephrases/densephrases/utils/embed_utils.py\", line 198, in write\n",
      "    fid = make_fid(name, mode, userblock_size, fapl, fcpl, swmr=swmr)\n",
      "  File \"/home/kiki/miniconda3/envs/densephrases/lib/python3.9/site-packages/h5py/_hl/files.py\", line 243, in make_fid\n",
      "    with h5py.File(output_dump_file, 'a') as f:\n",
      "  File \"/home/kiki/miniconda3/envs/densephrases/lib/python3.9/site-packages/h5py/_hl/files.py\", line 567, in __init__\n",
      "  File \"h5py/_objects.pyx\", line 55, in h5py._objects.with_phil.wrapper\n",
      "    with h5py.File(output_dump_file, 'a') as f:\n",
      "  File \"/home/kiki/miniconda3/envs/densephrases/lib/python3.9/site-packages/h5py/_hl/files.py\", line 567, in __init__\n",
      "  File \"h5py/h5f.pyx\", line 106, in h5py.h5f.open\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/kiki/dados_servidor/Servidor/InformationRetrival/densephrases/densephrases/utils/embed_utils.py\", line 198, in write\n",
      "    fid = make_fid(name, mode, userblock_size, fapl, fcpl, swmr=swmr)\n",
      "  File \"/home/kiki/miniconda3/envs/densephrases/lib/python3.9/site-packages/h5py/_hl/files.py\", line 243, in make_fid\n",
      "    fid = h5f.open(name, h5f.ACC_RDWR, fapl=fapl)\n",
      "  File \"h5py/_objects.pyx\", line 54, in h5py._objects.with_phil.wrapper\n",
      "    fid = make_fid(name, mode, userblock_size, fapl, fcpl, swmr=swmr)\n",
      "  File \"/home/kiki/miniconda3/envs/densephrases/lib/python3.9/site-packages/h5py/_hl/files.py\", line 243, in make_fid\n",
      "    fid = h5f.open(name, h5f.ACC_RDWR, fapl=fapl)\n",
      "  File \"h5py/_objects.pyx\", line 54, in h5py._objects.with_phil.wrapper\n",
      "Exception in thread Thread-123:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/kiki/miniconda3/envs/densephrases/lib/python3.9/threading.py\", line 950, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/kiki/miniconda3/envs/densephrases/lib/python3.9/threading.py\", line 888, in run\n",
      "    fid = h5f.open(name, h5f.ACC_RDWR, fapl=fapl)\n",
      "  File \"h5py/_objects.pyx\", line 54, in h5py._objects.with_phil.wrapper\n",
      "  File \"h5py/_objects.pyx\", line 55, in h5py._objects.with_phil.wrapper\n",
      "  File \"h5py/h5f.pyx\", line 106, in h5py.h5f.open\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/kiki/dados_servidor/Servidor/InformationRetrival/densephrases/densephrases/utils/embed_utils.py\", line 198, in write\n",
      "  File \"h5py/_objects.pyx\", line 55, in h5py._objects.with_phil.wrapper\n",
      "    with h5py.File(output_dump_file, 'a') as f:\n",
      "  File \"/home/kiki/miniconda3/envs/densephrases/lib/python3.9/site-packages/h5py/_hl/files.py\", line 567, in __init__\n",
      "Exception in thread Thread-124:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/kiki/miniconda3/envs/densephrases/lib/python3.9/threading.py\", line 950, in _bootstrap_inner\n",
      "    with h5py.File(output_dump_file, 'a') as f:\n",
      "  File \"/home/kiki/miniconda3/envs/densephrases/lib/python3.9/site-packages/h5py/_hl/files.py\", line 567, in __init__\n",
      "  File \"h5py/_objects.pyx\", line 55, in h5py._objects.with_phil.wrapper\n",
      "Exception in thread Thread-125:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/kiki/miniconda3/envs/densephrases/lib/python3.9/threading.py\", line 950, in _bootstrap_inner\n",
      "    fid = make_fid(name, mode, userblock_size, fapl, fcpl, swmr=swmr)\n",
      "  File \"/home/kiki/miniconda3/envs/densephrases/lib/python3.9/site-packages/h5py/_hl/files.py\", line 243, in make_fid\n",
      "OSError: Unable to open file (truncated file: eof = 96, sblock->base_addr = 0, stored_eof = 2048)\n",
      "Exception in thread Thread-126:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/kiki/miniconda3/envs/densephrases/lib/python3.9/threading.py\", line 950, in _bootstrap_inner\n",
      "    fid = h5f.open(name, h5f.ACC_RDWR, fapl=fapl)\n",
      "  File \"h5py/_objects.pyx\", line 54, in h5py._objects.with_phil.wrapper\n",
      "Exception in thread Thread-127:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/kiki/miniconda3/envs/densephrases/lib/python3.9/threading.py\", line 950, in _bootstrap_inner\n",
      "  File \"h5py/h5f.pyx\", line 106, in h5py.h5f.open\n",
      "OSError: Unable to open file (truncated file: eof = 96, sblock->base_addr = 0, stored_eof = 2048)\n",
      "Exception in thread Thread-128:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/kiki/miniconda3/envs/densephrases/lib/python3.9/threading.py\", line 950, in _bootstrap_inner\n",
      "    fid = make_fid(name, mode, userblock_size, fapl, fcpl, swmr=swmr)\n",
      "  File \"/home/kiki/miniconda3/envs/densephrases/lib/python3.9/site-packages/h5py/_hl/files.py\", line 243, in make_fid\n",
      "    with h5py.File(output_dump_file, 'a') as f:\n",
      "  File \"/home/kiki/miniconda3/envs/densephrases/lib/python3.9/site-packages/h5py/_hl/files.py\", line 567, in __init__\n",
      "  File \"h5py/h5f.pyx\", line 106, in h5py.h5f.open\n",
      "OSError: Unable to open file (truncated file: eof = 96, sblock->base_addr = 0, stored_eof = 2048)\n",
      "  File \"h5py/h5f.pyx\", line 106, in h5py.h5f.open\n",
      "    self.run()\n",
      "  File \"/home/kiki/miniconda3/envs/densephrases/lib/python3.9/threading.py\", line 888, in run\n",
      "OSError: Unable to open file (truncated file: eof = 96, sblock->base_addr = 0, stored_eof = 2048)\n",
      "    fid = h5f.open(name, h5f.ACC_RDWR, fapl=fapl)\n",
      "  File \"h5py/_objects.pyx\", line 54, in h5py._objects.with_phil.wrapper\n",
      "OSError: Unable to open file (truncated file: eof = 96, sblock->base_addr = 0, stored_eof = 2048)\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/kiki/dados_servidor/Servidor/InformationRetrival/densephrases/densephrases/utils/embed_utils.py\", line 198, in write\n",
      "    with h5py.File(output_dump_file, 'a') as f:\n",
      "  File \"/home/kiki/miniconda3/envs/densephrases/lib/python3.9/site-packages/h5py/_hl/files.py\", line 567, in __init__\n",
      "    fid = make_fid(name, mode, userblock_size, fapl, fcpl, swmr=swmr)\n",
      "  File \"/home/kiki/miniconda3/envs/densephrases/lib/python3.9/site-packages/h5py/_hl/files.py\", line 243, in make_fid\n",
      "    fid = h5f.open(name, h5f.ACC_RDWR, fapl=fapl)\n",
      "  File \"h5py/_objects.pyx\", line 54, in h5py._objects.with_phil.wrapper\n",
      "    self.run()\n",
      "  File \"/home/kiki/miniconda3/envs/densephrases/lib/python3.9/threading.py\", line 888, in run\n",
      "Exception in thread Thread-129:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/kiki/miniconda3/envs/densephrases/lib/python3.9/threading.py\", line 950, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/kiki/miniconda3/envs/densephrases/lib/python3.9/threading.py\", line 888, in run\n",
      "Exception in thread Thread-130:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/kiki/miniconda3/envs/densephrases/lib/python3.9/threading.py\", line 950, in _bootstrap_inner\n",
      "  File \"h5py/_objects.pyx\", line 55, in h5py._objects.with_phil.wrapper\n",
      "    fid = make_fid(name, mode, userblock_size, fapl, fcpl, swmr=swmr)\n",
      "  File \"/home/kiki/miniconda3/envs/densephrases/lib/python3.9/site-packages/h5py/_hl/files.py\", line 243, in make_fid\n",
      "    self.run()\n",
      "  File \"/home/kiki/miniconda3/envs/densephrases/lib/python3.9/threading.py\", line 888, in run\n",
      "    fid = h5f.open(name, h5f.ACC_RDWR, fapl=fapl)\n",
      "  File \"h5py/_objects.pyx\", line 54, in h5py._objects.with_phil.wrapper\n",
      "Exception in thread Thread-131:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/kiki/miniconda3/envs/densephrases/lib/python3.9/threading.py\", line 950, in _bootstrap_inner\n",
      "  File \"h5py/_objects.pyx\", line 55, in h5py._objects.with_phil.wrapper\n",
      "Exception in thread Thread-132:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/kiki/miniconda3/envs/densephrases/lib/python3.9/threading.py\", line 950, in _bootstrap_inner\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/kiki/dados_servidor/Servidor/InformationRetrival/densephrases/densephrases/utils/embed_utils.py\", line 198, in write\n",
      "  File \"h5py/_objects.pyx\", line 55, in h5py._objects.with_phil.wrapper\n",
      "    self.run()\n",
      "  File \"/home/kiki/miniconda3/envs/densephrases/lib/python3.9/threading.py\", line 888, in run\n",
      "  File \"h5py/h5f.pyx\", line 106, in h5py.h5f.open\n",
      "    self.run()\n",
      "  File \"/home/kiki/miniconda3/envs/densephrases/lib/python3.9/threading.py\", line 888, in run\n",
      "Exception in thread Thread-133:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/kiki/miniconda3/envs/densephrases/lib/python3.9/threading.py\", line 950, in _bootstrap_inner\n",
      "OSError: Unable to open file (truncated file: eof = 96, sblock->base_addr = 0, stored_eof = 2048)\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/kiki/dados_servidor/Servidor/InformationRetrival/densephrases/densephrases/utils/embed_utils.py\", line 198, in write\n",
      "    self.run()\n",
      "  File \"/home/kiki/miniconda3/envs/densephrases/lib/python3.9/threading.py\", line 888, in run\n",
      "    self.run()\n",
      "  File \"/home/kiki/miniconda3/envs/densephrases/lib/python3.9/threading.py\", line 888, in run\n",
      "    with h5py.File(output_dump_file, 'a') as f:\n",
      "  File \"/home/kiki/miniconda3/envs/densephrases/lib/python3.9/site-packages/h5py/_hl/files.py\", line 567, in __init__\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/kiki/dados_servidor/Servidor/InformationRetrival/densephrases/densephrases/utils/embed_utils.py\", line 198, in write\n",
      "Exception in thread Thread-134:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/kiki/miniconda3/envs/densephrases/lib/python3.9/threading.py\", line 950, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/kiki/miniconda3/envs/densephrases/lib/python3.9/threading.py\", line 888, in run\n",
      "  File \"h5py/h5f.pyx\", line 106, in h5py.h5f.open\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/kiki/dados_servidor/Servidor/InformationRetrival/densephrases/densephrases/utils/embed_utils.py\", line 198, in write\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/kiki/dados_servidor/Servidor/InformationRetrival/densephrases/densephrases/utils/embed_utils.py\", line 198, in write\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/kiki/dados_servidor/Servidor/InformationRetrival/densephrases/densephrases/utils/embed_utils.py\", line 198, in write\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/kiki/dados_servidor/Servidor/InformationRetrival/densephrases/densephrases/utils/embed_utils.py\", line 198, in write\n",
      "    self.run()\n",
      "  File \"/home/kiki/miniconda3/envs/densephrases/lib/python3.9/threading.py\", line 888, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/kiki/dados_servidor/Servidor/InformationRetrival/densephrases/densephrases/utils/embed_utils.py\", line 198, in write\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/kiki/dados_servidor/Servidor/InformationRetrival/densephrases/densephrases/utils/embed_utils.py\", line 198, in write\n",
      "        with h5py.File(output_dump_file, 'a') as f:\n",
      "  File \"/home/kiki/miniconda3/envs/densephrases/lib/python3.9/site-packages/h5py/_hl/files.py\", line 567, in __init__\n",
      "    fid = make_fid(name, mode, userblock_size, fapl, fcpl, swmr=swmr)\n",
      "  File \"/home/kiki/miniconda3/envs/densephrases/lib/python3.9/site-packages/h5py/_hl/files.py\", line 243, in make_fid\n",
      "with h5py.File(output_dump_file, 'a') as f:\n",
      "  File \"/home/kiki/miniconda3/envs/densephrases/lib/python3.9/site-packages/h5py/_hl/files.py\", line 567, in __init__\n",
      "Exception in thread Thread-135:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/kiki/miniconda3/envs/densephrases/lib/python3.9/threading.py\", line 950, in _bootstrap_inner\n",
      "    with h5py.File(output_dump_file, 'a') as f:\n",
      "  File \"/home/kiki/miniconda3/envs/densephrases/lib/python3.9/site-packages/h5py/_hl/files.py\", line 567, in __init__\n",
      "    self.run()\n",
      "  File \"/home/kiki/miniconda3/envs/densephrases/lib/python3.9/threading.py\", line 888, in run\n",
      "    with h5py.File(output_dump_file, 'a') as f:\n",
      "  File \"/home/kiki/miniconda3/envs/densephrases/lib/python3.9/site-packages/h5py/_hl/files.py\", line 567, in __init__\n",
      "OSError: Unable to open file (truncated file: eof = 96, sblock->base_addr = 0, stored_eof = 2048)\n",
      "    with h5py.File(output_dump_file, 'a') as f:\n",
      "  File \"/home/kiki/miniconda3/envs/densephrases/lib/python3.9/site-packages/h5py/_hl/files.py\", line 567, in __init__\n",
      "    fid = h5f.open(name, h5f.ACC_RDWR, fapl=fapl)\n",
      "  File \"h5py/_objects.pyx\", line 54, in h5py._objects.with_phil.wrapper\n",
      "Exception in thread Thread-136:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/kiki/miniconda3/envs/densephrases/lib/python3.9/threading.py\", line 950, in _bootstrap_inner\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/kiki/dados_servidor/Servidor/InformationRetrival/densephrases/densephrases/utils/embed_utils.py\", line 198, in write\n",
      "    self.run()\n",
      "  File \"/home/kiki/miniconda3/envs/densephrases/lib/python3.9/threading.py\", line 888, in run\n",
      "    fid = make_fid(name, mode, userblock_size, fapl, fcpl, swmr=swmr)\n",
      "  File \"/home/kiki/miniconda3/envs/densephrases/lib/python3.9/site-packages/h5py/_hl/files.py\", line 243, in make_fid\n",
      "    with h5py.File(output_dump_file, 'a') as f:\n",
      "  File \"/home/kiki/miniconda3/envs/densephrases/lib/python3.9/site-packages/h5py/_hl/files.py\", line 567, in __init__\n",
      "    fid = make_fid(name, mode, userblock_size, fapl, fcpl, swmr=swmr)\n",
      "  File \"/home/kiki/miniconda3/envs/densephrases/lib/python3.9/site-packages/h5py/_hl/files.py\", line 243, in make_fid\n",
      "    fid = make_fid(name, mode, userblock_size, fapl, fcpl, swmr=swmr)\n",
      "  File \"/home/kiki/miniconda3/envs/densephrases/lib/python3.9/site-packages/h5py/_hl/files.py\", line 243, in make_fid\n",
      "    fid = make_fid(name, mode, userblock_size, fapl, fcpl, swmr=swmr)\n",
      "  File \"/home/kiki/miniconda3/envs/densephrases/lib/python3.9/site-packages/h5py/_hl/files.py\", line 243, in make_fid\n",
      "    with h5py.File(output_dump_file, 'a') as f:\n",
      "  File \"/home/kiki/miniconda3/envs/densephrases/lib/python3.9/site-packages/h5py/_hl/files.py\", line 567, in __init__\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/kiki/dados_servidor/Servidor/InformationRetrival/densephrases/densephrases/utils/embed_utils.py\", line 198, in write\n",
      "    fid = make_fid(name, mode, userblock_size, fapl, fcpl, swmr=swmr)\n",
      "  File \"/home/kiki/miniconda3/envs/densephrases/lib/python3.9/site-packages/h5py/_hl/files.py\", line 243, in make_fid\n",
      "    self.run()\n",
      "  File \"/home/kiki/miniconda3/envs/densephrases/lib/python3.9/threading.py\", line 888, in run\n",
      "    with h5py.File(output_dump_file, 'a') as f:\n",
      "  File \"/home/kiki/miniconda3/envs/densephrases/lib/python3.9/site-packages/h5py/_hl/files.py\", line 567, in __init__\n",
      "Exception in thread Thread-137:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/kiki/miniconda3/envs/densephrases/lib/python3.9/threading.py\", line 950, in _bootstrap_inner\n",
      "    fid = h5f.open(name, h5f.ACC_RDWR, fapl=fapl)\n",
      "  File \"h5py/_objects.pyx\", line 54, in h5py._objects.with_phil.wrapper\n",
      "    fid = h5f.open(name, h5f.ACC_RDWR, fapl=fapl)\n",
      "  File \"h5py/_objects.pyx\", line 54, in h5py._objects.with_phil.wrapper\n",
      "    fid = h5f.open(name, h5f.ACC_RDWR, fapl=fapl)\n",
      "  File \"h5py/_objects.pyx\", line 54, in h5py._objects.with_phil.wrapper\n",
      "Exception in thread Thread-138:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/kiki/miniconda3/envs/densephrases/lib/python3.9/threading.py\", line 950, in _bootstrap_inner\n",
      "  File \"h5py/h5f.pyx\", line 106, in h5py.h5f.open\n",
      "    with h5py.File(output_dump_file, 'a') as f:\n",
      "  File \"/home/kiki/miniconda3/envs/densephrases/lib/python3.9/site-packages/h5py/_hl/files.py\", line 567, in __init__\n",
      "  File \"h5py/_objects.pyx\", line 55, in h5py._objects.with_phil.wrapper\n",
      "    self.run()\n",
      "  File \"/home/kiki/miniconda3/envs/densephrases/lib/python3.9/threading.py\", line 888, in run\n",
      "Exception in thread Thread-139:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/kiki/miniconda3/envs/densephrases/lib/python3.9/threading.py\", line 950, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/kiki/miniconda3/envs/densephrases/lib/python3.9/threading.py\", line 888, in run\n",
      "    fid = make_fid(name, mode, userblock_size, fapl, fcpl, swmr=swmr)\n",
      "  File \"/home/kiki/miniconda3/envs/densephrases/lib/python3.9/site-packages/h5py/_hl/files.py\", line 243, in make_fid\n",
      "    with h5py.File(output_dump_file, 'a') as f:\n",
      "  File \"/home/kiki/miniconda3/envs/densephrases/lib/python3.9/site-packages/h5py/_hl/files.py\", line 567, in __init__\n",
      "    fid = h5f.open(name, h5f.ACC_RDWR, fapl=fapl)\n",
      "  File \"h5py/_objects.pyx\", line 54, in h5py._objects.with_phil.wrapper\n",
      "    fid = h5f.open(name, h5f.ACC_RDWR, fapl=fapl)\n",
      "  File \"h5py/_objects.pyx\", line 54, in h5py._objects.with_phil.wrapper\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/kiki/dados_servidor/Servidor/InformationRetrival/densephrases/densephrases/utils/embed_utils.py\", line 198, in write\n",
      "Exception in thread Thread-140:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/kiki/miniconda3/envs/densephrases/lib/python3.9/threading.py\", line 950, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/kiki/miniconda3/envs/densephrases/lib/python3.9/threading.py\", line 888, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/kiki/dados_servidor/Servidor/InformationRetrival/densephrases/densephrases/utils/embed_utils.py\", line 198, in write\n",
      "    with h5py.File(output_dump_file, 'a') as f:\n",
      "  File \"/home/kiki/miniconda3/envs/densephrases/lib/python3.9/site-packages/h5py/_hl/files.py\", line 567, in __init__\n",
      "  File \"h5py/h5f.pyx\", line 106, in h5py.h5f.open\n",
      "  File \"h5py/_objects.pyx\", line 55, in h5py._objects.with_phil.wrapper\n",
      "    fid = make_fid(name, mode, userblock_size, fapl, fcpl, swmr=swmr)\n",
      "  File \"/home/kiki/miniconda3/envs/densephrases/lib/python3.9/site-packages/h5py/_hl/files.py\", line 243, in make_fid\n",
      "Exception in thread Thread-141:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/kiki/miniconda3/envs/densephrases/lib/python3.9/threading.py\", line 950, in _bootstrap_inner\n",
      "    fid = h5f.open(name, h5f.ACC_RDWR, fapl=fapl)\n",
      "  File \"h5py/_objects.pyx\", line 54, in h5py._objects.with_phil.wrapper\n",
      "    with h5py.File(output_dump_file, 'a') as f:\n",
      "  File \"/home/kiki/miniconda3/envs/densephrases/lib/python3.9/site-packages/h5py/_hl/files.py\", line 567, in __init__\n",
      "    fid = make_fid(name, mode, userblock_size, fapl, fcpl, swmr=swmr)\n",
      "  File \"/home/kiki/miniconda3/envs/densephrases/lib/python3.9/site-packages/h5py/_hl/files.py\", line 243, in make_fid\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/kiki/dados_servidor/Servidor/InformationRetrival/densephrases/densephrases/utils/embed_utils.py\", line 198, in write\n",
      "OSError: Unable to open file (truncated file: eof = 96, sblock->base_addr = 0, stored_eof = 2048)\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/kiki/dados_servidor/Servidor/InformationRetrival/densephrases/densephrases/utils/embed_utils.py\", line 198, in write\n",
      "    with h5py.File(output_dump_file, 'a') as f:\n",
      "  File \"/home/kiki/miniconda3/envs/densephrases/lib/python3.9/site-packages/h5py/_hl/files.py\", line 567, in __init__\n",
      "  File \"h5py/_objects.pyx\", line 55, in h5py._objects.with_phil.wrapper\n",
      "    fid = make_fid(name, mode, userblock_size, fapl, fcpl, swmr=swmr)\n",
      "  File \"/home/kiki/miniconda3/envs/densephrases/lib/python3.9/site-packages/h5py/_hl/files.py\", line 243, in make_fid\n",
      "    self.run()\n",
      "  File \"/home/kiki/miniconda3/envs/densephrases/lib/python3.9/threading.py\", line 888, in run\n",
      "  File \"h5py/_objects.pyx\", line 55, in h5py._objects.with_phil.wrapper\n",
      "    fid = make_fid(name, mode, userblock_size, fapl, fcpl, swmr=swmr)\n",
      "  File \"/home/kiki/miniconda3/envs/densephrases/lib/python3.9/site-packages/h5py/_hl/files.py\", line 243, in make_fid\n",
      "    fid = make_fid(name, mode, userblock_size, fapl, fcpl, swmr=swmr)\n",
      "  File \"/home/kiki/miniconda3/envs/densephrases/lib/python3.9/site-packages/h5py/_hl/files.py\", line 243, in make_fid\n",
      "    fid = make_fid(name, mode, userblock_size, fapl, fcpl, swmr=swmr)\n",
      "  File \"/home/kiki/miniconda3/envs/densephrases/lib/python3.9/site-packages/h5py/_hl/files.py\", line 243, in make_fid\n",
      "    fid = make_fid(name, mode, userblock_size, fapl, fcpl, swmr=swmr)\n",
      "  File \"/home/kiki/miniconda3/envs/densephrases/lib/python3.9/site-packages/h5py/_hl/files.py\", line 243, in make_fid\n",
      "    self.run()\n",
      "  File \"/home/kiki/miniconda3/envs/densephrases/lib/python3.9/threading.py\", line 888, in run\n",
      "    fid = h5f.open(name, h5f.ACC_RDWR, fapl=fapl)\n",
      "  File \"h5py/_objects.pyx\", line 54, in h5py._objects.with_phil.wrapper\n",
      "  File \"h5py/_objects.pyx\", line 55, in h5py._objects.with_phil.wrapper\n",
      "    fid = h5f.open(name, h5f.ACC_RDWR, fapl=fapl)\n",
      "  File \"h5py/_objects.pyx\", line 54, in h5py._objects.with_phil.wrapper\n",
      "    fid = h5f.open(name, h5f.ACC_RDWR, fapl=fapl)\n",
      "  File \"h5py/_objects.pyx\", line 54, in h5py._objects.with_phil.wrapper\n",
      "    fid = h5f.open(name, h5f.ACC_RDWR, fapl=fapl)\n",
      "  File \"h5py/_objects.pyx\", line 54, in h5py._objects.with_phil.wrapper\n",
      "  File \"h5py/h5f.pyx\", line 106, in h5py.h5f.open\n",
      "  File \"h5py/_objects.pyx\", line 55, in h5py._objects.with_phil.wrapper\n",
      "    fid = h5f.open(name, h5f.ACC_RDWR, fapl=fapl)\n",
      "  File \"h5py/_objects.pyx\", line 54, in h5py._objects.with_phil.wrapper\n",
      "    with h5py.File(output_dump_file, 'a') as f:\n",
      "  File \"/home/kiki/miniconda3/envs/densephrases/lib/python3.9/site-packages/h5py/_hl/files.py\", line 567, in __init__\n",
      "  File \"h5py/h5f.pyx\", line 106, in h5py.h5f.open\n",
      "OSError: Unable to open file (truncated file: eof = 96, sblock->base_addr = 0, stored_eof = 2048)\n",
      "    fid = h5f.open(name, h5f.ACC_RDWR, fapl=fapl)\n",
      "  File \"h5py/_objects.pyx\", line 54, in h5py._objects.with_phil.wrapper\n",
      "  File \"h5py/_objects.pyx\", line 55, in h5py._objects.with_phil.wrapper\n",
      "  File \"h5py/_objects.pyx\", line 55, in h5py._objects.with_phil.wrapper\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/kiki/dados_servidor/Servidor/InformationRetrival/densephrases/densephrases/utils/embed_utils.py\", line 198, in write\n",
      "OSError: Unable to open file (truncated file: eof = 96, sblock->base_addr = 0, stored_eof = 2048)\n",
      "        self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/kiki/dados_servidor/Servidor/InformationRetrival/densephrases/densephrases/utils/embed_utils.py\", line 198, in write\n",
      "  File \"h5py/_objects.pyx\", line 55, in h5py._objects.with_phil.wrapper\n",
      "    fid = h5f.open(name, h5f.ACC_RDWR, fapl=fapl)\n",
      "  File \"h5py/_objects.pyx\", line 54, in h5py._objects.with_phil.wrapper\n",
      "  File \"h5py/_objects.pyx\", line 55, in h5py._objects.with_phil.wrapper\n",
      "fid = make_fid(name, mode, userblock_size, fapl, fcpl, swmr=swmr)\n",
      "  File \"/home/kiki/miniconda3/envs/densephrases/lib/python3.9/site-packages/h5py/_hl/files.py\", line 243, in make_fid\n",
      "  File \"h5py/h5f.pyx\", line 106, in h5py.h5f.open\n",
      "  File \"h5py/_objects.pyx\", line 55, in h5py._objects.with_phil.wrapper\n",
      "OSError: Unable to open file (truncated file: eof = 96, sblock->base_addr = 0, stored_eof = 2048)\n",
      "  File \"h5py/h5f.pyx\", line 106, in h5py.h5f.open\n",
      "  File \"h5py/_objects.pyx\", line 55, in h5py._objects.with_phil.wrapper\n",
      "  File \"h5py/_objects.pyx\", line 55, in h5py._objects.with_phil.wrapper\n",
      "    fid = h5f.open(name, h5f.ACC_RDWR, fapl=fapl)\n",
      "  File \"h5py/_objects.pyx\", line 54, in h5py._objects.with_phil.wrapper\n",
      "  File \"h5py/h5f.pyx\", line 106, in h5py.h5f.open\n",
      "  File \"h5py/h5f.pyx\", line 106, in h5py.h5f.open\n",
      "OSError: Unable to open file (truncated file: eof = 96, sblock->base_addr = 0, stored_eof = 2048)\n",
      "  File \"h5py/_objects.pyx\", line 55, in h5py._objects.with_phil.wrapper\n",
      "OSError: Unable to open file (truncated file: eof = 96, sblock->base_addr = 0, stored_eof = 2048)\n",
      "    with h5py.File(output_dump_file, 'a') as f:\n",
      "  File \"/home/kiki/miniconda3/envs/densephrases/lib/python3.9/site-packages/h5py/_hl/files.py\", line 567, in __init__\n",
      "  File \"h5py/_objects.pyx\", line 55, in h5py._objects.with_phil.wrapper\n",
      "OSError: Unable to open file (truncated file: eof = 96, sblock->base_addr = 0, stored_eof = 2048)\n",
      "OSError: Unable to open file (truncated file: eof = 96, sblock->base_addr = 0, stored_eof = 2048)\n",
      "  File \"h5py/h5f.pyx\", line 106, in h5py.h5f.open\n",
      "  File \"h5py/h5f.pyx\", line 106, in h5py.h5f.open\n",
      "    fid = make_fid(name, mode, userblock_size, fapl, fcpl, swmr=swmr)\n",
      "  File \"/home/kiki/miniconda3/envs/densephrases/lib/python3.9/site-packages/h5py/_hl/files.py\", line 243, in make_fid\n",
      "    with h5py.File(output_dump_file, 'a') as f:\n",
      "  File \"/home/kiki/miniconda3/envs/densephrases/lib/python3.9/site-packages/h5py/_hl/files.py\", line 567, in __init__\n",
      "  File \"h5py/_objects.pyx\", line 55, in h5py._objects.with_phil.wrapper\n",
      "    fid = h5f.open(name, h5f.ACC_RDWR, fapl=fapl)\n",
      "  File \"h5py/_objects.pyx\", line 54, in h5py._objects.with_phil.wrapper\n",
      "  File \"h5py/h5f.pyx\", line 106, in h5py.h5f.open\n",
      "  File \"h5py/h5f.pyx\", line 106, in h5py.h5f.open\n",
      "OSError: Unable to open file (truncated file: eof = 96, sblock->base_addr = 0, stored_eof = 2048)\n",
      "OSError: Unable to open file (truncated file: eof = 96, sblock->base_addr = 0, stored_eof = 2048)\n",
      "    fid = make_fid(name, mode, userblock_size, fapl, fcpl, swmr=swmr)\n",
      "  File \"/home/kiki/miniconda3/envs/densephrases/lib/python3.9/site-packages/h5py/_hl/files.py\", line 243, in make_fid\n",
      "  File \"h5py/h5f.pyx\", line 106, in h5py.h5f.open\n",
      "  File \"h5py/h5f.pyx\", line 106, in h5py.h5f.open\n",
      "    fid = h5f.open(name, h5f.ACC_RDWR, fapl=fapl)\n",
      "  File \"h5py/_objects.pyx\", line 54, in h5py._objects.with_phil.wrapper\n",
      "OSError: Unable to open file (truncated file: eof = 96, sblock->base_addr = 0, stored_eof = 2048)\n",
      "  File \"h5py/_objects.pyx\", line 55, in h5py._objects.with_phil.wrapper\n",
      "OSError: Unable to open file (truncated file: eof = 96, sblock->base_addr = 0, stored_eof = 2048)\n",
      "  File \"h5py/h5f.pyx\", line 106, in h5py.h5f.open\n",
      "Exception in thread Thread-142:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/kiki/miniconda3/envs/densephrases/lib/python3.9/threading.py\", line 950, in _bootstrap_inner\n",
      "  File \"h5py/_objects.pyx\", line 55, in h5py._objects.with_phil.wrapper\n",
      "  File \"h5py/h5f.pyx\", line 106, in h5py.h5f.open\n",
      "OSError: Unable to open file (truncated file: eof = 96, sblock->base_addr = 0, stored_eof = 2048)\n",
      "Exception in thread Thread-143:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/kiki/miniconda3/envs/densephrases/lib/python3.9/threading.py\", line 950, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/kiki/miniconda3/envs/densephrases/lib/python3.9/threading.py\", line 888, in run\n",
      "  File \"h5py/h5f.pyx\", line 106, in h5py.h5f.open\n",
      "    self.run()\n",
      "  File \"/home/kiki/miniconda3/envs/densephrases/lib/python3.9/threading.py\", line 888, in run\n",
      "  File \"h5py/h5f.pyx\", line 106, in h5py.h5f.open\n",
      "OSError: Unable to open file (truncated file: eof = 96, sblock->base_addr = 0, stored_eof = 2048)\n",
      "  File \"h5py/h5f.pyx\", line 106, in h5py.h5f.open\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/kiki/dados_servidor/Servidor/InformationRetrival/densephrases/densephrases/utils/embed_utils.py\", line 198, in write\n",
      "OSError: Unable to open file (truncated file: eof = 96, sblock->base_addr = 0, stored_eof = 2048)\n",
      "Exception in thread Thread-144:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/kiki/miniconda3/envs/densephrases/lib/python3.9/threading.py\", line 950, in _bootstrap_inner\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/kiki/dados_servidor/Servidor/InformationRetrival/densephrases/densephrases/utils/embed_utils.py\", line 198, in write\n",
      "    self.run()\n",
      "  File \"/home/kiki/miniconda3/envs/densephrases/lib/python3.9/threading.py\", line 888, in run\n",
      "Exception in thread Thread-145:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/kiki/miniconda3/envs/densephrases/lib/python3.9/threading.py\", line 950, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/kiki/miniconda3/envs/densephrases/lib/python3.9/threading.py\", line 888, in run\n",
      "    with h5py.File(output_dump_file, 'a') as f:\n",
      "  File \"/home/kiki/miniconda3/envs/densephrases/lib/python3.9/site-packages/h5py/_hl/files.py\", line 567, in __init__\n",
      "OSError: Unable to open file (truncated file: eof = 96, sblock->base_addr = 0, stored_eof = 2048)\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/kiki/dados_servidor/Servidor/InformationRetrival/densephrases/densephrases/utils/embed_utils.py\", line 198, in write\n",
      "    with h5py.File(output_dump_file, 'a') as f:\n",
      "  File \"/home/kiki/miniconda3/envs/densephrases/lib/python3.9/site-packages/h5py/_hl/files.py\", line 567, in __init__\n",
      "    fid = make_fid(name, mode, userblock_size, fapl, fcpl, swmr=swmr)\n",
      "  File \"/home/kiki/miniconda3/envs/densephrases/lib/python3.9/site-packages/h5py/_hl/files.py\", line 243, in make_fid\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/kiki/dados_servidor/Servidor/InformationRetrival/densephrases/densephrases/utils/embed_utils.py\", line 198, in write\n",
      "    with h5py.File(output_dump_file, 'a') as f:\n",
      "  File \"/home/kiki/miniconda3/envs/densephrases/lib/python3.9/site-packages/h5py/_hl/files.py\", line 567, in __init__\n",
      "    fid = make_fid(name, mode, userblock_size, fapl, fcpl, swmr=swmr)\n",
      "  File \"/home/kiki/miniconda3/envs/densephrases/lib/python3.9/site-packages/h5py/_hl/files.py\", line 243, in make_fid\n",
      "    fid = make_fid(name, mode, userblock_size, fapl, fcpl, swmr=swmr)\n",
      "  File \"/home/kiki/miniconda3/envs/densephrases/lib/python3.9/site-packages/h5py/_hl/files.py\", line 243, in make_fid\n",
      "    fid = h5f.open(name, h5f.ACC_RDWR, fapl=fapl)\n",
      "  File \"h5py/_objects.pyx\", line 54, in h5py._objects.with_phil.wrapper\n",
      "OSError: Unable to open file (truncated file: eof = 96, sblock->base_addr = 0, stored_eof = 2048)\n",
      "Exception in thread Thread-146:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/kiki/miniconda3/envs/densephrases/lib/python3.9/threading.py\", line 950, in _bootstrap_inner\n",
      "    fid = h5f.open(name, h5f.ACC_RDWR, fapl=fapl)\n",
      "  File \"h5py/_objects.pyx\", line 54, in h5py._objects.with_phil.wrapper\n",
      "Exception in thread Thread-147:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/kiki/miniconda3/envs/densephrases/lib/python3.9/threading.py\", line 950, in _bootstrap_inner\n",
      "  File \"h5py/_objects.pyx\", line 55, in h5py._objects.with_phil.wrapper\n",
      "OSError: Unable to open file (truncated file: eof = 96, sblock->base_addr = 0, stored_eof = 2048)\n",
      "    self.run()\n",
      "  File \"/home/kiki/miniconda3/envs/densephrases/lib/python3.9/threading.py\", line 888, in run\n",
      "  File \"h5py/h5f.pyx\", line 106, in h5py.h5f.open\n",
      "OSError: Unable to open file (truncated file: eof = 96, sblock->base_addr = 0, stored_eof = 2048)\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/kiki/dados_servidor/Servidor/InformationRetrival/densephrases/densephrases/utils/embed_utils.py\", line 198, in write\n",
      "    fid = h5f.open(name, h5f.ACC_RDWR, fapl=fapl)\n",
      "  File \"h5py/_objects.pyx\", line 54, in h5py._objects.with_phil.wrapper\n",
      "  File \"h5py/_objects.pyx\", line 55, in h5py._objects.with_phil.wrapper\n",
      "  File \"h5py/h5f.pyx\", line 106, in h5py.h5f.open\n",
      "OSError: Unable to open file (truncated file: eof = 96, sblock->base_addr = 0, stored_eof = 2048)\n",
      "  File \"h5py/_objects.pyx\", line 55, in h5py._objects.with_phil.wrapper\n",
      "  File \"h5py/h5f.pyx\", line 106, in h5py.h5f.open\n",
      "OSError: Unable to open file (truncated file: eof = 96, sblock->base_addr = 0, stored_eof = 2048)\n",
      "  0%|          | 0/19194 [00:00<?, ?it/s]OSError: Unable to open file (truncated file: eof = 96, sblock->base_addr = 0, stored_eof = 2048)\n",
      "Exception in thread Thread-148:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/kiki/miniconda3/envs/densephrases/lib/python3.9/threading.py\", line 950, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/kiki/miniconda3/envs/densephrases/lib/python3.9/threading.py\", line 888, in run\n",
      "    self.run()\n",
      "  File \"/home/kiki/miniconda3/envs/densephrases/lib/python3.9/threading.py\", line 888, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/kiki/dados_servidor/Servidor/InformationRetrival/densephrases/densephrases/utils/embed_utils.py\", line 198, in write\n",
      "Exception in thread Thread-149:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/kiki/miniconda3/envs/densephrases/lib/python3.9/threading.py\", line 950, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/kiki/miniconda3/envs/densephrases/lib/python3.9/threading.py\", line 888, in run\n",
      "    with h5py.File(output_dump_file, 'a') as f:\n",
      "  File \"/home/kiki/miniconda3/envs/densephrases/lib/python3.9/site-packages/h5py/_hl/files.py\", line 567, in __init__\n",
      "    with h5py.File(output_dump_file, 'a') as f:\n",
      "  File \"/home/kiki/miniconda3/envs/densephrases/lib/python3.9/site-packages/h5py/_hl/files.py\", line 567, in __init__\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/kiki/dados_servidor/Servidor/InformationRetrival/densephrases/densephrases/utils/embed_utils.py\", line 198, in write\n",
      "    fid = make_fid(name, mode, userblock_size, fapl, fcpl, swmr=swmr)\n",
      "  File \"/home/kiki/miniconda3/envs/densephrases/lib/python3.9/site-packages/h5py/_hl/files.py\", line 243, in make_fid\n",
      "    with h5py.File(output_dump_file, 'a') as f:\n",
      "  File \"/home/kiki/miniconda3/envs/densephrases/lib/python3.9/site-packages/h5py/_hl/files.py\", line 567, in __init__\n",
      "    with h5py.File(output_dump_file, 'a') as f:\n",
      "  File \"/home/kiki/miniconda3/envs/densephrases/lib/python3.9/site-packages/h5py/_hl/files.py\", line 567, in __init__\n",
      "    fid = h5f.open(name, h5f.ACC_RDWR, fapl=fapl)\n",
      "  File \"h5py/_objects.pyx\", line 54, in h5py._objects.with_phil.wrapper\n",
      "    fid = make_fid(name, mode, userblock_size, fapl, fcpl, swmr=swmr)\n",
      "  File \"/home/kiki/miniconda3/envs/densephrases/lib/python3.9/site-packages/h5py/_hl/files.py\", line 243, in make_fid\n",
      "  File \"h5py/_objects.pyx\", line 55, in h5py._objects.with_phil.wrapper\n",
      "    fid = make_fid(name, mode, userblock_size, fapl, fcpl, swmr=swmr)\n",
      "  File \"/home/kiki/miniconda3/envs/densephrases/lib/python3.9/site-packages/h5py/_hl/files.py\", line 243, in make_fid\n",
      "  File \"h5py/h5f.pyx\", line 106, in h5py.h5f.open\n",
      "OSError: Unable to open file (truncated file: eof = 96, sblock->base_addr = 0, stored_eof = 2048)\n",
      "    fid = h5f.open(name, h5f.ACC_RDWR, fapl=fapl)\n",
      "  File \"h5py/_objects.pyx\", line 54, in h5py._objects.with_phil.wrapper\n",
      "  File \"h5py/_objects.pyx\", line 55, in h5py._objects.with_phil.wrapper\n",
      "  File \"h5py/h5f.pyx\", line 106, in h5py.h5f.open\n",
      "    fid = h5f.open(name, h5f.ACC_RDWR, fapl=fapl)\n",
      "  File \"h5py/_objects.pyx\", line 54, in h5py._objects.with_phil.wrapper\n",
      "Exception in thread Thread-150:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/kiki/miniconda3/envs/densephrases/lib/python3.9/threading.py\", line 950, in _bootstrap_inner\n",
      "  File \"h5py/_objects.pyx\", line 55, in h5py._objects.with_phil.wrapper\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/kiki/dados_servidor/Servidor/InformationRetrival/densephrases/densephrases/utils/embed_utils.py\", line 198, in write\n",
      "    self.run()\n",
      "  File \"/home/kiki/miniconda3/envs/densephrases/lib/python3.9/threading.py\", line 888, in run\n",
      "OSError: Unable to open file (truncated file: eof = 96, sblock->base_addr = 0, stored_eof = 2048)\n",
      "    with h5py.File(output_dump_file, 'a') as f:\n",
      "  File \"/home/kiki/miniconda3/envs/densephrases/lib/python3.9/site-packages/h5py/_hl/files.py\", line 567, in __init__\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/kiki/dados_servidor/Servidor/InformationRetrival/densephrases/densephrases/utils/embed_utils.py\", line 198, in write\n",
      "        with h5py.File(output_dump_file, 'a') as f:\n",
      "  File \"/home/kiki/miniconda3/envs/densephrases/lib/python3.9/site-packages/h5py/_hl/files.py\", line 567, in __init__\n",
      "    fid = make_fid(name, mode, userblock_size, fapl, fcpl, swmr=swmr)\n",
      "  File \"/home/kiki/miniconda3/envs/densephrases/lib/python3.9/site-packages/h5py/_hl/files.py\", line 243, in make_fid\n",
      "fid = make_fid(name, mode, userblock_size, fapl, fcpl, swmr=swmr)\n",
      "  File \"/home/kiki/miniconda3/envs/densephrases/lib/python3.9/site-packages/h5py/_hl/files.py\", line 243, in make_fid\n",
      "  File \"h5py/h5f.pyx\", line 106, in h5py.h5f.open\n",
      "OSError: Unable to open file (truncated file: eof = 96, sblock->base_addr = 0, stored_eof = 2048)\n",
      "    fid = make_fid(name, mode, userblock_size, fapl, fcpl, swmr=swmr)\n",
      "  File \"/home/kiki/miniconda3/envs/densephrases/lib/python3.9/site-packages/h5py/_hl/files.py\", line 243, in make_fid\n",
      "    fid = h5f.open(name, h5f.ACC_RDWR, fapl=fapl)\n",
      "  File \"h5py/_objects.pyx\", line 54, in h5py._objects.with_phil.wrapper\n",
      "    fid = h5f.open(name, h5f.ACC_RDWR, fapl=fapl)\n",
      "  File \"h5py/_objects.pyx\", line 54, in h5py._objects.with_phil.wrapper\n",
      "    fid = h5f.open(name, h5f.ACC_RDWR, fapl=fapl)\n",
      "  File \"h5py/_objects.pyx\", line 54, in h5py._objects.with_phil.wrapper\n",
      "  File \"h5py/_objects.pyx\", line 55, in h5py._objects.with_phil.wrapper\n",
      "  File \"h5py/h5f.pyx\", line 106, in h5py.h5f.open\n",
      "  File \"h5py/_objects.pyx\", line 55, in h5py._objects.with_phil.wrapper\n",
      "Exception in thread Thread-151:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/kiki/miniconda3/envs/densephrases/lib/python3.9/threading.py\", line 950, in _bootstrap_inner\n",
      "OSError: Unable to open file (truncated file: eof = 96, sblock->base_addr = 0, stored_eof = 2048)\n",
      "Exception in thread Thread-152:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/kiki/miniconda3/envs/densephrases/lib/python3.9/threading.py\", line 950, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/kiki/miniconda3/envs/densephrases/lib/python3.9/threading.py\", line 888, in run\n",
      "    self.run()\n",
      "  File \"/home/kiki/miniconda3/envs/densephrases/lib/python3.9/threading.py\", line 888, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/kiki/dados_servidor/Servidor/InformationRetrival/densephrases/densephrases/utils/embed_utils.py\", line 198, in write\n",
      "Exception in thread Thread-153:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/kiki/miniconda3/envs/densephrases/lib/python3.9/threading.py\", line 950, in _bootstrap_inner\n",
      "  File \"h5py/h5f.pyx\", line 106, in h5py.h5f.open\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/kiki/dados_servidor/Servidor/InformationRetrival/densephrases/densephrases/utils/embed_utils.py\", line 198, in write\n",
      "    self.run()\n",
      "  File \"/home/kiki/miniconda3/envs/densephrases/lib/python3.9/threading.py\", line 888, in run\n",
      "    with h5py.File(output_dump_file, 'a') as f:\n",
      "  File \"/home/kiki/miniconda3/envs/densephrases/lib/python3.9/site-packages/h5py/_hl/files.py\", line 567, in __init__\n",
      "  File \"h5py/_objects.pyx\", line 55, in h5py._objects.with_phil.wrapper\n",
      "Exception in thread Thread-154:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/kiki/miniconda3/envs/densephrases/lib/python3.9/threading.py\", line 950, in _bootstrap_inner\n",
      "    with h5py.File(output_dump_file, 'a') as f:\n",
      "  File \"/home/kiki/miniconda3/envs/densephrases/lib/python3.9/site-packages/h5py/_hl/files.py\", line 567, in __init__\n",
      "    fid = make_fid(name, mode, userblock_size, fapl, fcpl, swmr=swmr)\n",
      "  File \"/home/kiki/miniconda3/envs/densephrases/lib/python3.9/site-packages/h5py/_hl/files.py\", line 243, in make_fid\n",
      "    self.run()\n",
      "  File \"/home/kiki/miniconda3/envs/densephrases/lib/python3.9/threading.py\", line 888, in run\n",
      "    fid = h5f.open(name, h5f.ACC_RDWR, fapl=fapl)\n",
      "  File \"h5py/_objects.pyx\", line 54, in h5py._objects.with_phil.wrapper\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/kiki/dados_servidor/Servidor/InformationRetrival/densephrases/densephrases/utils/embed_utils.py\", line 198, in write\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/kiki/dados_servidor/Servidor/InformationRetrival/densephrases/densephrases/utils/embed_utils.py\", line 198, in write\n",
      "  0%|          | 0/19194 [00:00<?, ?it/s]  File \"h5py/_objects.pyx\", line 55, in h5py._objects.with_phil.wrapper\n",
      "  File \"h5py/h5f.pyx\", line 106, in h5py.h5f.open\n",
      "    with h5py.File(output_dump_file, 'a') as f:\n",
      "  File \"/home/kiki/miniconda3/envs/densephrases/lib/python3.9/site-packages/h5py/_hl/files.py\", line 567, in __init__\n",
      "    fid = make_fid(name, mode, userblock_size, fapl, fcpl, swmr=swmr)\n",
      "  File \"/home/kiki/miniconda3/envs/densephrases/lib/python3.9/site-packages/h5py/_hl/files.py\", line 243, in make_fid\n",
      "  File \"h5py/h5f.pyx\", line 106, in h5py.h5f.open\n",
      "    fid = h5f.open(name, h5f.ACC_RDWR, fapl=fapl)\n",
      "  File \"h5py/_objects.pyx\", line 54, in h5py._objects.with_phil.wrapper\n",
      "\n",
      "Exception in thread Thread-155:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/kiki/miniconda3/envs/densephrases/lib/python3.9/threading.py\", line 950, in _bootstrap_inner\n",
      "OSError: Unable to open file (truncated file: eof = 96, sblock->base_addr = 0, stored_eof = 2048)\n",
      "    fid = make_fid(name, mode, userblock_size, fapl, fcpl, swmr=swmr)\n",
      "  File \"/home/kiki/miniconda3/envs/densephrases/lib/python3.9/site-packages/h5py/_hl/files.py\", line 243, in make_fid\n",
      "    with h5py.File(output_dump_file, 'a') as f:\n",
      "  File \"/home/kiki/miniconda3/envs/densephrases/lib/python3.9/site-packages/h5py/_hl/files.py\", line 567, in __init__\n",
      "    self.run()\n",
      "  File \"/home/kiki/miniconda3/envs/densephrases/lib/python3.9/threading.py\", line 888, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/kiki/dados_servidor/Servidor/InformationRetrival/densephrases/densephrases/utils/embed_utils.py\", line 198, in write\n",
      "    fid = h5f.open(name, h5f.ACC_RDWR, fapl=fapl)\n",
      "  File \"h5py/_objects.pyx\", line 54, in h5py._objects.with_phil.wrapper\n",
      "    fid = make_fid(name, mode, userblock_size, fapl, fcpl, swmr=swmr)\n",
      "  File \"/home/kiki/miniconda3/envs/densephrases/lib/python3.9/site-packages/h5py/_hl/files.py\", line 243, in make_fid\n",
      "  File \"h5py/_objects.pyx\", line 55, in h5py._objects.with_phil.wrapper\n",
      "    with h5py.File(output_dump_file, 'a') as f:\n",
      "  File \"/home/kiki/miniconda3/envs/densephrases/lib/python3.9/site-packages/h5py/_hl/files.py\", line 567, in __init__\n",
      "  File \"h5py/_objects.pyx\", line 55, in h5py._objects.with_phil.wrapper\n",
      "    fid = h5f.open(name, h5f.ACC_RDWR, fapl=fapl)\n",
      "  File \"h5py/_objects.pyx\", line 54, in h5py._objects.with_phil.wrapper\n",
      "  File \"h5py/h5f.pyx\", line 106, in h5py.h5f.open\n",
      "OSError: Unable to open file (truncated file: eof = 96, sblock->base_addr = 0, stored_eof = 2048)\n",
      "OSError: Unable to open file (truncated file: eof = 96, sblock->base_addr = 0, stored_eof = 2048)\n",
      "OSError: Unable to open file (truncated file: eof = 96, sblock->base_addr = 0, stored_eof = 2048)\n",
      "  File \"h5py/_objects.pyx\", line 55, in h5py._objects.with_phil.wrapper\n",
      "  File \"h5py/h5f.pyx\", line 106, in h5py.h5f.open\n",
      "    fid = make_fid(name, mode, userblock_size, fapl, fcpl, swmr=swmr)\n",
      "  File \"/home/kiki/miniconda3/envs/densephrases/lib/python3.9/site-packages/h5py/_hl/files.py\", line 243, in make_fid\n",
      "    fid = h5f.open(name, h5f.ACC_RDWR, fapl=fapl)\n",
      "  File \"h5py/_objects.pyx\", line 54, in h5py._objects.with_phil.wrapper\n",
      "  File \"h5py/_objects.pyx\", line 55, in h5py._objects.with_phil.wrapper\n",
      "OSError: Unable to open file (truncated file: eof = 96, sblock->base_addr = 0, stored_eof = 2048)\n",
      "  File \"h5py/h5f.pyx\", line 106, in h5py.h5f.open\n",
      "OSError: Unable to open file (truncated file: eof = 96, sblock->base_addr = 0, stored_eof = 2048)\n",
      "  File \"h5py/h5f.pyx\", line 106, in h5py.h5f.open\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'QuestionAnsweringTrainer' object has no attribute '_past'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m args\u001b[39m.\u001b[39mn_gpu\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m\n\u001b[1;32m     16\u001b[0m args\u001b[39m.\u001b[39mpast_index\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m\n\u001b[0;32m---> 17\u001b[0m dump_phrases(args, model, tokenizer, filter_only\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n",
      "Cell \u001b[0;32mIn[15], line 138\u001b[0m, in \u001b[0;36mdump_phrases\u001b[0;34m(args, model, tokenizer, filter_only)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[39m# Use trainer for predict\u001b[39;00m\n\u001b[1;32m    132\u001b[0m trainer \u001b[39m=\u001b[39m QuestionAnsweringTrainer(\n\u001b[1;32m    133\u001b[0m     model\u001b[39m=\u001b[39mmodel,\n\u001b[1;32m    134\u001b[0m     args\u001b[39m=\u001b[39margs,\n\u001b[1;32m    135\u001b[0m     tokenizer\u001b[39m=\u001b[39mtokenizer,\n\u001b[1;32m    136\u001b[0m     data_collator\u001b[39m=\u001b[39mdata_collator,\n\u001b[1;32m    137\u001b[0m )\n\u001b[0;32m--> 138\u001b[0m trainer\u001b[39m.\u001b[39;49mgenerate_phrase_vecs(dataset, examples, output_dump_file, offset, args)\n\u001b[1;32m    140\u001b[0m evalTime \u001b[39m=\u001b[39m timeit\u001b[39m.\u001b[39mdefault_timer() \u001b[39m-\u001b[39m start_time\n\u001b[1;32m    141\u001b[0m logger\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39mEvaluation done in total \u001b[39m\u001b[39m%f\u001b[39;00m\u001b[39m secs (\u001b[39m\u001b[39m%f\u001b[39;00m\u001b[39m sec per example)\u001b[39m\u001b[39m\"\u001b[39m, evalTime, evalTime \u001b[39m/\u001b[39m \u001b[39mlen\u001b[39m(dataset))\n",
      "File \u001b[0;32m~/dados_servidor/Servidor/InformationRetrival/densephrases/densephrases/utils/trainer_qa.py:227\u001b[0m, in \u001b[0;36mQuestionAnsweringTrainer.generate_phrase_vecs\u001b[0;34m(self, dataset, examples, output_dump_file, offset, args, ignore_keys)\u001b[0m\n\u001b[1;32m    219\u001b[0m eval_loop \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39myield_phrases_loop\n\u001b[1;32m    220\u001b[0m output \u001b[39m=\u001b[39m eval_loop(\n\u001b[1;32m    221\u001b[0m     predict_dataloader,\n\u001b[1;32m    222\u001b[0m     examples,\n\u001b[1;32m    223\u001b[0m     description\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mPrediction\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    224\u001b[0m     ignore_keys\u001b[39m=\u001b[39mignore_keys,\n\u001b[1;32m    225\u001b[0m )\n\u001b[0;32m--> 227\u001b[0m write_phrases(examples, dataset, output, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtokenizer, output_dump_file, offset, args)\n",
      "File \u001b[0;32m~/dados_servidor/Servidor/InformationRetrival/densephrases/densephrases/utils/embed_utils.py:245\u001b[0m, in \u001b[0;36mwrite_phrases\u001b[0;34m(all_examples, all_features, all_results, tokenizer, output_dump_file, offset, args)\u001b[0m\n\u001b[1;32m    242\u001b[0m     out_p\u001b[39m.\u001b[39mstart()\n\u001b[1;32m    244\u001b[0m start_time \u001b[39m=\u001b[39m time()\n\u001b[0;32m--> 245\u001b[0m \u001b[39mfor\u001b[39;00m count, result \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(tqdm(all_results, total\u001b[39m=\u001b[39m\u001b[39mlen\u001b[39m(all_features))):\n\u001b[1;32m    246\u001b[0m     example \u001b[39m=\u001b[39m fid2example[result[\u001b[39m'\u001b[39m\u001b[39mfeature_id\u001b[39m\u001b[39m'\u001b[39m]]\n\u001b[1;32m    247\u001b[0m     feature \u001b[39m=\u001b[39m fid2feature[result[\u001b[39m'\u001b[39m\u001b[39mfeature_id\u001b[39m\u001b[39m'\u001b[39m]]\n",
      "File \u001b[0;32m~/miniconda3/envs/densephrases/lib/python3.9/site-packages/tqdm/std.py:1178\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1175\u001b[0m time \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_time\n\u001b[1;32m   1177\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1178\u001b[0m     \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m iterable:\n\u001b[1;32m   1179\u001b[0m         \u001b[39myield\u001b[39;00m obj\n\u001b[1;32m   1180\u001b[0m         \u001b[39m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1181\u001b[0m         \u001b[39m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/dados_servidor/Servidor/InformationRetrival/densephrases/densephrases/utils/trainer_qa.py:204\u001b[0m, in \u001b[0;36mQuestionAnsweringTrainer.yield_phrases_loop\u001b[0;34m(self, dataloader, examples, description, prediction_loss_only, ignore_keys)\u001b[0m\n\u001b[1;32m    201\u001b[0m         batch_size \u001b[39m=\u001b[39m observed_batch_size\n\u001b[1;32m    203\u001b[0m \u001b[39m# Prediction step (loss, logits, labels) => get only (start, end, filter_start_logits, filter_end_logits)\u001b[39;00m\n\u001b[0;32m--> 204\u001b[0m mini_batch \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprediction_step(model, inputs, prediction_loss_only, ignore_keys\u001b[39m=\u001b[39;49mignore_keys)[\u001b[39m1\u001b[39m]\n\u001b[1;32m    205\u001b[0m \u001b[39mfor\u001b[39;00m item_idx, (start, end, filter_start, filter_end) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mmini_batch)):\n\u001b[1;32m    206\u001b[0m     \u001b[39myield\u001b[39;00m {\n\u001b[1;32m    207\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mexample_id\u001b[39m\u001b[39m'\u001b[39m: inputs[\u001b[39m'\u001b[39m\u001b[39mexample_id\u001b[39m\u001b[39m'\u001b[39m][item_idx]\u001b[39m.\u001b[39mitem(),\n\u001b[1;32m    208\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mfeature_id\u001b[39m\u001b[39m'\u001b[39m: feature_id, \u001b[39m# dataloader should not be shuffled\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    212\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mfilter_end\u001b[39m\u001b[39m'\u001b[39m: to_numpy(filter_end),\n\u001b[1;32m    213\u001b[0m     }\n",
      "File \u001b[0;32m~/miniconda3/envs/densephrases/lib/python3.9/site-packages/transformers/trainer.py:2490\u001b[0m, in \u001b[0;36mTrainer.prediction_step\u001b[0;34m(self, model, inputs, prediction_loss_only, ignore_keys)\u001b[0m\n\u001b[1;32m   2466\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   2467\u001b[0m \u001b[39mPerform an evaluation step on :obj:`model` using obj:`inputs`.\u001b[39;00m\n\u001b[1;32m   2468\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2487\u001b[0m \u001b[39m    logits and labels (each being optional).\u001b[39;00m\n\u001b[1;32m   2488\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   2489\u001b[0m has_labels \u001b[39m=\u001b[39m \u001b[39mall\u001b[39m(inputs\u001b[39m.\u001b[39mget(k) \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlabel_names)\n\u001b[0;32m-> 2490\u001b[0m inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_prepare_inputs(inputs)\n\u001b[1;32m   2491\u001b[0m \u001b[39mif\u001b[39;00m ignore_keys \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   2492\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel, \u001b[39m\"\u001b[39m\u001b[39mconfig\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[0;32m~/miniconda3/envs/densephrases/lib/python3.9/site-packages/transformers/trainer.py:1837\u001b[0m, in \u001b[0;36mTrainer._prepare_inputs\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   1832\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1833\u001b[0m \u001b[39mPrepare :obj:`inputs` before feeding them to the model, converting them to tensors if they are not already and\u001b[39;00m\n\u001b[1;32m   1834\u001b[0m \u001b[39mhandling potential state.\u001b[39;00m\n\u001b[1;32m   1835\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1836\u001b[0m inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prepare_input(inputs)\n\u001b[0;32m-> 1837\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mpast_index \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_past \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1838\u001b[0m     inputs[\u001b[39m\"\u001b[39m\u001b[39mmems\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_past\n\u001b[1;32m   1840\u001b[0m \u001b[39mreturn\u001b[39;00m inputs\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'QuestionAnsweringTrainer' object has no attribute '_past'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OSError: Unable to open file (truncated file: eof = 96, sblock->base_addr = 0, stored_eof = 2048)\n"
     ]
    }
   ],
   "source": [
    "# Create phrase vectors\n",
    "args.draft = False\n",
    "args.pad_to_max_length=True\n",
    "args.append_title=False\n",
    "args.convert_squad_to_hf=True\n",
    "args.remove_unused_columns=True\n",
    "args.use_legacy_prediction_loop=False\n",
    "args.world_size=1\n",
    "args.eval_batch_size=8\n",
    "args.dataloader_drop_last=True\n",
    "args.dataloader_pin_memory=True\n",
    "args.output_dir='/home/kiki/dados_servidor/Servidor/InformationRetrival/densephrases/output/'\n",
    "args.test_file='/home/kiki/dados_servidor/Servidor/InformationRetrival/densephrases/wikidump/wiki-dev/0000'\n",
    "args.prediction_loss_only=False\n",
    "args.n_gpu=1\n",
    "args.past_index=1\n",
    "dump_phrases(args, model, tokenizer, filter_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "densephrases",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "490f05e18e9d9e2815c936015f51d731fe8a4f36aba15fb5a9bf8bf51d6fdb7c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
