{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install rank-bm25\n",
    "# !pip uninstall fastbm25 -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.1+cu117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/miu/miniconda3/envs/torch/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "torch.cuda.is_available()\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Iterator\n",
    "import time\n",
    "\n",
    "import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "from tokenizers import BertWordPieceTokenizer, Regex, normalizers\n",
    "from tqdm import tqdm\n",
    "from transformers import (\n",
    "    BertConfig,\n",
    "    BertForMaskedLM,\n",
    "    BertTokenizerFast,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "import math\n",
    "from functools import reduce\n",
    "from collections import namedtuple\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "# from reformer_pytorch import ReformerLM\n",
    "# from x_transformers import TransformerWrapper, Decoder,Encoder\n",
    "from torch.utils.data import DataLoader\n",
    "import math \n",
    "\n",
    "from accelerate import Accelerator\n",
    "import torch.nn.functional as F\n",
    "# from einops import rearrange, pack, unpack\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.1+cu117\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "torch.cuda.is_available()\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Iterator\n",
    "import time\n",
    "\n",
    "import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "from tokenizers import BertWordPieceTokenizer, Regex, normalizers\n",
    "from tqdm import tqdm\n",
    "from transformers import (\n",
    "    BertConfig,\n",
    "    BertForMaskedLM,\n",
    "    BertTokenizerFast,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "import math\n",
    "from functools import reduce\n",
    "from collections import namedtuple\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "# from reformer_pytorch import ReformerLM\n",
    "# from x_transformers import TransformerWrapper, Decoder,Encoder\n",
    "from torch.utils.data import DataLoader\n",
    "import math \n",
    "\n",
    "from accelerate import Accelerator\n",
    "import torch.nn.functional as F\n",
    "# from einops import rearrange, pack, unpack\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL_RETRIEVAL_MAX_SEQ_LEN:  420\n",
      "batch_size:  2040\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Resolving data files: 100%|██████████| 53/53 [00:01<00:00, 47.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded dataset!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feeding samples to tokenizer: 100%|██████████| 10000/10000 [00:00<00:00, 16726.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Tokenizer trained in 1.3 seconds.\n",
      "[MASK] 4\n"
     ]
    }
   ],
   "source": [
    "model_training='tokemonster'#'bert' , 'tokemonster\n",
    "# LIMIT_DATASET = 2016 * 4  # keep small for development, set to None for full dataset\n",
    "#(3090) Para 128 tokens(Bert) e 32000 vocab: 1000 = 6 seg, 10000 = 1min, 100000 = 10min, 1m = 100min, 10m = 16h, 100m = 160h ou 6.6 dias\n",
    "#(3090) Para 84 tokens(tokenmonster salva 35%) e 24000 vocab(Bert): 1000 = 4.3s, 10000 = 43s, 100000 = 7.1m, 1m = 71min, 10m = 11.8h, 100m = 118h ou 5 dias\n",
    "#(4090) Para 84 tokens(tokenmonster salva 35%) e 24000 vocab(Electra): 1000 = 3.5s, 10000 = 35s, 100000 = 5.9m, 1m = 59min, 10m = 9.8h, 100m = 98h ou 4 dias\n",
    "#(4090) Para 84 tokens(tokenmonster salva 35%) e 16000 vocab (Electra): 1000 = 2.1s, 10000 = 21s, 100000 = 3.53m, 1m = 35.3min, 10m = 5.8h, 100m = 58h ou 2.4 dias\n",
    "\n",
    "#com retnet. Confirmado que é O(n). Se lembrando que não existe mta vantagem além do custo crescer sequencialmente, já que o conteudo \n",
    "#(4090) Para 84 tokens(tokenmonster salva 35%) batch size 200 e 16000 vocab (Electra): 1000 = 2.1s, 10000 = 21s, 100000 = 3.53m, 1m = 35.3min, 10m = 5.8h, 100m = 58h ou 2.4 dias\n",
    "#(4090) Para 168 tokens(tokenmonster salva 35%) batch size 100 e 16000 vocab (Electra): 1000 = 3.7s, 10000 = 37.5s,\n",
    "#(4090) Para 1000 tokens(tokenmonster salva 35%) batch size 12 e 16000 vocab (Electra):            , 10000 = 4min32s,\n",
    "\n",
    "\n",
    "#HIPOTÉTICO(Se conseguisse representar um texto com menos tokens):\n",
    "#(4090) Para 24 tokens(tokenmonster salva 35%)(Electra): 1000 = , 10000 = , 100000 = 3.19m, 1m = 32min, 10m = 6.5h, 100m = 65h ou 2.7 dias\n",
    "#HIPOTÉTICO(Se diminuisse o vocabulario para 1024 tokens diferentes)\n",
    "#(4090) Para 84 tokens(tokenmonster salva 35%) e 1024 vocab: 1000 =1.6s , 10000 = 16.2s,\n",
    "\n",
    "#losses:\n",
    "#1m dataset, electra, tokenmonster: iteration:  7800 , total_loss:  26.277032788594564\n",
    "#10m dataset, 84 tokens(tokenmonster salva 35%) e 16000 vocab (Electra) iteration:  49980 , total_loss:  20.8733185450236. 339min total\n",
    "\n",
    "\n",
    "LIMIT_DATASET = 10_000\n",
    "RANDOM_SEED = 42\n",
    "NUM_TOKENIZER_TRAINING_ITEMS = 1_000_000  # I made this up, but it seems reasonable\n",
    "if model_training=='bert':\n",
    "    VOCAB_SIZE = 32_768  # from Cramming\n",
    "    DEVICE_BATCH_SIZE = 100 # aprox 128, adjust to get near 100% gpu memory use\n",
    "    MODEL_MAX_SEQ_LEN = 128  # from Cramming\n",
    "else:\n",
    "    VOCAB_SIZE = 16_000  # tokenmonster\n",
    "    # VOCAB_SIZE = 1_024  # tokenmonster\n",
    "    # DEVICE_BATCH_SIZE = 200 # Token monster aguenta um batch size de (200-248)!! Geralmente melhora a qualidade do treino\n",
    "    MODEL_MAX_SEQ_LEN = 84  # token_monster\n",
    "\n",
    "    #para retrieval\n",
    "    DEVICE_BATCH_SIZE=60\n",
    "    RETRIEVAL_BATCH_SIZE=5\n",
    "    MODEL_RETRIEVAL_MAX_SEQ_LEN=MODEL_MAX_SEQ_LEN\n",
    "\n",
    "    #abordagem 1: So concatenar a query com as passagens. Problema em transformers simples: é quadratico o aumento na seq. Retnet diz ser O(n) entao da.\n",
    "    MODEL_RETRIEVAL_MAX_SEQ_LEN*=RETRIEVAL_BATCH_SIZE\n",
    "    print('MODEL_RETRIEVAL_MAX_SEQ_LEN: ',MODEL_RETRIEVAL_MAX_SEQ_LEN)\n",
    "\n",
    "    # #abordagem 2: multiplas amostras da msm query com uma passagem diferente\n",
    "    # #precisa ser o dobro para comportar a query e a passagem\n",
    "    # MODEL_MAX_SEQ_LEN *=2\n",
    "\n",
    "\n",
    "MASK_ID=4\n",
    "PAD_ID=0\n",
    "gradient_accumulation_steps = 2048 // DEVICE_BATCH_SIZE  # roughly based on Cramming\n",
    "batch_size = DEVICE_BATCH_SIZE * gradient_accumulation_steps\n",
    "print('batch_size: ',batch_size)\n",
    "RUN_DIR = Path(\"data\") / f\"run_{time.strftime('%Y%m%d-%H%M%S')}\"\n",
    "CHECKPOINT_DIR = RUN_DIR / \"training_checkpoints\"\n",
    "MODEL_DIR = RUN_DIR / \"model\"\n",
    "TOKENIZER_PATH = RUN_DIR / \"tokenizer.json\"\n",
    "TRAINER_HISTORY_PATH = RUN_DIR / \"trainer_history.json\"\n",
    "\n",
    "RUN_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "dataset = datasets.load_dataset(\n",
    "    \"sradc/chunked-shuffled-wikipedia20220301en-bookcorpusopen\",\n",
    "    split=f\"train[:{LIMIT_DATASET}]\" if LIMIT_DATASET else \"train\",\n",
    "    revision=\"0e6fada2dd43136e4a3f637da41de2e596aee674\",\n",
    ")\n",
    "print('loaded dataset!!')\n",
    "\n",
    "from process_tokenizer import get_tokenizer\n",
    "tokenizer,tokenized_dataset,norm,vocab=get_tokenizer(dataset,NUM_TOKENIZER_TRAINING_ITEMS,VOCAB_SIZE,\n",
    "                                                     TOKENIZER_PATH,model_training,MODEL_MAX_SEQ_LEN+2,\n",
    "                                                     )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from rank_bm25 import BM25Okapi\n",
    "from fastbm25 import fastbm25\n",
    "\n",
    "\n",
    "\n",
    "# removing punctuations and lowering the case\n",
    "corpus = [doc.translate(str.maketrans('', '', string.punctuation)).replace('\\n',\"\").lower().split() for doc in dataset['text']]\n",
    "# bm25 = BM25Okapi(corpus)\n",
    "# bm25 = BM25(corpus)\n",
    "bm25 = fastbm25(corpus)\n",
    "\n",
    "# #retrieve search results\n",
    "# query = \"american immigration\"\n",
    "# tokenized_query = query.lower().split(\" \")\n",
    "# scores=bm25.get_scores(tokenized_query)\n",
    "# #get sorted indices\n",
    "# sorted_scores = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)\n",
    "# #get top 10 results\n",
    "# top_10 = [dataset['text'][i] for i in sorted_scores[:10]]\n",
    "# sorted_scores[:10],top_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11,051,520 total parameters.\n"
     ]
    }
   ],
   "source": [
    "# Creating a RetNet model\n",
    "import torch\n",
    "from torchscale.architecture.config import RetNetConfig,EncoderDecoderConfig,EncoderConfig\n",
    "from torchscale.architecture.retnet import RetNetDecoder\n",
    "from torchscale.architecture.encoder_decoder import EncoderDecoder,FID\n",
    "from torchscale.architecture.encoder import Encoder\n",
    "\n",
    "from torchscale.component.embedding import PositionalEmbedding, TextEmbedding\n",
    "\n",
    "\n",
    "decoder_embed_dim=int(768*0.25)\n",
    "decoder_ffn_embed_dim=int(1536*0.25)\n",
    "recurrent_chunk_size=int(512*0.25)\n",
    "encoder_embed_tokens=TextEmbedding(VOCAB_SIZE+8, decoder_embed_dim)\n",
    "# config = EncoderDecoderConfig(vocab_size=VOCAB_SIZE+8,\n",
    "#                     max_target_positions=MODEL_MAX_SEQ_LEN,\n",
    "#                     max_source_positions=MODEL_MAX_SEQ_LEN,\n",
    "#                     decoder_layers=11,\n",
    "#                     decoder_retention_heads=3,\n",
    "#                     decoder_embed_dim=decoder_embed_dim,\n",
    "#                     decoder_ffn_embed_dim=decoder_ffn_embed_dim,\n",
    "#                     encoder_layers=11,\n",
    "#                     encoder_retention_heads=3,\n",
    "#                     encoder_embed_dim=decoder_embed_dim,\n",
    "#                     encoder_ffn_embed_dim=decoder_ffn_embed_dim,\n",
    "#                     recurrent_chunk_size=recurrent_chunk_size,\n",
    "#                     share_all_embeddings=True)\n",
    "# generator = FID(config,encoder_embed_tokens)\n",
    "config=RetNetConfig(vocab_size=VOCAB_SIZE+8,\n",
    "                    max_target_positions=MODEL_MAX_SEQ_LEN,\n",
    "                    max_source_positions=MODEL_MAX_SEQ_LEN,\n",
    "                    decoder_layers=11,\n",
    "                    decoder_retention_heads=3,\n",
    "                    decoder_embed_dim=decoder_embed_dim,\n",
    "                    decoder_ffn_embed_dim=decoder_ffn_embed_dim,\n",
    "                    recurrent_chunk_size=recurrent_chunk_size,\n",
    "                    share_all_embeddings=True)\n",
    "generator = RetNetDecoder(config,encoder_embed_tokens)\n",
    "# to_logits = torch.nn.Linear(decoder_embed_dim, VOCAB_SIZE+8)\n",
    "# generator.to_logits = to_logits\n",
    "\n",
    "#print number of parameters\n",
    "total_params = sum(p.numel() for p in generator.parameters())\n",
    "print(f'{total_params:,} total parameters.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prev_output_tokens = torch.zeros((2, MODEL_MAX_SEQ_LEN), dtype=torch.long)\n",
    "# o=generator(\n",
    "#     prev_output_tokens=prev_output_tokens,\n",
    "#     token_embeddings=None,\n",
    "#     features_only=True,\n",
    "# )\n",
    "# # logits=generator.to_logits(o[0])\n",
    "# o[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62,351,425 total parameters.\n"
     ]
    }
   ],
   "source": [
    "decoder_embed_dim=int(768*0.75)\n",
    "decoder_ffn_embed_dim=int(1536*0.75)\n",
    "recurrent_chunk_size=int(512*0.75)\n",
    "\n",
    "decoder_embed_tokens=TextEmbedding(VOCAB_SIZE+8, decoder_embed_dim)\n",
    "\n",
    "# config = EncoderDecoderConfig(vocab_size=VOCAB_SIZE+8,\n",
    "#                     max_target_positions=MODEL_MAX_SEQ_LEN,\n",
    "#                     max_source_positions=MODEL_RETRIEVAL_MAX_SEQ_LEN,\n",
    "#                     decoder_layers=5,\n",
    "#                     decoder_retention_heads=3,\n",
    "#                     decoder_embed_dim=decoder_embed_dim,\n",
    "#                     decoder_ffn_embed_dim=decoder_ffn_embed_dim,\n",
    "#                     encoder_layers=5,\n",
    "#                     encoder_retention_heads=3,\n",
    "#                     encoder_embed_dim=decoder_embed_dim,\n",
    "#                     encoder_ffn_embed_dim=decoder_ffn_embed_dim,\n",
    "#                     recurrent_chunk_size=recurrent_chunk_size,\n",
    "#                     share_all_embeddings=True                 \n",
    "#                     )\n",
    "config=RetNetConfig(vocab_size=VOCAB_SIZE+8,\n",
    "                    max_target_positions=MODEL_RETRIEVAL_MAX_SEQ_LEN,\n",
    "                    max_source_positions=MODEL_RETRIEVAL_MAX_SEQ_LEN,\n",
    "                    decoder_layers=11,\n",
    "                    decoder_retention_heads=3,\n",
    "                    decoder_embed_dim=decoder_embed_dim,\n",
    "                    decoder_ffn_embed_dim=decoder_ffn_embed_dim,\n",
    "                    recurrent_chunk_size=recurrent_chunk_size,\n",
    "                    share_all_embeddings=True)\n",
    "discriminator = RetNetDecoder(config,decoder_embed_tokens)\n",
    "# discriminator = FID(config,encoder_embed_tokens=encoder_embed_tokens,decoder_embed_tokens=encoder_embed_tokens)\n",
    "to_binary = torch.nn.Linear(decoder_embed_dim,1)\n",
    "discriminator.to_binary = to_binary\n",
    "## old disc 66,343,105 total parameters.\n",
    "\n",
    "#print number of parameters\n",
    "total_params = sum(p.numel() for p in discriminator.parameters())\n",
    "print(f'{total_params:,} total parameters.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from base_electra_model import *\n",
    "class Electra(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        generator,\n",
    "        discriminator,\n",
    "        *,\n",
    "        num_tokens = None,\n",
    "        discr_dim = -1,\n",
    "        discr_layer = -1,\n",
    "        mask_prob = 0.15,\n",
    "        replace_prob = 0.85,\n",
    "        random_token_prob = 0.,\n",
    "        mask_token_id = 2,\n",
    "        pad_token_id = 0,\n",
    "        mask_ignore_token_ids = [],\n",
    "        disc_weight = 50.,\n",
    "        gen_weight = 1.,\n",
    "        temperature = 1.):\n",
    "        super().__init__()\n",
    "\n",
    "        self.generator = generator\n",
    "        self.discriminator = discriminator\n",
    "\n",
    "        # if discr_dim > 0:\n",
    "        #     self.discriminator = nn.Sequential(\n",
    "        #         HiddenLayerExtractor(discriminator, layer = discr_layer),\n",
    "        #         nn.Linear(discr_dim, 1)\n",
    "        #     )\n",
    "\n",
    "        # mlm related probabilities\n",
    "        self.mask_prob = mask_prob\n",
    "        self.replace_prob = replace_prob\n",
    "\n",
    "        self.num_tokens = num_tokens\n",
    "        self.random_token_prob = random_token_prob\n",
    "\n",
    "        # token ids\n",
    "        self.pad_token_id = pad_token_id\n",
    "        self.mask_token_id = mask_token_id\n",
    "        self.mask_ignore_token_ids = set([*mask_ignore_token_ids, pad_token_id])\n",
    "\n",
    "        # sampling temperature\n",
    "        self.temperature = temperature\n",
    "\n",
    "        # loss weights\n",
    "        self.disc_weight = disc_weight\n",
    "        self.gen_weight = gen_weight\n",
    "\n",
    "    def forward(self, input,retrieval_input, **kwargs):\n",
    "        b, t = input.shape\n",
    "\n",
    "        replace_prob = prob_mask_like(input, self.replace_prob)\n",
    "\n",
    "        # do not mask [pad] tokens, or any other tokens in the tokens designated to be excluded ([cls], [sep])\n",
    "        # also do not include these special tokens in the tokens chosen at random\n",
    "        no_mask = mask_with_tokens(input, self.mask_ignore_token_ids)\n",
    "        mask = get_mask_subset_with_prob(~no_mask, self.mask_prob)\n",
    "\n",
    "        # get mask indices\n",
    "        mask_indices = torch.nonzero(mask, as_tuple=True)\n",
    "\n",
    "        # mask input with mask tokens with probability of `replace_prob` (keep tokens the same with probability 1 - replace_prob)\n",
    "        masked_input = input.clone().detach()\n",
    "\n",
    "        # set inverse of mask to padding tokens for labels\n",
    "        gen_labels = input.masked_fill(~mask, self.pad_token_id)\n",
    "\n",
    "        # clone the mask, for potential modification if random tokens are involved\n",
    "        # not to be mistakened for the mask above, which is for all tokens, whether not replaced nor replaced with random tokens\n",
    "        masking_mask = mask.clone()\n",
    "\n",
    "        # if random token probability > 0 for mlm\n",
    "        if self.random_token_prob > 0:\n",
    "            assert self.num_tokens is not None, 'Number of tokens (num_tokens) must be passed to Electra for randomizing tokens during masked language modeling'\n",
    "\n",
    "            random_token_prob = prob_mask_like(input, self.random_token_prob)\n",
    "            random_tokens = torch.randint(0, self.num_tokens, input.shape, device=input.device)\n",
    "            random_no_mask = mask_with_tokens(random_tokens, self.mask_ignore_token_ids)\n",
    "            random_token_prob &= ~random_no_mask\n",
    "            masked_input = torch.where(random_token_prob, random_tokens, masked_input)\n",
    "\n",
    "            # remove random token prob mask from masking mask\n",
    "            masking_mask = masking_mask & ~random_token_prob\n",
    "\n",
    "        # [mask] input\n",
    "        masked_input = masked_input.masked_fill(masking_mask * replace_prob, self.mask_token_id)\n",
    "        # get generator output and get mlm loss\n",
    "        # logits,_ = self.generator(masked_input,return_all_hiddens=False)\n",
    "        # logits,_ = self.generator(masked_input,masked_input,features_only=False,return_all_hiddens=False)\n",
    "        logits,_ = self.generator(masked_input,features_only=False,return_all_hiddens=False)\n",
    "        # logits=self.generator.to_logits(embs)\n",
    "\n",
    "        mlm_loss = F.cross_entropy(\n",
    "            logits.transpose(1, 2),\n",
    "            gen_labels,\n",
    "            ignore_index = self.pad_token_id\n",
    "        )\n",
    "\n",
    "        # use mask from before to select logits that need sampling\n",
    "        sample_logits = logits[mask_indices]\n",
    "\n",
    "        # sample\n",
    "        sampled = gumbel_sample(sample_logits, temperature = self.temperature)\n",
    "\n",
    "        # scatter the sampled values back to the input\n",
    "        disc_input = input.clone()\n",
    "        disc_input[mask_indices] = sampled.detach()\n",
    "\n",
    "        # generate discriminator labels, with replaced as True and original as False\n",
    "        disc_labels = (input != disc_input).float().detach()\n",
    "\n",
    "        # get discriminator predictions of replaced / original\n",
    "        non_padded_indices = torch.nonzero(input != self.pad_token_id, as_tuple=True)\n",
    "        \n",
    "        #replace first 84 retrieval token with disc_input\n",
    "        retrieval_input[:,:disc_input.shape[1]]=disc_input\n",
    "        #extend disc_labels(padding) to match retrieval_input\n",
    "        disc_labels=torch.cat((disc_labels,torch.zeros(retrieval_input.shape[0],retrieval_input.shape[1]-disc_labels.shape[1],dtype=torch.float32,device=disc_labels.device)),dim=1)\n",
    "        #exted mask\n",
    "        disc_mask=torch.cat((mask,torch.zeros(retrieval_input.shape[0],retrieval_input.shape[1]-mask.shape[1],dtype=torch.bool,device=mask.device)),dim=1)\n",
    "        # disc_embs,_ = self.discriminator(retrieval_input,retrieval_input,features_only=True,return_all_hiddens=False)\n",
    "        disc_embs,_ = self.discriminator(retrieval_input,features_only=True,return_all_hiddens=False)\n",
    "        # print('disc_embs.shape: ',disc_embs.shape)\n",
    "        disc_logits=self.discriminator.to_binary(disc_embs)\n",
    "        disc_logits = disc_logits.reshape_as(disc_labels)\n",
    "\n",
    "        disc_loss = F.binary_cross_entropy_with_logits(\n",
    "            disc_logits[non_padded_indices],\n",
    "            disc_labels[non_padded_indices]\n",
    "        )\n",
    "\n",
    "        # gather metrics\n",
    "        with torch.no_grad():\n",
    "            gen_predictions = torch.argmax(logits, dim=-1)\n",
    "            disc_predictions = torch.round((torch.sign(disc_logits) + 1.0) * 0.5)\n",
    "            gen_acc = (gen_labels[mask] == gen_predictions[mask]).float().mean()\n",
    "            disc_acc = 0.5 * (disc_labels[disc_mask] == disc_predictions[disc_mask]).float().mean() + 0.5 * (disc_labels[~disc_mask] == disc_predictions[~disc_mask]).float().mean()\n",
    "\n",
    "        # return weighted sum of losses\n",
    "        return Results(self.gen_weight * mlm_loss + self.disc_weight * disc_loss, mlm_loss, disc_loss, gen_acc, disc_acc, disc_labels, disc_predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "MASK_ID=4\n",
    "PAD_ID=0\n",
    "model = Electra(\n",
    "    generator,\n",
    "    discriminator,\n",
    "    discr_dim = 768,           # the embedding dimension of the discriminator\n",
    "    # discr_layer = 'reformer',   # the layer name in the discriminator, whose output would be used for predicting token is still the same or replaced\n",
    "    discr_layer = 'attn_layers',   # the layer name in the discriminator, whose output would be used for predicting token is still the same or replaced\n",
    "    mask_token_id = MASK_ID,          # the token id reserved for masking\n",
    "    pad_token_id = PAD_ID,           # the token id for padding\n",
    "    mask_prob = 0.15,           # masking probability for masked language modeling\n",
    "    mask_ignore_token_ids = []  # ids of tokens to ignore for mask modeling ex. (cls, sep)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_ids are the indices corresponding to each token in the sentence.\n",
    "# attention_mask indicates whether a token should be attended to or not.\n",
    "# token_type_ids identifies which sequence a token belongs to when there is more than one sequence\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "        tokenized_dataset, shuffle=True, batch_size=DEVICE_BATCH_SIZE\n",
    "    )\n",
    "from utils import get_optimizer_scheduler\n",
    "optimizer,lr_scheduler,max_train_steps = get_optimizer_scheduler(model,train_dataloader,gradient_accumulation_steps,learning_rate=5e-5,weight_decay=0, num_warmup_steps=0, max_train_steps=None,lr_scheduler_type='linear',num_train_epochs=1)\n",
    "\n",
    "accelerator = Accelerator()\n",
    "# Prepare everything with our `accelerator`.\n",
    "model, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(\n",
    "    model, optimizer, train_dataloader, lr_scheduler\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#cada batch demora 1.7 segundos\n",
    "#Em um dataset de 10k, são 10k/25(batch_size)=400 batches, 1.7*400 = 680 segundos = 11.3 minutos\n",
    "#Em um dataset de 100k, são 100k/25(batch_size)=4000 batches, 1.7*4000 = 6800 segundos = 113 minutos = 1.9 horas\n",
    "#Em um dataset de 1m, são 1m/25(batch_size)=40000 batches, 1.7*40000 = 68000 segundos = 1133 minutos = 18.8 horas\n",
    "#Em um dataset de 10m, são 10m/25(batch_size)=400000 batches, 1.7*400000 = 680000 segundos = 11333 minutos = 188.8 horas = 7.8 dias\n",
    "\n",
    "#tem que utilizar dominios, para diminuir o tempo gasto pelo bm25 em larga escala\n",
    "\n",
    "def add_retrieval_results(ids):\n",
    "    # #retrieve search results\n",
    "    new_batch=[]\n",
    "    for i in ids:\n",
    "        query = dataset['text'][i]\n",
    "        tokenized_query = query.lower().split(\" \")\n",
    "        # sorted_scores = bm25.get_top_n_indexes(tokenized_query, corpus)\n",
    "        result = bm25.top_k_sentence(tokenized_query,k=RETRIEVAL_BATCH_SIZE)\n",
    "        \n",
    "        sorted_scores=[r[1] for r in result]\n",
    "\n",
    "        # scores=bm25.get_scores(tokenized_query)\n",
    "        # #get sorted indices\n",
    "        # sorted_scores = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)\n",
    "        #get top n results\n",
    "        top_n = [dataset['text'][i] for i in sorted_scores[:RETRIEVAL_BATCH_SIZE]]\n",
    "        \n",
    "        for s in top_n:\n",
    "            s=tokenized_dataset.norm.normalize_str(dataset[\"text\"][i])\n",
    "            query+='[SEP]'+s\n",
    "        tokens = tokenized_dataset.vocab.tokenize(query).tolist()\n",
    "        \n",
    "        \n",
    "        #trucate\n",
    "        tokens=tokens[:MODEL_RETRIEVAL_MAX_SEQ_LEN - 2]\n",
    "        l=len(tokens)\n",
    "        for j in range(l,MODEL_RETRIEVAL_MAX_SEQ_LEN - 2):\n",
    "            tokens.append(0)\n",
    "        \n",
    "        tokens=torch.Tensor(tokens)\n",
    "        tokens=torch.as_tensor(tokens,dtype=torch.long)\n",
    "        new_batch.append(tokens)\n",
    "    new_batch=torch.stack(new_batch)\n",
    "    a=2/0\n",
    "    return new_batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/miu/Projects/NLP/LM_Pretraining/EfficientPretrain/electra_retrieval_retnet.ipynb Cell 13\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bmiu/home/miu/Projects/NLP/LM_Pretraining/EfficientPretrain/electra_retrieval_retnet.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m model\u001b[39m.\u001b[39mtrain()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bmiu/home/miu/Projects/NLP/LM_Pretraining/EfficientPretrain/electra_retrieval_retnet.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39mfor\u001b[39;00m step, batch \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(train_dataloader):\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bmiu/home/miu/Projects/NLP/LM_Pretraining/EfficientPretrain/electra_retrieval_retnet.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m     \u001b[39m# with accelerator.accumulate(model):\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bmiu/home/miu/Projects/NLP/LM_Pretraining/EfficientPretrain/electra_retrieval_retnet.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m     retrieval_input_ids\u001b[39m=\u001b[39madd_retrieval_results(batch[\u001b[39m'\u001b[39;49m\u001b[39mindex\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bmiu/home/miu/Projects/NLP/LM_Pretraining/EfficientPretrain/electra_retrieval_retnet.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m     \u001b[39m#to gpu\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bmiu/home/miu/Projects/NLP/LM_Pretraining/EfficientPretrain/electra_retrieval_retnet.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m     retrieval_input_ids\u001b[39m=\u001b[39mretrieval_input_ids\u001b[39m.\u001b[39mto(accelerator\u001b[39m.\u001b[39mdevice)\n",
      "\u001b[1;32m/home/miu/Projects/NLP/LM_Pretraining/EfficientPretrain/electra_retrieval_retnet.ipynb Cell 13\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bmiu/home/miu/Projects/NLP/LM_Pretraining/EfficientPretrain/electra_retrieval_retnet.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=39'>40</a>\u001b[0m     new_batch\u001b[39m.\u001b[39mappend(tokens)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bmiu/home/miu/Projects/NLP/LM_Pretraining/EfficientPretrain/electra_retrieval_retnet.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=40'>41</a>\u001b[0m new_batch\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mstack(new_batch)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bmiu/home/miu/Projects/NLP/LM_Pretraining/EfficientPretrain/electra_retrieval_retnet.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=41'>42</a>\u001b[0m a\u001b[39m=\u001b[39m\u001b[39m2\u001b[39;49m\u001b[39m/\u001b[39;49m\u001b[39m0\u001b[39;49m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bmiu/home/miu/Projects/NLP/LM_Pretraining/EfficientPretrain/electra_retrieval_retnet.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=42'>43</a>\u001b[0m \u001b[39mreturn\u001b[39;00m new_batch\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "# Only show the progress bar once on each machine.\n",
    "# progress_bar = tqdm(range(max_train_steps), disable=not accelerator.is_local_main_process)\n",
    "completed_steps = 0\n",
    "starting_epoch = 0\n",
    "\n",
    "# update the progress_bar if load from checkpoint\n",
    "total_loss=0\n",
    "# progress_bar.update(completed_steps)\n",
    "steps_log=30\n",
    "count_amostra=0\n",
    "num_train_epochs=1\n",
    "for epoch in range(starting_epoch, num_train_epochs):\n",
    "    model.train()\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        # with accelerator.accumulate(model):\n",
    "        retrieval_input_ids=add_retrieval_results(batch['index'])\n",
    "        #to gpu\n",
    "        retrieval_input_ids=retrieval_input_ids.to(accelerator.device)\n",
    "        \n",
    "        results=model(batch['input_ids'],retrieval_input_ids) \n",
    "\n",
    "        count_amostra+=int(len(batch['input_ids']))\n",
    "        loss = results.loss\n",
    "        # print(loss)\n",
    "        total_loss += loss.detach().float().cpu().numpy().item()\n",
    "        accelerator.backward(loss)\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()       \n",
    "        \n",
    "        if step%steps_log==0:\n",
    "            print('iteration: ',step,', total_loss: ',total_loss/steps_log)\n",
    "            print('count_amostra:',count_amostra)\n",
    "            total_loss=0\n",
    "    # Checks if the accelerator has performed an optimization step behind the scenes\n",
    "    if accelerator.sync_gradients:\n",
    "        # progress_bar.update(1)\n",
    "        completed_steps += 1\n",
    "\n",
    "    if completed_steps >= max_train_steps:\n",
    "        break\n",
    "\n",
    "    model.eval()\n",
    "    losses = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch['index'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "em 2min:\n",
    "1860*5=9300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
