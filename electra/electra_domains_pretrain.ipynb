{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(3090) Para 128 tokens(Bert) e 32000 vocab: 1000 = 6 seg, 10000 = 1min, 100000 = 10min, 1m = 100min, 10m = 16h, 100m = 160h ou 6.6 dias\n",
    "#(3090) Para 84 tokens(tokenmonster salva 35%) e 24000 vocab(Bert): 1000 = 4.3s, 10000 = 43s, 100000 = 7.1m, 1m = 71min, 10m = 11.8h, 100m = 118h ou 5 dias\n",
    "#(4090) Para 84 tokens(tokenmonster salva 35%) e 24000 vocab(Electra): 1000 = 3.5s, 10000 = 35s, 100000 = 5.9m, 1m = 59min, 10m = 9.8h, 100m = 98h ou 4 dias\n",
    "#(4090) Para 84 tokens(tokenmonster salva 35%) e 16000 vocab (Electra): 1000 = 2.1s, 10000 = 21s, 100000 = 3.53m, 1m = 35.3min, 10m = 5.8h, 100m = 58h ou 2.4 dias\n",
    "\n",
    "#HIPOTÉTICO:\n",
    "#(4090) Para 24 tokens(tokenmonster salva 35%)e 24000 vocab(Electra): 1000 = , 10000 = , 100000 = 3.19m, 1m = 32min, 10m = 6.5h, 100m = 65h ou 2.7 dias\n",
    "\n",
    "#losses:\n",
    "#1m dataset, 24000 vocab electra, tokenmonster: iteration:  7800 , total_loss:  26.277032788594564\n",
    "#100k dataset, 16000 vocab electra, tokenmonster: iteration:  4980 , total_loss:  24.833836555480957\n",
    "#10m dataset, 16vocab electra , tokenmonster, 10 dom, iteration:  99990 , total_loss:  20.500934664408366\n",
    "\n",
    "\n",
    "#1m dataset, dom_electra_discriminator_16kvocab_1m  = 40min . Loss: 24.15\n",
    "#1m dataset, 3_3hierdom_electra_discriminator_16kvocab_1m  = 41min . Loss: 24.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.1+cu117\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "print(torch.__version__)\n",
    "\n",
    "import os\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "# !pip install datasets transformers tqdm magic_timer pandas tokenizers matplotlib pynvml\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path='dom9_lv1/'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: WANDB_START_METHOD=thread\n",
      "env: WANDB_PROJECT=pretraining_BERT_the_notebook\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU: NVIDIA GeForce RTX 3090, 24576.0 MiB\n",
      "torch.cuda.is_available() = True\n",
      "DEVICE_BATCH_SIZE = 100\n",
      "gradient_accumulation_steps = 20\n",
      "batch_size = 2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/home/kiki/.cache/huggingface/datasets/sradc___parquet/sradc--chunked-shuffled-wikipedia20220301en-bookcorpusopen-ff5cb88917a65ec5/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset in 2.1 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%env WANDB_START_METHOD=thread\n",
    "%env WANDB_PROJECT=pretraining_BERT_the_notebook\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Iterator\n",
    "import time\n",
    "\n",
    "import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pynvml\n",
    "import torch\n",
    "from magic_timer import MagicTimer\n",
    "from tokenizers import BertWordPieceTokenizer, Regex, normalizers\n",
    "from tqdm import tqdm\n",
    "from transformers import (\n",
    "    BertConfig,\n",
    "    BertForMaskedLM,\n",
    "    BertTokenizerFast,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "\n",
    "# Print hardware information\n",
    "pynvml.nvmlInit()\n",
    "handle = pynvml.nvmlDeviceGetHandleByIndex(0)\n",
    "gpu_name = pynvml.nvmlDeviceGetName(handle)\n",
    "gpu_mem = pynvml.nvmlDeviceGetMemoryInfo(handle).total / (1024**2)\n",
    "print(f\"GPU: {gpu_name}, {gpu_mem} MiB\")\n",
    "print(f\"{torch.cuda.is_available() = }\")\n",
    "model_training='modular'#'bert' , 'electra',; modular\n",
    "# LIMIT_DATASET = 2016 * 4  # keep small for development, set to None for full dataset\n",
    "\n",
    "\n",
    "LIMIT_DATASET = 10_000\n",
    "RANDOM_SEED = 42\n",
    "NUM_TOKENIZER_TRAINING_ITEMS = 1_000_000  # I made this up, but it seems reasonable\n",
    "if model_training=='bert':\n",
    "    VOCAB_SIZE = 32_768  # from Cramming\n",
    "    DEVICE_BATCH_SIZE = 100 # aprox 128, adjust to get near 100% gpu memory use\n",
    "    MODEL_MAX_SEQ_LEN = 128  # from Cramming\n",
    "else:\n",
    "    VOCAB_SIZE = 16_000  # tokenmonster\n",
    "    DEVICE_BATCH_SIZE = 100 # Token monster aguenta um batch size de (200-248)!! Geralmente melhora a qualidade do treino\n",
    "    MODEL_MAX_SEQ_LEN = 84  # token_monster\n",
    "\n",
    "MASK_ID=4\n",
    "PAD_ID=0\n",
    "\n",
    "gradient_accumulation_steps = 2048 // DEVICE_BATCH_SIZE  # roughly based on Cramming\n",
    "batch_size = DEVICE_BATCH_SIZE * gradient_accumulation_steps\n",
    "print(f\"{DEVICE_BATCH_SIZE = }\")\n",
    "print(f\"{gradient_accumulation_steps = }\")\n",
    "print(f\"{batch_size = }\")\n",
    "RUN_DIR = Path(\"data\") / f\"run_{time.strftime('%Y%m%d-%H%M%S')}\"\n",
    "CHECKPOINT_DIR = RUN_DIR / \"training_checkpoints\"\n",
    "MODEL_DIR = RUN_DIR / \"model\"\n",
    "TOKENIZER_PATH = RUN_DIR / \"tokenizer.json\"\n",
    "TRAINER_HISTORY_PATH = RUN_DIR / \"trainer_history.json\"\n",
    "\n",
    "RUN_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "with MagicTimer() as timer:\n",
    "    dataset = datasets.load_dataset(\n",
    "        \"sradc/chunked-shuffled-wikipedia20220301en-bookcorpusopen\",\n",
    "        split=f\"train[:{LIMIT_DATASET}]\" if LIMIT_DATASET else \"train\",\n",
    "        revision=\"0e6fada2dd43136e4a3f637da41de2e596aee674\",\n",
    "    )\n",
    "print(f\"Loaded dataset in {timer}\")\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clusterize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_DOMAINS=9\n",
    "HIER_LEVEL=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !git clone https://github.com/kernelmachine/balanced-kmeans/\n",
    "# !cd balanced-kmeans && pip install --editable .\n",
    "# !pip install numba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from kmeans_pytorch import KMeans as BalancedKMeans\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_texts():\n",
    "    return[dataset[i]['text'] for i in range(len(dataset))]\n",
    "\n",
    "def number_normalizer(tokens):\n",
    "    \"\"\"Map all numeric tokens to a placeholder.\n",
    "\n",
    "    For many applications, tokens that begin with a number are not directly\n",
    "    useful, but the fact that such a token exists can be relevant.  By applying\n",
    "    this form of dimensionality reduction, some methods may perform better.\n",
    "    \"\"\"\n",
    "    return (\"#NUMBER\" if token[0].isdigit() else token for token in tokens)\n",
    "\n",
    "\n",
    "class NumberNormalizingVectorizer(TfidfVectorizer):\n",
    "    # this vectorizer replaces numbers with #NUMBER token\n",
    "    def build_tokenizer(self):\n",
    "        tokenize = super().build_tokenizer()\n",
    "        return lambda doc: list(number_normalizer(tokenize(doc)))\n",
    "\n",
    "\n",
    "def train_vectorizer(texts):\n",
    "    # english stopwords plus the #NUMBER dummy token\n",
    "    stop_words = list(text.ENGLISH_STOP_WORDS.union([\"#NUMBER\"]))\n",
    "\n",
    "    model = Pipeline([('tfidf', NumberNormalizingVectorizer(stop_words=stop_words)),\n",
    "                      ('svd', TruncatedSVD(n_components=100)),\n",
    "                      ('normalizer', Normalizer(copy=False))])\n",
    "\n",
    "    model.fit(tqdm(texts[:1000000]))\n",
    "    vecs = model.transform(tqdm(texts))\n",
    "    return model, vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000000 [00:00<?, ?it/s]/home/miu/miniconda3/envs/lm/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['number'] not in stop_words.\n",
      "  warnings.warn(\n",
      "100%|██████████| 1000000/1000000 [00:42<00:00, 23534.89it/s]\n",
      "100%|██████████| 10000000/10000000 [07:31<00:00, 22151.43it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(10000000, 100)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts=get_texts()\n",
    "print(len(texts))\n",
    "tfidf_model, vecs=train_vectorizer(texts)\n",
    "vecs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, (20000, 100))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "        device = torch.device('cuda')\n",
    "else:\n",
    "        device = torch.device('cpu')\n",
    "\n",
    "if HIER_LEVEL==1:\n",
    "        kmeans = BalancedKMeans(n_clusters=int(math.sqrt(N_DOMAINS)), device=device, balanced=True)\n",
    "else:\n",
    "        kmeans = BalancedKMeans(n_clusters=N_DOMAINS, device=device, balanced=True)\n",
    "\n",
    "bs=500\n",
    "batches = np.array_split(vecs, bs, axis=0)\n",
    "len(batches),batches[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "500it [03:59,  2.09it/s]\n"
     ]
    }
   ],
   "source": [
    "for i, batch in tqdm(enumerate(batches)):\n",
    "        kmeans.fit(torch.from_numpy(batch), iter_limit=20, online=True, tqdm_flag=False,iter_k=i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "500it [00:26, 18.76it/s]\n"
     ]
    }
   ],
   "source": [
    "#LEVEL 0\n",
    "cluster_labels=[]\n",
    "for i, batch in tqdm(enumerate(batches)):\n",
    "    cluster_ids_y = kmeans.predict(\n",
    "        X=torch.from_numpy(batch).to(device),tqdm_flag=False\n",
    "    )\n",
    "    for c in cluster_ids_y:\n",
    "        cluster_labels.append(c.item())\n",
    "cluster_labels=np.asarray(cluster_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sub domain: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "500it [01:02,  8.00it/s]\n",
      "500it [00:07, 62.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sub domain: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "500it [01:27,  5.74it/s]\n",
      "500it [00:04, 109.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sub domain: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "500it [00:28, 17.56it/s]\n",
      "500it [00:07, 62.51it/s]\n"
     ]
    }
   ],
   "source": [
    "if HIER_LEVEL==1:\n",
    "    kmeans_level1=[]\n",
    "    cluster_labels_level1=[]\n",
    "    idxs_cluster_labels_level1=[]\n",
    "    for i in range(int(math.sqrt(N_DOMAINS))):\n",
    "        print('Sub domain:',i)\n",
    "        km=BalancedKMeans(n_clusters=int(math.sqrt(N_DOMAINS)), device=device, balanced=True)\n",
    "        idxs=np.where(cluster_labels==i)\n",
    "        batches_idxs = np.array_split(idxs[0], bs, axis=0)\n",
    "        \n",
    "        batches=[]\n",
    "        level0_idxs=[]\n",
    "        c=0\n",
    "        # for b in batches_idxs:\n",
    "        for k, b in tqdm(enumerate(batches_idxs)):\n",
    "            batch_data=[]\n",
    "            level0_batch_idxs=[]\n",
    "            for j in b:\n",
    "                batch_data.append(vecs[j])\n",
    "                level0_batch_idxs.append(j)\n",
    "                \n",
    "                \n",
    "            batch_data=np.asarray(batch_data)\n",
    "            batches.append(batch_data)\n",
    "            level0_idxs.append(level0_batch_idxs)\n",
    "            X=torch.from_numpy(batch_data).to(device)\n",
    "            km.fit(X, iter_limit=20, online=True, iter_k=c,tqdm_flag=False)\n",
    "            c+=1\n",
    "\n",
    "        subcluster_labels=[]\n",
    "        for k, batch in tqdm(enumerate(batches)):\n",
    "            cluster_ids_y = km.predict(\n",
    "                X=torch.from_numpy(batch).to(device),tqdm_flag=False\n",
    "            )\n",
    "            for z in cluster_ids_y:\n",
    "                subcluster_labels.append(z.item())\n",
    "            batch_idx=level0_idxs[k]\n",
    "            for o,j in enumerate(batch_idx):\n",
    "                idxs_cluster_labels_level1.append((i,j,subcluster_labels[o]))\n",
    "\n",
    "        cluster_labels_level1.append(subcluster_labels)\n",
    "\n",
    "        kmeans_level1.append(km)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2, 9999988, 1), (2, 9999990, 1), (2, 9999996, 1)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Domain index(level 0),text index(from txts and tfid vecs), subdomain index(level 1)\n",
    "idxs_cluster_labels_level1[-3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000000"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(idxs_cluster_labels_level1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0, 1), (0, 2, 0), (0, 4, 1), (0, 21, 2), (0, 22, 2)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idxs_cluster_labels_level1[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000000, 3827834, 2588738, 3583428)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cluster_labels),len(cluster_labels_level1[0]),len(cluster_labels_level1[1]),len(cluster_labels_level1[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(tfidf_model,open('models/'+model_path+'tfidf_model.p','wb'))\n",
    "pickle.dump(kmeans,open('models/'+model_path+'/kmeans.p','wb'))\n",
    "if HIER_LEVEL==1:\n",
    "    pickle.dump(kmeans_level1,open('models/'+model_path+'/kmeans_level1.p','wb'))\n",
    "    pickle.dump(idxs_cluster_labels_level1,open('models/'+model_path+'/idxs_cluster_labels_level1.p','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((array([0, 1, 2]), array([3827834, 2588738, 3583428])),\n",
       " (array([0, 1, 2]), array([1135508, 1492902, 1199424])))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(cluster_labels, return_counts=True),np.unique(np.asarray(cluster_labels_level1[0]), return_counts=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feeding samples to tokenizer: 100%|██████████| 1000000/1000000 [00:43<00:00, 23088.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Tokenizer trained in 56 seconds.\n"
     ]
    }
   ],
   "source": [
    "# !pip install tokenmonster\n",
    "from random import shuffle\n",
    "\n",
    "tokenizer = BertWordPieceTokenizer()\n",
    "tokenizer._tokenizer.normalizer = normalizers.Sequence(\n",
    "    [\n",
    "        normalizers.Replace(Regex(\"(``|'')\"), '\"'),\n",
    "        normalizers.NFD(),\n",
    "        normalizers.Lowercase(),\n",
    "        normalizers.StripAccents(),\n",
    "        normalizers.Replace(Regex(\" {2,}\"), \" \"),\n",
    "        normalizers.Replace(Regex(r\"[^\\x00-\\x7F]+\"), \"\"),\n",
    "    ]\n",
    ")  # Normalizer based on, https://github.com/JonasGeiping/cramming/blob/50bd06a65a4cd4a3dd6ee9ecce1809e1a9085374/cramming/data/tokenizer_preparation.py#L52\n",
    "def tokenizer_training_data() -> Iterator[str]:\n",
    "    for i in tqdm(\n",
    "        range(min(NUM_TOKENIZER_TRAINING_ITEMS, len(dataset))),\n",
    "        desc=\"Feeding samples to tokenizer\",\n",
    "    ):\n",
    "        yield dataset[i][\"text\"]\n",
    "\n",
    "\n",
    "with MagicTimer() as timer:\n",
    "    tokenizer.train_from_iterator(\n",
    "        tokenizer_training_data(),\n",
    "        vocab_size=VOCAB_SIZE,\n",
    "        min_frequency=2,\n",
    "    )\n",
    "print(f\"Tokenizer trained in {timer}.\")\n",
    "tokenizer.save(str(TOKENIZER_PATH))\n",
    "tokenizer = BertTokenizerFast(tokenizer_file=str(TOKENIZER_PATH))\n",
    "# tokenizer.unk_token,tokenizer.unk_token_id,tokenizer.sep_token,tokenizer.sep_token_id,tokenizer.pad_token,tokenizer.pad_token_id,tokenizer.cls_token,tokenizer.cls_token_id,tokenizer.mask_token,tokenizer.mask_token_id\n",
    "\n",
    "class HFTokenizedDataset(torch.utils.data.Dataset):\n",
    "    \"This wraps the dataset and tokenizes it, ready for the model\"\n",
    "\n",
    "    def __init__(self, dataset, tokenizer):\n",
    "        self.dataset = dataset\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        inps= self.tokenizer.encode(\n",
    "            self.dataset[i][\"text\"],\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            max_length=MODEL_MAX_SEQ_LEN - 2,\n",
    "            padding=\"max_length\",\n",
    "            return_special_tokens_mask=True,\n",
    "        )[0, ...]\n",
    "        return {'input_ids':inps}\n",
    "\n",
    "class TokenizedDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def convert_tokens_to_ids(self,s):\n",
    "        s=norm.normalize_str(s)\n",
    "        tokens = vocab.tokenize(s)\n",
    "        tokens=tokens[:MODEL_MAX_SEQ_LEN - 2]\n",
    "        tokens=torch.Tensor(tokens).long()\n",
    "        return tokens\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        s=norm.normalize_str(self.dataset[i][\"text\"])\n",
    "        tokens = vocab.tokenize(s).tolist()\n",
    "        \n",
    "        #trucate\n",
    "        tokens=tokens[:MODEL_MAX_SEQ_LEN - 2]\n",
    "        l=len(tokens)\n",
    "        for j in range(l,MODEL_MAX_SEQ_LEN - 2):\n",
    "            tokens.append(0)\n",
    "\n",
    "        tokens=torch.Tensor(tokens)\n",
    "        if MODEL_MAX_SEQ_LEN - 2>l:\n",
    "            att_mask=np.concatenate((np.ones(l),np.zeros(MODEL_MAX_SEQ_LEN - 2-l)))\n",
    "        else:\n",
    "            att_mask=np.ones(tokens.shape[0])\n",
    "        # print(l,att_mask.shape[0],MODEL_MAX_SEQ_LEN - 2)\n",
    "        assert tokens.shape[0]==att_mask.shape[0]\n",
    "\n",
    "        input_ids=tokens\n",
    "\n",
    "        input_ids=torch.as_tensor(input_ids,dtype=torch.long)\n",
    "        # labels = torch.zeros(input_ids.shape)\n",
    "        # probability_matrix = torch.full(labels.shape, 0.15)\n",
    "        # masked_indices = torch.bernoulli(probability_matrix).bool()\n",
    "        # indices_replaced = torch.bernoulli(torch.full(labels.shape, 1.0)).bool() & masked_indices\n",
    "        # labels[indices_replaced] =1\n",
    "        # mask=torch.as_tensor(labels,dtype=torch.bool)\n",
    "        \n",
    "        d={'input_ids':input_ids, \n",
    "            # 'token_type_ids':token_type_ids, \n",
    "            # 'attention_mask':attention_mask,\n",
    "            # 'mask':mask\n",
    "            }\n",
    "        return d\n",
    "\n",
    "def texts2mlm(texts,domain,subdomain1=0):\n",
    "    input_ids=[]\n",
    "    # token_type_ids=[]\n",
    "    # attention_mask=[]\n",
    "    # data=[]\n",
    "    inputs=[]\n",
    "    # masks=[]\n",
    "    for t in texts:\n",
    "        s=norm.normalize_str(t[\"text\"])\n",
    "        tokens = vocab.tokenize(s).tolist()\n",
    "        \n",
    "        #trucate\n",
    "        tokens=tokens[:MODEL_MAX_SEQ_LEN - 2]\n",
    "        l=len(tokens)\n",
    "        for j in range(l,MODEL_MAX_SEQ_LEN - 2):\n",
    "            tokens.append(0)\n",
    "\n",
    "        tokens=torch.Tensor(tokens)\n",
    "        if MODEL_MAX_SEQ_LEN - 2>l:\n",
    "            att_mask=np.concatenate((np.ones(l),np.zeros(MODEL_MAX_SEQ_LEN - 2-l)))\n",
    "        else:\n",
    "            att_mask=np.ones(tokens.shape[0])\n",
    "        # print(l,att_mask.shape[0],MODEL_MAX_SEQ_LEN - 2)\n",
    "        assert tokens.shape[0]==att_mask.shape[0]\n",
    "\n",
    "        input_ids=tokens\n",
    "\n",
    "        input_ids=torch.as_tensor(input_ids,dtype=torch.long)\n",
    "        # labels = torch.zeros(input_ids.shape)\n",
    "        # probability_matrix = torch.full(labels.shape, 0.15)\n",
    "        # masked_indices = torch.bernoulli(probability_matrix).bool()\n",
    "        # indices_replaced = torch.bernoulli(torch.full(labels.shape, 1.0)).bool() & masked_indices\n",
    "        # labels[indices_replaced] =1\n",
    "        # mask=torch.as_tensor(labels,dtype=torch.bool)\n",
    "        inputs.append(input_ids)\n",
    "        # masks.append(mask)\n",
    "    \n",
    "    return {'input_ids':torch.stack(inputs),'domain':domain,'subdomain1':(domain*N_DOMAINS)+subdomain1}\n",
    "\n",
    "def divide_chunks(l, n):\n",
    "    return [l[i:i + n] for i in range(0, len(l), n)] \n",
    "\n",
    "class CustomDataloader(torch.utils.data.Dataset):\n",
    "    def __init__(self,dataset,cluster_labels,n_domains) -> None:\n",
    "        self.dataset=dataset\n",
    "        self.cluster_labels=cluster_labels\n",
    "        self.bin_dataset={}\n",
    "        self.domains=[i for i in range(n_domains)]\n",
    "        self.batch_ordering=[]\n",
    "        self.current_domain=0\n",
    "        self.bs=DEVICE_BATCH_SIZE\n",
    "        self.fill_bins()\n",
    "\n",
    "    def fill_bins(self):\n",
    "        self.bin_dataset={}\n",
    "        for i,c in enumerate(self.cluster_labels):\n",
    "            if c not in self.bin_dataset:\n",
    "                self.bin_dataset[c]=[i]\n",
    "            else:\n",
    "                self.bin_dataset[c].append(i)\n",
    "        n_batches=0\n",
    "        domains=[]\n",
    "        for i in range(len(self.domains)):\n",
    "            self.bin_dataset[i]=divide_chunks(self.bin_dataset[i],DEVICE_BATCH_SIZE)\n",
    "            for k in range(len(self.bin_dataset[i])):\n",
    "                domains.append((i,k))\n",
    "        shuffle(domains)\n",
    "        self.batch_ordering=domains\n",
    "\n",
    "    def __getitem__(self, i) -> torch.Tensor:\n",
    "        indexes=self.bin_dataset[self.batch_ordering[i][0]][self.batch_ordering[i][1]]\n",
    "        batch_data=[]\n",
    "        for j in indexes:\n",
    "            batch_data.append(self.dataset[j])\n",
    "        # print('batch_data:',batch_data)\n",
    "        batch_data=texts2mlm(batch_data,self.batch_ordering[i][0])\n",
    "        # batch_data = torch.from_numpy(a).long()\n",
    "        return batch_data\n",
    "\n",
    "    def __len__(self):\n",
    "        #colocar uma margem de erro pra baixo\n",
    "        return len(self.batch_ordering)\n",
    "\n",
    "\n",
    "class CustomDataloaderLevel1(torch.utils.data.Dataset):\n",
    "    def __init__(self,dataset,cluster_labels,idxs_cluster_labels_level1,n_domains,n_subdomains) -> None:\n",
    "        self.dataset=dataset\n",
    "        self.cluster_labels=cluster_labels\n",
    "        self.idxs_cluster_labels_level1=idxs_cluster_labels_level1\n",
    "        self.bin_dataset={}\n",
    "        self.domains=[i for i in range(n_domains)]\n",
    "        self.subdomains=[i for i in range(n_subdomains)]\n",
    "        self.batch_ordering=[]\n",
    "        self.current_domain=0\n",
    "        self.bs=DEVICE_BATCH_SIZE\n",
    "        self.fill_bins()\n",
    "\n",
    "    def fill_bins(self):\n",
    "        self.bin_dataset={}\n",
    "        # for i,c in enumerate(self.cluster_labels):\n",
    "        #     if c not in self.bin_dataset:\n",
    "        #         self.bin_dataset[c]=[i]\n",
    "        #     else:\n",
    "        #         self.bin_dataset[c].append(i)\n",
    "        for dom,txt_id,subdom in self.idxs_cluster_labels_level1:\n",
    "            id_dom_sub=str(dom)+'_'+str(subdom)#identifica o dominio e o subdominio\n",
    "            if  id_dom_sub not in self.bin_dataset:\n",
    "                self.bin_dataset[id_dom_sub]=[txt_id]\n",
    "            else:\n",
    "                self.bin_dataset[id_dom_sub].append(txt_id)\n",
    "\n",
    "        n_batches=0\n",
    "        data_hier_idxs=[]\n",
    "        for i in range(len(self.domains)):\n",
    "            for j in range(len(self.subdomains)):\n",
    "                id_dom_sub=str(i)+'_'+str(j)#identifica o dominio e o subdominio\n",
    "                #OBS: subdivide a lista de todos os ids do msm dom e subdom em batches. O bin dataset e um dicionario\n",
    "                #que mapeia dom e subdom -> lista de batches\n",
    "                self.bin_dataset[id_dom_sub]=divide_chunks(self.bin_dataset[id_dom_sub],DEVICE_BATCH_SIZE)\n",
    "                for k in range(len(self.bin_dataset[id_dom_sub])):\n",
    "                    data_hier_idxs.append((id_dom_sub,k))\n",
    "        shuffle(data_hier_idxs)\n",
    "        #batch ordering é um vetor de tuplas de 2 elementos. O primeiro é o dominio e subdominio desse batch\n",
    "        # O segundo elemento é a posicao do batch no bin dataset. \n",
    "        # Ex: ('0_0',0) se lê: Batch com dominio e subdominio 0, esse batch é o numero 0 na lista de batches do bin_dataset para esse dom e subdom.\n",
    "        self.batch_ordering=data_hier_idxs\n",
    "\n",
    "    def __getitem__(self, i) -> torch.Tensor:\n",
    "        #o bin dataset tem como chaves o dominio e subdominio, e o valor é uma lista de batches de msm dom e subdom\n",
    "        indexes=self.bin_dataset[self.batch_ordering[i][0]][self.batch_ordering[i][1]]\n",
    "        batch_data=[]\n",
    "        for j in indexes:\n",
    "            batch_data.append(self.dataset[int(j)])\n",
    "        # print('batch_data:',batch_data)\n",
    "        if len(self.subdomains)>0:\n",
    "            d0,d1=self.batch_ordering[i][0].split('_')\n",
    "            batch_data=texts2mlm(batch_data,int(d0),int(d1))\n",
    "        else:\n",
    "            batch_data=texts2mlm(batch_data,self.batch_ordering[i][0])\n",
    "        # batch_data = torch.from_numpy(a).long()\n",
    "        return batch_data\n",
    "\n",
    "    def __len__(self):\n",
    "        #colocar uma margem de erro pra baixo\n",
    "        return len(self.batch_ordering)\n",
    "\n",
    "def get_vocab():\n",
    "    #### TokenMonster BRRR!!!\n",
    "    import tokenmonster\n",
    "    vocab = tokenmonster.load(\"englishcode-16000-balanced-v1\")\n",
    "    # vocab = TokenMonster.load('tokenizers_monster/english-24000-capcode.vocab')\n",
    "\n",
    "    norm=normalizers.Sequence(\n",
    "        [\n",
    "            normalizers.Replace(Regex(\"(``|'')\"), '\"'),\n",
    "            normalizers.NFD(),\n",
    "            normalizers.Lowercase(),\n",
    "            normalizers.StripAccents(),\n",
    "            normalizers.Replace(Regex(\" {2,}\"), \" \"),\n",
    "            normalizers.Replace(Regex(r\"[^\\x00-\\x7F]+\"), \"\"),\n",
    "        ]\n",
    "    )\n",
    "    vocab.modify(\"[EOS]\")\n",
    "    vocab.modify(\"[UNK]\")\n",
    "    vocab.modify(\"[SEP]\")\n",
    "    vocab.modify(\"[PAD]\")\n",
    "    vocab.modify(\"[CLS]\")\n",
    "    vocab.modify(\"[MASK]\")\n",
    "    return norm,vocab\n",
    "norm,vocab=get_vocab()\n",
    "MASK_ID=vocab.tokenize(\"[MASK]\")[0]\n",
    "PAD_ID=vocab.tokenize(\"[PAD]\")[0]\n",
    "\n",
    "if model_training=='bert':\n",
    "    tokenized_dataset = HFTokenizedDataset(dataset, tokenizer)\n",
    "elif model_training=='electra':\n",
    "    tokenized_dataset = TokenizedDataset(dataset)\n",
    "\n",
    "# tokenizer.mask_token,tokenizer.convert_tokens_to_ids(tokenizer.mask_token)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Electra"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from functools import reduce\n",
    "from collections import namedtuple\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# constants\n",
    "\n",
    "Results = namedtuple('Results', [\n",
    "    'loss',\n",
    "    'mlm_loss',\n",
    "    'disc_loss',\n",
    "    'gen_acc',\n",
    "    'disc_acc',\n",
    "    'disc_labels',\n",
    "    'disc_predictions'\n",
    "])\n",
    "\n",
    "# helpers\n",
    "def log(t, eps=1e-9):\n",
    "    return torch.log(t + eps)\n",
    "\n",
    "def gumbel_noise(t):\n",
    "    noise = torch.zeros_like(t).uniform_(0, 1)\n",
    "    return -log(-log(noise))\n",
    "\n",
    "def gumbel_sample(t, temperature = 1.):\n",
    "    return ((t / temperature) + gumbel_noise(t)).argmax(dim=-1)\n",
    "\n",
    "def prob_mask_like(t, prob):\n",
    "    return torch.zeros_like(t).float().uniform_(0, 1) < prob\n",
    "\n",
    "def mask_with_tokens(t, token_ids):\n",
    "    init_no_mask = torch.full_like(t, False, dtype=torch.bool)\n",
    "    mask = reduce(lambda acc, el: acc | (t == el), token_ids, init_no_mask)\n",
    "    return mask\n",
    "\n",
    "def get_mask_subset_with_prob(mask, prob):\n",
    "    batch, seq_len, device = *mask.shape, mask.device\n",
    "    max_masked = math.ceil(prob * seq_len)\n",
    "\n",
    "    num_tokens = mask.sum(dim=-1, keepdim=True)\n",
    "    mask_excess = (mask.cumsum(dim=-1) > (num_tokens * prob).ceil())\n",
    "    mask_excess = mask_excess[:, :max_masked]\n",
    "\n",
    "    rand = torch.rand((batch, seq_len), device=device).masked_fill(~mask, -1e9)\n",
    "    _, sampled_indices = rand.topk(max_masked, dim=-1)\n",
    "    sampled_indices = (sampled_indices + 1).masked_fill_(mask_excess, 0)\n",
    "\n",
    "    new_mask = torch.zeros((batch, seq_len + 1), device=device)\n",
    "    new_mask.scatter_(-1, sampled_indices, 1)\n",
    "    return new_mask[:, 1:].bool()\n",
    "\n",
    "\n",
    "# hidden layer extractor class, for magically adding adapter to language model to be pretrained\n",
    "class HiddenLayerExtractor(nn.Module):\n",
    "    def __init__(self, net, layer = -2):\n",
    "        super().__init__()\n",
    "        self.net = net\n",
    "        self.layer = layer\n",
    "\n",
    "        self.hidden = None\n",
    "        self.hook_registered = False\n",
    "\n",
    "    def _find_layer(self):\n",
    "        if type(self.layer) == str:\n",
    "            modules = dict([*self.net.named_modules()])\n",
    "            return modules.get(self.layer, None)\n",
    "        elif type(self.layer) == int:\n",
    "            children = [*self.net.children()]\n",
    "            return children[self.layer]\n",
    "        return None\n",
    "\n",
    "    def _hook(self, _, __, output):\n",
    "        self.hidden = output\n",
    "\n",
    "    def _register_hook(self):\n",
    "        layer = self._find_layer()\n",
    "        assert layer is not None, f'hidden layer ({self.layer}) not found'\n",
    "        handle = layer.register_forward_hook(self._hook)\n",
    "        self.hook_registered = True\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.layer == -1:\n",
    "            return self.net(x['input'],x['domain'])\n",
    "\n",
    "        if not self.hook_registered:\n",
    "            self._register_hook()\n",
    "\n",
    "        _ = self.net(x['input'],x['domain'])\n",
    "        hidden = self.hidden\n",
    "        self.hidden = None\n",
    "        assert hidden is not None, f'hidden layer {self.layer} never emitted an output'\n",
    "        return hidden\n",
    "    \n",
    "class Electra(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        generator,\n",
    "        discriminator,\n",
    "        *,\n",
    "        num_tokens = None,\n",
    "        discr_dim = -1,\n",
    "        discr_layer = -1,\n",
    "        mask_prob = 0.15,\n",
    "        replace_prob = 0.85,\n",
    "        random_token_prob = 0.,\n",
    "        mask_token_id = 2,\n",
    "        pad_token_id = 0,\n",
    "        mask_ignore_token_ids = [],\n",
    "        disc_weight = 50.,\n",
    "        gen_weight = 1.,\n",
    "        temperature = 1.,\n",
    "        ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.generator = generator\n",
    "        self.discriminator = discriminator\n",
    "\n",
    "        if discr_dim > 0:\n",
    "            self.discriminator = nn.Sequential(\n",
    "                HiddenLayerExtractor(discriminator, layer = discr_layer),\n",
    "                nn.Linear(discr_dim, 1)\n",
    "            )\n",
    "\n",
    "        # mlm related probabilities\n",
    "        self.mask_prob = mask_prob\n",
    "        self.replace_prob = replace_prob\n",
    "\n",
    "        self.num_tokens = num_tokens\n",
    "        self.random_token_prob = random_token_prob\n",
    "\n",
    "        # token ids\n",
    "        self.pad_token_id = pad_token_id\n",
    "        self.mask_token_id = mask_token_id\n",
    "        self.mask_ignore_token_ids = set([*mask_ignore_token_ids, pad_token_id])\n",
    "\n",
    "        # sampling temperature\n",
    "        self.temperature = temperature\n",
    "\n",
    "        # loss weights\n",
    "        self.disc_weight = disc_weight\n",
    "        self.gen_weight = gen_weight\n",
    "\n",
    "\n",
    "    def forward(self, inputs, **kwargs):\n",
    "        input=inputs['input']\n",
    "        b, t = input.shape\n",
    "\n",
    "        replace_prob = prob_mask_like(input, self.replace_prob)\n",
    "\n",
    "        # do not mask [pad] tokens, or any other tokens in the tokens designated to be excluded ([cls], [sep])\n",
    "        # also do not include these special tokens in the tokens chosen at random\n",
    "        no_mask = mask_with_tokens(input, self.mask_ignore_token_ids)\n",
    "        mask = get_mask_subset_with_prob(~no_mask, self.mask_prob)\n",
    "\n",
    "        # get mask indices\n",
    "        mask_indices = torch.nonzero(mask, as_tuple=True)\n",
    "\n",
    "        # mask input with mask tokens with probability of `replace_prob` (keep tokens the same with probability 1 - replace_prob)\n",
    "        masked_input = input.clone().detach()\n",
    "\n",
    "        # set inverse of mask to padding tokens for labels\n",
    "        gen_labels = input.masked_fill(~mask, self.pad_token_id)\n",
    "\n",
    "        # clone the mask, for potential modification if random tokens are involved\n",
    "        # not to be mistakened for the mask above, which is for all tokens, whether not replaced nor replaced with random tokens\n",
    "        masking_mask = mask.clone()\n",
    "\n",
    "        # if random token probability > 0 for mlm\n",
    "        if self.random_token_prob > 0:\n",
    "            assert self.num_tokens is not None, 'Number of tokens (num_tokens) must be passed to Electra for randomizing tokens during masked language modeling'\n",
    "\n",
    "            random_token_prob = prob_mask_like(input, self.random_token_prob)\n",
    "            random_tokens = torch.randint(0, self.num_tokens, input.shape, device=input.device)\n",
    "            random_no_mask = mask_with_tokens(random_tokens, self.mask_ignore_token_ids)\n",
    "            random_token_prob &= ~random_no_mask\n",
    "            masked_input = torch.where(random_token_prob, random_tokens, masked_input)\n",
    "\n",
    "            # remove random token prob mask from masking mask\n",
    "            masking_mask = masking_mask & ~random_token_prob\n",
    "\n",
    "        # [mask] input\n",
    "        masked_input = masked_input.masked_fill(masking_mask * replace_prob, self.mask_token_id)\n",
    "\n",
    "        # get generator output and get mlm loss\n",
    "        logits = self.generator(masked_input, **kwargs)\n",
    "\n",
    "        mlm_loss = F.cross_entropy(\n",
    "            logits.transpose(1, 2),\n",
    "            gen_labels,\n",
    "            ignore_index = self.pad_token_id\n",
    "        )\n",
    "\n",
    "        # use mask from before to select logits that need sampling\n",
    "        sample_logits = logits[mask_indices]\n",
    "\n",
    "        # sample\n",
    "        sampled = gumbel_sample(sample_logits, temperature = self.temperature)\n",
    "\n",
    "        # scatter the sampled values back to the input\n",
    "        disc_input = input.clone()\n",
    "        disc_input[mask_indices] = sampled.detach()\n",
    "\n",
    "        # generate discriminator labels, with replaced as True and original as False\n",
    "        disc_labels = (input != disc_input).float().detach()\n",
    "\n",
    "        # get discriminator predictions of replaced / original\n",
    "        non_padded_indices = torch.nonzero(input != self.pad_token_id, as_tuple=True)\n",
    "\n",
    "        # get discriminator output and binary cross entropy loss\n",
    "        disc_logits = self.discriminator({'input':disc_input,'domain':inputs['domain']}, **kwargs)\n",
    "        disc_logits = disc_logits.reshape_as(disc_labels)\n",
    "\n",
    "        disc_loss = F.binary_cross_entropy_with_logits(\n",
    "            disc_logits[non_padded_indices],\n",
    "            disc_labels[non_padded_indices]\n",
    "        )\n",
    "\n",
    "        # gather metrics\n",
    "        with torch.no_grad():\n",
    "            gen_predictions = torch.argmax(logits, dim=-1)\n",
    "            disc_predictions = torch.round((torch.sign(disc_logits) + 1.0) * 0.5)\n",
    "            gen_acc = (gen_labels[mask] == gen_predictions[mask]).float().mean()\n",
    "            disc_acc = 0.5 * (disc_labels[mask] == disc_predictions[mask]).float().mean() + 0.5 * (disc_labels[~mask] == disc_predictions[~mask]).float().mean()\n",
    "\n",
    "        # return weighted sum of losses\n",
    "        return Results(self.gen_weight * mlm_loss + self.disc_weight * disc_loss, mlm_loss, disc_loss, gen_acc, disc_acc, disc_labels, disc_predictions)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hier Domain Transformer\n",
      "Non-A100 GPU detected, using math or mem efficient attention if input tensor is on cuda\n"
     ]
    }
   ],
   "source": [
    "# pip install x_transformers reformer_pytorch accelerate einops\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from reformer_pytorch import ReformerLM\n",
    "from x_transformers import TransformerWrapper, Decoder,Encoder\n",
    "\n",
    "# (1) instantiate the generator and discriminator, making sure that the generator is roughly a quarter to a half of the size of the discriminator\n",
    "\n",
    "\n",
    "generator = TransformerWrapper(\n",
    "    num_tokens = VOCAB_SIZE+8,\n",
    "    max_seq_len = MODEL_MAX_SEQ_LEN,\n",
    "    attn_layers = Decoder(\n",
    "        dim = 256,\n",
    "        depth = 12,\n",
    "        heads = 4,\n",
    "        attn_flash = True,\n",
    "        domains=int(math.sqrt(N_DOMAINS)),\n",
    "        subdomains1=N_DOMAINS,\n",
    "        # rel_pos_bias = True \n",
    "    )\n",
    ")\n",
    "\n",
    "discriminator = TransformerWrapper(\n",
    "    num_tokens = VOCAB_SIZE+8,\n",
    "    max_seq_len = MODEL_MAX_SEQ_LEN,\n",
    "    attn_layers = Decoder(\n",
    "        dim = 768,\n",
    "        depth = 12,\n",
    "        heads = 12,\n",
    "        attn_flash = True,\n",
    "        domains=0,\n",
    "        # rel_pos_bias = True \n",
    "    )\n",
    ")\n",
    "\n",
    "#works with reformer!!!\n",
    "# generator.token_emb = discriminator.token_emb\n",
    "# generator.pos_emb = discriminator.pos_emb\n",
    "\n",
    "model = Electra(\n",
    "    generator,\n",
    "    discriminator,\n",
    "    discr_dim = 768,           # the embedding dimension of the discriminator\n",
    "    # discr_dim = 1024,           # the embedding dimension of the discriminator\n",
    "    # discr_layer = 'reformer',   # the layer name in the discriminator, whose output would be used for predicting token is still the same or replaced\n",
    "    discr_layer = 'attn_layers',   # the layer name in the discriminator, whose output would be used for predicting token is still the same or replaced\n",
    "    mask_token_id = MASK_ID,          # the token id reserved for masking\n",
    "    pad_token_id = PAD_ID,           # the token id for padding\n",
    "    mask_prob = 0.15,           # masking probability for masked language modeling\n",
    "    mask_ignore_token_ids = []  # ids of tokens to ignore for mask modeling ex. (cls, sep)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_ids are the indices corresponding to each token in the sentence.\n",
    "# attention_mask indicates whether a token should be attended to or not.\n",
    "# token_type_ids identifies which sequence a token belongs to when there is more than one sequence\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "if model_training=='modular':\n",
    "    if HIER_LEVEL==0:\n",
    "        train_dataloader = CustomDataloader(dataset,cluster_labels,N_DOMAINS)   \n",
    "    elif HIER_LEVEL==1:\n",
    "        #no dataloader os as var fr subdom sao do msm tamanho do dom. Exemplo: 3 dom e 3 subdom\n",
    "        #No modelo é diferente, temos uma primeira camada com 3 branches(dominio), e uma segunda camada com 9 branches(subdominios)\n",
    "        train_dataloader = CustomDataloaderLevel1(dataset,cluster_labels,idxs_cluster_labels_level1,int(math.sqrt(N_DOMAINS)),int(math.sqrt(N_DOMAINS)) ) \n",
    "else:\n",
    "    train_dataloader = DataLoader(\n",
    "            tokenized_dataset, shuffle=True, batch_size=DEVICE_BATCH_SIZE\n",
    "        )\n",
    "# Optimizer\n",
    "learning_rate=5e-5\n",
    "weight_decay=0\n",
    "\n",
    "gradient_accumulation_steps=1\n",
    "max_train_steps=None\n",
    "num_train_epochs=1\n",
    "lr_scheduler_type='linear'\n",
    "num_warmup_steps=0\n",
    "# Split weights in two groups, one with weight decay and the other not.\n",
    "no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "optimizer_grouped_parameters = [\n",
    "    {\n",
    "        \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "        \"weight_decay\": weight_decay,\n",
    "    },\n",
    "    {\n",
    "        \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "        \"weight_decay\": 0.0,\n",
    "    },\n",
    "]\n",
    "optimizer = torch.optim.AdamW(optimizer_grouped_parameters, lr=learning_rate)\n",
    "\n",
    "import math \n",
    "from transformers import (\n",
    "    get_scheduler,\n",
    ")\n",
    "# Scheduler and math around the number of training steps.\n",
    "overrode_max_train_steps = False\n",
    "num_update_steps_per_epoch = math.ceil(len(train_dataloader) / gradient_accumulation_steps)\n",
    "if max_train_steps is None:\n",
    "    max_train_steps = num_train_epochs * num_update_steps_per_epoch\n",
    "    overrode_max_train_steps = True\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=lr_scheduler_type,\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=num_warmup_steps * gradient_accumulation_steps,\n",
    "    num_training_steps=max_train_steps * gradient_accumulation_steps,\n",
    ")\n",
    "from accelerate import Accelerator\n",
    "\n",
    "accelerator = Accelerator()\n",
    "# Prepare everything with our `accelerator`.\n",
    "model, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(\n",
    "    model, optimizer, train_dataloader, lr_scheduler\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration:  0 , total_loss:  1.2910634358723958\n",
      "count_amostra: 100\n",
      "iteration:  30 , total_loss:  36.15121815999349\n",
      "count_amostra: 3100\n",
      "iteration:  60 , total_loss:  30.663614145914714\n",
      "count_amostra: 6100\n",
      "iteration:  90 , total_loss:  30.34499766031901\n",
      "count_amostra: 9100\n",
      "iteration:  120 , total_loss:  30.113268852233887\n",
      "count_amostra: 12100\n",
      "iteration:  150 , total_loss:  29.83952439626058\n",
      "count_amostra: 15100\n",
      "iteration:  180 , total_loss:  29.622913297017416\n",
      "count_amostra: 18100\n",
      "iteration:  210 , total_loss:  29.365022214253745\n",
      "count_amostra: 21100\n",
      "iteration:  240 , total_loss:  29.065749804178875\n",
      "count_amostra: 24100\n",
      "iteration:  270 , total_loss:  29.114762624104817\n",
      "count_amostra: 27100\n",
      "iteration:  300 , total_loss:  28.975903765360513\n",
      "count_amostra: 30100\n",
      "iteration:  330 , total_loss:  28.885704930623373\n",
      "count_amostra: 33100\n",
      "iteration:  360 , total_loss:  28.812674268086752\n",
      "count_amostra: 36100\n",
      "iteration:  390 , total_loss:  28.75842196146647\n",
      "count_amostra: 39100\n",
      "iteration:  420 , total_loss:  28.858734766642254\n",
      "count_amostra: 42100\n",
      "iteration:  450 , total_loss:  29.04611612955729\n",
      "count_amostra: 45100\n",
      "iteration:  480 , total_loss:  28.868082745869955\n",
      "count_amostra: 48100\n",
      "iteration:  510 , total_loss:  28.74900951385498\n",
      "count_amostra: 51100\n",
      "iteration:  540 , total_loss:  28.791536204020183\n",
      "count_amostra: 54100\n",
      "iteration:  570 , total_loss:  28.680887921651205\n",
      "count_amostra: 57100\n",
      "iteration:  600 , total_loss:  28.674227078755695\n",
      "count_amostra: 60100\n",
      "iteration:  630 , total_loss:  28.60933806101481\n",
      "count_amostra: 63100\n",
      "iteration:  660 , total_loss:  28.413976669311523\n",
      "count_amostra: 66100\n",
      "iteration:  690 , total_loss:  28.481268692016602\n",
      "count_amostra: 69100\n",
      "iteration:  720 , total_loss:  28.542180506388345\n",
      "count_amostra: 72100\n",
      "iteration:  750 , total_loss:  28.467857042948406\n",
      "count_amostra: 75100\n",
      "iteration:  780 , total_loss:  28.445674260457356\n",
      "count_amostra: 78100\n",
      "iteration:  810 , total_loss:  28.25187899271647\n",
      "count_amostra: 81100\n",
      "iteration:  840 , total_loss:  28.04248046875\n",
      "count_amostra: 84100\n",
      "iteration:  870 , total_loss:  28.249997011820476\n",
      "count_amostra: 87100\n",
      "iteration:  900 , total_loss:  28.000709088643394\n",
      "count_amostra: 90100\n",
      "iteration:  930 , total_loss:  28.14228458404541\n",
      "count_amostra: 93100\n",
      "iteration:  960 , total_loss:  27.84416929880778\n",
      "count_amostra: 96100\n",
      "iteration:  990 , total_loss:  28.02571004231771\n",
      "count_amostra: 99100\n",
      "iteration:  1020 , total_loss:  28.011335309346517\n",
      "count_amostra: 102100\n",
      "iteration:  1050 , total_loss:  27.905587577819823\n",
      "count_amostra: 105100\n",
      "iteration:  1080 , total_loss:  27.88407955169678\n",
      "count_amostra: 108100\n",
      "iteration:  1110 , total_loss:  27.843402036031087\n",
      "count_amostra: 111100\n",
      "iteration:  1140 , total_loss:  27.87696386973063\n",
      "count_amostra: 114100\n",
      "iteration:  1170 , total_loss:  27.958322715759277\n",
      "count_amostra: 117100\n",
      "iteration:  1200 , total_loss:  27.61166343688965\n",
      "count_amostra: 120100\n",
      "iteration:  1230 , total_loss:  27.762100728352866\n",
      "count_amostra: 123100\n",
      "iteration:  1260 , total_loss:  27.748229789733887\n",
      "count_amostra: 126100\n",
      "iteration:  1290 , total_loss:  27.65164763132731\n",
      "count_amostra: 129100\n",
      "iteration:  1320 , total_loss:  27.581937662760417\n",
      "count_amostra: 132100\n",
      "iteration:  1350 , total_loss:  27.580652554829914\n",
      "count_amostra: 135100\n",
      "iteration:  1380 , total_loss:  27.56360295613607\n",
      "count_amostra: 138100\n",
      "iteration:  1410 , total_loss:  27.42028948465983\n",
      "count_amostra: 141100\n",
      "iteration:  1440 , total_loss:  27.592041142781575\n",
      "count_amostra: 144100\n",
      "iteration:  1470 , total_loss:  27.396603139241538\n",
      "count_amostra: 147100\n",
      "iteration:  1500 , total_loss:  27.41363436381022\n",
      "count_amostra: 150100\n",
      "iteration:  1530 , total_loss:  27.555017534891764\n",
      "count_amostra: 153100\n",
      "iteration:  1560 , total_loss:  27.440121205647788\n",
      "count_amostra: 156100\n",
      "iteration:  1590 , total_loss:  27.467490577697752\n",
      "count_amostra: 159100\n",
      "iteration:  1620 , total_loss:  27.366211700439454\n",
      "count_amostra: 162100\n",
      "iteration:  1650 , total_loss:  27.471419779459634\n",
      "count_amostra: 165100\n",
      "iteration:  1680 , total_loss:  27.36481475830078\n",
      "count_amostra: 168100\n",
      "iteration:  1710 , total_loss:  27.18984578450521\n",
      "count_amostra: 171100\n",
      "iteration:  1740 , total_loss:  27.086691474914552\n",
      "count_amostra: 174100\n",
      "iteration:  1770 , total_loss:  27.26818981170654\n",
      "count_amostra: 177100\n",
      "iteration:  1800 , total_loss:  27.17304865519206\n",
      "count_amostra: 180100\n",
      "iteration:  1830 , total_loss:  27.176153945922852\n",
      "count_amostra: 183100\n",
      "iteration:  1860 , total_loss:  27.28295332590739\n",
      "count_amostra: 186100\n",
      "iteration:  1890 , total_loss:  27.00356648763021\n",
      "count_amostra: 189100\n",
      "iteration:  1920 , total_loss:  27.24529463450114\n",
      "count_amostra: 192100\n",
      "iteration:  1950 , total_loss:  27.059685134887694\n",
      "count_amostra: 195100\n",
      "iteration:  1980 , total_loss:  27.088489405314128\n",
      "count_amostra: 198100\n",
      "iteration:  2010 , total_loss:  27.052010536193848\n",
      "count_amostra: 201100\n",
      "iteration:  2040 , total_loss:  27.135623931884766\n",
      "count_amostra: 204100\n",
      "iteration:  2070 , total_loss:  26.939850934346516\n",
      "count_amostra: 207100\n",
      "iteration:  2100 , total_loss:  26.942885462443034\n",
      "count_amostra: 210100\n",
      "iteration:  2130 , total_loss:  27.026267051696777\n",
      "count_amostra: 213100\n",
      "iteration:  2160 , total_loss:  26.8183079401652\n",
      "count_amostra: 216100\n",
      "iteration:  2190 , total_loss:  26.764225260416666\n",
      "count_amostra: 219100\n",
      "iteration:  2220 , total_loss:  26.860125160217287\n",
      "count_amostra: 222100\n",
      "iteration:  2250 , total_loss:  26.800448608398437\n",
      "count_amostra: 225100\n",
      "iteration:  2280 , total_loss:  26.784298197428384\n",
      "count_amostra: 228100\n",
      "iteration:  2310 , total_loss:  26.803138478597006\n",
      "count_amostra: 231100\n",
      "iteration:  2340 , total_loss:  26.906390380859374\n",
      "count_amostra: 234100\n",
      "iteration:  2370 , total_loss:  26.663393529256187\n",
      "count_amostra: 237100\n",
      "iteration:  2400 , total_loss:  26.518606758117677\n",
      "count_amostra: 240100\n",
      "iteration:  2430 , total_loss:  26.583478991190592\n",
      "count_amostra: 243100\n",
      "iteration:  2460 , total_loss:  26.46598612467448\n",
      "count_amostra: 246100\n",
      "iteration:  2490 , total_loss:  26.362881215413413\n",
      "count_amostra: 249100\n",
      "iteration:  2520 , total_loss:  26.57959410349528\n",
      "count_amostra: 252100\n",
      "iteration:  2550 , total_loss:  26.494913164774577\n",
      "count_amostra: 255100\n",
      "iteration:  2580 , total_loss:  26.396753374735514\n",
      "count_amostra: 258100\n",
      "iteration:  2610 , total_loss:  26.29332866668701\n",
      "count_amostra: 261100\n",
      "iteration:  2640 , total_loss:  26.1620356241862\n",
      "count_amostra: 264100\n",
      "iteration:  2670 , total_loss:  26.385147857666016\n",
      "count_amostra: 267100\n",
      "iteration:  2700 , total_loss:  26.222725423177085\n",
      "count_amostra: 270100\n",
      "iteration:  2730 , total_loss:  26.202602322896322\n",
      "count_amostra: 273100\n",
      "iteration:  2760 , total_loss:  26.093724950154623\n",
      "count_amostra: 276100\n",
      "iteration:  2790 , total_loss:  25.905026308695476\n",
      "count_amostra: 279100\n",
      "iteration:  2820 , total_loss:  25.685905838012694\n",
      "count_amostra: 282100\n",
      "iteration:  2850 , total_loss:  25.81119244893392\n",
      "count_amostra: 285100\n",
      "iteration:  2880 , total_loss:  25.742030016581218\n",
      "count_amostra: 288100\n",
      "iteration:  2910 , total_loss:  25.620003191630044\n",
      "count_amostra: 291100\n",
      "iteration:  2940 , total_loss:  25.65248832702637\n",
      "count_amostra: 294100\n",
      "iteration:  2970 , total_loss:  25.7847194035848\n",
      "count_amostra: 297100\n",
      "iteration:  3000 , total_loss:  25.732263374328614\n",
      "count_amostra: 300100\n",
      "iteration:  3030 , total_loss:  25.535716374715168\n",
      "count_amostra: 303100\n",
      "iteration:  3060 , total_loss:  25.58082103729248\n",
      "count_amostra: 306100\n",
      "iteration:  3090 , total_loss:  25.415234565734863\n",
      "count_amostra: 309100\n",
      "iteration:  3120 , total_loss:  25.2195582707723\n",
      "count_amostra: 312100\n",
      "iteration:  3150 , total_loss:  25.439026578267416\n",
      "count_amostra: 315100\n",
      "iteration:  3180 , total_loss:  25.410216840108237\n",
      "count_amostra: 318100\n",
      "iteration:  3210 , total_loss:  25.23323300679525\n",
      "count_amostra: 321100\n",
      "iteration:  3240 , total_loss:  25.240825907389322\n",
      "count_amostra: 324100\n",
      "iteration:  3270 , total_loss:  25.28058141072591\n",
      "count_amostra: 327100\n",
      "iteration:  3300 , total_loss:  25.36379248301188\n",
      "count_amostra: 330100\n",
      "iteration:  3330 , total_loss:  25.21885242462158\n",
      "count_amostra: 333100\n",
      "iteration:  3360 , total_loss:  24.98494078318278\n",
      "count_amostra: 336100\n",
      "iteration:  3390 , total_loss:  25.064737828572593\n",
      "count_amostra: 339100\n",
      "iteration:  3420 , total_loss:  25.0476193745931\n",
      "count_amostra: 342100\n",
      "iteration:  3450 , total_loss:  24.951743443806965\n",
      "count_amostra: 345100\n",
      "iteration:  3480 , total_loss:  25.021271896362304\n",
      "count_amostra: 348100\n",
      "iteration:  3510 , total_loss:  24.880107561747234\n",
      "count_amostra: 351100\n",
      "iteration:  3540 , total_loss:  24.983455721537272\n",
      "count_amostra: 354100\n",
      "iteration:  3570 , total_loss:  24.60858809153239\n",
      "count_amostra: 357100\n",
      "iteration:  3600 , total_loss:  24.871787452697752\n",
      "count_amostra: 360100\n",
      "iteration:  3630 , total_loss:  24.660979334513346\n",
      "count_amostra: 363100\n",
      "iteration:  3660 , total_loss:  24.611418024698892\n",
      "count_amostra: 366100\n",
      "iteration:  3690 , total_loss:  24.657063929239907\n",
      "count_amostra: 369100\n",
      "iteration:  3720 , total_loss:  24.68293348948161\n",
      "count_amostra: 372100\n",
      "iteration:  3750 , total_loss:  24.832873407999674\n",
      "count_amostra: 375100\n",
      "iteration:  3780 , total_loss:  24.60308945973714\n",
      "count_amostra: 378100\n",
      "iteration:  3810 , total_loss:  24.458105723063152\n",
      "count_amostra: 381100\n",
      "iteration:  3840 , total_loss:  24.432538922627767\n",
      "count_amostra: 384100\n",
      "iteration:  3870 , total_loss:  24.340995915730794\n",
      "count_amostra: 387100\n",
      "iteration:  3900 , total_loss:  24.4232146581014\n",
      "count_amostra: 390100\n",
      "iteration:  3930 , total_loss:  24.371502494812013\n",
      "count_amostra: 393100\n",
      "iteration:  3960 , total_loss:  24.098820813496907\n",
      "count_amostra: 396100\n",
      "iteration:  3990 , total_loss:  24.42645575205485\n",
      "count_amostra: 399100\n",
      "iteration:  4020 , total_loss:  24.604595120747884\n",
      "count_amostra: 402100\n",
      "iteration:  4050 , total_loss:  24.63018576304118\n",
      "count_amostra: 405100\n",
      "iteration:  4080 , total_loss:  24.318194643656412\n",
      "count_amostra: 408100\n",
      "iteration:  4110 , total_loss:  24.477659225463867\n",
      "count_amostra: 411100\n",
      "iteration:  4140 , total_loss:  24.2434845606486\n",
      "count_amostra: 414100\n",
      "iteration:  4170 , total_loss:  24.283206367492674\n",
      "count_amostra: 417100\n",
      "iteration:  4200 , total_loss:  24.279754702250163\n",
      "count_amostra: 420100\n",
      "iteration:  4230 , total_loss:  24.308505948384603\n",
      "count_amostra: 423100\n",
      "iteration:  4260 , total_loss:  24.14309024810791\n",
      "count_amostra: 426100\n",
      "iteration:  4290 , total_loss:  24.557119369506836\n",
      "count_amostra: 429100\n",
      "iteration:  4320 , total_loss:  24.63676249186198\n",
      "count_amostra: 432100\n",
      "iteration:  4350 , total_loss:  24.011960983276367\n",
      "count_amostra: 435100\n",
      "iteration:  4380 , total_loss:  24.0779421488444\n",
      "count_amostra: 438100\n",
      "iteration:  4410 , total_loss:  24.21314748128255\n",
      "count_amostra: 441100\n",
      "iteration:  4440 , total_loss:  24.454840723673502\n",
      "count_amostra: 444100\n",
      "iteration:  4470 , total_loss:  24.28389924367269\n",
      "count_amostra: 447100\n",
      "iteration:  4500 , total_loss:  24.35980943044027\n",
      "count_amostra: 450100\n",
      "iteration:  4530 , total_loss:  24.25318781534831\n",
      "count_amostra: 453100\n",
      "iteration:  4560 , total_loss:  24.184797223409017\n",
      "count_amostra: 456100\n",
      "iteration:  4590 , total_loss:  24.246561940511068\n",
      "count_amostra: 459100\n",
      "iteration:  4620 , total_loss:  24.16773141225179\n",
      "count_amostra: 462100\n",
      "iteration:  4650 , total_loss:  24.302603022257486\n",
      "count_amostra: 465100\n",
      "iteration:  4680 , total_loss:  24.230244191487632\n",
      "count_amostra: 468100\n",
      "iteration:  4710 , total_loss:  24.042031733194985\n",
      "count_amostra: 471100\n",
      "iteration:  4740 , total_loss:  24.353271675109863\n",
      "count_amostra: 474100\n",
      "iteration:  4770 , total_loss:  24.4562172571818\n",
      "count_amostra: 477100\n",
      "iteration:  4800 , total_loss:  24.35402406056722\n",
      "count_amostra: 480100\n",
      "iteration:  4830 , total_loss:  24.05037104288737\n",
      "count_amostra: 483100\n",
      "iteration:  4860 , total_loss:  24.08580315907796\n",
      "count_amostra: 486100\n",
      "iteration:  4890 , total_loss:  24.159472783406574\n",
      "count_amostra: 489100\n",
      "iteration:  4920 , total_loss:  24.016076596577964\n",
      "count_amostra: 492100\n",
      "iteration:  4950 , total_loss:  24.24153429667155\n",
      "count_amostra: 495100\n",
      "iteration:  4980 , total_loss:  24.10865306854248\n",
      "count_amostra: 498100\n",
      "iteration:  5010 , total_loss:  24.282710456848143\n",
      "count_amostra: 501100\n",
      "iteration:  5040 , total_loss:  24.068575795491537\n",
      "count_amostra: 504100\n",
      "iteration:  5070 , total_loss:  23.746638679504393\n",
      "count_amostra: 507100\n",
      "iteration:  5100 , total_loss:  24.121681912740073\n",
      "count_amostra: 510100\n",
      "iteration:  5130 , total_loss:  23.936387825012208\n",
      "count_amostra: 513100\n",
      "iteration:  5160 , total_loss:  23.97085049947103\n",
      "count_amostra: 516100\n",
      "iteration:  5190 , total_loss:  24.12149689992269\n",
      "count_amostra: 519100\n",
      "iteration:  5220 , total_loss:  24.075031979878744\n",
      "count_amostra: 522100\n",
      "iteration:  5250 , total_loss:  23.996135584513347\n",
      "count_amostra: 525100\n",
      "iteration:  5280 , total_loss:  24.210676256815592\n",
      "count_amostra: 528100\n",
      "iteration:  5310 , total_loss:  23.99886786142985\n",
      "count_amostra: 531100\n",
      "iteration:  5340 , total_loss:  24.110573132832844\n",
      "count_amostra: 534100\n",
      "iteration:  5370 , total_loss:  24.070655632019044\n",
      "count_amostra: 537100\n",
      "iteration:  5400 , total_loss:  24.157172393798827\n",
      "count_amostra: 540100\n",
      "iteration:  5430 , total_loss:  24.067514864603677\n",
      "count_amostra: 543100\n",
      "iteration:  5460 , total_loss:  24.238598505655926\n",
      "count_amostra: 546100\n",
      "iteration:  5490 , total_loss:  24.210434595743816\n",
      "count_amostra: 549100\n",
      "iteration:  5520 , total_loss:  24.30493621826172\n",
      "count_amostra: 552100\n",
      "iteration:  5550 , total_loss:  24.2596342086792\n",
      "count_amostra: 555100\n",
      "iteration:  5580 , total_loss:  24.356435012817382\n",
      "count_amostra: 558100\n",
      "iteration:  5610 , total_loss:  23.876900482177735\n",
      "count_amostra: 561100\n",
      "iteration:  5640 , total_loss:  24.279629834493\n",
      "count_amostra: 564100\n",
      "iteration:  5670 , total_loss:  24.202446683247885\n",
      "count_amostra: 567100\n",
      "iteration:  5700 , total_loss:  24.257965087890625\n",
      "count_amostra: 570100\n",
      "iteration:  5730 , total_loss:  24.214838155110677\n",
      "count_amostra: 573100\n",
      "iteration:  5760 , total_loss:  23.946980412801107\n",
      "count_amostra: 576100\n",
      "iteration:  5790 , total_loss:  24.135635312398275\n",
      "count_amostra: 579100\n",
      "iteration:  5820 , total_loss:  24.080326716105144\n",
      "count_amostra: 582100\n",
      "iteration:  5850 , total_loss:  24.614276313781737\n",
      "count_amostra: 585100\n",
      "iteration:  5880 , total_loss:  24.143817138671874\n",
      "count_amostra: 588100\n",
      "iteration:  5910 , total_loss:  24.00899518330892\n",
      "count_amostra: 591100\n",
      "iteration:  5940 , total_loss:  24.31420987447103\n",
      "count_amostra: 594100\n",
      "iteration:  5970 , total_loss:  24.203179995218914\n",
      "count_amostra: 597100\n",
      "iteration:  6000 , total_loss:  24.37211685180664\n",
      "count_amostra: 600100\n",
      "iteration:  6030 , total_loss:  24.32027441660563\n",
      "count_amostra: 603100\n",
      "iteration:  6060 , total_loss:  23.93812510172526\n",
      "count_amostra: 606100\n",
      "iteration:  6090 , total_loss:  24.019760258992513\n",
      "count_amostra: 609100\n",
      "iteration:  6120 , total_loss:  24.19151102701823\n",
      "count_amostra: 612100\n",
      "iteration:  6150 , total_loss:  24.113060760498048\n",
      "count_amostra: 615100\n",
      "iteration:  6180 , total_loss:  24.06543140411377\n",
      "count_amostra: 618100\n",
      "iteration:  6210 , total_loss:  24.077731132507324\n",
      "count_amostra: 621100\n",
      "iteration:  6240 , total_loss:  24.23672612508138\n",
      "count_amostra: 624100\n",
      "iteration:  6270 , total_loss:  24.236866633097332\n",
      "count_amostra: 627100\n",
      "iteration:  6300 , total_loss:  24.08471533457438\n",
      "count_amostra: 630100\n",
      "iteration:  6330 , total_loss:  24.208860397338867\n",
      "count_amostra: 633100\n",
      "iteration:  6360 , total_loss:  24.15227959950765\n",
      "count_amostra: 636100\n",
      "iteration:  6390 , total_loss:  24.27830301920573\n",
      "count_amostra: 639100\n",
      "iteration:  6420 , total_loss:  24.171878814697266\n",
      "count_amostra: 642100\n",
      "iteration:  6450 , total_loss:  24.246288998921713\n",
      "count_amostra: 645100\n",
      "iteration:  6480 , total_loss:  23.995324007670085\n",
      "count_amostra: 648100\n",
      "iteration:  6510 , total_loss:  23.948279825846353\n",
      "count_amostra: 651100\n",
      "iteration:  6540 , total_loss:  23.99055722554525\n",
      "count_amostra: 654100\n",
      "iteration:  6570 , total_loss:  24.00318800608317\n",
      "count_amostra: 657100\n",
      "iteration:  6600 , total_loss:  23.938167826334634\n",
      "count_amostra: 660100\n",
      "iteration:  6630 , total_loss:  23.963144810994468\n",
      "count_amostra: 663100\n",
      "iteration:  6660 , total_loss:  24.053291765848794\n",
      "count_amostra: 666100\n",
      "iteration:  6690 , total_loss:  24.030423736572267\n",
      "count_amostra: 669100\n",
      "iteration:  6720 , total_loss:  24.107468287150066\n",
      "count_amostra: 672100\n",
      "iteration:  6750 , total_loss:  24.05790424346924\n",
      "count_amostra: 675100\n",
      "iteration:  6780 , total_loss:  23.784516843159995\n",
      "count_amostra: 678100\n",
      "iteration:  6810 , total_loss:  23.828768730163574\n",
      "count_amostra: 681100\n",
      "iteration:  6840 , total_loss:  24.009343401590982\n",
      "count_amostra: 684100\n",
      "iteration:  6870 , total_loss:  24.04160067240397\n",
      "count_amostra: 687100\n",
      "iteration:  6900 , total_loss:  24.111161041259766\n",
      "count_amostra: 690100\n",
      "iteration:  6930 , total_loss:  24.130636342366536\n",
      "count_amostra: 693100\n",
      "iteration:  6960 , total_loss:  24.095547231038413\n",
      "count_amostra: 696100\n",
      "iteration:  6990 , total_loss:  24.29804083506266\n",
      "count_amostra: 699100\n",
      "iteration:  7020 , total_loss:  23.96571610768636\n",
      "count_amostra: 702100\n",
      "iteration:  7050 , total_loss:  24.326553599039713\n",
      "count_amostra: 705100\n",
      "iteration:  7080 , total_loss:  24.162299410502115\n",
      "count_amostra: 708100\n",
      "iteration:  7110 , total_loss:  24.19263947804769\n",
      "count_amostra: 711100\n",
      "iteration:  7140 , total_loss:  24.09136899312337\n",
      "count_amostra: 714100\n",
      "iteration:  7170 , total_loss:  24.072881762186686\n",
      "count_amostra: 717100\n",
      "iteration:  7200 , total_loss:  23.864757347106934\n",
      "count_amostra: 720100\n",
      "iteration:  7230 , total_loss:  23.917197100321452\n",
      "count_amostra: 723100\n",
      "iteration:  7260 , total_loss:  24.03463509877523\n",
      "count_amostra: 726100\n",
      "iteration:  7290 , total_loss:  24.007157643636067\n",
      "count_amostra: 729100\n",
      "iteration:  7320 , total_loss:  23.926768620808918\n",
      "count_amostra: 732100\n",
      "iteration:  7350 , total_loss:  24.23730754852295\n",
      "count_amostra: 735100\n",
      "iteration:  7380 , total_loss:  23.97596327463786\n",
      "count_amostra: 738100\n",
      "iteration:  7410 , total_loss:  23.991591262817384\n",
      "count_amostra: 741100\n",
      "iteration:  7440 , total_loss:  23.796901512145997\n",
      "count_amostra: 744100\n",
      "iteration:  7470 , total_loss:  24.058567237854003\n",
      "count_amostra: 747100\n",
      "iteration:  7500 , total_loss:  23.945970217386883\n",
      "count_amostra: 750100\n",
      "iteration:  7530 , total_loss:  24.223491668701172\n",
      "count_amostra: 753100\n",
      "iteration:  7560 , total_loss:  24.181976890563966\n",
      "count_amostra: 756100\n",
      "iteration:  7590 , total_loss:  24.0748997370402\n",
      "count_amostra: 759100\n",
      "iteration:  7620 , total_loss:  24.0754695892334\n",
      "count_amostra: 762100\n",
      "iteration:  7650 , total_loss:  24.066972732543945\n",
      "count_amostra: 765100\n",
      "iteration:  7680 , total_loss:  23.853482055664063\n",
      "count_amostra: 768100\n",
      "iteration:  7710 , total_loss:  23.970108858744304\n",
      "count_amostra: 771100\n",
      "iteration:  7740 , total_loss:  23.63898131052653\n",
      "count_amostra: 774100\n",
      "iteration:  7770 , total_loss:  23.994013786315918\n",
      "count_amostra: 777100\n",
      "iteration:  7800 , total_loss:  24.16747907002767\n",
      "count_amostra: 780100\n",
      "iteration:  7830 , total_loss:  24.212185605367026\n",
      "count_amostra: 783100\n",
      "iteration:  7860 , total_loss:  23.770049730936687\n",
      "count_amostra: 786100\n",
      "iteration:  7890 , total_loss:  23.81194585164388\n",
      "count_amostra: 789100\n",
      "iteration:  7920 , total_loss:  23.952852376302083\n",
      "count_amostra: 792100\n",
      "iteration:  7950 , total_loss:  23.916931597391763\n",
      "count_amostra: 795100\n",
      "iteration:  7980 , total_loss:  23.931452369689943\n",
      "count_amostra: 798100\n",
      "iteration:  8010 , total_loss:  23.920890299479165\n",
      "count_amostra: 801100\n",
      "iteration:  8040 , total_loss:  23.864370091756186\n",
      "count_amostra: 804100\n",
      "iteration:  8070 , total_loss:  24.03385041554769\n",
      "count_amostra: 807100\n",
      "iteration:  8100 , total_loss:  23.886864153544106\n",
      "count_amostra: 810100\n",
      "iteration:  8130 , total_loss:  24.341201718648275\n",
      "count_amostra: 813100\n",
      "iteration:  8160 , total_loss:  23.841479619344074\n",
      "count_amostra: 816100\n",
      "iteration:  8190 , total_loss:  23.869885444641113\n",
      "count_amostra: 819100\n",
      "iteration:  8220 , total_loss:  23.811361440022786\n",
      "count_amostra: 822100\n",
      "iteration:  8250 , total_loss:  23.67497132619222\n",
      "count_amostra: 825100\n",
      "iteration:  8280 , total_loss:  23.793947728474937\n",
      "count_amostra: 828100\n",
      "iteration:  8310 , total_loss:  23.865982818603516\n",
      "count_amostra: 831100\n",
      "iteration:  8340 , total_loss:  23.775542958577475\n",
      "count_amostra: 834100\n",
      "iteration:  8370 , total_loss:  24.102484893798827\n",
      "count_amostra: 837100\n",
      "iteration:  8400 , total_loss:  23.848375765482583\n",
      "count_amostra: 840100\n",
      "iteration:  8430 , total_loss:  24.141016642252605\n",
      "count_amostra: 843100\n",
      "iteration:  8460 , total_loss:  23.72463607788086\n",
      "count_amostra: 846100\n",
      "iteration:  8490 , total_loss:  23.829321924845377\n",
      "count_amostra: 849100\n",
      "iteration:  8520 , total_loss:  23.817332140604655\n",
      "count_amostra: 852100\n",
      "iteration:  8550 , total_loss:  23.690455945332847\n",
      "count_amostra: 855100\n",
      "iteration:  8580 , total_loss:  24.104147911071777\n",
      "count_amostra: 858100\n",
      "iteration:  8610 , total_loss:  23.92122917175293\n",
      "count_amostra: 861100\n",
      "iteration:  8640 , total_loss:  23.795554796854656\n",
      "count_amostra: 864100\n",
      "iteration:  8670 , total_loss:  23.910512034098307\n",
      "count_amostra: 867100\n",
      "iteration:  8700 , total_loss:  23.662053044637045\n",
      "count_amostra: 870100\n",
      "iteration:  8730 , total_loss:  23.834933280944824\n",
      "count_amostra: 873100\n",
      "iteration:  8760 , total_loss:  23.833960723876952\n",
      "count_amostra: 876100\n",
      "iteration:  8790 , total_loss:  23.884799639383953\n",
      "count_amostra: 879100\n",
      "iteration:  8820 , total_loss:  23.89643955230713\n",
      "count_amostra: 882100\n",
      "iteration:  8850 , total_loss:  24.076849110921223\n",
      "count_amostra: 885100\n",
      "iteration:  8880 , total_loss:  23.9290797551473\n",
      "count_amostra: 888100\n",
      "iteration:  8910 , total_loss:  24.016349220275877\n",
      "count_amostra: 891100\n",
      "iteration:  8940 , total_loss:  23.876724751790366\n",
      "count_amostra: 894100\n",
      "iteration:  8970 , total_loss:  24.079109636942544\n",
      "count_amostra: 897100\n",
      "iteration:  9000 , total_loss:  23.83743362426758\n",
      "count_amostra: 900100\n",
      "iteration:  9030 , total_loss:  24.13733647664388\n",
      "count_amostra: 903100\n",
      "iteration:  9060 , total_loss:  23.604327964782716\n",
      "count_amostra: 906100\n",
      "iteration:  9090 , total_loss:  23.630743153889973\n",
      "count_amostra: 909100\n",
      "iteration:  9120 , total_loss:  23.90426451365153\n",
      "count_amostra: 912100\n",
      "iteration:  9150 , total_loss:  23.685783259073894\n",
      "count_amostra: 915100\n",
      "iteration:  9180 , total_loss:  23.63605047861735\n",
      "count_amostra: 918100\n",
      "iteration:  9210 , total_loss:  23.95309543609619\n",
      "count_amostra: 921100\n",
      "iteration:  9240 , total_loss:  23.614902623494466\n",
      "count_amostra: 924100\n",
      "iteration:  9270 , total_loss:  23.825410970052083\n",
      "count_amostra: 927100\n",
      "iteration:  9300 , total_loss:  23.92039546966553\n",
      "count_amostra: 930100\n",
      "iteration:  9330 , total_loss:  24.03594601949056\n",
      "count_amostra: 933100\n",
      "iteration:  9360 , total_loss:  23.730906804402668\n",
      "count_amostra: 936100\n",
      "iteration:  9390 , total_loss:  23.628365135192873\n",
      "count_amostra: 939100\n",
      "iteration:  9420 , total_loss:  23.580798212687174\n",
      "count_amostra: 942100\n",
      "iteration:  9450 , total_loss:  23.609823926289877\n",
      "count_amostra: 945100\n",
      "iteration:  9480 , total_loss:  23.985975329081217\n",
      "count_amostra: 948100\n",
      "iteration:  9510 , total_loss:  24.13389835357666\n",
      "count_amostra: 951100\n",
      "iteration:  9540 , total_loss:  23.65873425801595\n",
      "count_amostra: 954100\n",
      "iteration:  9570 , total_loss:  23.79412988026937\n",
      "count_amostra: 957100\n",
      "iteration:  9600 , total_loss:  23.43322359720866\n",
      "count_amostra: 960100\n",
      "iteration:  9630 , total_loss:  23.63320369720459\n",
      "count_amostra: 963100\n",
      "iteration:  9660 , total_loss:  23.792850812276203\n",
      "count_amostra: 966100\n",
      "iteration:  9690 , total_loss:  23.52384808858236\n",
      "count_amostra: 969100\n",
      "iteration:  9720 , total_loss:  23.67108465830485\n",
      "count_amostra: 972100\n",
      "iteration:  9750 , total_loss:  23.828833707173665\n",
      "count_amostra: 975100\n",
      "iteration:  9780 , total_loss:  23.65055211385091\n",
      "count_amostra: 978100\n",
      "iteration:  9810 , total_loss:  23.74742857615153\n",
      "count_amostra: 981100\n",
      "iteration:  9840 , total_loss:  23.7657439549764\n",
      "count_amostra: 984100\n",
      "iteration:  9870 , total_loss:  23.7432954788208\n",
      "count_amostra: 987100\n",
      "iteration:  9900 , total_loss:  23.61831309000651\n",
      "count_amostra: 990100\n",
      "iteration:  9930 , total_loss:  23.906760342915852\n",
      "count_amostra: 993100\n",
      "iteration:  9960 , total_loss:  23.559409777323406\n",
      "count_amostra: 996100\n",
      "iteration:  9990 , total_loss:  23.580700047810872\n",
      "count_amostra: 999100\n",
      "iteration:  10020 , total_loss:  23.55984878540039\n",
      "count_amostra: 1002100\n",
      "iteration:  10050 , total_loss:  23.77860107421875\n",
      "count_amostra: 1005100\n",
      "iteration:  10080 , total_loss:  23.831376139322916\n",
      "count_amostra: 1008100\n",
      "iteration:  10110 , total_loss:  23.4585365931193\n",
      "count_amostra: 1011100\n",
      "iteration:  10140 , total_loss:  23.617487335205077\n",
      "count_amostra: 1014100\n",
      "iteration:  10170 , total_loss:  23.45916067759196\n",
      "count_amostra: 1017100\n",
      "iteration:  10200 , total_loss:  23.355748875935873\n",
      "count_amostra: 1020100\n",
      "iteration:  10230 , total_loss:  23.254387791951498\n",
      "count_amostra: 1023100\n",
      "iteration:  10260 , total_loss:  23.789649454752603\n",
      "count_amostra: 1026100\n",
      "iteration:  10290 , total_loss:  23.368688837687174\n",
      "count_amostra: 1029100\n",
      "iteration:  10320 , total_loss:  23.607226181030274\n",
      "count_amostra: 1032100\n",
      "iteration:  10350 , total_loss:  23.587696901957194\n",
      "count_amostra: 1035100\n",
      "iteration:  10380 , total_loss:  23.519588724772134\n",
      "count_amostra: 1038100\n",
      "iteration:  10410 , total_loss:  23.954536120096844\n",
      "count_amostra: 1041100\n",
      "iteration:  10440 , total_loss:  23.6228728612264\n",
      "count_amostra: 1044100\n",
      "iteration:  10470 , total_loss:  23.54909553527832\n",
      "count_amostra: 1047100\n",
      "iteration:  10500 , total_loss:  23.34923528035482\n",
      "count_amostra: 1050100\n",
      "iteration:  10530 , total_loss:  23.70031172434489\n",
      "count_amostra: 1053100\n",
      "iteration:  10560 , total_loss:  23.492495028177895\n",
      "count_amostra: 1056100\n",
      "iteration:  10590 , total_loss:  23.57245489756266\n",
      "count_amostra: 1059100\n",
      "iteration:  10620 , total_loss:  23.691662788391113\n",
      "count_amostra: 1062100\n",
      "iteration:  10650 , total_loss:  23.346082369486492\n",
      "count_amostra: 1065100\n",
      "iteration:  10680 , total_loss:  23.3567990620931\n",
      "count_amostra: 1068100\n",
      "iteration:  10710 , total_loss:  23.8843256632487\n",
      "count_amostra: 1071100\n",
      "iteration:  10740 , total_loss:  23.448968251546223\n",
      "count_amostra: 1074100\n",
      "iteration:  10770 , total_loss:  23.552291297912596\n",
      "count_amostra: 1077100\n",
      "iteration:  10800 , total_loss:  23.67126178741455\n",
      "count_amostra: 1080100\n",
      "iteration:  10830 , total_loss:  23.727042071024577\n",
      "count_amostra: 1083100\n",
      "iteration:  10860 , total_loss:  23.625783093770345\n",
      "count_amostra: 1086100\n",
      "iteration:  10890 , total_loss:  23.38604958852132\n",
      "count_amostra: 1089100\n",
      "iteration:  10920 , total_loss:  23.531305821736655\n",
      "count_amostra: 1092100\n",
      "iteration:  10950 , total_loss:  23.46607322692871\n",
      "count_amostra: 1095100\n",
      "iteration:  10980 , total_loss:  23.575346755981446\n",
      "count_amostra: 1098100\n",
      "iteration:  11010 , total_loss:  23.35258191426595\n",
      "count_amostra: 1101100\n",
      "iteration:  11040 , total_loss:  23.631501770019533\n",
      "count_amostra: 1104100\n",
      "iteration:  11070 , total_loss:  23.2213649113973\n",
      "count_amostra: 1107100\n",
      "iteration:  11100 , total_loss:  23.45079860687256\n",
      "count_amostra: 1110100\n",
      "iteration:  11130 , total_loss:  23.40235424041748\n",
      "count_amostra: 1113100\n",
      "iteration:  11160 , total_loss:  23.50736910502116\n",
      "count_amostra: 1116100\n",
      "iteration:  11190 , total_loss:  23.405238723754884\n",
      "count_amostra: 1119100\n",
      "iteration:  11220 , total_loss:  23.479623476664226\n",
      "count_amostra: 1122100\n",
      "iteration:  11250 , total_loss:  23.410391108194986\n",
      "count_amostra: 1125100\n",
      "iteration:  11280 , total_loss:  23.5295259475708\n",
      "count_amostra: 1128100\n",
      "iteration:  11310 , total_loss:  23.5701935450236\n",
      "count_amostra: 1131100\n",
      "iteration:  11340 , total_loss:  23.35360507965088\n",
      "count_amostra: 1134100\n",
      "iteration:  11370 , total_loss:  23.226099332173664\n",
      "count_amostra: 1137100\n",
      "iteration:  11400 , total_loss:  23.724331665039063\n",
      "count_amostra: 1140100\n",
      "iteration:  11430 , total_loss:  23.449779637654622\n",
      "count_amostra: 1143100\n",
      "iteration:  11460 , total_loss:  23.44726759592692\n",
      "count_amostra: 1146100\n",
      "iteration:  11490 , total_loss:  23.370913124084474\n",
      "count_amostra: 1149100\n",
      "iteration:  11520 , total_loss:  23.682734743754068\n",
      "count_amostra: 1152100\n",
      "iteration:  11550 , total_loss:  23.416395950317384\n",
      "count_amostra: 1155100\n",
      "iteration:  11580 , total_loss:  23.4161776860555\n",
      "count_amostra: 1158100\n",
      "iteration:  11610 , total_loss:  23.557611401875814\n",
      "count_amostra: 1161100\n",
      "iteration:  11640 , total_loss:  23.506017112731932\n",
      "count_amostra: 1164100\n",
      "iteration:  11670 , total_loss:  23.395962079366047\n",
      "count_amostra: 1167100\n",
      "iteration:  11700 , total_loss:  23.420540428161623\n",
      "count_amostra: 1170100\n",
      "iteration:  11730 , total_loss:  23.335704167683918\n",
      "count_amostra: 1173100\n",
      "iteration:  11760 , total_loss:  23.38336404164632\n",
      "count_amostra: 1176100\n",
      "iteration:  11790 , total_loss:  23.300706481933595\n",
      "count_amostra: 1179100\n",
      "iteration:  11820 , total_loss:  23.074089558919272\n",
      "count_amostra: 1182100\n",
      "iteration:  11850 , total_loss:  23.359662437438963\n",
      "count_amostra: 1185100\n",
      "iteration:  11880 , total_loss:  23.184053230285645\n",
      "count_amostra: 1188100\n",
      "iteration:  11910 , total_loss:  23.368300692240396\n",
      "count_amostra: 1191100\n",
      "iteration:  11940 , total_loss:  23.34738343556722\n",
      "count_amostra: 1194100\n",
      "iteration:  11970 , total_loss:  23.277648735046387\n",
      "count_amostra: 1197100\n",
      "iteration:  12000 , total_loss:  23.154154968261718\n",
      "count_amostra: 1200100\n",
      "iteration:  12030 , total_loss:  23.361844062805176\n",
      "count_amostra: 1203100\n",
      "iteration:  12060 , total_loss:  23.03721701304118\n",
      "count_amostra: 1206100\n",
      "iteration:  12090 , total_loss:  23.370840326944986\n",
      "count_amostra: 1209100\n",
      "iteration:  12120 , total_loss:  23.248979568481445\n",
      "count_amostra: 1212100\n",
      "iteration:  12150 , total_loss:  23.26051400502523\n",
      "count_amostra: 1215100\n",
      "iteration:  12180 , total_loss:  23.187705675760906\n",
      "count_amostra: 1218100\n",
      "iteration:  12210 , total_loss:  23.080365562438963\n",
      "count_amostra: 1221100\n",
      "iteration:  12240 , total_loss:  22.992404429117837\n",
      "count_amostra: 1224100\n",
      "iteration:  12270 , total_loss:  23.110216077168783\n",
      "count_amostra: 1227100\n",
      "iteration:  12300 , total_loss:  23.441855748494465\n",
      "count_amostra: 1230100\n",
      "iteration:  12330 , total_loss:  23.3533478418986\n",
      "count_amostra: 1233100\n",
      "iteration:  12360 , total_loss:  23.29137903849284\n",
      "count_amostra: 1236100\n",
      "iteration:  12390 , total_loss:  23.211611239115395\n",
      "count_amostra: 1239100\n",
      "iteration:  12420 , total_loss:  23.32940616607666\n",
      "count_amostra: 1242100\n",
      "iteration:  12450 , total_loss:  22.9847900390625\n",
      "count_amostra: 1245100\n",
      "iteration:  12480 , total_loss:  23.458827273050943\n",
      "count_amostra: 1248100\n",
      "iteration:  12510 , total_loss:  23.27207546234131\n",
      "count_amostra: 1251100\n",
      "iteration:  12540 , total_loss:  22.9358034769694\n",
      "count_amostra: 1254100\n",
      "iteration:  12570 , total_loss:  23.213543128967284\n",
      "count_amostra: 1257100\n",
      "iteration:  12600 , total_loss:  23.57637093861898\n",
      "count_amostra: 1260100\n",
      "iteration:  12630 , total_loss:  22.920339393615723\n",
      "count_amostra: 1263100\n",
      "iteration:  12660 , total_loss:  23.099278577168782\n",
      "count_amostra: 1266100\n",
      "iteration:  12690 , total_loss:  23.23471476236979\n",
      "count_amostra: 1269100\n",
      "iteration:  12720 , total_loss:  23.015620994567872\n",
      "count_amostra: 1272100\n",
      "iteration:  12750 , total_loss:  23.34645741780599\n",
      "count_amostra: 1275100\n",
      "iteration:  12780 , total_loss:  23.212402280171712\n",
      "count_amostra: 1278100\n",
      "iteration:  12810 , total_loss:  23.171022097269695\n",
      "count_amostra: 1281100\n",
      "iteration:  12840 , total_loss:  23.007069396972657\n",
      "count_amostra: 1284100\n",
      "iteration:  12870 , total_loss:  23.00946159362793\n",
      "count_amostra: 1287100\n",
      "iteration:  12900 , total_loss:  23.27542559305827\n",
      "count_amostra: 1290100\n",
      "iteration:  12930 , total_loss:  22.893056869506836\n",
      "count_amostra: 1293100\n",
      "iteration:  12960 , total_loss:  23.121337763468425\n",
      "count_amostra: 1296100\n",
      "iteration:  12990 , total_loss:  22.958216921488443\n",
      "count_amostra: 1299100\n",
      "iteration:  13020 , total_loss:  23.202282524108888\n",
      "count_amostra: 1302100\n",
      "iteration:  13050 , total_loss:  23.252597427368165\n",
      "count_amostra: 1305100\n",
      "iteration:  13080 , total_loss:  23.041244761149088\n",
      "count_amostra: 1308100\n",
      "iteration:  13110 , total_loss:  23.269422022501626\n",
      "count_amostra: 1311100\n",
      "iteration:  13140 , total_loss:  23.52160619099935\n",
      "count_amostra: 1314100\n",
      "iteration:  13170 , total_loss:  23.45126132965088\n",
      "count_amostra: 1317100\n",
      "iteration:  13200 , total_loss:  23.09348055521647\n",
      "count_amostra: 1320100\n",
      "iteration:  13230 , total_loss:  23.10868091583252\n",
      "count_amostra: 1323100\n",
      "iteration:  13260 , total_loss:  23.429445457458495\n",
      "count_amostra: 1326100\n",
      "iteration:  13290 , total_loss:  23.137317657470703\n",
      "count_amostra: 1329100\n",
      "iteration:  13320 , total_loss:  22.902171071370443\n",
      "count_amostra: 1332100\n",
      "iteration:  13350 , total_loss:  23.17001094818115\n",
      "count_amostra: 1335100\n",
      "iteration:  13380 , total_loss:  23.146024322509767\n",
      "count_amostra: 1338100\n",
      "iteration:  13410 , total_loss:  23.2138547261556\n",
      "count_amostra: 1341100\n",
      "iteration:  13440 , total_loss:  22.916178639729818\n",
      "count_amostra: 1344100\n",
      "iteration:  13470 , total_loss:  23.033689117431642\n",
      "count_amostra: 1347100\n",
      "iteration:  13500 , total_loss:  22.82830193837484\n",
      "count_amostra: 1350100\n",
      "iteration:  13530 , total_loss:  23.334270985921226\n",
      "count_amostra: 1353100\n",
      "iteration:  13560 , total_loss:  23.24214547475179\n",
      "count_amostra: 1356100\n",
      "iteration:  13590 , total_loss:  23.12173817952474\n",
      "count_amostra: 1359100\n",
      "iteration:  13620 , total_loss:  22.97185688018799\n",
      "count_amostra: 1362100\n",
      "iteration:  13650 , total_loss:  22.983303705851238\n",
      "count_amostra: 1365100\n",
      "iteration:  13680 , total_loss:  22.982398096720377\n",
      "count_amostra: 1368100\n",
      "iteration:  13710 , total_loss:  22.99294490814209\n",
      "count_amostra: 1371100\n",
      "iteration:  13740 , total_loss:  23.210710334777833\n",
      "count_amostra: 1374100\n",
      "iteration:  13770 , total_loss:  23.06507943471273\n",
      "count_amostra: 1377100\n",
      "iteration:  13800 , total_loss:  22.601741091410318\n",
      "count_amostra: 1380100\n",
      "iteration:  13830 , total_loss:  22.87460403442383\n",
      "count_amostra: 1383100\n",
      "iteration:  13860 , total_loss:  23.024333063761393\n",
      "count_amostra: 1386100\n",
      "iteration:  13890 , total_loss:  23.203974596659343\n",
      "count_amostra: 1389100\n",
      "iteration:  13920 , total_loss:  22.95010617574056\n",
      "count_amostra: 1392100\n",
      "iteration:  13950 , total_loss:  22.678939946492513\n",
      "count_amostra: 1395100\n",
      "iteration:  13980 , total_loss:  22.814148966471354\n",
      "count_amostra: 1398100\n",
      "iteration:  14010 , total_loss:  23.022014236450197\n",
      "count_amostra: 1401100\n",
      "iteration:  14040 , total_loss:  22.75677070617676\n",
      "count_amostra: 1404100\n",
      "iteration:  14070 , total_loss:  23.141527620951333\n",
      "count_amostra: 1407100\n",
      "iteration:  14100 , total_loss:  23.07674198150635\n",
      "count_amostra: 1410100\n",
      "iteration:  14130 , total_loss:  23.102850596110027\n",
      "count_amostra: 1413100\n",
      "iteration:  14160 , total_loss:  23.076097551981608\n",
      "count_amostra: 1416100\n",
      "iteration:  14190 , total_loss:  22.861970774332683\n",
      "count_amostra: 1419100\n",
      "iteration:  14220 , total_loss:  23.006499608357746\n",
      "count_amostra: 1422100\n",
      "iteration:  14250 , total_loss:  22.865368270874022\n",
      "count_amostra: 1425100\n",
      "iteration:  14280 , total_loss:  23.174083836873372\n",
      "count_amostra: 1428100\n",
      "iteration:  14310 , total_loss:  22.835863049825033\n",
      "count_amostra: 1431100\n",
      "iteration:  14340 , total_loss:  22.74698397318522\n",
      "count_amostra: 1434100\n",
      "iteration:  14370 , total_loss:  23.21715348561605\n",
      "count_amostra: 1437100\n",
      "iteration:  14400 , total_loss:  22.75924078623454\n",
      "count_amostra: 1440100\n",
      "iteration:  14430 , total_loss:  23.08904946645101\n",
      "count_amostra: 1443100\n",
      "iteration:  14460 , total_loss:  23.135433642069497\n",
      "count_amostra: 1446100\n",
      "iteration:  14490 , total_loss:  23.153424135843913\n",
      "count_amostra: 1449100\n",
      "iteration:  14520 , total_loss:  22.93536771138509\n",
      "count_amostra: 1452100\n",
      "iteration:  14550 , total_loss:  22.682043329874674\n",
      "count_amostra: 1455100\n",
      "iteration:  14580 , total_loss:  23.08657887776693\n",
      "count_amostra: 1458100\n",
      "iteration:  14610 , total_loss:  22.6806084950765\n",
      "count_amostra: 1461100\n",
      "iteration:  14640 , total_loss:  23.150421714782716\n",
      "count_amostra: 1464100\n",
      "iteration:  14670 , total_loss:  23.154368718465168\n",
      "count_amostra: 1467100\n",
      "iteration:  14700 , total_loss:  23.008924420674642\n",
      "count_amostra: 1470100\n",
      "iteration:  14730 , total_loss:  22.864413833618165\n",
      "count_amostra: 1473100\n",
      "iteration:  14760 , total_loss:  22.7420139948527\n",
      "count_amostra: 1476100\n",
      "iteration:  14790 , total_loss:  23.007573890686036\n",
      "count_amostra: 1479100\n",
      "iteration:  14820 , total_loss:  22.910574213663736\n",
      "count_amostra: 1482100\n",
      "iteration:  14850 , total_loss:  22.80478795369466\n",
      "count_amostra: 1485100\n",
      "iteration:  14880 , total_loss:  23.10018679300944\n",
      "count_amostra: 1488100\n",
      "iteration:  14910 , total_loss:  22.7412930170695\n",
      "count_amostra: 1491100\n",
      "iteration:  14940 , total_loss:  23.2351957321167\n",
      "count_amostra: 1494100\n",
      "iteration:  14970 , total_loss:  22.66969254811605\n",
      "count_amostra: 1497100\n",
      "iteration:  15000 , total_loss:  23.175303840637206\n",
      "count_amostra: 1500100\n",
      "iteration:  15030 , total_loss:  23.030877939860027\n",
      "count_amostra: 1503100\n",
      "iteration:  15060 , total_loss:  23.002462577819824\n",
      "count_amostra: 1506100\n",
      "iteration:  15090 , total_loss:  22.993851725260416\n",
      "count_amostra: 1509100\n",
      "iteration:  15120 , total_loss:  22.782332865397134\n",
      "count_amostra: 1512100\n",
      "iteration:  15150 , total_loss:  22.90616855621338\n",
      "count_amostra: 1515100\n",
      "iteration:  15180 , total_loss:  22.906253178914387\n",
      "count_amostra: 1518100\n",
      "iteration:  15210 , total_loss:  22.656749089558918\n",
      "count_amostra: 1521100\n",
      "iteration:  15240 , total_loss:  22.764125887552897\n",
      "count_amostra: 1524100\n",
      "iteration:  15270 , total_loss:  23.061300786336265\n",
      "count_amostra: 1527100\n",
      "iteration:  15300 , total_loss:  22.91256809234619\n",
      "count_amostra: 1530100\n",
      "iteration:  15330 , total_loss:  22.874115816752116\n",
      "count_amostra: 1533100\n",
      "iteration:  15360 , total_loss:  22.98888390858968\n",
      "count_amostra: 1536100\n",
      "iteration:  15390 , total_loss:  23.122536404927573\n",
      "count_amostra: 1539100\n",
      "iteration:  15420 , total_loss:  22.708572006225587\n",
      "count_amostra: 1542100\n",
      "iteration:  15450 , total_loss:  23.007018597920737\n",
      "count_amostra: 1545100\n",
      "iteration:  15480 , total_loss:  22.91586888631185\n",
      "count_amostra: 1548100\n",
      "iteration:  15510 , total_loss:  22.687389373779297\n",
      "count_amostra: 1551100\n",
      "iteration:  15540 , total_loss:  22.795142491658527\n",
      "count_amostra: 1554100\n",
      "iteration:  15570 , total_loss:  23.18189131418864\n",
      "count_amostra: 1557100\n",
      "iteration:  15600 , total_loss:  23.012092526753744\n",
      "count_amostra: 1560100\n",
      "iteration:  15630 , total_loss:  23.017685890197754\n",
      "count_amostra: 1563100\n",
      "iteration:  15660 , total_loss:  22.7523499806722\n",
      "count_amostra: 1566100\n",
      "iteration:  15690 , total_loss:  22.801242637634278\n",
      "count_amostra: 1569100\n",
      "iteration:  15720 , total_loss:  22.797709910074868\n",
      "count_amostra: 1572100\n",
      "iteration:  15750 , total_loss:  22.452190844217935\n",
      "count_amostra: 1575100\n",
      "iteration:  15780 , total_loss:  22.676287587483724\n",
      "count_amostra: 1578100\n",
      "iteration:  15810 , total_loss:  22.59924659729004\n",
      "count_amostra: 1581100\n",
      "iteration:  15840 , total_loss:  22.732266171773276\n",
      "count_amostra: 1584100\n",
      "iteration:  15870 , total_loss:  22.884455172220864\n",
      "count_amostra: 1587100\n",
      "iteration:  15900 , total_loss:  22.478892135620118\n",
      "count_amostra: 1590100\n",
      "iteration:  15930 , total_loss:  22.313105392456055\n",
      "count_amostra: 1593100\n",
      "iteration:  15960 , total_loss:  23.01066557566325\n",
      "count_amostra: 1596100\n",
      "iteration:  15990 , total_loss:  22.7477689743042\n",
      "count_amostra: 1599100\n",
      "iteration:  16020 , total_loss:  22.8854008992513\n",
      "count_amostra: 1602100\n",
      "iteration:  16050 , total_loss:  22.922326215108235\n",
      "count_amostra: 1605100\n",
      "iteration:  16080 , total_loss:  23.458619753519695\n",
      "count_amostra: 1608100\n",
      "iteration:  16110 , total_loss:  22.610882822672526\n",
      "count_amostra: 1611100\n",
      "iteration:  16140 , total_loss:  22.967879740397134\n",
      "count_amostra: 1614100\n",
      "iteration:  16170 , total_loss:  22.57890682220459\n",
      "count_amostra: 1617100\n",
      "iteration:  16200 , total_loss:  22.89862937927246\n",
      "count_amostra: 1620100\n",
      "iteration:  16230 , total_loss:  22.576782671610513\n",
      "count_amostra: 1623100\n",
      "iteration:  16260 , total_loss:  22.725441869099935\n",
      "count_amostra: 1626100\n",
      "iteration:  16290 , total_loss:  22.758294932047527\n",
      "count_amostra: 1629100\n",
      "iteration:  16320 , total_loss:  22.925299390157065\n",
      "count_amostra: 1632100\n",
      "iteration:  16350 , total_loss:  22.715794626871745\n",
      "count_amostra: 1635100\n",
      "iteration:  16380 , total_loss:  22.90246524810791\n",
      "count_amostra: 1638100\n",
      "iteration:  16410 , total_loss:  22.711259651184083\n",
      "count_amostra: 1641100\n",
      "iteration:  16440 , total_loss:  22.41158898671468\n",
      "count_amostra: 1644100\n",
      "iteration:  16470 , total_loss:  22.807438977559407\n",
      "count_amostra: 1647100\n",
      "iteration:  16500 , total_loss:  22.830020332336424\n",
      "count_amostra: 1650100\n",
      "iteration:  16530 , total_loss:  22.715728187561034\n",
      "count_amostra: 1653100\n",
      "iteration:  16560 , total_loss:  22.413119633992512\n",
      "count_amostra: 1656100\n",
      "iteration:  16590 , total_loss:  22.990112686157225\n",
      "count_amostra: 1659100\n",
      "iteration:  16620 , total_loss:  22.994879913330077\n",
      "count_amostra: 1662100\n",
      "iteration:  16650 , total_loss:  23.036344401041667\n",
      "count_amostra: 1665100\n",
      "iteration:  16680 , total_loss:  22.643140983581542\n",
      "count_amostra: 1668100\n",
      "iteration:  16710 , total_loss:  22.709573618570964\n",
      "count_amostra: 1671100\n",
      "iteration:  16740 , total_loss:  22.955097325642903\n",
      "count_amostra: 1674100\n",
      "iteration:  16770 , total_loss:  22.427880414326985\n",
      "count_amostra: 1677100\n",
      "iteration:  16800 , total_loss:  22.9873779296875\n",
      "count_amostra: 1680100\n",
      "iteration:  16830 , total_loss:  22.578702608744305\n",
      "count_amostra: 1683100\n",
      "iteration:  16860 , total_loss:  22.755330085754395\n",
      "count_amostra: 1686100\n",
      "iteration:  16890 , total_loss:  22.851915804545083\n",
      "count_amostra: 1689100\n",
      "iteration:  16920 , total_loss:  22.663174438476563\n",
      "count_amostra: 1692100\n",
      "iteration:  16950 , total_loss:  22.979819997151694\n",
      "count_amostra: 1695100\n",
      "iteration:  16980 , total_loss:  22.92320416768392\n",
      "count_amostra: 1698100\n",
      "iteration:  17010 , total_loss:  22.772301610310873\n",
      "count_amostra: 1701100\n",
      "iteration:  17040 , total_loss:  22.273821449279787\n",
      "count_amostra: 1704100\n",
      "iteration:  17070 , total_loss:  22.622647921244305\n",
      "count_amostra: 1707100\n",
      "iteration:  17100 , total_loss:  22.73537343343099\n",
      "count_amostra: 1710100\n",
      "iteration:  17130 , total_loss:  22.547573725382488\n",
      "count_amostra: 1713100\n",
      "iteration:  17160 , total_loss:  22.83991044362386\n",
      "count_amostra: 1716100\n",
      "iteration:  17190 , total_loss:  22.73283996582031\n",
      "count_amostra: 1719100\n",
      "iteration:  17220 , total_loss:  22.873468844095864\n",
      "count_amostra: 1722100\n",
      "iteration:  17250 , total_loss:  22.595260747273763\n",
      "count_amostra: 1725100\n",
      "iteration:  17280 , total_loss:  22.85250301361084\n",
      "count_amostra: 1728100\n",
      "iteration:  17310 , total_loss:  22.78271598815918\n",
      "count_amostra: 1731100\n",
      "iteration:  17340 , total_loss:  22.858023770650227\n",
      "count_amostra: 1734100\n",
      "iteration:  17370 , total_loss:  22.761035601298016\n",
      "count_amostra: 1737100\n",
      "iteration:  17400 , total_loss:  22.418446222941082\n",
      "count_amostra: 1740100\n",
      "iteration:  17430 , total_loss:  22.57826919555664\n",
      "count_amostra: 1743100\n",
      "iteration:  17460 , total_loss:  22.8459446589152\n",
      "count_amostra: 1746100\n",
      "iteration:  17490 , total_loss:  22.609933153788248\n",
      "count_amostra: 1749100\n",
      "iteration:  17520 , total_loss:  22.462102953592936\n",
      "count_amostra: 1752100\n",
      "iteration:  17550 , total_loss:  22.50179640452067\n",
      "count_amostra: 1755100\n",
      "iteration:  17580 , total_loss:  22.51037534077962\n",
      "count_amostra: 1758100\n",
      "iteration:  17610 , total_loss:  22.826755205790203\n",
      "count_amostra: 1761100\n",
      "iteration:  17640 , total_loss:  22.926048723856606\n",
      "count_amostra: 1764100\n",
      "iteration:  17670 , total_loss:  22.735360145568848\n",
      "count_amostra: 1767100\n",
      "iteration:  17700 , total_loss:  22.35727926890055\n",
      "count_amostra: 1770100\n",
      "iteration:  17730 , total_loss:  22.987002881368003\n",
      "count_amostra: 1773100\n",
      "iteration:  17760 , total_loss:  22.56008701324463\n",
      "count_amostra: 1776100\n",
      "iteration:  17790 , total_loss:  22.81020882924398\n",
      "count_amostra: 1779100\n",
      "iteration:  17820 , total_loss:  22.516060002644856\n",
      "count_amostra: 1782100\n",
      "iteration:  17850 , total_loss:  22.52389170328776\n",
      "count_amostra: 1785100\n",
      "iteration:  17880 , total_loss:  22.538380432128907\n",
      "count_amostra: 1788100\n",
      "iteration:  17910 , total_loss:  22.512781461079914\n",
      "count_amostra: 1791100\n",
      "iteration:  17940 , total_loss:  22.72894859313965\n",
      "count_amostra: 1794100\n",
      "iteration:  17970 , total_loss:  22.59442768096924\n",
      "count_amostra: 1797100\n",
      "iteration:  18000 , total_loss:  22.631219228108723\n",
      "count_amostra: 1800100\n",
      "iteration:  18030 , total_loss:  22.49998009999593\n",
      "count_amostra: 1803100\n",
      "iteration:  18060 , total_loss:  22.354770215352378\n",
      "count_amostra: 1806100\n",
      "iteration:  18090 , total_loss:  22.549511082967122\n",
      "count_amostra: 1809100\n",
      "iteration:  18120 , total_loss:  22.78145783742269\n",
      "count_amostra: 1812100\n",
      "iteration:  18150 , total_loss:  22.836314074198405\n",
      "count_amostra: 1815100\n",
      "iteration:  18180 , total_loss:  22.626319122314452\n",
      "count_amostra: 1818100\n",
      "iteration:  18210 , total_loss:  22.77352854410807\n",
      "count_amostra: 1821100\n",
      "iteration:  18240 , total_loss:  22.49002145131429\n",
      "count_amostra: 1824100\n",
      "iteration:  18270 , total_loss:  22.57357095082601\n",
      "count_amostra: 1827100\n",
      "iteration:  18300 , total_loss:  22.79065793355306\n",
      "count_amostra: 1830100\n",
      "iteration:  18330 , total_loss:  22.40853150685628\n",
      "count_amostra: 1833100\n",
      "iteration:  18360 , total_loss:  22.616084861755372\n",
      "count_amostra: 1836100\n",
      "iteration:  18390 , total_loss:  22.72491734822591\n",
      "count_amostra: 1839100\n",
      "iteration:  18420 , total_loss:  22.561764589945476\n",
      "count_amostra: 1842100\n",
      "iteration:  18450 , total_loss:  22.557206916809083\n",
      "count_amostra: 1845100\n",
      "iteration:  18480 , total_loss:  22.218599319458008\n",
      "count_amostra: 1848100\n",
      "iteration:  18510 , total_loss:  22.742222531636557\n",
      "count_amostra: 1851100\n",
      "iteration:  18540 , total_loss:  22.65766372680664\n",
      "count_amostra: 1854100\n",
      "iteration:  18570 , total_loss:  22.754640197753908\n",
      "count_amostra: 1857100\n",
      "iteration:  18600 , total_loss:  22.482180976867674\n",
      "count_amostra: 1860100\n",
      "iteration:  18630 , total_loss:  22.442801348368327\n",
      "count_amostra: 1863100\n",
      "iteration:  18660 , total_loss:  22.51012929280599\n",
      "count_amostra: 1866100\n",
      "iteration:  18690 , total_loss:  22.41402931213379\n",
      "count_amostra: 1869100\n",
      "iteration:  18720 , total_loss:  22.890758323669434\n",
      "count_amostra: 1872100\n",
      "iteration:  18750 , total_loss:  22.489730453491212\n",
      "count_amostra: 1875100\n",
      "iteration:  18780 , total_loss:  22.483911069234214\n",
      "count_amostra: 1878100\n",
      "iteration:  18810 , total_loss:  22.406890614827475\n",
      "count_amostra: 1881100\n",
      "iteration:  18840 , total_loss:  22.536890284220377\n",
      "count_amostra: 1884100\n",
      "iteration:  18870 , total_loss:  22.548185857137046\n",
      "count_amostra: 1887100\n",
      "iteration:  18900 , total_loss:  22.642744954427084\n",
      "count_amostra: 1890100\n",
      "iteration:  18930 , total_loss:  22.376453272501628\n",
      "count_amostra: 1893100\n",
      "iteration:  18960 , total_loss:  22.415445963541668\n",
      "count_amostra: 1896034\n",
      "iteration:  18990 , total_loss:  22.743772888183592\n",
      "count_amostra: 1899034\n",
      "iteration:  19020 , total_loss:  22.32304267883301\n",
      "count_amostra: 1902034\n",
      "iteration:  19050 , total_loss:  22.484579594930015\n",
      "count_amostra: 1905034\n",
      "iteration:  19080 , total_loss:  22.701102701822915\n",
      "count_amostra: 1908034\n",
      "iteration:  19110 , total_loss:  22.366624450683595\n",
      "count_amostra: 1911034\n",
      "iteration:  19140 , total_loss:  22.99596824645996\n",
      "count_amostra: 1914034\n",
      "iteration:  19170 , total_loss:  22.45867265065511\n",
      "count_amostra: 1917034\n",
      "iteration:  19200 , total_loss:  22.811639658610027\n",
      "count_amostra: 1920034\n",
      "iteration:  19230 , total_loss:  22.189507929484048\n",
      "count_amostra: 1923034\n",
      "iteration:  19260 , total_loss:  22.193472544352215\n",
      "count_amostra: 1926034\n",
      "iteration:  19290 , total_loss:  23.13061955769857\n",
      "count_amostra: 1929034\n",
      "iteration:  19320 , total_loss:  22.377556228637694\n",
      "count_amostra: 1932034\n",
      "iteration:  19350 , total_loss:  22.432513999938966\n",
      "count_amostra: 1935034\n",
      "iteration:  19380 , total_loss:  22.59498462677002\n",
      "count_amostra: 1938034\n",
      "iteration:  19410 , total_loss:  22.479668362935385\n",
      "count_amostra: 1941034\n",
      "iteration:  19440 , total_loss:  22.429775365193684\n",
      "count_amostra: 1944034\n",
      "iteration:  19470 , total_loss:  22.510004615783693\n",
      "count_amostra: 1947034\n",
      "iteration:  19500 , total_loss:  22.417444483439127\n",
      "count_amostra: 1950034\n",
      "iteration:  19530 , total_loss:  22.52786782582601\n",
      "count_amostra: 1953034\n",
      "iteration:  19560 , total_loss:  22.503791745503744\n",
      "count_amostra: 1956034\n",
      "iteration:  19590 , total_loss:  22.76500129699707\n",
      "count_amostra: 1959034\n",
      "iteration:  19620 , total_loss:  22.454830614725747\n",
      "count_amostra: 1962034\n",
      "iteration:  19650 , total_loss:  22.23440456390381\n",
      "count_amostra: 1965034\n",
      "iteration:  19680 , total_loss:  22.410216776529946\n",
      "count_amostra: 1968034\n",
      "iteration:  19710 , total_loss:  22.488375854492187\n",
      "count_amostra: 1971034\n",
      "iteration:  19740 , total_loss:  22.48689359029134\n",
      "count_amostra: 1974034\n",
      "iteration:  19770 , total_loss:  22.515472221374512\n",
      "count_amostra: 1977034\n",
      "iteration:  19800 , total_loss:  22.71489709218343\n",
      "count_amostra: 1980034\n",
      "iteration:  19830 , total_loss:  22.517740058898926\n",
      "count_amostra: 1983034\n",
      "iteration:  19860 , total_loss:  22.502795600891112\n",
      "count_amostra: 1986034\n",
      "iteration:  19890 , total_loss:  22.45270195007324\n",
      "count_amostra: 1989034\n",
      "iteration:  19920 , total_loss:  22.630918884277342\n",
      "count_amostra: 1992034\n",
      "iteration:  19950 , total_loss:  22.398321533203124\n",
      "count_amostra: 1995034\n",
      "iteration:  19980 , total_loss:  22.704051462809243\n",
      "count_amostra: 1998034\n",
      "iteration:  20010 , total_loss:  22.5006202061971\n",
      "count_amostra: 2001034\n",
      "iteration:  20040 , total_loss:  22.446413294474283\n",
      "count_amostra: 2004034\n",
      "iteration:  20070 , total_loss:  22.519684028625488\n",
      "count_amostra: 2007034\n",
      "iteration:  20100 , total_loss:  22.7705104192098\n",
      "count_amostra: 2010034\n",
      "iteration:  20130 , total_loss:  22.15703639984131\n",
      "count_amostra: 2013034\n",
      "iteration:  20160 , total_loss:  22.17089589436849\n",
      "count_amostra: 2016034\n",
      "iteration:  20190 , total_loss:  22.513750966389974\n",
      "count_amostra: 2019034\n",
      "iteration:  20220 , total_loss:  22.608342615763345\n",
      "count_amostra: 2022034\n",
      "iteration:  20250 , total_loss:  22.400351778666177\n",
      "count_amostra: 2025034\n",
      "iteration:  20280 , total_loss:  22.447957611083986\n",
      "count_amostra: 2028034\n",
      "iteration:  20310 , total_loss:  22.271141306559244\n",
      "count_amostra: 2031034\n",
      "iteration:  20340 , total_loss:  22.492268880208332\n",
      "count_amostra: 2034034\n",
      "iteration:  20370 , total_loss:  22.83223203023275\n",
      "count_amostra: 2037034\n",
      "iteration:  20400 , total_loss:  22.356727091471353\n",
      "count_amostra: 2040034\n",
      "iteration:  20430 , total_loss:  22.493865903218587\n",
      "count_amostra: 2043034\n",
      "iteration:  20460 , total_loss:  22.699624888102214\n",
      "count_amostra: 2046034\n",
      "iteration:  20490 , total_loss:  22.33942699432373\n",
      "count_amostra: 2049034\n",
      "iteration:  20520 , total_loss:  22.377094968159994\n",
      "count_amostra: 2052034\n",
      "iteration:  20550 , total_loss:  22.359148343404133\n",
      "count_amostra: 2055034\n",
      "iteration:  20580 , total_loss:  22.210645357767742\n",
      "count_amostra: 2058034\n",
      "iteration:  20610 , total_loss:  22.611232821146647\n",
      "count_amostra: 2061034\n",
      "iteration:  20640 , total_loss:  22.552111434936524\n",
      "count_amostra: 2064034\n",
      "iteration:  20670 , total_loss:  22.398735682169598\n",
      "count_amostra: 2067034\n",
      "iteration:  20700 , total_loss:  22.553944269816082\n",
      "count_amostra: 2070034\n",
      "iteration:  20730 , total_loss:  22.735001373291016\n",
      "count_amostra: 2073034\n",
      "iteration:  20760 , total_loss:  22.580387941996257\n",
      "count_amostra: 2076034\n",
      "iteration:  20790 , total_loss:  22.482286326090495\n",
      "count_amostra: 2079034\n",
      "iteration:  20820 , total_loss:  22.41733570098877\n",
      "count_amostra: 2082034\n",
      "iteration:  20850 , total_loss:  22.501292165120443\n",
      "count_amostra: 2085034\n",
      "iteration:  20880 , total_loss:  22.643133799235027\n",
      "count_amostra: 2088034\n",
      "iteration:  20910 , total_loss:  22.452638626098633\n",
      "count_amostra: 2091034\n",
      "iteration:  20940 , total_loss:  22.473129399617513\n",
      "count_amostra: 2094034\n",
      "iteration:  20970 , total_loss:  22.440949567159016\n",
      "count_amostra: 2097034\n",
      "iteration:  21000 , total_loss:  22.05097796122233\n",
      "count_amostra: 2100034\n",
      "iteration:  21030 , total_loss:  22.241324933369956\n",
      "count_amostra: 2103034\n",
      "iteration:  21060 , total_loss:  22.393747329711914\n",
      "count_amostra: 2106034\n",
      "iteration:  21090 , total_loss:  22.638360595703126\n",
      "count_amostra: 2109034\n",
      "iteration:  21120 , total_loss:  22.45636157989502\n",
      "count_amostra: 2112034\n",
      "iteration:  21150 , total_loss:  22.150794092814127\n",
      "count_amostra: 2115034\n",
      "iteration:  21180 , total_loss:  22.61652399698893\n",
      "count_amostra: 2118034\n",
      "iteration:  21210 , total_loss:  22.65364195505778\n",
      "count_amostra: 2121034\n",
      "iteration:  21240 , total_loss:  22.650399017333985\n",
      "count_amostra: 2124034\n",
      "iteration:  21270 , total_loss:  22.640806452433267\n",
      "count_amostra: 2127034\n",
      "iteration:  21300 , total_loss:  22.720225524902343\n",
      "count_amostra: 2130034\n",
      "iteration:  21330 , total_loss:  22.384178924560548\n",
      "count_amostra: 2133034\n",
      "iteration:  21360 , total_loss:  22.4079927444458\n",
      "count_amostra: 2136034\n",
      "iteration:  21390 , total_loss:  22.344720204671223\n",
      "count_amostra: 2139034\n",
      "iteration:  21420 , total_loss:  22.004571151733398\n",
      "count_amostra: 2142034\n",
      "iteration:  21450 , total_loss:  21.976516342163087\n",
      "count_amostra: 2145034\n",
      "iteration:  21480 , total_loss:  22.463556861877443\n",
      "count_amostra: 2148034\n",
      "iteration:  21510 , total_loss:  22.471632703145346\n",
      "count_amostra: 2151034\n",
      "iteration:  21540 , total_loss:  22.55342044830322\n",
      "count_amostra: 2154034\n",
      "iteration:  21570 , total_loss:  22.375942611694335\n",
      "count_amostra: 2157034\n",
      "iteration:  21600 , total_loss:  22.384096336364745\n",
      "count_amostra: 2160034\n",
      "iteration:  21630 , total_loss:  22.06568342844645\n",
      "count_amostra: 2163034\n",
      "iteration:  21660 , total_loss:  22.209278043111166\n",
      "count_amostra: 2166034\n",
      "iteration:  21690 , total_loss:  22.614399719238282\n",
      "count_amostra: 2169034\n",
      "iteration:  21720 , total_loss:  22.298045221964518\n",
      "count_amostra: 2172034\n",
      "iteration:  21750 , total_loss:  22.208165232340495\n",
      "count_amostra: 2175034\n",
      "iteration:  21780 , total_loss:  22.587149492899577\n",
      "count_amostra: 2178034\n",
      "iteration:  21810 , total_loss:  22.25758736928304\n",
      "count_amostra: 2181034\n",
      "iteration:  21840 , total_loss:  22.980995750427248\n",
      "count_amostra: 2184034\n",
      "iteration:  21870 , total_loss:  22.69075190226237\n",
      "count_amostra: 2187034\n",
      "iteration:  21900 , total_loss:  22.502704747517903\n",
      "count_amostra: 2190034\n",
      "iteration:  21930 , total_loss:  22.174063046773274\n",
      "count_amostra: 2193034\n",
      "iteration:  21960 , total_loss:  22.497712326049804\n",
      "count_amostra: 2196034\n",
      "iteration:  21990 , total_loss:  22.735650889078777\n",
      "count_amostra: 2199034\n",
      "iteration:  22020 , total_loss:  22.726427841186524\n",
      "count_amostra: 2202034\n",
      "iteration:  22050 , total_loss:  22.4229310353597\n",
      "count_amostra: 2205034\n",
      "iteration:  22080 , total_loss:  22.483327293395995\n",
      "count_amostra: 2208034\n",
      "iteration:  22110 , total_loss:  22.254171943664552\n",
      "count_amostra: 2211034\n",
      "iteration:  22140 , total_loss:  22.550459988911946\n",
      "count_amostra: 2214034\n",
      "iteration:  22170 , total_loss:  22.31608632405599\n",
      "count_amostra: 2217034\n",
      "iteration:  22200 , total_loss:  22.297323799133302\n",
      "count_amostra: 2220034\n",
      "iteration:  22230 , total_loss:  22.306597010294595\n",
      "count_amostra: 2223034\n",
      "iteration:  22260 , total_loss:  22.524816958109536\n",
      "count_amostra: 2226034\n",
      "iteration:  22290 , total_loss:  22.341881815592448\n",
      "count_amostra: 2229034\n",
      "iteration:  22320 , total_loss:  22.380217679341634\n",
      "count_amostra: 2232034\n",
      "iteration:  22350 , total_loss:  22.171264521280925\n",
      "count_amostra: 2235034\n",
      "iteration:  22380 , total_loss:  22.23332748413086\n",
      "count_amostra: 2238034\n",
      "iteration:  22410 , total_loss:  22.465710894266763\n",
      "count_amostra: 2241034\n",
      "iteration:  22440 , total_loss:  22.444137001037596\n",
      "count_amostra: 2244034\n",
      "iteration:  22470 , total_loss:  22.36235809326172\n",
      "count_amostra: 2247034\n",
      "iteration:  22500 , total_loss:  22.61084575653076\n",
      "count_amostra: 2250034\n",
      "iteration:  22530 , total_loss:  22.201801808675132\n",
      "count_amostra: 2253034\n",
      "iteration:  22560 , total_loss:  22.37348601023356\n",
      "count_amostra: 2256034\n",
      "iteration:  22590 , total_loss:  22.597476514180503\n",
      "count_amostra: 2259034\n",
      "iteration:  22620 , total_loss:  22.361165936787923\n",
      "count_amostra: 2262034\n",
      "iteration:  22650 , total_loss:  22.414731407165526\n",
      "count_amostra: 2265034\n",
      "iteration:  22680 , total_loss:  22.39583174387614\n",
      "count_amostra: 2268034\n",
      "iteration:  22710 , total_loss:  22.497255325317383\n",
      "count_amostra: 2271034\n",
      "iteration:  22740 , total_loss:  22.36595090230306\n",
      "count_amostra: 2274034\n",
      "iteration:  22770 , total_loss:  22.83277619679769\n",
      "count_amostra: 2277034\n",
      "iteration:  22800 , total_loss:  22.577717336018882\n",
      "count_amostra: 2280034\n",
      "iteration:  22830 , total_loss:  22.41334883371989\n",
      "count_amostra: 2283034\n",
      "iteration:  22860 , total_loss:  22.229166730244955\n",
      "count_amostra: 2286034\n",
      "iteration:  22890 , total_loss:  22.42735989888509\n",
      "count_amostra: 2289034\n",
      "iteration:  22920 , total_loss:  22.522972869873048\n",
      "count_amostra: 2292034\n",
      "iteration:  22950 , total_loss:  22.143439292907715\n",
      "count_amostra: 2295034\n",
      "iteration:  22980 , total_loss:  22.297790908813475\n",
      "count_amostra: 2298034\n",
      "iteration:  23010 , total_loss:  22.64510300954183\n",
      "count_amostra: 2301034\n",
      "iteration:  23040 , total_loss:  22.48598658243815\n",
      "count_amostra: 2304034\n",
      "iteration:  23070 , total_loss:  22.369329071044923\n",
      "count_amostra: 2307034\n",
      "iteration:  23100 , total_loss:  22.30300350189209\n",
      "count_amostra: 2310034\n",
      "iteration:  23130 , total_loss:  22.084782727559407\n",
      "count_amostra: 2313034\n",
      "iteration:  23160 , total_loss:  22.272927029927573\n",
      "count_amostra: 2316034\n",
      "iteration:  23190 , total_loss:  22.356363932291668\n",
      "count_amostra: 2319034\n",
      "iteration:  23220 , total_loss:  22.48267765045166\n",
      "count_amostra: 2322034\n",
      "iteration:  23250 , total_loss:  22.092295455932618\n",
      "count_amostra: 2325034\n",
      "iteration:  23280 , total_loss:  22.135548273722332\n",
      "count_amostra: 2328034\n",
      "iteration:  23310 , total_loss:  22.574564361572264\n",
      "count_amostra: 2331034\n",
      "iteration:  23340 , total_loss:  21.905569076538086\n",
      "count_amostra: 2334034\n",
      "iteration:  23370 , total_loss:  22.262847836812337\n",
      "count_amostra: 2337034\n",
      "iteration:  23400 , total_loss:  22.234830156962076\n",
      "count_amostra: 2340034\n",
      "iteration:  23430 , total_loss:  22.466881942749023\n",
      "count_amostra: 2343034\n",
      "iteration:  23460 , total_loss:  22.19617748260498\n",
      "count_amostra: 2346034\n",
      "iteration:  23490 , total_loss:  22.199849955240886\n",
      "count_amostra: 2349034\n",
      "iteration:  23520 , total_loss:  22.768385950724284\n",
      "count_amostra: 2352034\n",
      "iteration:  23550 , total_loss:  22.492731285095214\n",
      "count_amostra: 2355034\n",
      "iteration:  23580 , total_loss:  21.93236192067464\n",
      "count_amostra: 2358034\n",
      "iteration:  23610 , total_loss:  22.38334929148356\n",
      "count_amostra: 2361034\n",
      "iteration:  23640 , total_loss:  22.04158986409505\n",
      "count_amostra: 2364034\n",
      "iteration:  23670 , total_loss:  22.148580042521157\n",
      "count_amostra: 2367034\n",
      "iteration:  23700 , total_loss:  22.277464612325033\n",
      "count_amostra: 2370034\n",
      "iteration:  23730 , total_loss:  22.055644734700522\n",
      "count_amostra: 2373034\n",
      "iteration:  23760 , total_loss:  22.643421745300294\n",
      "count_amostra: 2376034\n",
      "iteration:  23790 , total_loss:  22.063317743937173\n",
      "count_amostra: 2379034\n",
      "iteration:  23820 , total_loss:  21.930450757344563\n",
      "count_amostra: 2382034\n",
      "iteration:  23850 , total_loss:  22.24623826344808\n",
      "count_amostra: 2385034\n",
      "iteration:  23880 , total_loss:  22.51703866322835\n",
      "count_amostra: 2388034\n",
      "iteration:  23910 , total_loss:  22.756015078226724\n",
      "count_amostra: 2391034\n",
      "iteration:  23940 , total_loss:  22.360911178588868\n",
      "count_amostra: 2394034\n",
      "iteration:  23970 , total_loss:  22.667812538146972\n",
      "count_amostra: 2397034\n",
      "iteration:  24000 , total_loss:  22.448908615112305\n",
      "count_amostra: 2400034\n",
      "iteration:  24030 , total_loss:  22.210055923461915\n",
      "count_amostra: 2403034\n",
      "iteration:  24060 , total_loss:  22.291649500528973\n",
      "count_amostra: 2406034\n",
      "iteration:  24090 , total_loss:  21.938513247172036\n",
      "count_amostra: 2409034\n",
      "iteration:  24120 , total_loss:  22.336710611979168\n",
      "count_amostra: 2412034\n",
      "iteration:  24150 , total_loss:  21.989490381876628\n",
      "count_amostra: 2415034\n",
      "iteration:  24180 , total_loss:  22.082153256734212\n",
      "count_amostra: 2418034\n",
      "iteration:  24210 , total_loss:  22.486349868774415\n",
      "count_amostra: 2421034\n",
      "iteration:  24240 , total_loss:  22.209850947062176\n",
      "count_amostra: 2424034\n",
      "iteration:  24270 , total_loss:  22.432944043477377\n",
      "count_amostra: 2427034\n",
      "iteration:  24300 , total_loss:  22.257870038350422\n",
      "count_amostra: 2430034\n",
      "iteration:  24330 , total_loss:  22.39963461558024\n",
      "count_amostra: 2433034\n",
      "iteration:  24360 , total_loss:  22.0089516321818\n",
      "count_amostra: 2436034\n",
      "iteration:  24390 , total_loss:  22.02375405629476\n",
      "count_amostra: 2439034\n",
      "iteration:  24420 , total_loss:  22.4036345799764\n",
      "count_amostra: 2442034\n",
      "iteration:  24450 , total_loss:  21.986410331726074\n",
      "count_amostra: 2445034\n",
      "iteration:  24480 , total_loss:  22.173077138264976\n",
      "count_amostra: 2448034\n",
      "iteration:  24510 , total_loss:  22.512815221150717\n",
      "count_amostra: 2451034\n",
      "iteration:  24540 , total_loss:  22.07918186187744\n",
      "count_amostra: 2454034\n",
      "iteration:  24570 , total_loss:  22.217248280843098\n",
      "count_amostra: 2457034\n",
      "iteration:  24600 , total_loss:  22.225971539815266\n",
      "count_amostra: 2460034\n",
      "iteration:  24630 , total_loss:  22.335030364990235\n",
      "count_amostra: 2463034\n",
      "iteration:  24660 , total_loss:  22.01058184305827\n",
      "count_amostra: 2466034\n",
      "iteration:  24690 , total_loss:  22.19975299835205\n",
      "count_amostra: 2469034\n",
      "iteration:  24720 , total_loss:  22.312376085917155\n",
      "count_amostra: 2472034\n",
      "iteration:  24750 , total_loss:  22.38130963643392\n",
      "count_amostra: 2475034\n",
      "iteration:  24780 , total_loss:  22.12839698791504\n",
      "count_amostra: 2478034\n",
      "iteration:  24810 , total_loss:  22.402411270141602\n",
      "count_amostra: 2481034\n",
      "iteration:  24840 , total_loss:  22.319897651672363\n",
      "count_amostra: 2484034\n",
      "iteration:  24870 , total_loss:  22.461271540323892\n",
      "count_amostra: 2487034\n",
      "iteration:  24900 , total_loss:  22.39370320638021\n",
      "count_amostra: 2490034\n",
      "iteration:  24930 , total_loss:  22.06913038889567\n",
      "count_amostra: 2493034\n",
      "iteration:  24960 , total_loss:  22.338369941711427\n",
      "count_amostra: 2496034\n",
      "iteration:  24990 , total_loss:  22.455514081319173\n",
      "count_amostra: 2499034\n",
      "iteration:  25020 , total_loss:  22.02767251332601\n",
      "count_amostra: 2502034\n",
      "iteration:  25050 , total_loss:  22.263359705607098\n",
      "count_amostra: 2505034\n",
      "iteration:  25080 , total_loss:  22.15617478688558\n",
      "count_amostra: 2508034\n",
      "iteration:  25110 , total_loss:  22.646430905659994\n",
      "count_amostra: 2511034\n",
      "iteration:  25140 , total_loss:  22.071633911132814\n",
      "count_amostra: 2514034\n",
      "iteration:  25170 , total_loss:  22.066349093119303\n",
      "count_amostra: 2517034\n",
      "iteration:  25200 , total_loss:  22.229171180725096\n",
      "count_amostra: 2520034\n",
      "iteration:  25230 , total_loss:  22.415745290120444\n",
      "count_amostra: 2523034\n",
      "iteration:  25260 , total_loss:  22.190123558044434\n",
      "count_amostra: 2526034\n",
      "iteration:  25290 , total_loss:  22.383398628234865\n",
      "count_amostra: 2529034\n",
      "iteration:  25320 , total_loss:  22.206630261739097\n",
      "count_amostra: 2532034\n",
      "iteration:  25350 , total_loss:  22.241135279337566\n",
      "count_amostra: 2535034\n",
      "iteration:  25380 , total_loss:  22.18088239034017\n",
      "count_amostra: 2538034\n",
      "iteration:  25410 , total_loss:  22.58068758646647\n",
      "count_amostra: 2541034\n",
      "iteration:  25440 , total_loss:  22.50448195139567\n",
      "count_amostra: 2544034\n",
      "iteration:  25470 , total_loss:  22.256729634602866\n",
      "count_amostra: 2547034\n",
      "iteration:  25500 , total_loss:  22.15725326538086\n",
      "count_amostra: 2550034\n",
      "iteration:  25530 , total_loss:  22.13436082204183\n",
      "count_amostra: 2553034\n",
      "iteration:  25560 , total_loss:  22.409755643208822\n",
      "count_amostra: 2556034\n",
      "iteration:  25590 , total_loss:  22.195348993937174\n",
      "count_amostra: 2559034\n",
      "iteration:  25620 , total_loss:  21.76945972442627\n",
      "count_amostra: 2562034\n",
      "iteration:  25650 , total_loss:  22.306797218322753\n",
      "count_amostra: 2565034\n",
      "iteration:  25680 , total_loss:  22.34656359354655\n",
      "count_amostra: 2568034\n",
      "iteration:  25710 , total_loss:  22.14702752431234\n",
      "count_amostra: 2571034\n",
      "iteration:  25740 , total_loss:  22.550481033325195\n",
      "count_amostra: 2574034\n",
      "iteration:  25770 , total_loss:  22.113726870218912\n",
      "count_amostra: 2577034\n",
      "iteration:  25800 , total_loss:  22.060961214701333\n",
      "count_amostra: 2580034\n",
      "iteration:  25830 , total_loss:  22.130838902791343\n",
      "count_amostra: 2583034\n",
      "iteration:  25860 , total_loss:  21.868108558654786\n",
      "count_amostra: 2586034\n",
      "iteration:  25890 , total_loss:  22.118293380737306\n",
      "count_amostra: 2589034\n",
      "iteration:  25920 , total_loss:  22.066923522949217\n",
      "count_amostra: 2592034\n",
      "iteration:  25950 , total_loss:  22.305332120259603\n",
      "count_amostra: 2595034\n",
      "iteration:  25980 , total_loss:  22.055470657348632\n",
      "count_amostra: 2598034\n",
      "iteration:  26010 , total_loss:  22.021573130289713\n",
      "count_amostra: 2601034\n",
      "iteration:  26040 , total_loss:  22.083916854858398\n",
      "count_amostra: 2604034\n",
      "iteration:  26070 , total_loss:  21.879726028442384\n",
      "count_amostra: 2607034\n",
      "iteration:  26100 , total_loss:  22.532018597920736\n",
      "count_amostra: 2610034\n",
      "iteration:  26130 , total_loss:  22.287441571553547\n",
      "count_amostra: 2613034\n",
      "iteration:  26160 , total_loss:  22.06158873240153\n",
      "count_amostra: 2616034\n",
      "iteration:  26190 , total_loss:  22.462852223714194\n",
      "count_amostra: 2619034\n",
      "iteration:  26220 , total_loss:  22.110759735107422\n",
      "count_amostra: 2622034\n",
      "iteration:  26250 , total_loss:  22.260961278279623\n",
      "count_amostra: 2625034\n",
      "iteration:  26280 , total_loss:  22.072491709391276\n",
      "count_amostra: 2628034\n",
      "iteration:  26310 , total_loss:  21.97098509470622\n",
      "count_amostra: 2631034\n",
      "iteration:  26340 , total_loss:  22.283418655395508\n",
      "count_amostra: 2634034\n",
      "iteration:  26370 , total_loss:  22.06276003519694\n",
      "count_amostra: 2637034\n",
      "iteration:  26400 , total_loss:  22.35837434132894\n",
      "count_amostra: 2640034\n",
      "iteration:  26430 , total_loss:  22.469132296244304\n",
      "count_amostra: 2643034\n",
      "iteration:  26460 , total_loss:  22.002526473999023\n",
      "count_amostra: 2646034\n",
      "iteration:  26490 , total_loss:  21.66644566853841\n",
      "count_amostra: 2649034\n",
      "iteration:  26520 , total_loss:  22.343075625101726\n",
      "count_amostra: 2652034\n",
      "iteration:  26550 , total_loss:  22.066328239440917\n",
      "count_amostra: 2655034\n",
      "iteration:  26580 , total_loss:  21.81394894917806\n",
      "count_amostra: 2658034\n",
      "iteration:  26610 , total_loss:  22.009643236796062\n",
      "count_amostra: 2661034\n",
      "iteration:  26640 , total_loss:  22.253139305114747\n",
      "count_amostra: 2664034\n",
      "iteration:  26670 , total_loss:  22.0488489151001\n",
      "count_amostra: 2667034\n",
      "iteration:  26700 , total_loss:  22.145798619588216\n",
      "count_amostra: 2670034\n",
      "iteration:  26730 , total_loss:  21.978992907206216\n",
      "count_amostra: 2673034\n",
      "iteration:  26760 , total_loss:  21.892895317077638\n",
      "count_amostra: 2676034\n",
      "iteration:  26790 , total_loss:  22.057425689697265\n",
      "count_amostra: 2679034\n",
      "iteration:  26820 , total_loss:  21.987481117248535\n",
      "count_amostra: 2682034\n",
      "iteration:  26850 , total_loss:  22.071118609110513\n",
      "count_amostra: 2685034\n",
      "iteration:  26880 , total_loss:  22.328670692443847\n",
      "count_amostra: 2688034\n",
      "iteration:  26910 , total_loss:  22.492313575744628\n",
      "count_amostra: 2691034\n",
      "iteration:  26940 , total_loss:  22.309729957580565\n",
      "count_amostra: 2694034\n",
      "iteration:  26970 , total_loss:  21.946995862325032\n",
      "count_amostra: 2697034\n",
      "iteration:  27000 , total_loss:  22.261711438496906\n",
      "count_amostra: 2700034\n",
      "iteration:  27030 , total_loss:  22.083934529622397\n",
      "count_amostra: 2703034\n",
      "iteration:  27060 , total_loss:  22.346693166097005\n",
      "count_amostra: 2706034\n",
      "iteration:  27090 , total_loss:  21.95354830423991\n",
      "count_amostra: 2709034\n",
      "iteration:  27120 , total_loss:  22.234196917215982\n",
      "count_amostra: 2712034\n",
      "iteration:  27150 , total_loss:  22.054177220662435\n",
      "count_amostra: 2715034\n",
      "iteration:  27180 , total_loss:  22.189954566955567\n",
      "count_amostra: 2718034\n",
      "iteration:  27210 , total_loss:  22.15014623006185\n",
      "count_amostra: 2721034\n",
      "iteration:  27240 , total_loss:  21.681415303548178\n",
      "count_amostra: 2724034\n",
      "iteration:  27270 , total_loss:  22.482163365681966\n",
      "count_amostra: 2727034\n",
      "iteration:  27300 , total_loss:  22.156650161743165\n",
      "count_amostra: 2730034\n",
      "iteration:  27330 , total_loss:  22.254133224487305\n",
      "count_amostra: 2733034\n",
      "iteration:  27360 , total_loss:  22.1265406926473\n",
      "count_amostra: 2736034\n",
      "iteration:  27390 , total_loss:  22.00566711425781\n",
      "count_amostra: 2739034\n",
      "iteration:  27420 , total_loss:  22.487801615397135\n",
      "count_amostra: 2742034\n",
      "iteration:  27450 , total_loss:  21.999958419799803\n",
      "count_amostra: 2745034\n",
      "iteration:  27480 , total_loss:  22.02157300313314\n",
      "count_amostra: 2748034\n",
      "iteration:  27510 , total_loss:  22.103285535176596\n",
      "count_amostra: 2751034\n",
      "iteration:  27540 , total_loss:  22.25068645477295\n",
      "count_amostra: 2754034\n",
      "iteration:  27570 , total_loss:  21.951734479268392\n",
      "count_amostra: 2757034\n",
      "iteration:  27600 , total_loss:  21.603752772013348\n",
      "count_amostra: 2760034\n",
      "iteration:  27630 , total_loss:  21.941676584879556\n",
      "count_amostra: 2763034\n",
      "iteration:  27660 , total_loss:  21.972448094685873\n",
      "count_amostra: 2766034\n",
      "iteration:  27690 , total_loss:  21.82459201812744\n",
      "count_amostra: 2769034\n",
      "iteration:  27720 , total_loss:  21.935077412923178\n",
      "count_amostra: 2772034\n",
      "iteration:  27750 , total_loss:  22.000172805786132\n",
      "count_amostra: 2775034\n",
      "iteration:  27780 , total_loss:  21.895688565572105\n",
      "count_amostra: 2778034\n",
      "iteration:  27810 , total_loss:  22.02227191925049\n",
      "count_amostra: 2781034\n",
      "iteration:  27840 , total_loss:  22.04498373667399\n",
      "count_amostra: 2784034\n",
      "iteration:  27870 , total_loss:  22.24708302815755\n",
      "count_amostra: 2787034\n",
      "iteration:  27900 , total_loss:  22.4755189259847\n",
      "count_amostra: 2790034\n",
      "iteration:  27930 , total_loss:  22.159611447652182\n",
      "count_amostra: 2793034\n",
      "iteration:  27960 , total_loss:  22.08457419077555\n",
      "count_amostra: 2796034\n",
      "iteration:  27990 , total_loss:  22.161058616638183\n",
      "count_amostra: 2799034\n",
      "iteration:  28020 , total_loss:  22.024708875020345\n",
      "count_amostra: 2802034\n",
      "iteration:  28050 , total_loss:  22.148387273152668\n",
      "count_amostra: 2805034\n",
      "iteration:  28080 , total_loss:  22.064822324117024\n",
      "count_amostra: 2808034\n",
      "iteration:  28110 , total_loss:  22.076116371154786\n",
      "count_amostra: 2811034\n",
      "iteration:  28140 , total_loss:  21.902886962890626\n",
      "count_amostra: 2814034\n",
      "iteration:  28170 , total_loss:  22.1626438776652\n",
      "count_amostra: 2817034\n",
      "iteration:  28200 , total_loss:  22.126621691385903\n",
      "count_amostra: 2820034\n",
      "iteration:  28230 , total_loss:  22.210111300150555\n",
      "count_amostra: 2823034\n",
      "iteration:  28260 , total_loss:  22.02423686981201\n",
      "count_amostra: 2826034\n",
      "iteration:  28290 , total_loss:  21.96695302327474\n",
      "count_amostra: 2829034\n",
      "iteration:  28320 , total_loss:  22.066459719340006\n",
      "count_amostra: 2832034\n",
      "iteration:  28350 , total_loss:  22.068608729044595\n",
      "count_amostra: 2835034\n",
      "iteration:  28380 , total_loss:  21.914798927307128\n",
      "count_amostra: 2838034\n",
      "iteration:  28410 , total_loss:  22.086626116434733\n",
      "count_amostra: 2841034\n",
      "iteration:  28440 , total_loss:  21.593984349568686\n",
      "count_amostra: 2844034\n",
      "iteration:  28470 , total_loss:  22.059974225362144\n",
      "count_amostra: 2847034\n",
      "iteration:  28500 , total_loss:  22.174424171447754\n",
      "count_amostra: 2850034\n",
      "iteration:  28530 , total_loss:  21.856497891743977\n",
      "count_amostra: 2853034\n",
      "iteration:  28560 , total_loss:  22.11841894785563\n",
      "count_amostra: 2856034\n",
      "iteration:  28590 , total_loss:  22.106072235107423\n",
      "count_amostra: 2859034\n",
      "iteration:  28620 , total_loss:  21.742725308736166\n",
      "count_amostra: 2862034\n",
      "iteration:  28650 , total_loss:  21.982607714335124\n",
      "count_amostra: 2865034\n",
      "iteration:  28680 , total_loss:  22.238748423258464\n",
      "count_amostra: 2868034\n",
      "iteration:  28710 , total_loss:  21.85167465209961\n",
      "count_amostra: 2871034\n",
      "iteration:  28740 , total_loss:  22.035143343607583\n",
      "count_amostra: 2874034\n",
      "iteration:  28770 , total_loss:  21.942917251586913\n",
      "count_amostra: 2877034\n",
      "iteration:  28800 , total_loss:  22.191975529988607\n",
      "count_amostra: 2880034\n",
      "iteration:  28830 , total_loss:  21.808639017740884\n",
      "count_amostra: 2883034\n",
      "iteration:  28860 , total_loss:  22.033689371744792\n",
      "count_amostra: 2886034\n",
      "iteration:  28890 , total_loss:  22.03449058532715\n",
      "count_amostra: 2889034\n",
      "iteration:  28920 , total_loss:  22.34321854909261\n",
      "count_amostra: 2892034\n",
      "iteration:  28950 , total_loss:  22.233271980285643\n",
      "count_amostra: 2895034\n",
      "iteration:  28980 , total_loss:  21.932010078430174\n",
      "count_amostra: 2898034\n",
      "iteration:  29010 , total_loss:  22.066469637552895\n",
      "count_amostra: 2901034\n",
      "iteration:  29040 , total_loss:  21.81767152150472\n",
      "count_amostra: 2904034\n",
      "iteration:  29070 , total_loss:  22.03987922668457\n",
      "count_amostra: 2907034\n",
      "iteration:  29100 , total_loss:  21.929920959472657\n",
      "count_amostra: 2910034\n",
      "iteration:  29130 , total_loss:  22.05668067932129\n",
      "count_amostra: 2913034\n",
      "iteration:  29160 , total_loss:  22.235885429382325\n",
      "count_amostra: 2916034\n",
      "iteration:  29190 , total_loss:  22.22814521789551\n",
      "count_amostra: 2919034\n",
      "iteration:  29220 , total_loss:  21.647708574930828\n",
      "count_amostra: 2922034\n",
      "iteration:  29250 , total_loss:  21.778781572977703\n",
      "count_amostra: 2925034\n",
      "iteration:  29280 , total_loss:  22.198293177286782\n",
      "count_amostra: 2928034\n",
      "iteration:  29310 , total_loss:  21.955821482340493\n",
      "count_amostra: 2931034\n",
      "iteration:  29340 , total_loss:  22.050577799479168\n",
      "count_amostra: 2934034\n",
      "iteration:  29370 , total_loss:  22.24325033823649\n",
      "count_amostra: 2937034\n",
      "iteration:  29400 , total_loss:  22.128715070088706\n",
      "count_amostra: 2940034\n",
      "iteration:  29430 , total_loss:  22.427527554829915\n",
      "count_amostra: 2943034\n",
      "iteration:  29460 , total_loss:  21.975603357950845\n",
      "count_amostra: 2946034\n",
      "iteration:  29490 , total_loss:  22.202307573954265\n",
      "count_amostra: 2949034\n",
      "iteration:  29520 , total_loss:  22.01141357421875\n",
      "count_amostra: 2952034\n",
      "iteration:  29550 , total_loss:  22.2015661239624\n",
      "count_amostra: 2955034\n",
      "iteration:  29580 , total_loss:  21.7331262588501\n",
      "count_amostra: 2958034\n",
      "iteration:  29610 , total_loss:  21.75602003733317\n",
      "count_amostra: 2961034\n",
      "iteration:  29640 , total_loss:  22.406241035461427\n",
      "count_amostra: 2964034\n",
      "iteration:  29670 , total_loss:  21.890383211771645\n",
      "count_amostra: 2967034\n",
      "iteration:  29700 , total_loss:  21.903723780314127\n",
      "count_amostra: 2970034\n",
      "iteration:  29730 , total_loss:  21.968829981486003\n",
      "count_amostra: 2973034\n",
      "iteration:  29760 , total_loss:  22.219263140360514\n",
      "count_amostra: 2976034\n",
      "iteration:  29790 , total_loss:  21.95378131866455\n",
      "count_amostra: 2979034\n",
      "iteration:  29820 , total_loss:  22.009972254435223\n",
      "count_amostra: 2982034\n",
      "iteration:  29850 , total_loss:  21.571246910095216\n",
      "count_amostra: 2985034\n",
      "iteration:  29880 , total_loss:  21.723538398742676\n",
      "count_amostra: 2988034\n",
      "iteration:  29910 , total_loss:  21.908609771728514\n",
      "count_amostra: 2991034\n",
      "iteration:  29940 , total_loss:  22.20681355794271\n",
      "count_amostra: 2994034\n",
      "iteration:  29970 , total_loss:  21.63473262786865\n",
      "count_amostra: 2997034\n",
      "iteration:  30000 , total_loss:  21.877094586690266\n",
      "count_amostra: 3000034\n",
      "iteration:  30030 , total_loss:  21.873352750142416\n",
      "count_amostra: 3003034\n",
      "iteration:  30060 , total_loss:  22.07468401590983\n",
      "count_amostra: 3006034\n",
      "iteration:  30090 , total_loss:  21.772663815816244\n",
      "count_amostra: 3009034\n",
      "iteration:  30120 , total_loss:  21.85994555155436\n",
      "count_amostra: 3012034\n",
      "iteration:  30150 , total_loss:  21.630006980895995\n",
      "count_amostra: 3015034\n",
      "iteration:  30180 , total_loss:  21.70578409830729\n",
      "count_amostra: 3018034\n",
      "iteration:  30210 , total_loss:  22.150471496582032\n",
      "count_amostra: 3021034\n",
      "iteration:  30240 , total_loss:  21.93814500172933\n",
      "count_amostra: 3024034\n",
      "iteration:  30270 , total_loss:  21.819082514444986\n",
      "count_amostra: 3027034\n",
      "iteration:  30300 , total_loss:  21.952787844340005\n",
      "count_amostra: 3030034\n",
      "iteration:  30330 , total_loss:  21.978131993611655\n",
      "count_amostra: 3033034\n",
      "iteration:  30360 , total_loss:  21.86833356221517\n",
      "count_amostra: 3036034\n",
      "iteration:  30390 , total_loss:  22.047849146525063\n",
      "count_amostra: 3039034\n",
      "iteration:  30420 , total_loss:  22.31116091410319\n",
      "count_amostra: 3042034\n",
      "iteration:  30450 , total_loss:  22.399590301513673\n",
      "count_amostra: 3045034\n",
      "iteration:  30480 , total_loss:  22.091571807861328\n",
      "count_amostra: 3048034\n",
      "iteration:  30510 , total_loss:  22.052548980712892\n",
      "count_amostra: 3051034\n",
      "iteration:  30540 , total_loss:  21.876979700724284\n",
      "count_amostra: 3054034\n",
      "iteration:  30570 , total_loss:  22.051225662231445\n",
      "count_amostra: 3057034\n",
      "iteration:  30600 , total_loss:  21.648670959472657\n",
      "count_amostra: 3060034\n",
      "iteration:  30630 , total_loss:  21.841778246561685\n",
      "count_amostra: 3063034\n",
      "iteration:  30660 , total_loss:  22.297735977172852\n",
      "count_amostra: 3066034\n",
      "iteration:  30690 , total_loss:  21.845390192667644\n",
      "count_amostra: 3069034\n",
      "iteration:  30720 , total_loss:  22.40806573232015\n",
      "count_amostra: 3072034\n",
      "iteration:  30750 , total_loss:  22.126295280456542\n",
      "count_amostra: 3075034\n",
      "iteration:  30780 , total_loss:  22.068683052062987\n",
      "count_amostra: 3078034\n",
      "iteration:  30810 , total_loss:  21.40288054148356\n",
      "count_amostra: 3081034\n",
      "iteration:  30840 , total_loss:  21.822698656717936\n",
      "count_amostra: 3084034\n",
      "iteration:  30870 , total_loss:  22.15486895243327\n",
      "count_amostra: 3087034\n",
      "iteration:  30900 , total_loss:  21.83810838063558\n",
      "count_amostra: 3090034\n",
      "iteration:  30930 , total_loss:  22.01067237854004\n",
      "count_amostra: 3093034\n",
      "iteration:  30960 , total_loss:  21.834923362731935\n",
      "count_amostra: 3096034\n",
      "iteration:  30990 , total_loss:  21.59502207438151\n",
      "count_amostra: 3099034\n",
      "iteration:  31020 , total_loss:  22.054430770874024\n",
      "count_amostra: 3102034\n",
      "iteration:  31050 , total_loss:  21.795142046610515\n",
      "count_amostra: 3105034\n",
      "iteration:  31080 , total_loss:  22.037893613179524\n",
      "count_amostra: 3108034\n",
      "iteration:  31110 , total_loss:  21.92712624867757\n",
      "count_amostra: 3111034\n",
      "iteration:  31140 , total_loss:  22.125467109680176\n",
      "count_amostra: 3114034\n",
      "iteration:  31170 , total_loss:  21.92064208984375\n",
      "count_amostra: 3117034\n",
      "iteration:  31200 , total_loss:  21.89331480662028\n",
      "count_amostra: 3120034\n",
      "iteration:  31230 , total_loss:  22.317740567525227\n",
      "count_amostra: 3123034\n",
      "iteration:  31260 , total_loss:  22.210119819641115\n",
      "count_amostra: 3126034\n",
      "iteration:  31290 , total_loss:  22.103724479675293\n",
      "count_amostra: 3129034\n",
      "iteration:  31320 , total_loss:  22.147774314880373\n",
      "count_amostra: 3132034\n",
      "iteration:  31350 , total_loss:  22.02008539835612\n",
      "count_amostra: 3135034\n",
      "iteration:  31380 , total_loss:  21.650917116800944\n",
      "count_amostra: 3138034\n",
      "iteration:  31410 , total_loss:  22.01418228149414\n",
      "count_amostra: 3141034\n",
      "iteration:  31440 , total_loss:  22.323077774047853\n",
      "count_amostra: 3144034\n",
      "iteration:  31470 , total_loss:  21.751468976338703\n",
      "count_amostra: 3147034\n",
      "iteration:  31500 , total_loss:  21.98845717112223\n",
      "count_amostra: 3150034\n",
      "iteration:  31530 , total_loss:  21.715164756774904\n",
      "count_amostra: 3153034\n",
      "iteration:  31560 , total_loss:  21.769228998819987\n",
      "count_amostra: 3156034\n",
      "iteration:  31590 , total_loss:  22.232061513264973\n",
      "count_amostra: 3159034\n",
      "iteration:  31620 , total_loss:  21.732922490437826\n",
      "count_amostra: 3162034\n",
      "iteration:  31650 , total_loss:  22.11865037282308\n",
      "count_amostra: 3165034\n",
      "iteration:  31680 , total_loss:  21.65409711201986\n",
      "count_amostra: 3168034\n",
      "iteration:  31710 , total_loss:  22.250747680664062\n",
      "count_amostra: 3171034\n",
      "iteration:  31740 , total_loss:  22.51797129313151\n",
      "count_amostra: 3174034\n",
      "iteration:  31770 , total_loss:  21.68785228729248\n",
      "count_amostra: 3177034\n",
      "iteration:  31800 , total_loss:  21.958278592427572\n",
      "count_amostra: 3180034\n",
      "iteration:  31830 , total_loss:  21.6900151570638\n",
      "count_amostra: 3183034\n",
      "iteration:  31860 , total_loss:  22.01901194254557\n",
      "count_amostra: 3186034\n",
      "iteration:  31890 , total_loss:  21.963214937845866\n",
      "count_amostra: 3189034\n",
      "iteration:  31920 , total_loss:  22.022978274027505\n",
      "count_amostra: 3192034\n",
      "iteration:  31950 , total_loss:  21.907288869222004\n",
      "count_amostra: 3195034\n",
      "iteration:  31980 , total_loss:  21.877388827006023\n",
      "count_amostra: 3198034\n",
      "iteration:  32010 , total_loss:  22.06903991699219\n",
      "count_amostra: 3201034\n",
      "iteration:  32040 , total_loss:  22.228216552734374\n",
      "count_amostra: 3204034\n",
      "iteration:  32070 , total_loss:  22.065058453877768\n",
      "count_amostra: 3207034\n",
      "iteration:  32100 , total_loss:  21.66856435139974\n",
      "count_amostra: 3210034\n",
      "iteration:  32130 , total_loss:  22.262078603108723\n",
      "count_amostra: 3213034\n",
      "iteration:  32160 , total_loss:  22.13498134613037\n",
      "count_amostra: 3216034\n",
      "iteration:  32190 , total_loss:  21.631357765197755\n",
      "count_amostra: 3219034\n",
      "iteration:  32220 , total_loss:  22.05527318318685\n",
      "count_amostra: 3222034\n",
      "iteration:  32250 , total_loss:  22.152817026774088\n",
      "count_amostra: 3225034\n",
      "iteration:  32280 , total_loss:  21.777231152852377\n",
      "count_amostra: 3228034\n",
      "iteration:  32310 , total_loss:  22.032683436075846\n",
      "count_amostra: 3231034\n",
      "iteration:  32340 , total_loss:  21.70680872599284\n",
      "count_amostra: 3234034\n",
      "iteration:  32370 , total_loss:  21.896998850504556\n",
      "count_amostra: 3237034\n",
      "iteration:  32400 , total_loss:  22.288750394185385\n",
      "count_amostra: 3240034\n",
      "iteration:  32430 , total_loss:  22.05271987915039\n",
      "count_amostra: 3243034\n",
      "iteration:  32460 , total_loss:  21.75420862833659\n",
      "count_amostra: 3246034\n",
      "iteration:  32490 , total_loss:  21.874671109517415\n",
      "count_amostra: 3249034\n",
      "iteration:  32520 , total_loss:  22.38211008707682\n",
      "count_amostra: 3252034\n",
      "iteration:  32550 , total_loss:  21.869205220540366\n",
      "count_amostra: 3255034\n",
      "iteration:  32580 , total_loss:  22.021648788452147\n",
      "count_amostra: 3258034\n",
      "iteration:  32610 , total_loss:  21.558735275268553\n",
      "count_amostra: 3261034\n",
      "iteration:  32640 , total_loss:  21.80511817932129\n",
      "count_amostra: 3264034\n",
      "iteration:  32670 , total_loss:  22.021979649861652\n",
      "count_amostra: 3267034\n",
      "iteration:  32700 , total_loss:  21.852613576253255\n",
      "count_amostra: 3270034\n",
      "iteration:  32730 , total_loss:  21.8620704015096\n",
      "count_amostra: 3273034\n",
      "iteration:  32760 , total_loss:  21.896992365519207\n",
      "count_amostra: 3276034\n",
      "iteration:  32790 , total_loss:  22.145707511901854\n",
      "count_amostra: 3279034\n",
      "iteration:  32820 , total_loss:  21.989467811584472\n",
      "count_amostra: 3282034\n",
      "iteration:  32850 , total_loss:  21.91337121327718\n",
      "count_amostra: 3285034\n",
      "iteration:  32880 , total_loss:  21.907592837015788\n",
      "count_amostra: 3288034\n",
      "iteration:  32910 , total_loss:  21.69754556020101\n",
      "count_amostra: 3291034\n",
      "iteration:  32940 , total_loss:  21.907186953226724\n",
      "count_amostra: 3294034\n",
      "iteration:  32970 , total_loss:  21.747776540120444\n",
      "count_amostra: 3297034\n",
      "iteration:  33000 , total_loss:  22.02391268412272\n",
      "count_amostra: 3300034\n",
      "iteration:  33030 , total_loss:  21.872334289550782\n",
      "count_amostra: 3303034\n",
      "iteration:  33060 , total_loss:  22.05149580637614\n",
      "count_amostra: 3306034\n",
      "iteration:  33090 , total_loss:  21.747810045878094\n",
      "count_amostra: 3309034\n",
      "iteration:  33120 , total_loss:  21.893913650512694\n",
      "count_amostra: 3312034\n",
      "iteration:  33150 , total_loss:  21.81378084818522\n",
      "count_amostra: 3315034\n",
      "iteration:  33180 , total_loss:  22.001919492085776\n",
      "count_amostra: 3318034\n",
      "iteration:  33210 , total_loss:  21.90663725535075\n",
      "count_amostra: 3321034\n",
      "iteration:  33240 , total_loss:  21.87626724243164\n",
      "count_amostra: 3324034\n",
      "iteration:  33270 , total_loss:  21.76304753621419\n",
      "count_amostra: 3327034\n",
      "iteration:  33300 , total_loss:  21.904536056518555\n",
      "count_amostra: 3330034\n",
      "iteration:  33330 , total_loss:  21.729789543151856\n",
      "count_amostra: 3333034\n",
      "iteration:  33360 , total_loss:  21.934245681762697\n",
      "count_amostra: 3336034\n",
      "iteration:  33390 , total_loss:  21.880576833089194\n",
      "count_amostra: 3339034\n",
      "iteration:  33420 , total_loss:  22.23869654337565\n",
      "count_amostra: 3342034\n",
      "iteration:  33450 , total_loss:  21.633938026428222\n",
      "count_amostra: 3345034\n",
      "iteration:  33480 , total_loss:  21.720734469095866\n",
      "count_amostra: 3348034\n",
      "iteration:  33510 , total_loss:  21.953090985616047\n",
      "count_amostra: 3351034\n",
      "iteration:  33540 , total_loss:  22.04329147338867\n",
      "count_amostra: 3354034\n",
      "iteration:  33570 , total_loss:  21.919238917032878\n",
      "count_amostra: 3357034\n",
      "iteration:  33600 , total_loss:  21.862609545389812\n",
      "count_amostra: 3360034\n",
      "iteration:  33630 , total_loss:  21.62492758433024\n",
      "count_amostra: 3363034\n",
      "iteration:  33660 , total_loss:  21.735165278116863\n",
      "count_amostra: 3366034\n",
      "iteration:  33690 , total_loss:  21.885940996805825\n",
      "count_amostra: 3369034\n",
      "iteration:  33720 , total_loss:  22.354426193237305\n",
      "count_amostra: 3372034\n",
      "iteration:  33750 , total_loss:  21.96819636027018\n",
      "count_amostra: 3375034\n",
      "iteration:  33780 , total_loss:  21.872010548909504\n",
      "count_amostra: 3378034\n",
      "iteration:  33810 , total_loss:  21.78994140625\n",
      "count_amostra: 3381034\n",
      "iteration:  33840 , total_loss:  21.908060137430827\n",
      "count_amostra: 3384034\n",
      "iteration:  33870 , total_loss:  21.859034792582193\n",
      "count_amostra: 3387034\n",
      "iteration:  33900 , total_loss:  21.57651856740316\n",
      "count_amostra: 3390034\n",
      "iteration:  33930 , total_loss:  21.745278867085776\n",
      "count_amostra: 3393034\n",
      "iteration:  33960 , total_loss:  22.157775751749675\n",
      "count_amostra: 3396034\n",
      "iteration:  33990 , total_loss:  21.480551274617515\n",
      "count_amostra: 3399034\n",
      "iteration:  34020 , total_loss:  21.912060991923013\n",
      "count_amostra: 3402034\n",
      "iteration:  34050 , total_loss:  21.79851722717285\n",
      "count_amostra: 3405034\n",
      "iteration:  34080 , total_loss:  21.72184467315674\n",
      "count_amostra: 3408034\n",
      "iteration:  34110 , total_loss:  21.954217274983723\n",
      "count_amostra: 3411034\n",
      "iteration:  34140 , total_loss:  22.479868380228677\n",
      "count_amostra: 3414034\n",
      "iteration:  34170 , total_loss:  21.83711706797282\n",
      "count_amostra: 3417034\n",
      "iteration:  34200 , total_loss:  21.6604248046875\n",
      "count_amostra: 3420034\n",
      "iteration:  34230 , total_loss:  21.827139727274577\n",
      "count_amostra: 3423034\n",
      "iteration:  34260 , total_loss:  21.759325218200683\n",
      "count_amostra: 3426034\n",
      "iteration:  34290 , total_loss:  21.991197649637858\n",
      "count_amostra: 3429034\n",
      "iteration:  34320 , total_loss:  22.169359525044758\n",
      "count_amostra: 3432034\n",
      "iteration:  34350 , total_loss:  22.044352086385093\n",
      "count_amostra: 3435034\n",
      "iteration:  34380 , total_loss:  22.076994196573892\n",
      "count_amostra: 3438034\n",
      "iteration:  34410 , total_loss:  21.750735600789387\n",
      "count_amostra: 3441034\n",
      "iteration:  34440 , total_loss:  21.62133248647054\n",
      "count_amostra: 3444034\n",
      "iteration:  34470 , total_loss:  21.700162633260092\n",
      "count_amostra: 3447034\n",
      "iteration:  34500 , total_loss:  22.10654614766439\n",
      "count_amostra: 3450034\n",
      "iteration:  34530 , total_loss:  21.988358052571616\n",
      "count_amostra: 3453034\n",
      "iteration:  34560 , total_loss:  21.614010938008626\n",
      "count_amostra: 3456034\n",
      "iteration:  34590 , total_loss:  21.882062339782713\n",
      "count_amostra: 3459034\n",
      "iteration:  34620 , total_loss:  22.021992492675782\n",
      "count_amostra: 3462034\n",
      "iteration:  34650 , total_loss:  22.179696464538573\n",
      "count_amostra: 3465034\n",
      "iteration:  34680 , total_loss:  21.878282610575358\n",
      "count_amostra: 3468034\n",
      "iteration:  34710 , total_loss:  21.19668197631836\n",
      "count_amostra: 3471034\n",
      "iteration:  34740 , total_loss:  22.083911895751953\n",
      "count_amostra: 3474034\n",
      "iteration:  34770 , total_loss:  21.710598627726238\n",
      "count_amostra: 3477034\n",
      "iteration:  34800 , total_loss:  22.17192726135254\n",
      "count_amostra: 3480034\n",
      "iteration:  34830 , total_loss:  21.659606742858887\n",
      "count_amostra: 3483034\n",
      "iteration:  34860 , total_loss:  21.962592188517252\n",
      "count_amostra: 3486034\n",
      "iteration:  34890 , total_loss:  21.911136627197266\n",
      "count_amostra: 3489034\n",
      "iteration:  34920 , total_loss:  22.065828196207683\n",
      "count_amostra: 3492034\n",
      "iteration:  34950 , total_loss:  21.78867441813151\n",
      "count_amostra: 3495034\n",
      "iteration:  34980 , total_loss:  21.848787562052408\n",
      "count_amostra: 3498034\n",
      "iteration:  35010 , total_loss:  21.5651948928833\n",
      "count_amostra: 3501034\n",
      "iteration:  35040 , total_loss:  21.74320723215739\n",
      "count_amostra: 3504034\n",
      "iteration:  35070 , total_loss:  21.98746649424235\n",
      "count_amostra: 3507034\n",
      "iteration:  35100 , total_loss:  21.6758628209432\n",
      "count_amostra: 3510034\n",
      "iteration:  35130 , total_loss:  21.652655537923177\n",
      "count_amostra: 3513034\n",
      "iteration:  35160 , total_loss:  21.61894105275472\n",
      "count_amostra: 3516034\n",
      "iteration:  35190 , total_loss:  21.700306129455566\n",
      "count_amostra: 3519034\n",
      "iteration:  35220 , total_loss:  21.56069672902425\n",
      "count_amostra: 3522034\n",
      "iteration:  35250 , total_loss:  21.748881530761718\n",
      "count_amostra: 3525034\n",
      "iteration:  35280 , total_loss:  21.73648872375488\n",
      "count_amostra: 3528034\n",
      "iteration:  35310 , total_loss:  21.66048304239909\n",
      "count_amostra: 3531034\n",
      "iteration:  35340 , total_loss:  21.826634661356607\n",
      "count_amostra: 3534034\n",
      "iteration:  35370 , total_loss:  21.82170886993408\n",
      "count_amostra: 3537034\n",
      "iteration:  35400 , total_loss:  22.139525159200033\n",
      "count_amostra: 3540034\n",
      "iteration:  35430 , total_loss:  21.944384320576987\n",
      "count_amostra: 3543034\n",
      "iteration:  35460 , total_loss:  22.023185602823894\n",
      "count_amostra: 3546034\n",
      "iteration:  35490 , total_loss:  21.91185805002848\n",
      "count_amostra: 3549034\n",
      "iteration:  35520 , total_loss:  22.31280460357666\n",
      "count_amostra: 3552034\n",
      "iteration:  35550 , total_loss:  21.79523893992106\n",
      "count_amostra: 3555034\n",
      "iteration:  35580 , total_loss:  21.676342646280926\n",
      "count_amostra: 3558034\n",
      "iteration:  35610 , total_loss:  21.51588306427002\n",
      "count_amostra: 3561034\n",
      "iteration:  35640 , total_loss:  22.03194382985433\n",
      "count_amostra: 3564034\n",
      "iteration:  35670 , total_loss:  21.832853507995605\n",
      "count_amostra: 3567034\n",
      "iteration:  35700 , total_loss:  21.667057291666666\n",
      "count_amostra: 3570034\n",
      "iteration:  35730 , total_loss:  21.661433474222818\n",
      "count_amostra: 3573034\n",
      "iteration:  35760 , total_loss:  21.793618138631185\n",
      "count_amostra: 3576034\n",
      "iteration:  35790 , total_loss:  21.567282168070474\n",
      "count_amostra: 3579034\n",
      "iteration:  35820 , total_loss:  21.801356887817384\n",
      "count_amostra: 3582034\n",
      "iteration:  35850 , total_loss:  22.086721674601236\n",
      "count_amostra: 3585034\n",
      "iteration:  35880 , total_loss:  21.538306744893394\n",
      "count_amostra: 3588034\n",
      "iteration:  35910 , total_loss:  21.769260915120444\n",
      "count_amostra: 3591034\n",
      "iteration:  35940 , total_loss:  21.549640464782716\n",
      "count_amostra: 3594034\n",
      "iteration:  35970 , total_loss:  22.081086603800454\n",
      "count_amostra: 3597034\n",
      "iteration:  36000 , total_loss:  21.893495496114095\n",
      "count_amostra: 3600034\n",
      "iteration:  36030 , total_loss:  22.073644574483236\n",
      "count_amostra: 3603034\n",
      "iteration:  36060 , total_loss:  21.68446922302246\n",
      "count_amostra: 3606034\n",
      "iteration:  36090 , total_loss:  21.925577672322593\n",
      "count_amostra: 3609034\n",
      "iteration:  36120 , total_loss:  22.18570016225179\n",
      "count_amostra: 3612034\n",
      "iteration:  36150 , total_loss:  21.916618855794272\n",
      "count_amostra: 3615034\n",
      "iteration:  36180 , total_loss:  21.821829096476236\n",
      "count_amostra: 3618034\n",
      "iteration:  36210 , total_loss:  21.7696870803833\n",
      "count_amostra: 3621034\n",
      "iteration:  36240 , total_loss:  22.02504908243815\n",
      "count_amostra: 3624034\n",
      "iteration:  36270 , total_loss:  21.889133008321128\n",
      "count_amostra: 3627034\n",
      "iteration:  36300 , total_loss:  21.981551106770834\n",
      "count_amostra: 3630034\n",
      "iteration:  36330 , total_loss:  21.761406262715656\n",
      "count_amostra: 3633034\n",
      "iteration:  36360 , total_loss:  22.07547092437744\n",
      "count_amostra: 3636034\n",
      "iteration:  36390 , total_loss:  21.927594693501792\n",
      "count_amostra: 3639034\n",
      "iteration:  36420 , total_loss:  21.799945958455403\n",
      "count_amostra: 3642034\n",
      "iteration:  36450 , total_loss:  21.462739753723145\n",
      "count_amostra: 3645034\n",
      "iteration:  36480 , total_loss:  21.78440329233805\n",
      "count_amostra: 3648034\n",
      "iteration:  36510 , total_loss:  21.60293057759603\n",
      "count_amostra: 3651034\n",
      "iteration:  36540 , total_loss:  21.927084096272786\n",
      "count_amostra: 3654034\n",
      "iteration:  36570 , total_loss:  21.591272672017414\n",
      "count_amostra: 3657034\n",
      "iteration:  36600 , total_loss:  21.44380741119385\n",
      "count_amostra: 3660034\n",
      "iteration:  36630 , total_loss:  21.495359738667805\n",
      "count_amostra: 3663034\n",
      "iteration:  36660 , total_loss:  21.841425895690918\n",
      "count_amostra: 3666034\n",
      "iteration:  36690 , total_loss:  22.02364133199056\n",
      "count_amostra: 3669034\n",
      "iteration:  36720 , total_loss:  21.826425298055014\n",
      "count_amostra: 3672034\n",
      "iteration:  36750 , total_loss:  21.498711331685385\n",
      "count_amostra: 3675034\n",
      "iteration:  36780 , total_loss:  21.655447642008465\n",
      "count_amostra: 3678034\n",
      "iteration:  36810 , total_loss:  21.79193852742513\n",
      "count_amostra: 3681034\n",
      "iteration:  36840 , total_loss:  21.891846084594725\n",
      "count_amostra: 3684034\n",
      "iteration:  36870 , total_loss:  21.873477300008137\n",
      "count_amostra: 3687034\n",
      "iteration:  36900 , total_loss:  21.611825116475423\n",
      "count_amostra: 3690034\n",
      "iteration:  36930 , total_loss:  21.822678756713866\n",
      "count_amostra: 3693034\n",
      "iteration:  36960 , total_loss:  21.551888084411623\n",
      "count_amostra: 3696034\n",
      "iteration:  36990 , total_loss:  22.036628659566244\n",
      "count_amostra: 3699034\n",
      "iteration:  37020 , total_loss:  21.83146718343099\n",
      "count_amostra: 3702034\n",
      "iteration:  37050 , total_loss:  21.826836268107098\n",
      "count_amostra: 3705034\n",
      "iteration:  37080 , total_loss:  21.73929074605306\n",
      "count_amostra: 3708034\n",
      "iteration:  37110 , total_loss:  22.068708610534667\n",
      "count_amostra: 3711034\n",
      "iteration:  37140 , total_loss:  21.832083447774252\n",
      "count_amostra: 3714034\n",
      "iteration:  37170 , total_loss:  21.90255686442057\n",
      "count_amostra: 3717034\n",
      "iteration:  37200 , total_loss:  21.8341547648112\n",
      "count_amostra: 3720034\n",
      "iteration:  37230 , total_loss:  21.608174006144207\n",
      "count_amostra: 3723034\n",
      "iteration:  37260 , total_loss:  21.31798210144043\n",
      "count_amostra: 3726034\n",
      "iteration:  37290 , total_loss:  21.615308634440105\n",
      "count_amostra: 3729034\n",
      "iteration:  37320 , total_loss:  21.62893091837565\n",
      "count_amostra: 3732034\n",
      "iteration:  37350 , total_loss:  21.73745829264323\n",
      "count_amostra: 3735034\n",
      "iteration:  37380 , total_loss:  21.943634287516275\n",
      "count_amostra: 3738034\n",
      "iteration:  37410 , total_loss:  21.632883644104005\n",
      "count_amostra: 3741034\n",
      "iteration:  37440 , total_loss:  21.25368175506592\n",
      "count_amostra: 3744034\n",
      "iteration:  37470 , total_loss:  21.603714179992675\n",
      "count_amostra: 3747034\n",
      "iteration:  37500 , total_loss:  22.00921065012614\n",
      "count_amostra: 3750034\n",
      "iteration:  37530 , total_loss:  21.818765513102214\n",
      "count_amostra: 3753034\n",
      "iteration:  37560 , total_loss:  21.81738058725993\n",
      "count_amostra: 3756034\n",
      "iteration:  37590 , total_loss:  21.837023735046387\n",
      "count_amostra: 3759034\n",
      "iteration:  37620 , total_loss:  21.67454408009847\n",
      "count_amostra: 3762034\n",
      "iteration:  37650 , total_loss:  21.755523427327475\n",
      "count_amostra: 3765034\n",
      "iteration:  37680 , total_loss:  21.424224789937337\n",
      "count_amostra: 3768034\n",
      "iteration:  37710 , total_loss:  21.515533447265625\n",
      "count_amostra: 3771034\n",
      "iteration:  37740 , total_loss:  21.450260861714682\n",
      "count_amostra: 3774034\n",
      "iteration:  37770 , total_loss:  21.563138707478842\n",
      "count_amostra: 3777034\n",
      "iteration:  37800 , total_loss:  21.99333922068278\n",
      "count_amostra: 3780034\n",
      "iteration:  37830 , total_loss:  21.78632335662842\n",
      "count_amostra: 3783034\n",
      "iteration:  37860 , total_loss:  21.828017807006837\n",
      "count_amostra: 3786034\n",
      "iteration:  37890 , total_loss:  21.709471638997396\n",
      "count_amostra: 3789034\n",
      "iteration:  37920 , total_loss:  22.042979303995768\n",
      "count_amostra: 3792034\n",
      "iteration:  37950 , total_loss:  21.531891759236654\n",
      "count_amostra: 3795034\n",
      "iteration:  37980 , total_loss:  21.65738150278727\n",
      "count_amostra: 3798034\n",
      "iteration:  38010 , total_loss:  21.93353926340739\n",
      "count_amostra: 3801034\n",
      "iteration:  38040 , total_loss:  21.67932154337565\n",
      "count_amostra: 3804034\n",
      "iteration:  38070 , total_loss:  21.925915654500326\n",
      "count_amostra: 3807034\n",
      "iteration:  38100 , total_loss:  21.833302815755207\n",
      "count_amostra: 3810034\n",
      "iteration:  38130 , total_loss:  22.026192283630373\n",
      "count_amostra: 3813034\n",
      "iteration:  38160 , total_loss:  21.823665301005047\n",
      "count_amostra: 3816034\n",
      "iteration:  38190 , total_loss:  21.971316846211753\n",
      "count_amostra: 3819034\n",
      "iteration:  38220 , total_loss:  21.401553217569987\n",
      "count_amostra: 3822034\n",
      "iteration:  38250 , total_loss:  21.68517106374105\n",
      "count_amostra: 3825034\n",
      "iteration:  38280 , total_loss:  21.645572217305503\n",
      "count_amostra: 3828034\n",
      "iteration:  38310 , total_loss:  22.057936032613117\n",
      "count_amostra: 3831034\n",
      "iteration:  38340 , total_loss:  21.904064051310222\n",
      "count_amostra: 3834034\n",
      "iteration:  38370 , total_loss:  21.71020622253418\n",
      "count_amostra: 3837034\n",
      "iteration:  38400 , total_loss:  21.49904810587565\n",
      "count_amostra: 3840034\n",
      "iteration:  38430 , total_loss:  21.957991282145183\n",
      "count_amostra: 3843034\n",
      "iteration:  38460 , total_loss:  21.55640837351481\n",
      "count_amostra: 3846034\n",
      "iteration:  38490 , total_loss:  21.87449900309245\n",
      "count_amostra: 3849034\n",
      "iteration:  38520 , total_loss:  21.716590054829915\n",
      "count_amostra: 3852034\n",
      "iteration:  38550 , total_loss:  22.185864893595376\n",
      "count_amostra: 3855034\n",
      "iteration:  38580 , total_loss:  21.91548868815104\n",
      "count_amostra: 3858034\n",
      "iteration:  38610 , total_loss:  21.587856356302897\n",
      "count_amostra: 3861034\n",
      "iteration:  38640 , total_loss:  21.771974436442058\n",
      "count_amostra: 3864034\n",
      "iteration:  38670 , total_loss:  21.945239893595378\n",
      "count_amostra: 3867034\n",
      "iteration:  38700 , total_loss:  21.938500340779623\n",
      "count_amostra: 3870034\n",
      "iteration:  38730 , total_loss:  21.55855261484782\n",
      "count_amostra: 3873034\n",
      "iteration:  38760 , total_loss:  21.807002321879068\n",
      "count_amostra: 3876034\n",
      "iteration:  38790 , total_loss:  21.988239415486653\n",
      "count_amostra: 3879034\n",
      "iteration:  38820 , total_loss:  22.09380200703939\n",
      "count_amostra: 3882034\n",
      "iteration:  38850 , total_loss:  21.53175163269043\n",
      "count_amostra: 3885034\n",
      "iteration:  38880 , total_loss:  21.752300516764322\n",
      "count_amostra: 3888034\n",
      "iteration:  38910 , total_loss:  21.765526835123698\n",
      "count_amostra: 3891034\n",
      "iteration:  38940 , total_loss:  21.62947311401367\n",
      "count_amostra: 3894034\n",
      "iteration:  38970 , total_loss:  21.936972745259602\n",
      "count_amostra: 3897034\n",
      "iteration:  39000 , total_loss:  21.553512064615884\n",
      "count_amostra: 3900034\n",
      "iteration:  39030 , total_loss:  21.766810862223306\n",
      "count_amostra: 3903034\n",
      "iteration:  39060 , total_loss:  21.44933910369873\n",
      "count_amostra: 3906034\n",
      "iteration:  39090 , total_loss:  21.300343704223632\n",
      "count_amostra: 3909034\n",
      "iteration:  39120 , total_loss:  21.597182337443034\n",
      "count_amostra: 3912034\n",
      "iteration:  39150 , total_loss:  21.310934766133627\n",
      "count_amostra: 3915034\n",
      "iteration:  39180 , total_loss:  21.585135078430177\n",
      "count_amostra: 3918034\n",
      "iteration:  39210 , total_loss:  21.752958488464355\n",
      "count_amostra: 3921034\n",
      "iteration:  39240 , total_loss:  21.978331820170084\n",
      "count_amostra: 3924034\n",
      "iteration:  39270 , total_loss:  21.680608622233073\n",
      "count_amostra: 3927034\n",
      "iteration:  39300 , total_loss:  21.76449546813965\n",
      "count_amostra: 3930034\n",
      "iteration:  39330 , total_loss:  21.74636567433675\n",
      "count_amostra: 3933034\n",
      "iteration:  39360 , total_loss:  21.91178258260091\n",
      "count_amostra: 3936034\n",
      "iteration:  39390 , total_loss:  22.178561973571778\n",
      "count_amostra: 3939034\n",
      "iteration:  39420 , total_loss:  21.688242403666177\n",
      "count_amostra: 3942034\n",
      "iteration:  39450 , total_loss:  21.579377110799154\n",
      "count_amostra: 3945034\n",
      "iteration:  39480 , total_loss:  21.872603352864584\n",
      "count_amostra: 3948034\n",
      "iteration:  39510 , total_loss:  21.7648567199707\n",
      "count_amostra: 3951034\n",
      "iteration:  39540 , total_loss:  21.706432469685872\n",
      "count_amostra: 3954034\n",
      "iteration:  39570 , total_loss:  22.117582066853842\n",
      "count_amostra: 3957034\n",
      "iteration:  39600 , total_loss:  21.782426834106445\n",
      "count_amostra: 3960034\n",
      "iteration:  39630 , total_loss:  21.636678314208986\n",
      "count_amostra: 3963034\n",
      "iteration:  39660 , total_loss:  21.832459195454916\n",
      "count_amostra: 3966034\n",
      "iteration:  39690 , total_loss:  21.583097457885742\n",
      "count_amostra: 3969034\n",
      "iteration:  39720 , total_loss:  21.466954930623373\n",
      "count_amostra: 3972034\n",
      "iteration:  39750 , total_loss:  21.959847513834635\n",
      "count_amostra: 3975034\n",
      "iteration:  39780 , total_loss:  21.55113608042399\n",
      "count_amostra: 3978034\n",
      "iteration:  39810 , total_loss:  21.576976203918456\n",
      "count_amostra: 3981034\n",
      "iteration:  39840 , total_loss:  21.518059094746906\n",
      "count_amostra: 3984034\n",
      "iteration:  39870 , total_loss:  21.525508562723797\n",
      "count_amostra: 3987034\n",
      "iteration:  39900 , total_loss:  21.64389279683431\n",
      "count_amostra: 3990034\n",
      "iteration:  39930 , total_loss:  21.65713621775309\n",
      "count_amostra: 3993034\n",
      "iteration:  39960 , total_loss:  21.423101743062336\n",
      "count_amostra: 3996034\n",
      "iteration:  39990 , total_loss:  21.70386397043864\n",
      "count_amostra: 3999034\n",
      "iteration:  40020 , total_loss:  21.915593528747557\n",
      "count_amostra: 4002034\n",
      "iteration:  40050 , total_loss:  21.61278870900472\n",
      "count_amostra: 4005034\n",
      "iteration:  40080 , total_loss:  21.87670103708903\n",
      "count_amostra: 4008034\n",
      "iteration:  40110 , total_loss:  21.759121958414713\n",
      "count_amostra: 4011034\n",
      "iteration:  40140 , total_loss:  21.755350240071614\n",
      "count_amostra: 4014034\n",
      "iteration:  40170 , total_loss:  21.936799176534016\n",
      "count_amostra: 4017034\n",
      "iteration:  40200 , total_loss:  21.766879018147787\n",
      "count_amostra: 4020034\n",
      "iteration:  40230 , total_loss:  21.991819000244142\n",
      "count_amostra: 4023034\n",
      "iteration:  40260 , total_loss:  21.69019826253255\n",
      "count_amostra: 4026034\n",
      "iteration:  40290 , total_loss:  21.90685157775879\n",
      "count_amostra: 4029034\n",
      "iteration:  40320 , total_loss:  21.3590576171875\n",
      "count_amostra: 4032034\n",
      "iteration:  40350 , total_loss:  21.742576026916502\n",
      "count_amostra: 4035034\n",
      "iteration:  40380 , total_loss:  21.465416463216147\n",
      "count_amostra: 4038034\n",
      "iteration:  40410 , total_loss:  21.5691125869751\n",
      "count_amostra: 4041034\n",
      "iteration:  40440 , total_loss:  21.73700942993164\n",
      "count_amostra: 4044034\n",
      "iteration:  40470 , total_loss:  21.602112197875975\n",
      "count_amostra: 4047034\n",
      "iteration:  40500 , total_loss:  21.83675340016683\n",
      "count_amostra: 4050034\n",
      "iteration:  40530 , total_loss:  21.608104197184243\n",
      "count_amostra: 4053034\n",
      "iteration:  40560 , total_loss:  21.731558100382486\n",
      "count_amostra: 4056034\n",
      "iteration:  40590 , total_loss:  22.07927392323812\n",
      "count_amostra: 4059034\n",
      "iteration:  40620 , total_loss:  21.41007219950358\n",
      "count_amostra: 4062034\n",
      "iteration:  40650 , total_loss:  22.064280700683593\n",
      "count_amostra: 4065034\n",
      "iteration:  40680 , total_loss:  21.456806882222494\n",
      "count_amostra: 4068034\n",
      "iteration:  40710 , total_loss:  21.688501993815105\n",
      "count_amostra: 4071034\n",
      "iteration:  40740 , total_loss:  21.463695526123047\n",
      "count_amostra: 4074034\n",
      "iteration:  40770 , total_loss:  21.92030512491862\n",
      "count_amostra: 4077034\n",
      "iteration:  40800 , total_loss:  21.590036964416505\n",
      "count_amostra: 4080034\n",
      "iteration:  40830 , total_loss:  21.542430114746093\n",
      "count_amostra: 4083034\n",
      "iteration:  40860 , total_loss:  21.74274298350016\n",
      "count_amostra: 4086034\n",
      "iteration:  40890 , total_loss:  21.747896258036295\n",
      "count_amostra: 4089034\n",
      "iteration:  40920 , total_loss:  21.546838188171385\n",
      "count_amostra: 4092034\n",
      "iteration:  40950 , total_loss:  21.797343699137368\n",
      "count_amostra: 4095034\n",
      "iteration:  40980 , total_loss:  21.92694721221924\n",
      "count_amostra: 4098034\n",
      "iteration:  41010 , total_loss:  21.089912541707356\n",
      "count_amostra: 4101034\n",
      "iteration:  41040 , total_loss:  21.61541131337484\n",
      "count_amostra: 4104034\n",
      "iteration:  41070 , total_loss:  21.781860160827637\n",
      "count_amostra: 4107034\n",
      "iteration:  41100 , total_loss:  21.688682556152344\n",
      "count_amostra: 4110034\n",
      "iteration:  41130 , total_loss:  21.968148930867514\n",
      "count_amostra: 4113034\n",
      "iteration:  41160 , total_loss:  21.937927118937175\n",
      "count_amostra: 4116034\n",
      "iteration:  41190 , total_loss:  21.79521961212158\n",
      "count_amostra: 4119034\n",
      "iteration:  41220 , total_loss:  21.80945370992025\n",
      "count_amostra: 4122034\n",
      "iteration:  41250 , total_loss:  21.48463389078776\n",
      "count_amostra: 4125034\n",
      "iteration:  41280 , total_loss:  21.57037982940674\n",
      "count_amostra: 4128034\n",
      "iteration:  41310 , total_loss:  21.326766586303712\n",
      "count_amostra: 4131034\n",
      "iteration:  41340 , total_loss:  21.55082950592041\n",
      "count_amostra: 4134034\n",
      "iteration:  41370 , total_loss:  21.529471079508465\n",
      "count_amostra: 4137034\n",
      "iteration:  41400 , total_loss:  21.566512807210287\n",
      "count_amostra: 4140034\n",
      "iteration:  41430 , total_loss:  21.44688866933187\n",
      "count_amostra: 4143034\n",
      "iteration:  41460 , total_loss:  21.82466214497884\n",
      "count_amostra: 4146034\n",
      "iteration:  41490 , total_loss:  21.645948727925617\n",
      "count_amostra: 4149034\n",
      "iteration:  41520 , total_loss:  21.673073514302573\n",
      "count_amostra: 4152034\n",
      "iteration:  41550 , total_loss:  21.68769105275472\n",
      "count_amostra: 4155034\n",
      "iteration:  41580 , total_loss:  21.78247814178467\n",
      "count_amostra: 4158034\n",
      "iteration:  41610 , total_loss:  21.607665888468425\n",
      "count_amostra: 4161034\n",
      "iteration:  41640 , total_loss:  21.673659960428875\n",
      "count_amostra: 4164034\n",
      "iteration:  41670 , total_loss:  21.56189187367757\n",
      "count_amostra: 4167034\n",
      "iteration:  41700 , total_loss:  21.605736796061198\n",
      "count_amostra: 4170034\n",
      "iteration:  41730 , total_loss:  21.67829786936442\n",
      "count_amostra: 4173034\n",
      "iteration:  41760 , total_loss:  21.563658078511555\n",
      "count_amostra: 4176034\n",
      "iteration:  41790 , total_loss:  21.5773219426473\n",
      "count_amostra: 4179034\n",
      "iteration:  41820 , total_loss:  21.38384437561035\n",
      "count_amostra: 4182034\n",
      "iteration:  41850 , total_loss:  21.497745259602866\n",
      "count_amostra: 4185034\n",
      "iteration:  41880 , total_loss:  21.664347012837727\n",
      "count_amostra: 4188034\n",
      "iteration:  41910 , total_loss:  21.756034723917644\n",
      "count_amostra: 4191034\n",
      "iteration:  41940 , total_loss:  21.62105655670166\n",
      "count_amostra: 4194034\n",
      "iteration:  41970 , total_loss:  21.998794937133788\n",
      "count_amostra: 4197034\n",
      "iteration:  42000 , total_loss:  21.81360975901286\n",
      "count_amostra: 4200034\n",
      "iteration:  42030 , total_loss:  21.31352570851644\n",
      "count_amostra: 4203034\n",
      "iteration:  42060 , total_loss:  21.997257296244303\n",
      "count_amostra: 4206034\n",
      "iteration:  42090 , total_loss:  21.943284797668458\n",
      "count_amostra: 4209034\n",
      "iteration:  42120 , total_loss:  21.470411618550617\n",
      "count_amostra: 4212034\n",
      "iteration:  42150 , total_loss:  21.810301526387533\n",
      "count_amostra: 4215034\n",
      "iteration:  42180 , total_loss:  21.373812866210937\n",
      "count_amostra: 4218034\n",
      "iteration:  42210 , total_loss:  21.598574701944987\n",
      "count_amostra: 4221034\n",
      "iteration:  42240 , total_loss:  21.815826988220216\n",
      "count_amostra: 4224034\n",
      "iteration:  42270 , total_loss:  21.5620174407959\n",
      "count_amostra: 4227034\n",
      "iteration:  42300 , total_loss:  21.540808296203615\n",
      "count_amostra: 4230034\n",
      "iteration:  42330 , total_loss:  21.63991813659668\n",
      "count_amostra: 4233034\n",
      "iteration:  42360 , total_loss:  21.49316323598226\n",
      "count_amostra: 4236034\n",
      "iteration:  42390 , total_loss:  21.70365441640218\n",
      "count_amostra: 4239034\n",
      "iteration:  42420 , total_loss:  21.69769026438395\n",
      "count_amostra: 4242034\n",
      "iteration:  42450 , total_loss:  22.148520278930665\n",
      "count_amostra: 4245034\n",
      "iteration:  42480 , total_loss:  21.725518163045248\n",
      "count_amostra: 4248034\n",
      "iteration:  42510 , total_loss:  21.451852734883627\n",
      "count_amostra: 4251034\n",
      "iteration:  42540 , total_loss:  21.6851988474528\n",
      "count_amostra: 4254034\n",
      "iteration:  42570 , total_loss:  22.073619206746418\n",
      "count_amostra: 4257034\n",
      "iteration:  42600 , total_loss:  21.327908770243326\n",
      "count_amostra: 4260034\n",
      "iteration:  42630 , total_loss:  21.945416196187338\n",
      "count_amostra: 4263034\n",
      "iteration:  42660 , total_loss:  21.573174921671548\n",
      "count_amostra: 4266034\n",
      "iteration:  42690 , total_loss:  21.62980251312256\n",
      "count_amostra: 4269034\n",
      "iteration:  42720 , total_loss:  21.59277566274007\n",
      "count_amostra: 4272034\n",
      "iteration:  42750 , total_loss:  21.495359484354655\n",
      "count_amostra: 4275034\n",
      "iteration:  42780 , total_loss:  21.521846199035643\n",
      "count_amostra: 4278034\n",
      "iteration:  42810 , total_loss:  21.58127358754476\n",
      "count_amostra: 4281034\n",
      "iteration:  42840 , total_loss:  21.51561991373698\n",
      "count_amostra: 4284034\n",
      "iteration:  42870 , total_loss:  21.254778734842937\n",
      "count_amostra: 4287034\n",
      "iteration:  42900 , total_loss:  21.448879114786784\n",
      "count_amostra: 4290034\n",
      "iteration:  42930 , total_loss:  21.963568369547527\n",
      "count_amostra: 4293034\n",
      "iteration:  42960 , total_loss:  22.010415013631185\n",
      "count_amostra: 4296034\n",
      "iteration:  42990 , total_loss:  21.79746259053548\n",
      "count_amostra: 4299034\n",
      "iteration:  43020 , total_loss:  21.55661538441976\n",
      "count_amostra: 4302034\n",
      "iteration:  43050 , total_loss:  21.88423334757487\n",
      "count_amostra: 4305034\n",
      "iteration:  43080 , total_loss:  21.549673398335774\n",
      "count_amostra: 4308034\n",
      "iteration:  43110 , total_loss:  21.594649759928384\n",
      "count_amostra: 4311034\n",
      "iteration:  43140 , total_loss:  22.085926183064778\n",
      "count_amostra: 4314034\n",
      "iteration:  43170 , total_loss:  21.333944892883302\n",
      "count_amostra: 4317034\n",
      "iteration:  43200 , total_loss:  21.585816701253254\n",
      "count_amostra: 4320034\n",
      "iteration:  43230 , total_loss:  21.7956792195638\n",
      "count_amostra: 4323034\n",
      "iteration:  43260 , total_loss:  21.51324717203776\n",
      "count_amostra: 4326034\n",
      "iteration:  43290 , total_loss:  21.76386241912842\n",
      "count_amostra: 4329034\n",
      "iteration:  43320 , total_loss:  21.539736048380533\n",
      "count_amostra: 4332034\n",
      "iteration:  43350 , total_loss:  21.54783687591553\n",
      "count_amostra: 4335034\n",
      "iteration:  43380 , total_loss:  21.449157587687175\n",
      "count_amostra: 4338034\n",
      "iteration:  43410 , total_loss:  21.209456634521484\n",
      "count_amostra: 4341034\n",
      "iteration:  43440 , total_loss:  21.294964536031088\n",
      "count_amostra: 4344034\n",
      "iteration:  43470 , total_loss:  21.778361256917318\n",
      "count_amostra: 4347034\n",
      "iteration:  43500 , total_loss:  21.981255849202473\n",
      "count_amostra: 4350034\n",
      "iteration:  43530 , total_loss:  21.602979850769042\n",
      "count_amostra: 4353034\n",
      "iteration:  43560 , total_loss:  21.770036061604817\n",
      "count_amostra: 4356034\n",
      "iteration:  43590 , total_loss:  21.818205070495605\n",
      "count_amostra: 4359034\n",
      "iteration:  43620 , total_loss:  21.560392570495605\n",
      "count_amostra: 4362034\n",
      "iteration:  43650 , total_loss:  21.721596844991048\n",
      "count_amostra: 4365034\n",
      "iteration:  43680 , total_loss:  21.671421623229982\n",
      "count_amostra: 4368034\n",
      "iteration:  43710 , total_loss:  22.003494771321616\n",
      "count_amostra: 4371034\n",
      "iteration:  43740 , total_loss:  21.45872713724772\n",
      "count_amostra: 4374034\n",
      "iteration:  43770 , total_loss:  21.62343495686849\n",
      "count_amostra: 4377034\n",
      "iteration:  43800 , total_loss:  21.462550417582193\n",
      "count_amostra: 4380034\n",
      "iteration:  43830 , total_loss:  21.42382183074951\n",
      "count_amostra: 4383034\n",
      "iteration:  43860 , total_loss:  21.554199282328288\n",
      "count_amostra: 4386034\n",
      "iteration:  43890 , total_loss:  21.747011375427245\n",
      "count_amostra: 4389034\n",
      "iteration:  43920 , total_loss:  21.81274503072103\n",
      "count_amostra: 4392034\n",
      "iteration:  43950 , total_loss:  21.43762270609538\n",
      "count_amostra: 4395034\n",
      "iteration:  43980 , total_loss:  21.632747332255047\n",
      "count_amostra: 4398034\n",
      "iteration:  44010 , total_loss:  21.651740137736002\n",
      "count_amostra: 4401034\n",
      "iteration:  44040 , total_loss:  21.71540075937907\n",
      "count_amostra: 4404034\n",
      "iteration:  44070 , total_loss:  21.99966812133789\n",
      "count_amostra: 4407034\n",
      "iteration:  44100 , total_loss:  21.849025281270347\n",
      "count_amostra: 4410034\n",
      "iteration:  44130 , total_loss:  21.399605560302735\n",
      "count_amostra: 4413034\n",
      "iteration:  44160 , total_loss:  21.4126916885376\n",
      "count_amostra: 4416034\n",
      "iteration:  44190 , total_loss:  21.613633410135904\n",
      "count_amostra: 4419034\n",
      "iteration:  44220 , total_loss:  21.257500330607098\n",
      "count_amostra: 4422034\n",
      "iteration:  44250 , total_loss:  21.7356055577596\n",
      "count_amostra: 4425034\n",
      "iteration:  44280 , total_loss:  21.620909118652342\n",
      "count_amostra: 4428034\n",
      "iteration:  44310 , total_loss:  21.124013264973957\n",
      "count_amostra: 4431034\n",
      "iteration:  44340 , total_loss:  21.634581247965496\n",
      "count_amostra: 4434034\n",
      "iteration:  44370 , total_loss:  21.66735699971517\n",
      "count_amostra: 4437034\n",
      "iteration:  44400 , total_loss:  21.632046763102213\n",
      "count_amostra: 4440034\n",
      "iteration:  44430 , total_loss:  21.695600700378417\n",
      "count_amostra: 4443034\n",
      "iteration:  44460 , total_loss:  21.479275258382163\n",
      "count_amostra: 4446034\n",
      "iteration:  44490 , total_loss:  21.975577926635744\n",
      "count_amostra: 4449034\n",
      "iteration:  44520 , total_loss:  21.387428919474285\n",
      "count_amostra: 4452034\n",
      "iteration:  44550 , total_loss:  22.092975997924803\n",
      "count_amostra: 4455034\n",
      "iteration:  44580 , total_loss:  21.400533803304036\n",
      "count_amostra: 4458034\n",
      "iteration:  44610 , total_loss:  21.762950960795084\n",
      "count_amostra: 4461034\n",
      "iteration:  44640 , total_loss:  21.26230812072754\n",
      "count_amostra: 4464034\n",
      "iteration:  44670 , total_loss:  21.29718869527181\n",
      "count_amostra: 4467034\n",
      "iteration:  44700 , total_loss:  21.461678314208985\n",
      "count_amostra: 4470034\n",
      "iteration:  44730 , total_loss:  21.503884442647298\n",
      "count_amostra: 4473034\n",
      "iteration:  44760 , total_loss:  21.473241170247395\n",
      "count_amostra: 4476034\n",
      "iteration:  44790 , total_loss:  21.50570551554362\n",
      "count_amostra: 4479034\n",
      "iteration:  44820 , total_loss:  22.015800031026206\n",
      "count_amostra: 4482034\n",
      "iteration:  44850 , total_loss:  21.570188077290855\n",
      "count_amostra: 4485034\n",
      "iteration:  44880 , total_loss:  21.634586334228516\n",
      "count_amostra: 4488034\n",
      "iteration:  44910 , total_loss:  21.515723419189452\n",
      "count_amostra: 4491034\n",
      "iteration:  44940 , total_loss:  21.622748311360677\n",
      "count_amostra: 4494034\n",
      "iteration:  44970 , total_loss:  21.565594291687013\n",
      "count_amostra: 4497034\n",
      "iteration:  45000 , total_loss:  21.36855500539144\n",
      "count_amostra: 4500034\n",
      "iteration:  45030 , total_loss:  21.635368665059406\n",
      "count_amostra: 4503034\n",
      "iteration:  45060 , total_loss:  21.458443705240885\n",
      "count_amostra: 4506034\n",
      "iteration:  45090 , total_loss:  21.65993537902832\n",
      "count_amostra: 4509034\n",
      "iteration:  45120 , total_loss:  21.436362584431965\n",
      "count_amostra: 4512034\n",
      "iteration:  45150 , total_loss:  21.387698682149253\n",
      "count_amostra: 4515034\n",
      "iteration:  45180 , total_loss:  21.813791529337564\n",
      "count_amostra: 4518034\n",
      "iteration:  45210 , total_loss:  21.746648025512695\n",
      "count_amostra: 4521034\n",
      "iteration:  45240 , total_loss:  21.830058415730793\n",
      "count_amostra: 4524034\n",
      "iteration:  45270 , total_loss:  21.01981372833252\n",
      "count_amostra: 4527034\n",
      "iteration:  45300 , total_loss:  21.55568510691325\n",
      "count_amostra: 4530034\n",
      "iteration:  45330 , total_loss:  21.92708905537923\n",
      "count_amostra: 4533034\n",
      "iteration:  45360 , total_loss:  21.37721169789632\n",
      "count_amostra: 4536034\n",
      "iteration:  45390 , total_loss:  21.877579307556154\n",
      "count_amostra: 4539034\n",
      "iteration:  45420 , total_loss:  21.588375536600747\n",
      "count_amostra: 4542034\n",
      "iteration:  45450 , total_loss:  21.647996711730958\n",
      "count_amostra: 4545034\n",
      "iteration:  45480 , total_loss:  21.480354181925456\n",
      "count_amostra: 4548034\n",
      "iteration:  45510 , total_loss:  21.392910448710122\n",
      "count_amostra: 4551034\n",
      "iteration:  45540 , total_loss:  21.745519002278645\n",
      "count_amostra: 4554034\n",
      "iteration:  45570 , total_loss:  22.107838567097982\n",
      "count_amostra: 4557034\n",
      "iteration:  45600 , total_loss:  21.276874351501466\n",
      "count_amostra: 4560034\n",
      "iteration:  45630 , total_loss:  21.963717714945474\n",
      "count_amostra: 4563034\n",
      "iteration:  45660 , total_loss:  21.73623212178548\n",
      "count_amostra: 4566034\n",
      "iteration:  45690 , total_loss:  21.66275151570638\n",
      "count_amostra: 4569034\n",
      "iteration:  45720 , total_loss:  21.55360221862793\n",
      "count_amostra: 4572034\n",
      "iteration:  45750 , total_loss:  21.156747563680014\n",
      "count_amostra: 4575034\n",
      "iteration:  45780 , total_loss:  21.161000124613444\n",
      "count_amostra: 4578034\n",
      "iteration:  45810 , total_loss:  21.4314395904541\n",
      "count_amostra: 4581034\n",
      "iteration:  45840 , total_loss:  21.652079518636068\n",
      "count_amostra: 4584034\n",
      "iteration:  45870 , total_loss:  21.19356524149577\n",
      "count_amostra: 4587034\n",
      "iteration:  45900 , total_loss:  21.564085706075033\n",
      "count_amostra: 4590034\n",
      "iteration:  45930 , total_loss:  21.393908882141112\n",
      "count_amostra: 4593034\n",
      "iteration:  45960 , total_loss:  21.83902498881022\n",
      "count_amostra: 4596034\n",
      "iteration:  45990 , total_loss:  21.7414608001709\n",
      "count_amostra: 4599034\n",
      "iteration:  46020 , total_loss:  21.602939478556316\n",
      "count_amostra: 4602034\n",
      "iteration:  46050 , total_loss:  21.692336908976237\n",
      "count_amostra: 4605034\n",
      "iteration:  46080 , total_loss:  21.35766232808431\n",
      "count_amostra: 4608034\n",
      "iteration:  46110 , total_loss:  21.332643381754558\n",
      "count_amostra: 4611034\n",
      "iteration:  46140 , total_loss:  21.493605168660483\n",
      "count_amostra: 4614034\n",
      "iteration:  46170 , total_loss:  21.74913075764974\n",
      "count_amostra: 4617034\n",
      "iteration:  46200 , total_loss:  21.845085906982423\n",
      "count_amostra: 4620034\n",
      "iteration:  46230 , total_loss:  21.213009516398113\n",
      "count_amostra: 4623034\n",
      "iteration:  46260 , total_loss:  21.50094725290934\n",
      "count_amostra: 4626034\n",
      "iteration:  46290 , total_loss:  21.648961766560873\n",
      "count_amostra: 4629034\n",
      "iteration:  46320 , total_loss:  21.33187255859375\n",
      "count_amostra: 4632034\n",
      "iteration:  46350 , total_loss:  21.633018430074056\n",
      "count_amostra: 4635034\n",
      "iteration:  46380 , total_loss:  21.468513679504394\n",
      "count_amostra: 4638034\n",
      "iteration:  46410 , total_loss:  22.031402778625488\n",
      "count_amostra: 4641034\n",
      "iteration:  46440 , total_loss:  21.625131797790527\n",
      "count_amostra: 4644034\n",
      "iteration:  46470 , total_loss:  21.62945556640625\n",
      "count_amostra: 4647034\n",
      "iteration:  46500 , total_loss:  21.02497533162435\n",
      "count_amostra: 4650034\n",
      "iteration:  46530 , total_loss:  21.497882080078124\n",
      "count_amostra: 4653034\n",
      "iteration:  46560 , total_loss:  21.700165112813313\n",
      "count_amostra: 4656034\n",
      "iteration:  46590 , total_loss:  21.515243657430013\n",
      "count_amostra: 4659034\n",
      "iteration:  46620 , total_loss:  21.661296590169272\n",
      "count_amostra: 4662034\n",
      "iteration:  46650 , total_loss:  21.616670926411945\n",
      "count_amostra: 4665034\n",
      "iteration:  46680 , total_loss:  21.54056390126546\n",
      "count_amostra: 4668034\n",
      "iteration:  46710 , total_loss:  21.40957571665446\n",
      "count_amostra: 4671034\n",
      "iteration:  46740 , total_loss:  21.620170911153156\n",
      "count_amostra: 4674034\n",
      "iteration:  46770 , total_loss:  21.621842257181804\n",
      "count_amostra: 4677034\n",
      "iteration:  46800 , total_loss:  21.6756415049235\n",
      "count_amostra: 4680034\n",
      "iteration:  46830 , total_loss:  21.504233105977377\n",
      "count_amostra: 4683034\n",
      "iteration:  46860 , total_loss:  21.96522464752197\n",
      "count_amostra: 4686034\n",
      "iteration:  46890 , total_loss:  21.282023429870605\n",
      "count_amostra: 4689034\n",
      "iteration:  46920 , total_loss:  21.48853454589844\n",
      "count_amostra: 4692034\n",
      "iteration:  46950 , total_loss:  21.807627995808918\n",
      "count_amostra: 4695034\n",
      "iteration:  46980 , total_loss:  21.682277234395347\n",
      "count_amostra: 4698034\n",
      "iteration:  47010 , total_loss:  21.37174784342448\n",
      "count_amostra: 4701034\n",
      "iteration:  47040 , total_loss:  21.76045347849528\n",
      "count_amostra: 4704034\n",
      "iteration:  47070 , total_loss:  21.51960868835449\n",
      "count_amostra: 4707034\n",
      "iteration:  47100 , total_loss:  21.566508102416993\n",
      "count_amostra: 4710034\n",
      "iteration:  47130 , total_loss:  22.28573570251465\n",
      "count_amostra: 4713034\n",
      "iteration:  47160 , total_loss:  22.67149213155111\n",
      "count_amostra: 4716034\n",
      "iteration:  47190 , total_loss:  22.63929869333903\n",
      "count_amostra: 4719034\n",
      "iteration:  47220 , total_loss:  23.00832894643148\n",
      "count_amostra: 4722034\n",
      "iteration:  47250 , total_loss:  23.05496966044108\n",
      "count_amostra: 4725034\n",
      "iteration:  47280 , total_loss:  22.855776087443033\n",
      "count_amostra: 4728034\n",
      "iteration:  47310 , total_loss:  22.729662704467774\n",
      "count_amostra: 4731034\n",
      "iteration:  47340 , total_loss:  22.639958953857423\n",
      "count_amostra: 4734034\n",
      "iteration:  47370 , total_loss:  22.968712043762206\n",
      "count_amostra: 4737034\n",
      "iteration:  47400 , total_loss:  22.667375119527183\n",
      "count_amostra: 4740034\n",
      "iteration:  47430 , total_loss:  22.676838429768882\n",
      "count_amostra: 4743034\n",
      "iteration:  47460 , total_loss:  22.53994566599528\n",
      "count_amostra: 4746034\n",
      "iteration:  47490 , total_loss:  22.426108996073406\n",
      "count_amostra: 4749034\n",
      "iteration:  47520 , total_loss:  22.72502810160319\n",
      "count_amostra: 4752034\n",
      "iteration:  47550 , total_loss:  22.873388671875\n",
      "count_amostra: 4755034\n",
      "iteration:  47580 , total_loss:  22.493273226420083\n",
      "count_amostra: 4758034\n",
      "iteration:  47610 , total_loss:  22.491927973429362\n",
      "count_amostra: 4761034\n",
      "iteration:  47640 , total_loss:  22.754395802815754\n",
      "count_amostra: 4764034\n",
      "iteration:  47670 , total_loss:  22.85054086049398\n",
      "count_amostra: 4767034\n",
      "iteration:  47700 , total_loss:  22.727791786193848\n",
      "count_amostra: 4770034\n",
      "iteration:  47730 , total_loss:  22.50377960205078\n",
      "count_amostra: 4773034\n",
      "iteration:  47760 , total_loss:  22.978153165181478\n",
      "count_amostra: 4776034\n",
      "iteration:  47790 , total_loss:  22.603288014729817\n",
      "count_amostra: 4779034\n",
      "iteration:  47820 , total_loss:  23.055814615885417\n",
      "count_amostra: 4782034\n",
      "iteration:  47850 , total_loss:  22.82777474721273\n",
      "count_amostra: 4785034\n",
      "iteration:  47880 , total_loss:  22.706777890523274\n",
      "count_amostra: 4788034\n",
      "iteration:  47910 , total_loss:  22.747975540161132\n",
      "count_amostra: 4791034\n",
      "iteration:  47940 , total_loss:  22.923674201965333\n",
      "count_amostra: 4794034\n",
      "iteration:  47970 , total_loss:  22.754383341471353\n",
      "count_amostra: 4797034\n",
      "iteration:  48000 , total_loss:  22.939156659444173\n",
      "count_amostra: 4800034\n",
      "iteration:  48030 , total_loss:  22.728962453206382\n",
      "count_amostra: 4803034\n",
      "iteration:  48060 , total_loss:  22.62131576538086\n",
      "count_amostra: 4806034\n",
      "iteration:  48090 , total_loss:  22.593675994873045\n",
      "count_amostra: 4809034\n",
      "iteration:  48120 , total_loss:  22.588666852315267\n",
      "count_amostra: 4812034\n",
      "iteration:  48150 , total_loss:  22.909656969706216\n",
      "count_amostra: 4815034\n",
      "iteration:  48180 , total_loss:  22.5816748936971\n",
      "count_amostra: 4818034\n",
      "iteration:  48210 , total_loss:  22.500843556722007\n",
      "count_amostra: 4821034\n",
      "iteration:  48240 , total_loss:  22.708230400085448\n",
      "count_amostra: 4824034\n",
      "iteration:  48270 , total_loss:  22.979181226094564\n",
      "count_amostra: 4827034\n",
      "iteration:  48300 , total_loss:  22.43420302073161\n",
      "count_amostra: 4830034\n",
      "iteration:  48330 , total_loss:  22.814476585388185\n",
      "count_amostra: 4833034\n",
      "iteration:  48360 , total_loss:  22.783175913492837\n",
      "count_amostra: 4836034\n",
      "iteration:  48390 , total_loss:  22.709062639872233\n",
      "count_amostra: 4839034\n",
      "iteration:  48420 , total_loss:  22.536808077494303\n",
      "count_amostra: 4842034\n",
      "iteration:  48450 , total_loss:  22.681273905436196\n",
      "count_amostra: 4845034\n",
      "iteration:  48480 , total_loss:  22.694614982604982\n",
      "count_amostra: 4848034\n",
      "iteration:  48510 , total_loss:  22.658391253153482\n",
      "count_amostra: 4851034\n",
      "iteration:  48540 , total_loss:  22.8167875289917\n",
      "count_amostra: 4854034\n",
      "iteration:  48570 , total_loss:  22.685479799906414\n",
      "count_amostra: 4857034\n",
      "iteration:  48600 , total_loss:  22.851690673828124\n",
      "count_amostra: 4860034\n",
      "iteration:  48630 , total_loss:  22.46955992380778\n",
      "count_amostra: 4863034\n",
      "iteration:  48660 , total_loss:  22.62773100535075\n",
      "count_amostra: 4866034\n",
      "iteration:  48690 , total_loss:  22.8429874420166\n",
      "count_amostra: 4869034\n",
      "iteration:  48720 , total_loss:  22.363917477925618\n",
      "count_amostra: 4872034\n",
      "iteration:  48750 , total_loss:  22.67270628611247\n",
      "count_amostra: 4875034\n",
      "iteration:  48780 , total_loss:  22.533939425150553\n",
      "count_amostra: 4878034\n",
      "iteration:  48810 , total_loss:  22.635067240397134\n",
      "count_amostra: 4881034\n",
      "iteration:  48840 , total_loss:  22.66081097920736\n",
      "count_amostra: 4884034\n",
      "iteration:  48870 , total_loss:  22.69194310506185\n",
      "count_amostra: 4887034\n",
      "iteration:  48900 , total_loss:  23.12646376291911\n",
      "count_amostra: 4890034\n",
      "iteration:  48930 , total_loss:  22.648769887288413\n",
      "count_amostra: 4893034\n",
      "iteration:  48960 , total_loss:  22.815264956156412\n",
      "count_amostra: 4896034\n",
      "iteration:  48990 , total_loss:  22.72635269165039\n",
      "count_amostra: 4899034\n",
      "iteration:  49020 , total_loss:  22.848531659444173\n",
      "count_amostra: 4902034\n",
      "iteration:  49050 , total_loss:  22.802932357788087\n",
      "count_amostra: 4905034\n",
      "iteration:  49080 , total_loss:  22.761835861206055\n",
      "count_amostra: 4908034\n",
      "iteration:  49110 , total_loss:  22.436175028483074\n",
      "count_amostra: 4911034\n",
      "iteration:  49140 , total_loss:  22.44853547414144\n",
      "count_amostra: 4914034\n",
      "iteration:  49170 , total_loss:  22.57563851674398\n",
      "count_amostra: 4917034\n",
      "iteration:  49200 , total_loss:  22.637686030069986\n",
      "count_amostra: 4920034\n",
      "iteration:  49230 , total_loss:  22.860654830932617\n",
      "count_amostra: 4923034\n",
      "iteration:  49260 , total_loss:  23.236428451538085\n",
      "count_amostra: 4926034\n",
      "iteration:  49290 , total_loss:  22.6953608194987\n",
      "count_amostra: 4929034\n",
      "iteration:  49320 , total_loss:  22.924710973103842\n",
      "count_amostra: 4932034\n",
      "iteration:  49350 , total_loss:  22.68780975341797\n",
      "count_amostra: 4935034\n",
      "iteration:  49380 , total_loss:  22.890954081217448\n",
      "count_amostra: 4938034\n",
      "iteration:  49410 , total_loss:  22.552390225728352\n",
      "count_amostra: 4941034\n",
      "iteration:  49440 , total_loss:  22.489243189493816\n",
      "count_amostra: 4944034\n",
      "iteration:  49470 , total_loss:  22.553089777628582\n",
      "count_amostra: 4947034\n",
      "iteration:  49500 , total_loss:  22.66984004974365\n",
      "count_amostra: 4950034\n",
      "iteration:  49530 , total_loss:  22.7035120010376\n",
      "count_amostra: 4953034\n",
      "iteration:  49560 , total_loss:  22.151507759094237\n",
      "count_amostra: 4956034\n",
      "iteration:  49590 , total_loss:  22.474126688639323\n",
      "count_amostra: 4959034\n",
      "iteration:  49620 , total_loss:  22.11619415283203\n",
      "count_amostra: 4962034\n",
      "iteration:  49650 , total_loss:  22.185535685221353\n",
      "count_amostra: 4965034\n",
      "iteration:  49680 , total_loss:  22.297765668233236\n",
      "count_amostra: 4968034\n",
      "iteration:  49710 , total_loss:  22.024762662251792\n",
      "count_amostra: 4971034\n",
      "iteration:  49740 , total_loss:  21.97065436045329\n",
      "count_amostra: 4974034\n",
      "iteration:  49770 , total_loss:  21.76820494333903\n",
      "count_amostra: 4977034\n",
      "iteration:  49800 , total_loss:  21.71459134419759\n",
      "count_amostra: 4980034\n",
      "iteration:  49830 , total_loss:  22.217667706807454\n",
      "count_amostra: 4983034\n",
      "iteration:  49860 , total_loss:  21.46697883605957\n",
      "count_amostra: 4986034\n",
      "iteration:  49890 , total_loss:  21.399069849650065\n",
      "count_amostra: 4989034\n",
      "iteration:  49920 , total_loss:  21.760062599182127\n",
      "count_amostra: 4992034\n",
      "iteration:  49950 , total_loss:  21.560420672098797\n",
      "count_amostra: 4995034\n",
      "iteration:  49980 , total_loss:  21.738567860921226\n",
      "count_amostra: 4998034\n",
      "iteration:  50010 , total_loss:  21.695469347635903\n",
      "count_amostra: 5001034\n",
      "iteration:  50040 , total_loss:  21.955266761779786\n",
      "count_amostra: 5004034\n",
      "iteration:  50070 , total_loss:  22.100993347167968\n",
      "count_amostra: 5007034\n",
      "iteration:  50100 , total_loss:  21.66330846150716\n",
      "count_amostra: 5010034\n",
      "iteration:  50130 , total_loss:  22.130162302652995\n",
      "count_amostra: 5013034\n",
      "iteration:  50160 , total_loss:  21.660034497578938\n",
      "count_amostra: 5016034\n",
      "iteration:  50190 , total_loss:  21.610684458414713\n",
      "count_amostra: 5019034\n",
      "iteration:  50220 , total_loss:  21.447686513264973\n",
      "count_amostra: 5022034\n",
      "iteration:  50250 , total_loss:  21.59268118540446\n",
      "count_amostra: 5025034\n",
      "iteration:  50280 , total_loss:  21.21422595977783\n",
      "count_amostra: 5028034\n",
      "iteration:  50310 , total_loss:  21.970214080810546\n",
      "count_amostra: 5031034\n",
      "iteration:  50340 , total_loss:  21.66408856709798\n",
      "count_amostra: 5034034\n",
      "iteration:  50370 , total_loss:  21.76455281575521\n",
      "count_amostra: 5037034\n",
      "iteration:  50400 , total_loss:  21.690526644388836\n",
      "count_amostra: 5040034\n",
      "iteration:  50430 , total_loss:  21.73807563781738\n",
      "count_amostra: 5043034\n",
      "iteration:  50460 , total_loss:  21.69741579691569\n",
      "count_amostra: 5046034\n",
      "iteration:  50490 , total_loss:  21.742632484436037\n",
      "count_amostra: 5049034\n",
      "iteration:  50520 , total_loss:  21.6035857518514\n",
      "count_amostra: 5052034\n",
      "iteration:  50550 , total_loss:  21.71549917856852\n",
      "count_amostra: 5055034\n",
      "iteration:  50580 , total_loss:  21.524196243286134\n",
      "count_amostra: 5058034\n",
      "iteration:  50610 , total_loss:  21.849526596069335\n",
      "count_amostra: 5061034\n",
      "iteration:  50640 , total_loss:  21.403054682413735\n",
      "count_amostra: 5064034\n",
      "iteration:  50670 , total_loss:  21.418837547302246\n",
      "count_amostra: 5067034\n",
      "iteration:  50700 , total_loss:  21.635950152079264\n",
      "count_amostra: 5070034\n",
      "iteration:  50730 , total_loss:  22.289807891845705\n",
      "count_amostra: 5073034\n",
      "iteration:  50760 , total_loss:  21.964500872294106\n",
      "count_amostra: 5076034\n",
      "iteration:  50790 , total_loss:  21.230305226643882\n",
      "count_amostra: 5079034\n",
      "iteration:  50820 , total_loss:  21.500049018859862\n",
      "count_amostra: 5082034\n",
      "iteration:  50850 , total_loss:  21.608535130818684\n",
      "count_amostra: 5085034\n",
      "iteration:  50880 , total_loss:  21.815764300028484\n",
      "count_amostra: 5088034\n",
      "iteration:  50910 , total_loss:  21.299727821350096\n",
      "count_amostra: 5091034\n",
      "iteration:  50940 , total_loss:  21.550187174479166\n",
      "count_amostra: 5094034\n",
      "iteration:  50970 , total_loss:  21.840770149230956\n",
      "count_amostra: 5097034\n",
      "iteration:  51000 , total_loss:  21.322415669759113\n",
      "count_amostra: 5100034\n",
      "iteration:  51030 , total_loss:  21.597849082946777\n",
      "count_amostra: 5103034\n",
      "iteration:  51060 , total_loss:  21.11708418528239\n",
      "count_amostra: 5106034\n",
      "iteration:  51090 , total_loss:  21.31864725748698\n",
      "count_amostra: 5109034\n",
      "iteration:  51120 , total_loss:  21.615861829121908\n",
      "count_amostra: 5112034\n",
      "iteration:  51150 , total_loss:  21.645355161031087\n",
      "count_amostra: 5115034\n",
      "iteration:  51180 , total_loss:  21.751526578267416\n",
      "count_amostra: 5118034\n",
      "iteration:  51210 , total_loss:  21.114810880025228\n",
      "count_amostra: 5121034\n",
      "iteration:  51240 , total_loss:  20.976338577270507\n",
      "count_amostra: 5124034\n",
      "iteration:  51270 , total_loss:  21.604634475708007\n",
      "count_amostra: 5127034\n",
      "iteration:  51300 , total_loss:  21.75077330271403\n",
      "count_amostra: 5130034\n",
      "iteration:  51330 , total_loss:  21.658142280578613\n",
      "count_amostra: 5133034\n",
      "iteration:  51360 , total_loss:  21.555367279052735\n",
      "count_amostra: 5136034\n",
      "iteration:  51390 , total_loss:  21.905663681030273\n",
      "count_amostra: 5139034\n",
      "iteration:  51420 , total_loss:  21.427717463175455\n",
      "count_amostra: 5142034\n",
      "iteration:  51450 , total_loss:  21.73488368988037\n",
      "count_amostra: 5145034\n",
      "iteration:  51480 , total_loss:  21.255745061238606\n",
      "count_amostra: 5148034\n",
      "iteration:  51510 , total_loss:  21.537548319498697\n",
      "count_amostra: 5151034\n",
      "iteration:  51540 , total_loss:  21.38238582611084\n",
      "count_amostra: 5154034\n",
      "iteration:  51570 , total_loss:  21.6286039352417\n",
      "count_amostra: 5157034\n",
      "iteration:  51600 , total_loss:  21.537011845906576\n",
      "count_amostra: 5160034\n",
      "iteration:  51630 , total_loss:  21.60726877848307\n",
      "count_amostra: 5163034\n",
      "iteration:  51660 , total_loss:  21.508339564005535\n",
      "count_amostra: 5166034\n",
      "iteration:  51690 , total_loss:  21.79129695892334\n",
      "count_amostra: 5169034\n",
      "iteration:  51720 , total_loss:  21.235678545633952\n",
      "count_amostra: 5172034\n",
      "iteration:  51750 , total_loss:  21.313163057963052\n",
      "count_amostra: 5175034\n",
      "iteration:  51780 , total_loss:  21.065089162190755\n",
      "count_amostra: 5178034\n",
      "iteration:  51810 , total_loss:  21.569892756144206\n",
      "count_amostra: 5181034\n",
      "iteration:  51840 , total_loss:  21.234531529744466\n",
      "count_amostra: 5184034\n",
      "iteration:  51870 , total_loss:  21.40078639984131\n",
      "count_amostra: 5187034\n",
      "iteration:  51900 , total_loss:  21.148743120829263\n",
      "count_amostra: 5190034\n",
      "iteration:  51930 , total_loss:  21.434298833211262\n",
      "count_amostra: 5193034\n",
      "iteration:  51960 , total_loss:  21.844836616516112\n",
      "count_amostra: 5196034\n",
      "iteration:  51990 , total_loss:  21.4974396387736\n",
      "count_amostra: 5199034\n",
      "iteration:  52020 , total_loss:  21.601183319091795\n",
      "count_amostra: 5202034\n",
      "iteration:  52050 , total_loss:  21.53575191497803\n",
      "count_amostra: 5205034\n",
      "iteration:  52080 , total_loss:  21.351414235432944\n",
      "count_amostra: 5208034\n",
      "iteration:  52110 , total_loss:  21.281263669331867\n",
      "count_amostra: 5211034\n",
      "iteration:  52140 , total_loss:  21.507338269551596\n",
      "count_amostra: 5214034\n",
      "iteration:  52170 , total_loss:  21.530785624186198\n",
      "count_amostra: 5217034\n",
      "iteration:  52200 , total_loss:  21.79565887451172\n",
      "count_amostra: 5220034\n",
      "iteration:  52230 , total_loss:  21.584400622049966\n",
      "count_amostra: 5223034\n",
      "iteration:  52260 , total_loss:  21.23288764953613\n",
      "count_amostra: 5226034\n",
      "iteration:  52290 , total_loss:  21.54451732635498\n",
      "count_amostra: 5229034\n",
      "iteration:  52320 , total_loss:  21.25923360188802\n",
      "count_amostra: 5232034\n",
      "iteration:  52350 , total_loss:  21.84756342569987\n",
      "count_amostra: 5235034\n",
      "iteration:  52380 , total_loss:  21.560108184814453\n",
      "count_amostra: 5238034\n",
      "iteration:  52410 , total_loss:  21.84708372751872\n",
      "count_amostra: 5241034\n",
      "iteration:  52440 , total_loss:  20.883973757425945\n",
      "count_amostra: 5244034\n",
      "iteration:  52470 , total_loss:  21.171545855204265\n",
      "count_amostra: 5247034\n",
      "iteration:  52500 , total_loss:  21.32089500427246\n",
      "count_amostra: 5250034\n",
      "iteration:  52530 , total_loss:  21.450197664896645\n",
      "count_amostra: 5253034\n",
      "iteration:  52560 , total_loss:  21.7666316986084\n",
      "count_amostra: 5256034\n",
      "iteration:  52590 , total_loss:  21.259423955281576\n",
      "count_amostra: 5259034\n",
      "iteration:  52620 , total_loss:  21.43396390279134\n",
      "count_amostra: 5262034\n",
      "iteration:  52650 , total_loss:  21.351954587300618\n",
      "count_amostra: 5265034\n",
      "iteration:  52680 , total_loss:  21.327051226298014\n",
      "count_amostra: 5268034\n",
      "iteration:  52710 , total_loss:  21.725697199503582\n",
      "count_amostra: 5271034\n",
      "iteration:  52740 , total_loss:  21.47313028971354\n",
      "count_amostra: 5274034\n",
      "iteration:  52770 , total_loss:  21.24638188680013\n",
      "count_amostra: 5277034\n",
      "iteration:  52800 , total_loss:  21.765588569641114\n",
      "count_amostra: 5280034\n",
      "iteration:  52830 , total_loss:  21.875664647420248\n",
      "count_amostra: 5283034\n",
      "iteration:  52860 , total_loss:  21.25411211649577\n",
      "count_amostra: 5286034\n",
      "iteration:  52890 , total_loss:  21.35916779836019\n",
      "count_amostra: 5289034\n",
      "iteration:  52920 , total_loss:  21.199913533528647\n",
      "count_amostra: 5292034\n",
      "iteration:  52950 , total_loss:  21.839865493774415\n",
      "count_amostra: 5295034\n",
      "iteration:  52980 , total_loss:  21.33324146270752\n",
      "count_amostra: 5298034\n",
      "iteration:  53010 , total_loss:  21.361726252237954\n",
      "count_amostra: 5301034\n",
      "iteration:  53040 , total_loss:  21.255422401428223\n",
      "count_amostra: 5304034\n",
      "iteration:  53070 , total_loss:  21.20665168762207\n",
      "count_amostra: 5307034\n",
      "iteration:  53100 , total_loss:  21.26501318613688\n",
      "count_amostra: 5310034\n",
      "iteration:  53130 , total_loss:  21.13141860961914\n",
      "count_amostra: 5313034\n",
      "iteration:  53160 , total_loss:  21.540309079488118\n",
      "count_amostra: 5316034\n",
      "iteration:  53190 , total_loss:  21.572442944844564\n",
      "count_amostra: 5319034\n",
      "iteration:  53220 , total_loss:  20.91201572418213\n",
      "count_amostra: 5322034\n",
      "iteration:  53250 , total_loss:  21.561612701416017\n",
      "count_amostra: 5325034\n",
      "iteration:  53280 , total_loss:  21.13566951751709\n",
      "count_amostra: 5328034\n",
      "iteration:  53310 , total_loss:  21.125971539815268\n",
      "count_amostra: 5331034\n",
      "iteration:  53340 , total_loss:  21.413613255818685\n",
      "count_amostra: 5334034\n",
      "iteration:  53370 , total_loss:  21.031318283081056\n",
      "count_amostra: 5337034\n",
      "iteration:  53400 , total_loss:  21.54637279510498\n",
      "count_amostra: 5340034\n",
      "iteration:  53430 , total_loss:  21.500454457600913\n",
      "count_amostra: 5343034\n",
      "iteration:  53460 , total_loss:  21.39318733215332\n",
      "count_amostra: 5346034\n",
      "iteration:  53490 , total_loss:  21.411618804931642\n",
      "count_amostra: 5349034\n",
      "iteration:  53520 , total_loss:  21.49870891571045\n",
      "count_amostra: 5352034\n",
      "iteration:  53550 , total_loss:  21.514484469095866\n",
      "count_amostra: 5355034\n",
      "iteration:  53580 , total_loss:  21.738561312357586\n",
      "count_amostra: 5358034\n",
      "iteration:  53610 , total_loss:  21.00585912068685\n",
      "count_amostra: 5361034\n",
      "iteration:  53640 , total_loss:  21.248061434427896\n",
      "count_amostra: 5364034\n",
      "iteration:  53670 , total_loss:  21.324266942342124\n",
      "count_amostra: 5367034\n",
      "iteration:  53700 , total_loss:  21.25938148498535\n",
      "count_amostra: 5370034\n",
      "iteration:  53730 , total_loss:  21.203666178385415\n",
      "count_amostra: 5373034\n",
      "iteration:  53760 , total_loss:  21.238921801249187\n",
      "count_amostra: 5376034\n",
      "iteration:  53790 , total_loss:  21.367947324117026\n",
      "count_amostra: 5379034\n",
      "iteration:  53820 , total_loss:  21.33200168609619\n",
      "count_amostra: 5382034\n",
      "iteration:  53850 , total_loss:  21.565309651692708\n",
      "count_amostra: 5385034\n",
      "iteration:  53880 , total_loss:  21.394096438090006\n",
      "count_amostra: 5388034\n",
      "iteration:  53910 , total_loss:  21.4259007136027\n",
      "count_amostra: 5391034\n",
      "iteration:  53940 , total_loss:  20.87531852722168\n",
      "count_amostra: 5394034\n",
      "iteration:  53970 , total_loss:  21.290245628356935\n",
      "count_amostra: 5397034\n",
      "iteration:  54000 , total_loss:  21.597597058614095\n",
      "count_amostra: 5400034\n",
      "iteration:  54030 , total_loss:  21.119277572631837\n",
      "count_amostra: 5403034\n",
      "iteration:  54060 , total_loss:  21.271568234761556\n",
      "count_amostra: 5406034\n",
      "iteration:  54090 , total_loss:  21.587401898701987\n",
      "count_amostra: 5409034\n",
      "iteration:  54120 , total_loss:  21.206986300150554\n",
      "count_amostra: 5412034\n",
      "iteration:  54150 , total_loss:  21.694826062520345\n",
      "count_amostra: 5415034\n",
      "iteration:  54180 , total_loss:  21.34962050120036\n",
      "count_amostra: 5418034\n",
      "iteration:  54210 , total_loss:  21.133010737101237\n",
      "count_amostra: 5421034\n",
      "iteration:  54240 , total_loss:  21.582240867614747\n",
      "count_amostra: 5424034\n",
      "iteration:  54270 , total_loss:  21.56909726460775\n",
      "count_amostra: 5427034\n",
      "iteration:  54300 , total_loss:  21.14557514190674\n",
      "count_amostra: 5430034\n",
      "iteration:  54330 , total_loss:  21.489732678731283\n",
      "count_amostra: 5433034\n",
      "iteration:  54360 , total_loss:  21.66307627360026\n",
      "count_amostra: 5436034\n",
      "iteration:  54390 , total_loss:  21.221921920776367\n",
      "count_amostra: 5439034\n",
      "iteration:  54420 , total_loss:  21.420335896809895\n",
      "count_amostra: 5442034\n",
      "iteration:  54450 , total_loss:  21.034365781148274\n",
      "count_amostra: 5445034\n",
      "iteration:  54480 , total_loss:  21.4069979985555\n",
      "count_amostra: 5448034\n",
      "iteration:  54510 , total_loss:  21.866317621866862\n",
      "count_amostra: 5451034\n",
      "iteration:  54540 , total_loss:  21.47360521952311\n",
      "count_amostra: 5454034\n",
      "iteration:  54570 , total_loss:  21.379333877563475\n",
      "count_amostra: 5457034\n",
      "iteration:  54600 , total_loss:  21.17819945017497\n",
      "count_amostra: 5460034\n",
      "iteration:  54630 , total_loss:  21.67705529530843\n",
      "count_amostra: 5463034\n",
      "iteration:  54660 , total_loss:  21.441842969258627\n",
      "count_amostra: 5466034\n",
      "iteration:  54690 , total_loss:  21.624221547444662\n",
      "count_amostra: 5469034\n",
      "iteration:  54720 , total_loss:  21.244556554158528\n",
      "count_amostra: 5472034\n",
      "iteration:  54750 , total_loss:  21.22484073638916\n",
      "count_amostra: 5475034\n",
      "iteration:  54780 , total_loss:  20.97923730214437\n",
      "count_amostra: 5478034\n",
      "iteration:  54810 , total_loss:  21.43721758524577\n",
      "count_amostra: 5481034\n",
      "iteration:  54840 , total_loss:  21.950403531392414\n",
      "count_amostra: 5484034\n",
      "iteration:  54870 , total_loss:  21.103864224751792\n",
      "count_amostra: 5487034\n",
      "iteration:  54900 , total_loss:  21.839682197570802\n",
      "count_amostra: 5489972\n",
      "iteration:  54930 , total_loss:  21.41466522216797\n",
      "count_amostra: 5492972\n",
      "iteration:  54960 , total_loss:  21.47418581644694\n",
      "count_amostra: 5495972\n",
      "iteration:  54990 , total_loss:  21.864983495076498\n",
      "count_amostra: 5498972\n",
      "iteration:  55020 , total_loss:  20.983585421244303\n",
      "count_amostra: 5501972\n",
      "iteration:  55050 , total_loss:  21.378081957499187\n",
      "count_amostra: 5504972\n",
      "iteration:  55080 , total_loss:  20.809066327412925\n",
      "count_amostra: 5507972\n",
      "iteration:  55110 , total_loss:  21.14573529561361\n",
      "count_amostra: 5510972\n",
      "iteration:  55140 , total_loss:  21.680192375183104\n",
      "count_amostra: 5513972\n",
      "iteration:  55170 , total_loss:  21.52715841929118\n",
      "count_amostra: 5516972\n",
      "iteration:  55200 , total_loss:  21.59107913970947\n",
      "count_amostra: 5519972\n",
      "iteration:  55230 , total_loss:  21.335710207621258\n",
      "count_amostra: 5522972\n",
      "iteration:  55260 , total_loss:  21.12311986287435\n",
      "count_amostra: 5525972\n",
      "iteration:  55290 , total_loss:  21.502503967285158\n",
      "count_amostra: 5528972\n",
      "iteration:  55320 , total_loss:  21.08244603474935\n",
      "count_amostra: 5531972\n",
      "iteration:  55350 , total_loss:  21.41473280588786\n",
      "count_amostra: 5534972\n",
      "iteration:  55380 , total_loss:  21.28882255554199\n",
      "count_amostra: 5537972\n",
      "iteration:  55410 , total_loss:  21.582447497049966\n",
      "count_amostra: 5540972\n",
      "iteration:  55440 , total_loss:  21.53248945871989\n",
      "count_amostra: 5543972\n",
      "iteration:  55470 , total_loss:  21.13128401438395\n",
      "count_amostra: 5546972\n",
      "iteration:  55500 , total_loss:  21.270789210001627\n",
      "count_amostra: 5549972\n",
      "iteration:  55530 , total_loss:  21.49751434326172\n",
      "count_amostra: 5552972\n",
      "iteration:  55560 , total_loss:  21.579394912719728\n",
      "count_amostra: 5555972\n",
      "iteration:  55590 , total_loss:  21.285984611511232\n",
      "count_amostra: 5558972\n",
      "iteration:  55620 , total_loss:  21.68215649922689\n",
      "count_amostra: 5561972\n",
      "iteration:  55650 , total_loss:  21.539283816019694\n",
      "count_amostra: 5564972\n",
      "iteration:  55680 , total_loss:  21.651596132914225\n",
      "count_amostra: 5567972\n",
      "iteration:  55710 , total_loss:  21.58088181813558\n",
      "count_amostra: 5570972\n",
      "iteration:  55740 , total_loss:  21.27503922780355\n",
      "count_amostra: 5573972\n",
      "iteration:  55770 , total_loss:  21.10319849650065\n",
      "count_amostra: 5576972\n",
      "iteration:  55800 , total_loss:  21.702689297993977\n",
      "count_amostra: 5579972\n",
      "iteration:  55830 , total_loss:  21.752807744344075\n",
      "count_amostra: 5582972\n",
      "iteration:  55860 , total_loss:  21.50382506052653\n",
      "count_amostra: 5585972\n",
      "iteration:  55890 , total_loss:  21.82220567067464\n",
      "count_amostra: 5588972\n",
      "iteration:  55920 , total_loss:  21.576570638020833\n",
      "count_amostra: 5591972\n",
      "iteration:  55950 , total_loss:  20.8567502339681\n",
      "count_amostra: 5594972\n",
      "iteration:  55980 , total_loss:  21.755006663004558\n",
      "count_amostra: 5597972\n",
      "iteration:  56010 , total_loss:  21.414728101094564\n",
      "count_amostra: 5600972\n",
      "iteration:  56040 , total_loss:  21.506690788269044\n",
      "count_amostra: 5603972\n",
      "iteration:  56070 , total_loss:  21.53682912190755\n",
      "count_amostra: 5606972\n",
      "iteration:  56100 , total_loss:  21.20309632619222\n",
      "count_amostra: 5609972\n",
      "iteration:  56130 , total_loss:  21.60179475148519\n",
      "count_amostra: 5612972\n",
      "iteration:  56160 , total_loss:  21.196633593241373\n",
      "count_amostra: 5615972\n",
      "iteration:  56190 , total_loss:  21.070803515116374\n",
      "count_amostra: 5618972\n",
      "iteration:  56220 , total_loss:  21.699286460876465\n",
      "count_amostra: 5621972\n",
      "iteration:  56250 , total_loss:  21.43200626373291\n",
      "count_amostra: 5624972\n",
      "iteration:  56280 , total_loss:  20.944222005208335\n",
      "count_amostra: 5627972\n",
      "iteration:  56310 , total_loss:  21.043605359395347\n",
      "count_amostra: 5630972\n",
      "iteration:  56340 , total_loss:  21.19352804819743\n",
      "count_amostra: 5633972\n",
      "iteration:  56370 , total_loss:  21.36945629119873\n",
      "count_amostra: 5636972\n",
      "iteration:  56400 , total_loss:  21.08021380106608\n",
      "count_amostra: 5639972\n",
      "iteration:  56430 , total_loss:  21.154946200052898\n",
      "count_amostra: 5642972\n",
      "iteration:  56460 , total_loss:  21.219973119099937\n",
      "count_amostra: 5645972\n",
      "iteration:  56490 , total_loss:  21.47412738800049\n",
      "count_amostra: 5648972\n",
      "iteration:  56520 , total_loss:  21.154249064127605\n",
      "count_amostra: 5651972\n",
      "iteration:  56550 , total_loss:  21.6178991317749\n",
      "count_amostra: 5654972\n",
      "iteration:  56580 , total_loss:  20.980301284790038\n",
      "count_amostra: 5657972\n",
      "iteration:  56610 , total_loss:  21.48259080251058\n",
      "count_amostra: 5660972\n",
      "iteration:  56640 , total_loss:  21.943271509806316\n",
      "count_amostra: 5663972\n",
      "iteration:  56670 , total_loss:  21.532761001586913\n",
      "count_amostra: 5666972\n",
      "iteration:  56700 , total_loss:  21.688203557332358\n",
      "count_amostra: 5669972\n",
      "iteration:  56730 , total_loss:  21.08649597167969\n",
      "count_amostra: 5672972\n",
      "iteration:  56760 , total_loss:  21.388197898864746\n",
      "count_amostra: 5675972\n",
      "iteration:  56790 , total_loss:  21.465185165405273\n",
      "count_amostra: 5678972\n",
      "iteration:  56820 , total_loss:  21.356575457255044\n",
      "count_amostra: 5681972\n",
      "iteration:  56850 , total_loss:  21.10977579752604\n",
      "count_amostra: 5684972\n",
      "iteration:  56880 , total_loss:  21.508935356140135\n",
      "count_amostra: 5687972\n",
      "iteration:  56910 , total_loss:  21.47238032023112\n",
      "count_amostra: 5690972\n",
      "iteration:  56940 , total_loss:  21.12854963938395\n",
      "count_amostra: 5693972\n",
      "iteration:  56970 , total_loss:  21.59080778757731\n",
      "count_amostra: 5696972\n",
      "iteration:  57000 , total_loss:  21.610057004292806\n",
      "count_amostra: 5699972\n",
      "iteration:  57030 , total_loss:  21.319296073913574\n",
      "count_amostra: 5702972\n",
      "iteration:  57060 , total_loss:  20.848108355204264\n",
      "count_amostra: 5705972\n",
      "iteration:  57090 , total_loss:  21.75749708811442\n",
      "count_amostra: 5708972\n",
      "iteration:  57120 , total_loss:  21.751860109965005\n",
      "count_amostra: 5711972\n",
      "iteration:  57150 , total_loss:  21.475597763061522\n",
      "count_amostra: 5714972\n",
      "iteration:  57180 , total_loss:  21.908297538757324\n",
      "count_amostra: 5717972\n",
      "iteration:  57210 , total_loss:  21.378162956237794\n",
      "count_amostra: 5720972\n",
      "iteration:  57240 , total_loss:  21.406012789408365\n",
      "count_amostra: 5723972\n",
      "iteration:  57270 , total_loss:  21.12831433614095\n",
      "count_amostra: 5726972\n",
      "iteration:  57300 , total_loss:  21.23641980489095\n",
      "count_amostra: 5729972\n",
      "iteration:  57330 , total_loss:  21.87383944193522\n",
      "count_amostra: 5732972\n",
      "iteration:  57360 , total_loss:  21.25499897003174\n",
      "count_amostra: 5735972\n",
      "iteration:  57390 , total_loss:  21.277233123779297\n",
      "count_amostra: 5738972\n",
      "iteration:  57420 , total_loss:  21.36648610432943\n",
      "count_amostra: 5741972\n",
      "iteration:  57450 , total_loss:  21.009205691019694\n",
      "count_amostra: 5744972\n",
      "iteration:  57480 , total_loss:  21.572860463460287\n",
      "count_amostra: 5747972\n",
      "iteration:  57510 , total_loss:  21.594721921284993\n",
      "count_amostra: 5750972\n",
      "iteration:  57540 , total_loss:  21.65522117614746\n",
      "count_amostra: 5753972\n",
      "iteration:  57570 , total_loss:  21.273412450154623\n",
      "count_amostra: 5756972\n",
      "iteration:  57600 , total_loss:  21.27690010070801\n",
      "count_amostra: 5759972\n",
      "iteration:  57630 , total_loss:  21.030744997660317\n",
      "count_amostra: 5762972\n",
      "iteration:  57660 , total_loss:  21.134087562561035\n",
      "count_amostra: 5765972\n",
      "iteration:  57690 , total_loss:  21.312328910827638\n",
      "count_amostra: 5768972\n",
      "iteration:  57720 , total_loss:  21.472296460469565\n",
      "count_amostra: 5771972\n",
      "iteration:  57750 , total_loss:  21.222092310587566\n",
      "count_amostra: 5774972\n",
      "iteration:  57780 , total_loss:  20.972683397928872\n",
      "count_amostra: 5777972\n",
      "iteration:  57810 , total_loss:  21.73390916188558\n",
      "count_amostra: 5780972\n",
      "iteration:  57840 , total_loss:  21.055355517069497\n",
      "count_amostra: 5783972\n",
      "iteration:  57870 , total_loss:  21.39454116821289\n",
      "count_amostra: 5786972\n",
      "iteration:  57900 , total_loss:  21.224880854288738\n",
      "count_amostra: 5789972\n",
      "iteration:  57930 , total_loss:  20.995417086283364\n",
      "count_amostra: 5792972\n",
      "iteration:  57960 , total_loss:  20.94445718129476\n",
      "count_amostra: 5795972\n",
      "iteration:  57990 , total_loss:  21.327228164672853\n",
      "count_amostra: 5798972\n",
      "iteration:  58020 , total_loss:  21.622984250386555\n",
      "count_amostra: 5801972\n",
      "iteration:  58050 , total_loss:  21.47289301554362\n",
      "count_amostra: 5804972\n",
      "iteration:  58080 , total_loss:  20.92333787282308\n",
      "count_amostra: 5807972\n",
      "iteration:  58110 , total_loss:  21.528354136149087\n",
      "count_amostra: 5810972\n",
      "iteration:  58140 , total_loss:  21.077325185139973\n",
      "count_amostra: 5813972\n",
      "iteration:  58170 , total_loss:  21.64105504353841\n",
      "count_amostra: 5816972\n",
      "iteration:  58200 , total_loss:  21.62679100036621\n",
      "count_amostra: 5819972\n",
      "iteration:  58230 , total_loss:  21.010738817850747\n",
      "count_amostra: 5822972\n",
      "iteration:  58260 , total_loss:  21.31388339996338\n",
      "count_amostra: 5825972\n",
      "iteration:  58290 , total_loss:  21.022269630432127\n",
      "count_amostra: 5828972\n",
      "iteration:  58320 , total_loss:  21.538487497965495\n",
      "count_amostra: 5831972\n",
      "iteration:  58350 , total_loss:  21.229929224650064\n",
      "count_amostra: 5834972\n",
      "iteration:  58380 , total_loss:  21.178138160705565\n",
      "count_amostra: 5837972\n",
      "iteration:  58410 , total_loss:  21.447504997253418\n",
      "count_amostra: 5840972\n",
      "iteration:  58440 , total_loss:  21.481261761983237\n",
      "count_amostra: 5843972\n",
      "iteration:  58470 , total_loss:  20.952131334940592\n",
      "count_amostra: 5846972\n",
      "iteration:  58500 , total_loss:  21.408023389180503\n",
      "count_amostra: 5849972\n",
      "iteration:  58530 , total_loss:  21.155605125427247\n",
      "count_amostra: 5852972\n",
      "iteration:  58560 , total_loss:  21.212779235839843\n",
      "count_amostra: 5855972\n",
      "iteration:  58590 , total_loss:  20.961256408691405\n",
      "count_amostra: 5858972\n",
      "iteration:  58620 , total_loss:  20.936576271057127\n",
      "count_amostra: 5861972\n",
      "iteration:  58650 , total_loss:  21.431237920125326\n",
      "count_amostra: 5864972\n",
      "iteration:  58680 , total_loss:  21.447505251566568\n",
      "count_amostra: 5867972\n",
      "iteration:  58710 , total_loss:  21.30035400390625\n",
      "count_amostra: 5870972\n",
      "iteration:  58740 , total_loss:  21.149276288350425\n",
      "count_amostra: 5873972\n",
      "iteration:  58770 , total_loss:  21.39235521952311\n",
      "count_amostra: 5876972\n",
      "iteration:  58800 , total_loss:  21.35762971242269\n",
      "count_amostra: 5879972\n",
      "iteration:  58830 , total_loss:  21.265221214294435\n",
      "count_amostra: 5882972\n",
      "iteration:  58860 , total_loss:  21.221152305603027\n",
      "count_amostra: 5885972\n",
      "iteration:  58890 , total_loss:  21.330897839864097\n",
      "count_amostra: 5888972\n",
      "iteration:  58920 , total_loss:  21.340687878926595\n",
      "count_amostra: 5891972\n",
      "iteration:  58950 , total_loss:  21.053859583536784\n",
      "count_amostra: 5894972\n",
      "iteration:  58980 , total_loss:  21.185029411315917\n",
      "count_amostra: 5897972\n",
      "iteration:  59010 , total_loss:  21.305941772460937\n",
      "count_amostra: 5900972\n",
      "iteration:  59040 , total_loss:  21.36942329406738\n",
      "count_amostra: 5903972\n",
      "iteration:  59070 , total_loss:  21.236553700764976\n",
      "count_amostra: 5906972\n",
      "iteration:  59100 , total_loss:  21.15967362721761\n",
      "count_amostra: 5909972\n",
      "iteration:  59130 , total_loss:  21.19898535410563\n",
      "count_amostra: 5912972\n",
      "iteration:  59160 , total_loss:  21.53928540547689\n",
      "count_amostra: 5915972\n",
      "iteration:  59190 , total_loss:  21.099255752563476\n",
      "count_amostra: 5918972\n",
      "iteration:  59220 , total_loss:  21.20309689839681\n",
      "count_amostra: 5921972\n",
      "iteration:  59250 , total_loss:  21.33129990895589\n",
      "count_amostra: 5924972\n",
      "iteration:  59280 , total_loss:  21.159243202209474\n",
      "count_amostra: 5927972\n",
      "iteration:  59310 , total_loss:  21.742893664042153\n",
      "count_amostra: 5930972\n",
      "iteration:  59340 , total_loss:  21.177658971150716\n",
      "count_amostra: 5933972\n",
      "iteration:  59370 , total_loss:  21.204960759480795\n",
      "count_amostra: 5936972\n",
      "iteration:  59400 , total_loss:  21.218940099080402\n",
      "count_amostra: 5939972\n",
      "iteration:  59430 , total_loss:  21.09574178059896\n",
      "count_amostra: 5942972\n",
      "iteration:  59460 , total_loss:  20.799596786499023\n",
      "count_amostra: 5945972\n",
      "iteration:  59490 , total_loss:  21.426620038350425\n",
      "count_amostra: 5948972\n",
      "iteration:  59520 , total_loss:  21.344062678019206\n",
      "count_amostra: 5951972\n",
      "iteration:  59550 , total_loss:  21.12192039489746\n",
      "count_amostra: 5954972\n",
      "iteration:  59580 , total_loss:  21.119952328999837\n",
      "count_amostra: 5957972\n",
      "iteration:  59610 , total_loss:  21.323126157124836\n",
      "count_amostra: 5960972\n",
      "iteration:  59640 , total_loss:  21.299786949157713\n",
      "count_amostra: 5963972\n",
      "iteration:  59670 , total_loss:  21.557724380493163\n",
      "count_amostra: 5966972\n",
      "iteration:  59700 , total_loss:  21.572026189168295\n",
      "count_amostra: 5969972\n",
      "iteration:  59730 , total_loss:  21.282071685791017\n",
      "count_amostra: 5972972\n",
      "iteration:  59760 , total_loss:  21.106919479370116\n",
      "count_amostra: 5975972\n",
      "iteration:  59790 , total_loss:  21.417857297261556\n",
      "count_amostra: 5978972\n",
      "iteration:  59820 , total_loss:  21.46851660410563\n",
      "count_amostra: 5981972\n",
      "iteration:  59850 , total_loss:  21.15599670410156\n",
      "count_amostra: 5984972\n",
      "iteration:  59880 , total_loss:  21.55645618438721\n",
      "count_amostra: 5987972\n",
      "iteration:  59910 , total_loss:  21.249730110168457\n",
      "count_amostra: 5990972\n",
      "iteration:  59940 , total_loss:  20.78470687866211\n",
      "count_amostra: 5993972\n",
      "iteration:  59970 , total_loss:  21.185901069641112\n",
      "count_amostra: 5996972\n",
      "iteration:  60000 , total_loss:  21.4503781636556\n",
      "count_amostra: 5999972\n",
      "iteration:  60030 , total_loss:  21.401765886942545\n",
      "count_amostra: 6002972\n",
      "iteration:  60060 , total_loss:  21.22759501139323\n",
      "count_amostra: 6005972\n",
      "iteration:  60090 , total_loss:  21.107889556884764\n",
      "count_amostra: 6008972\n",
      "iteration:  60120 , total_loss:  21.285896428426106\n",
      "count_amostra: 6011972\n",
      "iteration:  60150 , total_loss:  21.403098169962565\n",
      "count_amostra: 6014972\n",
      "iteration:  60180 , total_loss:  21.45122388203939\n",
      "count_amostra: 6017972\n",
      "iteration:  60210 , total_loss:  20.981631978352866\n",
      "count_amostra: 6020972\n",
      "iteration:  60240 , total_loss:  21.453415552775066\n",
      "count_amostra: 6023972\n",
      "iteration:  60270 , total_loss:  21.177052370707194\n",
      "count_amostra: 6026972\n",
      "iteration:  60300 , total_loss:  21.506241226196288\n",
      "count_amostra: 6029972\n",
      "iteration:  60330 , total_loss:  21.274855931599934\n",
      "count_amostra: 6032972\n",
      "iteration:  60360 , total_loss:  21.484420903523763\n",
      "count_amostra: 6035972\n",
      "iteration:  60390 , total_loss:  21.422107442220053\n",
      "count_amostra: 6038972\n",
      "iteration:  60420 , total_loss:  21.095613797505695\n",
      "count_amostra: 6041972\n",
      "iteration:  60450 , total_loss:  21.16351712544759\n",
      "count_amostra: 6044972\n",
      "iteration:  60480 , total_loss:  21.094437026977538\n",
      "count_amostra: 6047972\n",
      "iteration:  60510 , total_loss:  21.49527791341146\n",
      "count_amostra: 6050972\n",
      "iteration:  60540 , total_loss:  21.21598046620687\n",
      "count_amostra: 6053972\n",
      "iteration:  60570 , total_loss:  20.972669665018717\n",
      "count_amostra: 6056972\n",
      "iteration:  60600 , total_loss:  21.32230962117513\n",
      "count_amostra: 6059972\n",
      "iteration:  60630 , total_loss:  21.261186345418295\n",
      "count_amostra: 6062972\n",
      "iteration:  60660 , total_loss:  21.454963302612306\n",
      "count_amostra: 6065972\n",
      "iteration:  60690 , total_loss:  21.638603528340656\n",
      "count_amostra: 6068972\n",
      "iteration:  60720 , total_loss:  21.13409843444824\n",
      "count_amostra: 6071972\n",
      "iteration:  60750 , total_loss:  21.244061533610026\n",
      "count_amostra: 6074972\n",
      "iteration:  60780 , total_loss:  21.665812174479168\n",
      "count_amostra: 6077972\n",
      "iteration:  60810 , total_loss:  21.34264939626058\n",
      "count_amostra: 6080972\n",
      "iteration:  60840 , total_loss:  21.153048451741537\n",
      "count_amostra: 6083972\n",
      "iteration:  60870 , total_loss:  21.00186405181885\n",
      "count_amostra: 6086972\n",
      "iteration:  60900 , total_loss:  21.367651494344077\n",
      "count_amostra: 6089972\n",
      "iteration:  60930 , total_loss:  21.163948567708335\n",
      "count_amostra: 6092972\n",
      "iteration:  60960 , total_loss:  21.11951592763265\n",
      "count_amostra: 6095972\n",
      "iteration:  60990 , total_loss:  21.234203720092772\n",
      "count_amostra: 6098972\n",
      "iteration:  61020 , total_loss:  21.163805198669433\n",
      "count_amostra: 6101972\n",
      "iteration:  61050 , total_loss:  21.297144571940105\n",
      "count_amostra: 6104972\n",
      "iteration:  61080 , total_loss:  21.210307375590006\n",
      "count_amostra: 6107972\n",
      "iteration:  61110 , total_loss:  21.430248578389484\n",
      "count_amostra: 6110972\n",
      "iteration:  61140 , total_loss:  21.355265045166014\n",
      "count_amostra: 6113972\n",
      "iteration:  61170 , total_loss:  21.251110204060872\n",
      "count_amostra: 6116972\n",
      "iteration:  61200 , total_loss:  21.165943654378257\n",
      "count_amostra: 6119972\n",
      "iteration:  61230 , total_loss:  20.804082043965657\n",
      "count_amostra: 6122972\n",
      "iteration:  61260 , total_loss:  21.458166758219402\n",
      "count_amostra: 6125972\n",
      "iteration:  61290 , total_loss:  21.160028839111327\n",
      "count_amostra: 6128972\n",
      "iteration:  61320 , total_loss:  21.391994603474934\n",
      "count_amostra: 6131972\n",
      "iteration:  61350 , total_loss:  21.342803891499837\n",
      "count_amostra: 6134972\n",
      "iteration:  61380 , total_loss:  20.881228764851887\n",
      "count_amostra: 6137972\n",
      "iteration:  61410 , total_loss:  20.91716251373291\n",
      "count_amostra: 6140972\n",
      "iteration:  61440 , total_loss:  20.991245206197103\n",
      "count_amostra: 6143972\n",
      "iteration:  61470 , total_loss:  20.93322474161784\n",
      "count_amostra: 6146972\n",
      "iteration:  61500 , total_loss:  21.31294714609782\n",
      "count_amostra: 6149972\n",
      "iteration:  61530 , total_loss:  20.946986134847005\n",
      "count_amostra: 6152972\n",
      "iteration:  61560 , total_loss:  21.402988942464194\n",
      "count_amostra: 6155972\n",
      "iteration:  61590 , total_loss:  21.479403368632\n",
      "count_amostra: 6158972\n",
      "iteration:  61620 , total_loss:  21.50012149810791\n",
      "count_amostra: 6161972\n",
      "iteration:  61650 , total_loss:  21.13254674275716\n",
      "count_amostra: 6164972\n",
      "iteration:  61680 , total_loss:  21.6393035252889\n",
      "count_amostra: 6167972\n",
      "iteration:  61710 , total_loss:  21.36423905690511\n",
      "count_amostra: 6170972\n",
      "iteration:  61740 , total_loss:  20.823767344156902\n",
      "count_amostra: 6173972\n",
      "iteration:  61770 , total_loss:  21.395647239685058\n",
      "count_amostra: 6176972\n",
      "iteration:  61800 , total_loss:  21.18560924530029\n",
      "count_amostra: 6179972\n",
      "iteration:  61830 , total_loss:  21.212878036499024\n",
      "count_amostra: 6182972\n",
      "iteration:  61860 , total_loss:  21.309435907999674\n",
      "count_amostra: 6185972\n",
      "iteration:  61890 , total_loss:  21.199100240071616\n",
      "count_amostra: 6188972\n",
      "iteration:  61920 , total_loss:  21.51218916575114\n",
      "count_amostra: 6191972\n",
      "iteration:  61950 , total_loss:  21.462701416015626\n",
      "count_amostra: 6194972\n",
      "iteration:  61980 , total_loss:  21.117022132873537\n",
      "count_amostra: 6197972\n",
      "iteration:  62010 , total_loss:  21.33634599049886\n",
      "count_amostra: 6200972\n",
      "iteration:  62040 , total_loss:  20.934918721516926\n",
      "count_amostra: 6203972\n",
      "iteration:  62070 , total_loss:  21.411526044209797\n",
      "count_amostra: 6206972\n",
      "iteration:  62100 , total_loss:  21.049648094177247\n",
      "count_amostra: 6209972\n",
      "iteration:  62130 , total_loss:  21.30702559153239\n",
      "count_amostra: 6212972\n",
      "iteration:  62160 , total_loss:  20.930495262145996\n",
      "count_amostra: 6215972\n",
      "iteration:  62190 , total_loss:  21.12672125498454\n",
      "count_amostra: 6218972\n",
      "iteration:  62220 , total_loss:  21.332665125528973\n",
      "count_amostra: 6221972\n",
      "iteration:  62250 , total_loss:  21.041914812723796\n",
      "count_amostra: 6224972\n",
      "iteration:  62280 , total_loss:  20.709101931254068\n",
      "count_amostra: 6227972\n",
      "iteration:  62310 , total_loss:  21.32454999287923\n",
      "count_amostra: 6230972\n",
      "iteration:  62340 , total_loss:  21.312564404805503\n",
      "count_amostra: 6233972\n",
      "iteration:  62370 , total_loss:  21.262593523661295\n",
      "count_amostra: 6236972\n",
      "iteration:  62400 , total_loss:  21.131074333190917\n",
      "count_amostra: 6239972\n",
      "iteration:  62430 , total_loss:  21.374413935343423\n",
      "count_amostra: 6242972\n",
      "iteration:  62460 , total_loss:  21.411172294616698\n",
      "count_amostra: 6245972\n",
      "iteration:  62490 , total_loss:  21.24877211252848\n",
      "count_amostra: 6248972\n",
      "iteration:  62520 , total_loss:  21.54307270050049\n",
      "count_amostra: 6251972\n",
      "iteration:  62550 , total_loss:  21.034875615437826\n",
      "count_amostra: 6254972\n",
      "iteration:  62580 , total_loss:  20.99724489847819\n",
      "count_amostra: 6257972\n",
      "iteration:  62610 , total_loss:  21.050317128499348\n",
      "count_amostra: 6260972\n",
      "iteration:  62640 , total_loss:  21.505983479817708\n",
      "count_amostra: 6263972\n",
      "iteration:  62670 , total_loss:  21.517050234476724\n",
      "count_amostra: 6266972\n",
      "iteration:  62700 , total_loss:  21.32988166809082\n",
      "count_amostra: 6269972\n",
      "iteration:  62730 , total_loss:  21.251216888427734\n",
      "count_amostra: 6272972\n",
      "iteration:  62760 , total_loss:  21.05451176961263\n",
      "count_amostra: 6275972\n",
      "iteration:  62790 , total_loss:  21.627725664774577\n",
      "count_amostra: 6278972\n",
      "iteration:  62820 , total_loss:  21.13633015950521\n",
      "count_amostra: 6281972\n",
      "iteration:  62850 , total_loss:  21.27346852620443\n",
      "count_amostra: 6284972\n",
      "iteration:  62880 , total_loss:  21.24837220509847\n",
      "count_amostra: 6287972\n",
      "iteration:  62910 , total_loss:  20.887174606323242\n",
      "count_amostra: 6290972\n",
      "iteration:  62940 , total_loss:  21.40407396952311\n",
      "count_amostra: 6293972\n",
      "iteration:  62970 , total_loss:  21.438044548034668\n",
      "count_amostra: 6296972\n",
      "iteration:  63000 , total_loss:  21.04841079711914\n",
      "count_amostra: 6299972\n",
      "iteration:  63030 , total_loss:  21.277458572387694\n",
      "count_amostra: 6302972\n",
      "iteration:  63060 , total_loss:  21.028694725036623\n",
      "count_amostra: 6305972\n",
      "iteration:  63090 , total_loss:  21.255969683329266\n",
      "count_amostra: 6308972\n",
      "iteration:  63120 , total_loss:  21.328675015767416\n",
      "count_amostra: 6311972\n",
      "iteration:  63150 , total_loss:  21.202225303649904\n",
      "count_amostra: 6314972\n",
      "iteration:  63180 , total_loss:  21.318793932596844\n",
      "count_amostra: 6317972\n",
      "iteration:  63210 , total_loss:  21.183345731099447\n",
      "count_amostra: 6320972\n",
      "iteration:  63240 , total_loss:  21.486901219685873\n",
      "count_amostra: 6323972\n",
      "iteration:  63270 , total_loss:  21.389116032918295\n",
      "count_amostra: 6326972\n",
      "iteration:  63300 , total_loss:  21.281093788146972\n",
      "count_amostra: 6329972\n",
      "iteration:  63330 , total_loss:  21.196595891316733\n",
      "count_amostra: 6332972\n",
      "iteration:  63360 , total_loss:  21.069460423787437\n",
      "count_amostra: 6335972\n",
      "iteration:  63390 , total_loss:  21.518428484598797\n",
      "count_amostra: 6338972\n",
      "iteration:  63420 , total_loss:  21.4669984181722\n",
      "count_amostra: 6341972\n",
      "iteration:  63450 , total_loss:  20.795474942525228\n",
      "count_amostra: 6344972\n",
      "iteration:  63480 , total_loss:  20.929716873168946\n",
      "count_amostra: 6347972\n",
      "iteration:  63510 , total_loss:  21.266726938883462\n",
      "count_amostra: 6350972\n",
      "iteration:  63540 , total_loss:  21.116728337605796\n",
      "count_amostra: 6353972\n",
      "iteration:  63570 , total_loss:  21.139638646443686\n",
      "count_amostra: 6356972\n",
      "iteration:  63600 , total_loss:  20.864427947998045\n",
      "count_amostra: 6359972\n",
      "iteration:  63630 , total_loss:  21.39003760019938\n",
      "count_amostra: 6362972\n",
      "iteration:  63660 , total_loss:  21.20982093811035\n",
      "count_amostra: 6365972\n",
      "iteration:  63690 , total_loss:  21.0753630956014\n",
      "count_amostra: 6368972\n",
      "iteration:  63720 , total_loss:  21.45622793833415\n",
      "count_amostra: 6371972\n",
      "iteration:  63750 , total_loss:  20.96025816599528\n",
      "count_amostra: 6374972\n",
      "iteration:  63780 , total_loss:  20.695658874511718\n",
      "count_amostra: 6377972\n",
      "iteration:  63810 , total_loss:  21.00757624308268\n",
      "count_amostra: 6380972\n",
      "iteration:  63840 , total_loss:  21.57223351796468\n",
      "count_amostra: 6383972\n",
      "iteration:  63870 , total_loss:  21.262351671854656\n",
      "count_amostra: 6386900\n",
      "iteration:  63900 , total_loss:  21.343902587890625\n",
      "count_amostra: 6389900\n",
      "iteration:  63930 , total_loss:  21.171084594726562\n",
      "count_amostra: 6392900\n",
      "iteration:  63960 , total_loss:  21.12442175547282\n",
      "count_amostra: 6395900\n",
      "iteration:  63990 , total_loss:  21.185675366719565\n",
      "count_amostra: 6398900\n",
      "iteration:  64020 , total_loss:  21.133429336547852\n",
      "count_amostra: 6401900\n",
      "iteration:  64050 , total_loss:  21.195200538635255\n",
      "count_amostra: 6404900\n",
      "iteration:  64080 , total_loss:  21.135912450154624\n",
      "count_amostra: 6407900\n",
      "iteration:  64110 , total_loss:  21.129863675435384\n",
      "count_amostra: 6410900\n",
      "iteration:  64140 , total_loss:  21.12836348215739\n",
      "count_amostra: 6413900\n",
      "iteration:  64170 , total_loss:  21.24253616333008\n",
      "count_amostra: 6416900\n",
      "iteration:  64200 , total_loss:  21.078879102071127\n",
      "count_amostra: 6419900\n",
      "iteration:  64230 , total_loss:  21.17038040161133\n",
      "count_amostra: 6422900\n",
      "iteration:  64260 , total_loss:  20.897201093037925\n",
      "count_amostra: 6425900\n",
      "iteration:  64290 , total_loss:  21.1778621673584\n",
      "count_amostra: 6428900\n",
      "iteration:  64320 , total_loss:  21.644221623738606\n",
      "count_amostra: 6431900\n",
      "iteration:  64350 , total_loss:  21.055183919270835\n",
      "count_amostra: 6434900\n",
      "iteration:  64380 , total_loss:  21.22191390991211\n",
      "count_amostra: 6437900\n",
      "iteration:  64410 , total_loss:  21.159700711568195\n",
      "count_amostra: 6440900\n",
      "iteration:  64440 , total_loss:  20.849256197611492\n",
      "count_amostra: 6443900\n",
      "iteration:  64470 , total_loss:  21.46955369313558\n",
      "count_amostra: 6446900\n",
      "iteration:  64500 , total_loss:  21.36514155069987\n",
      "count_amostra: 6449900\n",
      "iteration:  64530 , total_loss:  21.40064303080241\n",
      "count_amostra: 6452900\n",
      "iteration:  64560 , total_loss:  21.038761711120607\n",
      "count_amostra: 6455900\n",
      "iteration:  64590 , total_loss:  21.100570742289225\n",
      "count_amostra: 6458900\n",
      "iteration:  64620 , total_loss:  21.193614768981934\n",
      "count_amostra: 6461900\n",
      "iteration:  64650 , total_loss:  21.232889684041343\n",
      "count_amostra: 6464900\n",
      "iteration:  64680 , total_loss:  21.06670722961426\n",
      "count_amostra: 6467900\n",
      "iteration:  64710 , total_loss:  21.1703893661499\n",
      "count_amostra: 6470900\n",
      "iteration:  64740 , total_loss:  21.245263226826985\n",
      "count_amostra: 6473900\n",
      "iteration:  64770 , total_loss:  20.842268053690592\n",
      "count_amostra: 6476900\n",
      "iteration:  64800 , total_loss:  21.511277643839517\n",
      "count_amostra: 6479900\n",
      "iteration:  64830 , total_loss:  20.979498036702473\n",
      "count_amostra: 6482900\n",
      "iteration:  64860 , total_loss:  21.033184496561685\n",
      "count_amostra: 6485900\n",
      "iteration:  64890 , total_loss:  20.96974360148112\n",
      "count_amostra: 6488900\n",
      "iteration:  64920 , total_loss:  21.375445874532065\n",
      "count_amostra: 6491900\n",
      "iteration:  64950 , total_loss:  21.03259391784668\n",
      "count_amostra: 6494900\n",
      "iteration:  64980 , total_loss:  21.407345453898113\n",
      "count_amostra: 6497900\n",
      "iteration:  65010 , total_loss:  21.271423276265462\n",
      "count_amostra: 6500900\n",
      "iteration:  65040 , total_loss:  21.009554862976074\n",
      "count_amostra: 6503900\n",
      "iteration:  65070 , total_loss:  21.348499806722007\n",
      "count_amostra: 6506900\n",
      "iteration:  65100 , total_loss:  21.474628067016603\n",
      "count_amostra: 6509900\n",
      "iteration:  65130 , total_loss:  21.28780352274577\n",
      "count_amostra: 6512900\n",
      "iteration:  65160 , total_loss:  21.349357414245606\n",
      "count_amostra: 6515900\n",
      "iteration:  65190 , total_loss:  21.196699587504067\n",
      "count_amostra: 6518900\n",
      "iteration:  65220 , total_loss:  21.139562034606932\n",
      "count_amostra: 6521900\n",
      "iteration:  65250 , total_loss:  21.38695500691732\n",
      "count_amostra: 6524900\n",
      "iteration:  65280 , total_loss:  20.748856671651204\n",
      "count_amostra: 6527900\n",
      "iteration:  65310 , total_loss:  20.888736724853516\n",
      "count_amostra: 6530900\n",
      "iteration:  65340 , total_loss:  21.125721740722657\n",
      "count_amostra: 6533900\n",
      "iteration:  65370 , total_loss:  20.972719446818033\n",
      "count_amostra: 6536900\n",
      "iteration:  65400 , total_loss:  20.824682744344077\n",
      "count_amostra: 6539900\n",
      "iteration:  65430 , total_loss:  21.094943364461262\n",
      "count_amostra: 6542900\n",
      "iteration:  65460 , total_loss:  21.362180773417155\n",
      "count_amostra: 6545900\n",
      "iteration:  65490 , total_loss:  20.967815844217935\n",
      "count_amostra: 6548900\n",
      "iteration:  65520 , total_loss:  21.40916093190511\n",
      "count_amostra: 6551900\n",
      "iteration:  65550 , total_loss:  21.128423754374186\n",
      "count_amostra: 6554900\n",
      "iteration:  65580 , total_loss:  21.12588405609131\n",
      "count_amostra: 6557900\n",
      "iteration:  65610 , total_loss:  21.602494557698567\n",
      "count_amostra: 6560900\n",
      "iteration:  65640 , total_loss:  21.366954612731934\n",
      "count_amostra: 6563900\n",
      "iteration:  65670 , total_loss:  21.230872027079265\n",
      "count_amostra: 6566900\n",
      "iteration:  65700 , total_loss:  21.11107546488444\n",
      "count_amostra: 6569900\n",
      "iteration:  65730 , total_loss:  21.507488377888997\n",
      "count_amostra: 6572900\n",
      "iteration:  65760 , total_loss:  20.892806434631346\n",
      "count_amostra: 6575900\n",
      "iteration:  65790 , total_loss:  20.82142422993978\n",
      "count_amostra: 6578900\n",
      "iteration:  65820 , total_loss:  21.16565138498942\n",
      "count_amostra: 6581900\n",
      "iteration:  65850 , total_loss:  20.98463789621989\n",
      "count_amostra: 6584900\n",
      "iteration:  65880 , total_loss:  21.553989855448403\n",
      "count_amostra: 6587900\n",
      "iteration:  65910 , total_loss:  21.288408470153808\n",
      "count_amostra: 6590900\n",
      "iteration:  65940 , total_loss:  20.954999033610026\n",
      "count_amostra: 6593900\n",
      "iteration:  65970 , total_loss:  21.206847190856934\n",
      "count_amostra: 6596900\n",
      "iteration:  66000 , total_loss:  21.494459597269692\n",
      "count_amostra: 6599900\n",
      "iteration:  66030 , total_loss:  21.12958380381266\n",
      "count_amostra: 6602900\n",
      "iteration:  66060 , total_loss:  20.99060656229655\n",
      "count_amostra: 6605900\n",
      "iteration:  66090 , total_loss:  21.031354649861655\n",
      "count_amostra: 6608900\n",
      "iteration:  66120 , total_loss:  20.88996810913086\n",
      "count_amostra: 6611900\n",
      "iteration:  66150 , total_loss:  21.15406920115153\n",
      "count_amostra: 6614900\n",
      "iteration:  66180 , total_loss:  20.981490325927734\n",
      "count_amostra: 6617900\n",
      "iteration:  66210 , total_loss:  21.17248675028483\n",
      "count_amostra: 6620900\n",
      "iteration:  66240 , total_loss:  21.43325220743815\n",
      "count_amostra: 6623900\n",
      "iteration:  66270 , total_loss:  21.04587790171305\n",
      "count_amostra: 6626900\n",
      "iteration:  66300 , total_loss:  20.634214210510255\n",
      "count_amostra: 6629900\n",
      "iteration:  66330 , total_loss:  21.28357016245524\n",
      "count_amostra: 6632900\n",
      "iteration:  66360 , total_loss:  21.107079696655273\n",
      "count_amostra: 6635900\n",
      "iteration:  66390 , total_loss:  21.77986583709717\n",
      "count_amostra: 6638900\n",
      "iteration:  66420 , total_loss:  21.27400894165039\n",
      "count_amostra: 6641900\n",
      "iteration:  66450 , total_loss:  21.026062520345054\n",
      "count_amostra: 6644900\n",
      "iteration:  66480 , total_loss:  20.923062070210776\n",
      "count_amostra: 6647900\n",
      "iteration:  66510 , total_loss:  21.331459999084473\n",
      "count_amostra: 6650900\n",
      "iteration:  66540 , total_loss:  21.41109447479248\n",
      "count_amostra: 6653900\n",
      "iteration:  66570 , total_loss:  21.103383763631186\n",
      "count_amostra: 6656900\n",
      "iteration:  66600 , total_loss:  21.286213048299153\n",
      "count_amostra: 6659900\n",
      "iteration:  66630 , total_loss:  21.406787745157878\n",
      "count_amostra: 6662900\n",
      "iteration:  66660 , total_loss:  20.963473892211915\n",
      "count_amostra: 6665900\n",
      "iteration:  66690 , total_loss:  21.46819756825765\n",
      "count_amostra: 6668900\n",
      "iteration:  66720 , total_loss:  20.982635180155437\n",
      "count_amostra: 6671900\n",
      "iteration:  66750 , total_loss:  20.965042304992675\n",
      "count_amostra: 6674900\n",
      "iteration:  66780 , total_loss:  21.094436581929525\n",
      "count_amostra: 6677900\n",
      "iteration:  66810 , total_loss:  21.170107460021974\n",
      "count_amostra: 6680900\n",
      "iteration:  66840 , total_loss:  21.43729591369629\n",
      "count_amostra: 6683900\n",
      "iteration:  66870 , total_loss:  21.161373583475747\n",
      "count_amostra: 6686900\n",
      "iteration:  66900 , total_loss:  21.35426928202311\n",
      "count_amostra: 6689900\n",
      "iteration:  66930 , total_loss:  21.001655197143556\n",
      "count_amostra: 6692900\n",
      "iteration:  66960 , total_loss:  21.310810597737632\n",
      "count_amostra: 6695900\n",
      "iteration:  66990 , total_loss:  20.8829195022583\n",
      "count_amostra: 6698900\n",
      "iteration:  67020 , total_loss:  21.1176274617513\n",
      "count_amostra: 6701900\n",
      "iteration:  67050 , total_loss:  20.91307169596354\n",
      "count_amostra: 6704900\n",
      "iteration:  67080 , total_loss:  21.393364969889323\n",
      "count_amostra: 6707900\n",
      "iteration:  67110 , total_loss:  21.00909277598063\n",
      "count_amostra: 6710900\n",
      "iteration:  67140 , total_loss:  20.926211675008137\n",
      "count_amostra: 6713900\n",
      "iteration:  67170 , total_loss:  21.14023036956787\n",
      "count_amostra: 6716900\n",
      "iteration:  67200 , total_loss:  21.05654671986898\n",
      "count_amostra: 6719900\n",
      "iteration:  67230 , total_loss:  21.337480099995933\n",
      "count_amostra: 6722900\n",
      "iteration:  67260 , total_loss:  21.064400418599448\n",
      "count_amostra: 6725900\n",
      "iteration:  67290 , total_loss:  21.2788880666097\n",
      "count_amostra: 6728900\n",
      "iteration:  67320 , total_loss:  20.978389167785643\n",
      "count_amostra: 6731900\n",
      "iteration:  67350 , total_loss:  20.70216312408447\n",
      "count_amostra: 6734900\n",
      "iteration:  67380 , total_loss:  21.403756014506023\n",
      "count_amostra: 6737900\n",
      "iteration:  67410 , total_loss:  21.159073066711425\n",
      "count_amostra: 6740900\n",
      "iteration:  67440 , total_loss:  21.053792254130045\n",
      "count_amostra: 6743900\n",
      "iteration:  67470 , total_loss:  21.55632807413737\n",
      "count_amostra: 6746900\n",
      "iteration:  67500 , total_loss:  21.33639399210612\n",
      "count_amostra: 6749900\n",
      "iteration:  67530 , total_loss:  20.82523333231608\n",
      "count_amostra: 6752900\n",
      "iteration:  67560 , total_loss:  21.020505777994792\n",
      "count_amostra: 6755900\n",
      "iteration:  67590 , total_loss:  20.979247983296712\n",
      "count_amostra: 6758900\n",
      "iteration:  67620 , total_loss:  21.41551316579183\n",
      "count_amostra: 6761900\n",
      "iteration:  67650 , total_loss:  20.996835136413573\n",
      "count_amostra: 6764900\n",
      "iteration:  67680 , total_loss:  21.101971054077147\n",
      "count_amostra: 6767900\n",
      "iteration:  67710 , total_loss:  21.53036092122396\n",
      "count_amostra: 6770900\n",
      "iteration:  67740 , total_loss:  20.662243143717447\n",
      "count_amostra: 6773900\n",
      "iteration:  67770 , total_loss:  20.75887050628662\n",
      "count_amostra: 6776900\n",
      "iteration:  67800 , total_loss:  21.22917308807373\n",
      "count_amostra: 6779900\n",
      "iteration:  67830 , total_loss:  20.94388058980306\n",
      "count_amostra: 6782900\n",
      "iteration:  67860 , total_loss:  20.911568895975748\n",
      "count_amostra: 6785900\n",
      "iteration:  67890 , total_loss:  21.29676628112793\n",
      "count_amostra: 6788900\n",
      "iteration:  67920 , total_loss:  21.26497548421224\n",
      "count_amostra: 6791900\n",
      "iteration:  67950 , total_loss:  21.085139910380047\n",
      "count_amostra: 6794900\n",
      "iteration:  67980 , total_loss:  21.020185534159342\n",
      "count_amostra: 6797900\n",
      "iteration:  68010 , total_loss:  21.488397534688314\n",
      "count_amostra: 6800900\n",
      "iteration:  68040 , total_loss:  20.83080005645752\n",
      "count_amostra: 6803900\n",
      "iteration:  68070 , total_loss:  21.12646427154541\n",
      "count_amostra: 6806900\n",
      "iteration:  68100 , total_loss:  21.33431714375814\n",
      "count_amostra: 6809900\n",
      "iteration:  68130 , total_loss:  21.63909009297689\n",
      "count_amostra: 6812900\n",
      "iteration:  68160 , total_loss:  21.049142519632976\n",
      "count_amostra: 6815900\n",
      "iteration:  68190 , total_loss:  21.284127108256023\n",
      "count_amostra: 6818900\n",
      "iteration:  68220 , total_loss:  21.23072954813639\n",
      "count_amostra: 6821900\n",
      "iteration:  68250 , total_loss:  21.092855580647786\n",
      "count_amostra: 6824900\n",
      "iteration:  68280 , total_loss:  21.302739524841307\n",
      "count_amostra: 6827900\n",
      "iteration:  68310 , total_loss:  21.150563685099282\n",
      "count_amostra: 6830900\n",
      "iteration:  68340 , total_loss:  21.26059824625651\n",
      "count_amostra: 6833900\n",
      "iteration:  68370 , total_loss:  21.40760943094889\n",
      "count_amostra: 6836900\n",
      "iteration:  68400 , total_loss:  21.157009251912434\n",
      "count_amostra: 6839900\n",
      "iteration:  68430 , total_loss:  21.117918268839517\n",
      "count_amostra: 6842900\n",
      "iteration:  68460 , total_loss:  21.52578811645508\n",
      "count_amostra: 6845900\n",
      "iteration:  68490 , total_loss:  21.0789888381958\n",
      "count_amostra: 6848900\n",
      "iteration:  68520 , total_loss:  20.607605934143066\n",
      "count_amostra: 6851900\n",
      "iteration:  68550 , total_loss:  21.241035652160644\n",
      "count_amostra: 6854900\n",
      "iteration:  68580 , total_loss:  21.16546719868978\n",
      "count_amostra: 6857900\n",
      "iteration:  68610 , total_loss:  20.992833836873373\n",
      "count_amostra: 6860900\n",
      "iteration:  68640 , total_loss:  20.917992464701335\n",
      "count_amostra: 6863900\n",
      "iteration:  68670 , total_loss:  21.377016512552895\n",
      "count_amostra: 6866900\n",
      "iteration:  68700 , total_loss:  21.098188145955405\n",
      "count_amostra: 6869900\n",
      "iteration:  68730 , total_loss:  21.069290161132812\n",
      "count_amostra: 6872900\n",
      "iteration:  68760 , total_loss:  20.824062792460122\n",
      "count_amostra: 6875900\n",
      "iteration:  68790 , total_loss:  20.659591102600096\n",
      "count_amostra: 6878900\n",
      "iteration:  68820 , total_loss:  21.061903444925942\n",
      "count_amostra: 6881900\n",
      "iteration:  68850 , total_loss:  20.813794136047363\n",
      "count_amostra: 6884900\n",
      "iteration:  68880 , total_loss:  20.83555450439453\n",
      "count_amostra: 6887900\n",
      "iteration:  68910 , total_loss:  21.090440877278645\n",
      "count_amostra: 6890900\n",
      "iteration:  68940 , total_loss:  21.243603515625\n",
      "count_amostra: 6893900\n",
      "iteration:  68970 , total_loss:  21.791727574666343\n",
      "count_amostra: 6896900\n",
      "iteration:  69000 , total_loss:  20.901675860087078\n",
      "count_amostra: 6899900\n",
      "iteration:  69030 , total_loss:  21.190616289774578\n",
      "count_amostra: 6902900\n",
      "iteration:  69060 , total_loss:  21.287654940287272\n",
      "count_amostra: 6905900\n",
      "iteration:  69090 , total_loss:  21.408611043294272\n",
      "count_amostra: 6908900\n",
      "iteration:  69120 , total_loss:  20.95378894805908\n",
      "count_amostra: 6911900\n",
      "iteration:  69150 , total_loss:  21.491053835550943\n",
      "count_amostra: 6914900\n",
      "iteration:  69180 , total_loss:  21.00722999572754\n",
      "count_amostra: 6917900\n",
      "iteration:  69210 , total_loss:  20.62015743255615\n",
      "count_amostra: 6920900\n",
      "iteration:  69240 , total_loss:  21.24649124145508\n",
      "count_amostra: 6923900\n",
      "iteration:  69270 , total_loss:  21.09244753519694\n",
      "count_amostra: 6926900\n",
      "iteration:  69300 , total_loss:  20.883416175842285\n",
      "count_amostra: 6929900\n",
      "iteration:  69330 , total_loss:  21.306597073872883\n",
      "count_amostra: 6932900\n",
      "iteration:  69360 , total_loss:  21.34303398132324\n",
      "count_amostra: 6935900\n",
      "iteration:  69390 , total_loss:  21.04900506337484\n",
      "count_amostra: 6938900\n",
      "iteration:  69420 , total_loss:  21.28990739186605\n",
      "count_amostra: 6941900\n",
      "iteration:  69450 , total_loss:  21.18920135498047\n",
      "count_amostra: 6944900\n",
      "iteration:  69480 , total_loss:  20.616203943888348\n",
      "count_amostra: 6947900\n",
      "iteration:  69510 , total_loss:  21.13978385925293\n",
      "count_amostra: 6950900\n",
      "iteration:  69540 , total_loss:  21.319004440307616\n",
      "count_amostra: 6953900\n",
      "iteration:  69570 , total_loss:  21.55524470011393\n",
      "count_amostra: 6956900\n",
      "iteration:  69600 , total_loss:  21.359455617268882\n",
      "count_amostra: 6959900\n",
      "iteration:  69630 , total_loss:  21.487522570292153\n",
      "count_amostra: 6962900\n",
      "iteration:  69660 , total_loss:  21.194611422220866\n",
      "count_amostra: 6965900\n",
      "iteration:  69690 , total_loss:  21.310962359110516\n",
      "count_amostra: 6968900\n",
      "iteration:  69720 , total_loss:  21.006233914693198\n",
      "count_amostra: 6971900\n",
      "iteration:  69750 , total_loss:  21.098164494832357\n",
      "count_amostra: 6974900\n",
      "iteration:  69780 , total_loss:  21.6476105372111\n",
      "count_amostra: 6977900\n",
      "iteration:  69810 , total_loss:  21.170104217529296\n",
      "count_amostra: 6980900\n",
      "iteration:  69840 , total_loss:  21.5313471476237\n",
      "count_amostra: 6983900\n",
      "iteration:  69870 , total_loss:  21.412707583109537\n",
      "count_amostra: 6986900\n",
      "iteration:  69900 , total_loss:  21.31904754638672\n",
      "count_amostra: 6989900\n",
      "iteration:  69930 , total_loss:  20.88246580759684\n",
      "count_amostra: 6992900\n",
      "iteration:  69960 , total_loss:  21.26589438120524\n",
      "count_amostra: 6995900\n",
      "iteration:  69990 , total_loss:  21.309434127807616\n",
      "count_amostra: 6998900\n",
      "iteration:  70020 , total_loss:  20.982020314534505\n",
      "count_amostra: 7001900\n",
      "iteration:  70050 , total_loss:  20.808325576782227\n",
      "count_amostra: 7004900\n",
      "iteration:  70080 , total_loss:  20.766618283589683\n",
      "count_amostra: 7007900\n",
      "iteration:  70110 , total_loss:  21.078602027893066\n",
      "count_amostra: 7010900\n",
      "iteration:  70140 , total_loss:  21.27785962422689\n",
      "count_amostra: 7013900\n",
      "iteration:  70170 , total_loss:  21.512285486857095\n",
      "count_amostra: 7016900\n",
      "iteration:  70200 , total_loss:  21.17791493733724\n",
      "count_amostra: 7019900\n",
      "iteration:  70230 , total_loss:  21.625291315714517\n",
      "count_amostra: 7022900\n",
      "iteration:  70260 , total_loss:  21.06253089904785\n",
      "count_amostra: 7025900\n",
      "iteration:  70290 , total_loss:  21.209281730651856\n",
      "count_amostra: 7028900\n",
      "iteration:  70320 , total_loss:  21.1236021677653\n",
      "count_amostra: 7031900\n",
      "iteration:  70350 , total_loss:  21.219772656758625\n",
      "count_amostra: 7034900\n",
      "iteration:  70380 , total_loss:  20.832988230387368\n",
      "count_amostra: 7037900\n",
      "iteration:  70410 , total_loss:  21.323859659830728\n",
      "count_amostra: 7040900\n",
      "iteration:  70440 , total_loss:  20.95868714650472\n",
      "count_amostra: 7043900\n",
      "iteration:  70470 , total_loss:  21.065166346232097\n",
      "count_amostra: 7046900\n",
      "iteration:  70500 , total_loss:  21.172845458984376\n",
      "count_amostra: 7049900\n",
      "iteration:  70530 , total_loss:  20.743708101908364\n",
      "count_amostra: 7052900\n",
      "iteration:  70560 , total_loss:  20.771649233500163\n",
      "count_amostra: 7055900\n",
      "iteration:  70590 , total_loss:  20.74223607381185\n",
      "count_amostra: 7058900\n",
      "iteration:  70620 , total_loss:  21.24821974436442\n",
      "count_amostra: 7061900\n",
      "iteration:  70650 , total_loss:  20.77048994700114\n",
      "count_amostra: 7064900\n",
      "iteration:  70680 , total_loss:  21.251844088236492\n",
      "count_amostra: 7067900\n",
      "iteration:  70710 , total_loss:  21.368018023173015\n",
      "count_amostra: 7070900\n",
      "iteration:  70740 , total_loss:  20.985661443074545\n",
      "count_amostra: 7073900\n",
      "iteration:  70770 , total_loss:  21.181165568033855\n",
      "count_amostra: 7076900\n",
      "iteration:  70800 , total_loss:  20.891751035054526\n",
      "count_amostra: 7079900\n",
      "iteration:  70830 , total_loss:  20.83579864501953\n",
      "count_amostra: 7082900\n",
      "iteration:  70860 , total_loss:  20.967062950134277\n",
      "count_amostra: 7085900\n",
      "iteration:  70890 , total_loss:  21.318320274353027\n",
      "count_amostra: 7088900\n",
      "iteration:  70920 , total_loss:  20.976123174031574\n",
      "count_amostra: 7091900\n",
      "iteration:  70950 , total_loss:  21.214067522684733\n",
      "count_amostra: 7094900\n",
      "iteration:  70980 , total_loss:  21.100119972229002\n",
      "count_amostra: 7097900\n",
      "iteration:  71010 , total_loss:  20.9684019724528\n",
      "count_amostra: 7100900\n",
      "iteration:  71040 , total_loss:  20.64430898030599\n",
      "count_amostra: 7103900\n",
      "iteration:  71070 , total_loss:  21.1248810450236\n",
      "count_amostra: 7106900\n",
      "iteration:  71100 , total_loss:  20.929375584920248\n",
      "count_amostra: 7109900\n",
      "iteration:  71130 , total_loss:  21.02507603963216\n",
      "count_amostra: 7112900\n",
      "iteration:  71160 , total_loss:  21.00072809855143\n",
      "count_amostra: 7115900\n",
      "iteration:  71190 , total_loss:  21.477936871846516\n",
      "count_amostra: 7118900\n",
      "iteration:  71220 , total_loss:  21.414764722188313\n",
      "count_amostra: 7121900\n",
      "iteration:  71250 , total_loss:  21.059363047281902\n",
      "count_amostra: 7124900\n",
      "iteration:  71280 , total_loss:  21.221176401774088\n",
      "count_amostra: 7127900\n",
      "iteration:  71310 , total_loss:  20.950068283081055\n",
      "count_amostra: 7130900\n",
      "iteration:  71340 , total_loss:  21.14736270904541\n",
      "count_amostra: 7133900\n",
      "iteration:  71370 , total_loss:  21.322527503967287\n",
      "count_amostra: 7136900\n",
      "iteration:  71400 , total_loss:  20.88776067097982\n",
      "count_amostra: 7139900\n",
      "iteration:  71430 , total_loss:  20.95462277730306\n",
      "count_amostra: 7142900\n",
      "iteration:  71460 , total_loss:  20.84831288655599\n",
      "count_amostra: 7145900\n",
      "iteration:  71490 , total_loss:  21.310426330566408\n",
      "count_amostra: 7148900\n",
      "iteration:  71520 , total_loss:  21.161369705200194\n",
      "count_amostra: 7151900\n",
      "iteration:  71550 , total_loss:  20.916762161254884\n",
      "count_amostra: 7154900\n",
      "iteration:  71580 , total_loss:  20.957043393452963\n",
      "count_amostra: 7157900\n",
      "iteration:  71610 , total_loss:  20.94887555440267\n",
      "count_amostra: 7160900\n",
      "iteration:  71640 , total_loss:  21.304289881388346\n",
      "count_amostra: 7163900\n",
      "iteration:  71670 , total_loss:  20.99803460439046\n",
      "count_amostra: 7166900\n",
      "iteration:  71700 , total_loss:  21.377627182006837\n",
      "count_amostra: 7169900\n",
      "iteration:  71730 , total_loss:  21.097311782836915\n",
      "count_amostra: 7172900\n",
      "iteration:  71760 , total_loss:  21.09244753519694\n",
      "count_amostra: 7175900\n",
      "iteration:  71790 , total_loss:  21.032821210225425\n",
      "count_amostra: 7178900\n",
      "iteration:  71820 , total_loss:  20.942017300923666\n",
      "count_amostra: 7181900\n",
      "iteration:  71850 , total_loss:  21.082534917195638\n",
      "count_amostra: 7184900\n",
      "iteration:  71880 , total_loss:  21.075670115152995\n",
      "count_amostra: 7187900\n",
      "iteration:  71910 , total_loss:  21.138181813557942\n",
      "count_amostra: 7190900\n",
      "iteration:  71940 , total_loss:  21.38221575419108\n",
      "count_amostra: 7193900\n",
      "iteration:  71970 , total_loss:  20.696184603373208\n",
      "count_amostra: 7196900\n",
      "iteration:  72000 , total_loss:  21.061791102091473\n",
      "count_amostra: 7199900\n",
      "iteration:  72030 , total_loss:  20.716464805603028\n",
      "count_amostra: 7202900\n",
      "iteration:  72060 , total_loss:  21.388413111368816\n",
      "count_amostra: 7205900\n",
      "iteration:  72090 , total_loss:  21.35253791809082\n",
      "count_amostra: 7208900\n",
      "iteration:  72120 , total_loss:  21.08322270711263\n",
      "count_amostra: 7211900\n",
      "iteration:  72150 , total_loss:  21.022380828857422\n",
      "count_amostra: 7214900\n",
      "iteration:  72180 , total_loss:  20.721790504455566\n",
      "count_amostra: 7217900\n",
      "iteration:  72210 , total_loss:  21.07061456044515\n",
      "count_amostra: 7220900\n",
      "iteration:  72240 , total_loss:  21.029926109313966\n",
      "count_amostra: 7223900\n",
      "iteration:  72270 , total_loss:  21.104393895467123\n",
      "count_amostra: 7226900\n",
      "iteration:  72300 , total_loss:  20.839218266805013\n",
      "count_amostra: 7229900\n",
      "iteration:  72330 , total_loss:  20.72099641164144\n",
      "count_amostra: 7232900\n",
      "iteration:  72360 , total_loss:  21.493586413065593\n",
      "count_amostra: 7235900\n",
      "iteration:  72390 , total_loss:  20.99333101908366\n",
      "count_amostra: 7238900\n",
      "iteration:  72420 , total_loss:  21.095900026957192\n",
      "count_amostra: 7241900\n",
      "iteration:  72450 , total_loss:  20.88887882232666\n",
      "count_amostra: 7244900\n",
      "iteration:  72480 , total_loss:  21.282254791259767\n",
      "count_amostra: 7247900\n",
      "iteration:  72510 , total_loss:  21.169290351867676\n",
      "count_amostra: 7250900\n",
      "iteration:  72540 , total_loss:  20.9536252339681\n",
      "count_amostra: 7253900\n",
      "iteration:  72570 , total_loss:  21.26757221221924\n",
      "count_amostra: 7256900\n",
      "iteration:  72600 , total_loss:  20.75330613454183\n",
      "count_amostra: 7259900\n",
      "iteration:  72630 , total_loss:  20.919841003417968\n",
      "count_amostra: 7262900\n",
      "iteration:  72660 , total_loss:  20.69308522542318\n",
      "count_amostra: 7265900\n",
      "iteration:  72690 , total_loss:  21.15416170756022\n",
      "count_amostra: 7268900\n",
      "iteration:  72720 , total_loss:  21.210301653544107\n",
      "count_amostra: 7271900\n",
      "iteration:  72750 , total_loss:  21.2252072652181\n",
      "count_amostra: 7274900\n",
      "iteration:  72780 , total_loss:  21.01015421549479\n",
      "count_amostra: 7277900\n",
      "iteration:  72810 , total_loss:  21.126949628194172\n",
      "count_amostra: 7280900\n",
      "iteration:  72840 , total_loss:  21.14841365814209\n",
      "count_amostra: 7283900\n",
      "iteration:  72870 , total_loss:  21.194405428568523\n",
      "count_amostra: 7286900\n",
      "iteration:  72900 , total_loss:  21.55570411682129\n",
      "count_amostra: 7289900\n",
      "iteration:  72930 , total_loss:  21.048553276062012\n",
      "count_amostra: 7292900\n",
      "iteration:  72960 , total_loss:  21.260576057434083\n",
      "count_amostra: 7295900\n",
      "iteration:  72990 , total_loss:  21.114619000752768\n",
      "count_amostra: 7298900\n",
      "iteration:  73020 , total_loss:  20.810055224100747\n",
      "count_amostra: 7301900\n",
      "iteration:  73050 , total_loss:  21.020499165852865\n",
      "count_amostra: 7304900\n",
      "iteration:  73080 , total_loss:  20.898051516215006\n",
      "count_amostra: 7307900\n",
      "iteration:  73110 , total_loss:  20.822733688354493\n",
      "count_amostra: 7310900\n",
      "iteration:  73140 , total_loss:  20.94950440724691\n",
      "count_amostra: 7313900\n",
      "iteration:  73170 , total_loss:  20.964414405822755\n",
      "count_amostra: 7316900\n",
      "iteration:  73200 , total_loss:  21.6828182220459\n",
      "count_amostra: 7319900\n",
      "iteration:  73230 , total_loss:  21.314185841878256\n",
      "count_amostra: 7322900\n",
      "iteration:  73260 , total_loss:  20.9405642191569\n",
      "count_amostra: 7325900\n",
      "iteration:  73290 , total_loss:  21.002629280090332\n",
      "count_amostra: 7328900\n",
      "iteration:  73320 , total_loss:  21.05767739613851\n",
      "count_amostra: 7331900\n",
      "iteration:  73350 , total_loss:  21.01992155710856\n",
      "count_amostra: 7334900\n",
      "iteration:  73380 , total_loss:  21.151191838582356\n",
      "count_amostra: 7337900\n",
      "iteration:  73410 , total_loss:  20.787775484720864\n",
      "count_amostra: 7340900\n",
      "iteration:  73440 , total_loss:  20.699574279785157\n",
      "count_amostra: 7343900\n",
      "iteration:  73470 , total_loss:  21.029254213968912\n",
      "count_amostra: 7346900\n",
      "iteration:  73500 , total_loss:  21.143667856852215\n",
      "count_amostra: 7349900\n",
      "iteration:  73530 , total_loss:  21.065495236714682\n",
      "count_amostra: 7352900\n",
      "iteration:  73560 , total_loss:  21.211889012654623\n",
      "count_amostra: 7355900\n",
      "iteration:  73590 , total_loss:  21.173392486572265\n",
      "count_amostra: 7358900\n",
      "iteration:  73620 , total_loss:  21.399762471516926\n",
      "count_amostra: 7361900\n",
      "iteration:  73650 , total_loss:  20.66769358317057\n",
      "count_amostra: 7364900\n",
      "iteration:  73680 , total_loss:  21.12711353302002\n",
      "count_amostra: 7367900\n",
      "iteration:  73710 , total_loss:  21.12446492513021\n",
      "count_amostra: 7370900\n",
      "iteration:  73740 , total_loss:  21.019369824727377\n",
      "count_amostra: 7373900\n",
      "iteration:  73770 , total_loss:  21.029239718119303\n",
      "count_amostra: 7376900\n",
      "iteration:  73800 , total_loss:  21.26404062906901\n",
      "count_amostra: 7379900\n",
      "iteration:  73830 , total_loss:  21.371446927388508\n",
      "count_amostra: 7382900\n",
      "iteration:  73860 , total_loss:  21.235660107930503\n",
      "count_amostra: 7385900\n",
      "iteration:  73890 , total_loss:  20.597429211934408\n",
      "count_amostra: 7388900\n",
      "iteration:  73920 , total_loss:  21.107714207967124\n",
      "count_amostra: 7391900\n",
      "iteration:  73950 , total_loss:  20.67421309153239\n",
      "count_amostra: 7394900\n",
      "iteration:  73980 , total_loss:  21.139000511169435\n",
      "count_amostra: 7397900\n",
      "iteration:  74010 , total_loss:  20.875979487101237\n",
      "count_amostra: 7400900\n",
      "iteration:  74040 , total_loss:  20.684254964192707\n",
      "count_amostra: 7403900\n",
      "iteration:  74070 , total_loss:  20.667377090454103\n",
      "count_amostra: 7406900\n",
      "iteration:  74100 , total_loss:  20.92918847401937\n",
      "count_amostra: 7409900\n",
      "iteration:  74130 , total_loss:  20.89669221242269\n",
      "count_amostra: 7412900\n",
      "iteration:  74160 , total_loss:  21.297904014587402\n",
      "count_amostra: 7415900\n",
      "iteration:  74190 , total_loss:  21.0441535949707\n",
      "count_amostra: 7418900\n",
      "iteration:  74220 , total_loss:  21.30156046549479\n",
      "count_amostra: 7421900\n",
      "iteration:  74250 , total_loss:  20.90717690785726\n",
      "count_amostra: 7424900\n",
      "iteration:  74280 , total_loss:  21.16147518157959\n",
      "count_amostra: 7427900\n",
      "iteration:  74310 , total_loss:  21.03505204518636\n",
      "count_amostra: 7430900\n",
      "iteration:  74340 , total_loss:  20.82514165242513\n",
      "count_amostra: 7433900\n",
      "iteration:  74370 , total_loss:  20.939603487650555\n",
      "count_amostra: 7436900\n",
      "iteration:  74400 , total_loss:  20.703280448913574\n",
      "count_amostra: 7439900\n",
      "iteration:  74430 , total_loss:  21.351622772216796\n",
      "count_amostra: 7442900\n",
      "iteration:  74460 , total_loss:  20.90485045115153\n",
      "count_amostra: 7445900\n",
      "iteration:  74490 , total_loss:  21.1499174118042\n",
      "count_amostra: 7448900\n",
      "iteration:  74520 , total_loss:  20.80547243754069\n",
      "count_amostra: 7451900\n",
      "iteration:  74550 , total_loss:  20.967000198364257\n",
      "count_amostra: 7454900\n",
      "iteration:  74580 , total_loss:  20.67058130900065\n",
      "count_amostra: 7457900\n",
      "iteration:  74610 , total_loss:  21.53541113535563\n",
      "count_amostra: 7460900\n",
      "iteration:  74640 , total_loss:  21.043068822224935\n",
      "count_amostra: 7463900\n",
      "iteration:  74670 , total_loss:  21.18030808766683\n",
      "count_amostra: 7466900\n",
      "iteration:  74700 , total_loss:  21.33595415751139\n",
      "count_amostra: 7469900\n",
      "iteration:  74730 , total_loss:  20.66986230214437\n",
      "count_amostra: 7472900\n",
      "iteration:  74760 , total_loss:  21.34260018666585\n",
      "count_amostra: 7475900\n",
      "iteration:  74790 , total_loss:  21.164607365926106\n",
      "count_amostra: 7478900\n",
      "iteration:  74820 , total_loss:  21.186358896891274\n",
      "count_amostra: 7481900\n",
      "iteration:  74850 , total_loss:  21.05877997080485\n",
      "count_amostra: 7484900\n",
      "iteration:  74880 , total_loss:  21.266811879475913\n",
      "count_amostra: 7487900\n",
      "iteration:  74910 , total_loss:  21.10884863535563\n",
      "count_amostra: 7490900\n",
      "iteration:  74940 , total_loss:  21.33441734313965\n",
      "count_amostra: 7493900\n",
      "iteration:  74970 , total_loss:  20.982861455281576\n",
      "count_amostra: 7496900\n",
      "iteration:  75000 , total_loss:  20.82446543375651\n",
      "count_amostra: 7499900\n",
      "iteration:  75030 , total_loss:  21.111672337849935\n",
      "count_amostra: 7502900\n",
      "iteration:  75060 , total_loss:  21.04488754272461\n",
      "count_amostra: 7505900\n",
      "iteration:  75090 , total_loss:  21.04866631825765\n",
      "count_amostra: 7508900\n",
      "iteration:  75120 , total_loss:  20.772661145528158\n",
      "count_amostra: 7511900\n",
      "iteration:  75150 , total_loss:  21.226456960042317\n",
      "count_amostra: 7514900\n",
      "iteration:  75180 , total_loss:  21.309462801615396\n",
      "count_amostra: 7517900\n",
      "iteration:  75210 , total_loss:  21.010487365722657\n",
      "count_amostra: 7520900\n",
      "iteration:  75240 , total_loss:  21.066679191589355\n",
      "count_amostra: 7523900\n",
      "iteration:  75270 , total_loss:  20.840748405456544\n",
      "count_amostra: 7526900\n",
      "iteration:  75300 , total_loss:  21.25320472717285\n",
      "count_amostra: 7529900\n",
      "iteration:  75330 , total_loss:  20.88823038736979\n",
      "count_amostra: 7532900\n",
      "iteration:  75360 , total_loss:  20.72702840169271\n",
      "count_amostra: 7535900\n",
      "iteration:  75390 , total_loss:  21.137432479858397\n",
      "count_amostra: 7538900\n",
      "iteration:  75420 , total_loss:  21.405541102091473\n",
      "count_amostra: 7541900\n",
      "iteration:  75450 , total_loss:  20.948139508565266\n",
      "count_amostra: 7544900\n",
      "iteration:  75480 , total_loss:  21.093382263183592\n",
      "count_amostra: 7547900\n",
      "iteration:  75510 , total_loss:  20.64708875020345\n",
      "count_amostra: 7550900\n",
      "iteration:  75540 , total_loss:  20.976298077901205\n",
      "count_amostra: 7553900\n",
      "iteration:  75570 , total_loss:  20.965903155008952\n",
      "count_amostra: 7556900\n",
      "iteration:  75600 , total_loss:  21.060139465332032\n",
      "count_amostra: 7559900\n",
      "iteration:  75630 , total_loss:  21.20135981241862\n",
      "count_amostra: 7562900\n",
      "iteration:  75660 , total_loss:  21.06399917602539\n",
      "count_amostra: 7565900\n",
      "iteration:  75690 , total_loss:  20.68177007039388\n",
      "count_amostra: 7568900\n",
      "iteration:  75720 , total_loss:  20.8226287206014\n",
      "count_amostra: 7571900\n",
      "iteration:  75750 , total_loss:  21.551816813151042\n",
      "count_amostra: 7574900\n",
      "iteration:  75780 , total_loss:  20.83769588470459\n",
      "count_amostra: 7577900\n",
      "iteration:  75810 , total_loss:  20.92775821685791\n",
      "count_amostra: 7580900\n",
      "iteration:  75840 , total_loss:  20.984875043233234\n",
      "count_amostra: 7583900\n",
      "iteration:  75870 , total_loss:  21.092853673299153\n",
      "count_amostra: 7586900\n",
      "iteration:  75900 , total_loss:  21.171154022216797\n",
      "count_amostra: 7589900\n",
      "iteration:  75930 , total_loss:  21.029200299580893\n",
      "count_amostra: 7592900\n",
      "iteration:  75960 , total_loss:  21.274939028422036\n",
      "count_amostra: 7595900\n",
      "iteration:  75990 , total_loss:  20.765638287862142\n",
      "count_amostra: 7598900\n",
      "iteration:  76020 , total_loss:  21.05659745534261\n",
      "count_amostra: 7601900\n",
      "iteration:  76050 , total_loss:  20.797258822123208\n",
      "count_amostra: 7604900\n",
      "iteration:  76080 , total_loss:  21.260778999328615\n",
      "count_amostra: 7607900\n",
      "iteration:  76110 , total_loss:  21.367749722798667\n",
      "count_amostra: 7610900\n",
      "iteration:  76140 , total_loss:  20.815711339314777\n",
      "count_amostra: 7613900\n",
      "iteration:  76170 , total_loss:  21.359248288472493\n",
      "count_amostra: 7616900\n",
      "iteration:  76200 , total_loss:  21.116363779703775\n",
      "count_amostra: 7619900\n",
      "iteration:  76230 , total_loss:  21.165622202555337\n",
      "count_amostra: 7622900\n",
      "iteration:  76260 , total_loss:  21.30613479614258\n",
      "count_amostra: 7625900\n",
      "iteration:  76290 , total_loss:  20.88154150644938\n",
      "count_amostra: 7628900\n",
      "iteration:  76320 , total_loss:  21.11895669301351\n",
      "count_amostra: 7631900\n",
      "iteration:  76350 , total_loss:  20.97001469930013\n",
      "count_amostra: 7634900\n",
      "iteration:  76380 , total_loss:  21.055393600463866\n",
      "count_amostra: 7637900\n",
      "iteration:  76410 , total_loss:  20.863364537556965\n",
      "count_amostra: 7640900\n",
      "iteration:  76440 , total_loss:  20.835388056437175\n",
      "count_amostra: 7643900\n",
      "iteration:  76470 , total_loss:  21.0583070119222\n",
      "count_amostra: 7646900\n",
      "iteration:  76500 , total_loss:  20.993408648173013\n",
      "count_amostra: 7649900\n",
      "iteration:  76530 , total_loss:  20.922541236877443\n",
      "count_amostra: 7652900\n",
      "iteration:  76560 , total_loss:  21.37098585764567\n",
      "count_amostra: 7655900\n",
      "iteration:  76590 , total_loss:  21.091170756022134\n",
      "count_amostra: 7658900\n",
      "iteration:  76620 , total_loss:  20.851693216959635\n",
      "count_amostra: 7661900\n",
      "iteration:  76650 , total_loss:  21.26187744140625\n",
      "count_amostra: 7664900\n",
      "iteration:  76680 , total_loss:  20.87192096710205\n",
      "count_amostra: 7667900\n",
      "iteration:  76710 , total_loss:  21.428761609395345\n",
      "count_amostra: 7670900\n",
      "iteration:  76740 , total_loss:  21.2633846282959\n",
      "count_amostra: 7673900\n",
      "iteration:  76770 , total_loss:  21.07009073893229\n",
      "count_amostra: 7676900\n",
      "iteration:  76800 , total_loss:  20.843564987182617\n",
      "count_amostra: 7679900\n",
      "iteration:  76830 , total_loss:  20.712181981404623\n",
      "count_amostra: 7682900\n",
      "iteration:  76860 , total_loss:  20.89410400390625\n",
      "count_amostra: 7685900\n",
      "iteration:  76890 , total_loss:  21.289213371276855\n",
      "count_amostra: 7688900\n",
      "iteration:  76920 , total_loss:  20.91896457672119\n",
      "count_amostra: 7691900\n",
      "iteration:  76950 , total_loss:  21.318544133504233\n",
      "count_amostra: 7694900\n",
      "iteration:  76980 , total_loss:  21.321217091878257\n",
      "count_amostra: 7697900\n",
      "iteration:  77010 , total_loss:  20.793341064453124\n",
      "count_amostra: 7700900\n",
      "iteration:  77040 , total_loss:  20.654448382059734\n",
      "count_amostra: 7703900\n",
      "iteration:  77070 , total_loss:  21.17984930674235\n",
      "count_amostra: 7706900\n",
      "iteration:  77100 , total_loss:  20.762086232503254\n",
      "count_amostra: 7709900\n",
      "iteration:  77130 , total_loss:  20.77728207906087\n",
      "count_amostra: 7712900\n",
      "iteration:  77160 , total_loss:  21.48365650177002\n",
      "count_amostra: 7715900\n",
      "iteration:  77190 , total_loss:  20.81628958384196\n",
      "count_amostra: 7718900\n",
      "iteration:  77220 , total_loss:  21.01754461924235\n",
      "count_amostra: 7721900\n",
      "iteration:  77250 , total_loss:  20.81036154429118\n",
      "count_amostra: 7724900\n",
      "iteration:  77280 , total_loss:  21.148362731933595\n",
      "count_amostra: 7727900\n",
      "iteration:  77310 , total_loss:  20.97195555369059\n",
      "count_amostra: 7730900\n",
      "iteration:  77340 , total_loss:  20.757871754964192\n",
      "count_amostra: 7733900\n",
      "iteration:  77370 , total_loss:  21.035360781351724\n",
      "count_amostra: 7736900\n",
      "iteration:  77400 , total_loss:  20.729681650797527\n",
      "count_amostra: 7739900\n",
      "iteration:  77430 , total_loss:  21.02729860941569\n",
      "count_amostra: 7742900\n",
      "iteration:  77460 , total_loss:  21.353172938028973\n",
      "count_amostra: 7745900\n",
      "iteration:  77490 , total_loss:  20.997569211324056\n",
      "count_amostra: 7748900\n",
      "iteration:  77520 , total_loss:  21.040300814310708\n",
      "count_amostra: 7751900\n",
      "iteration:  77550 , total_loss:  20.98838996887207\n",
      "count_amostra: 7754900\n",
      "iteration:  77580 , total_loss:  21.1870423634847\n",
      "count_amostra: 7757900\n",
      "iteration:  77610 , total_loss:  21.239421145121256\n",
      "count_amostra: 7760900\n",
      "iteration:  77640 , total_loss:  20.827367528279623\n",
      "count_amostra: 7763900\n",
      "iteration:  77670 , total_loss:  20.85780487060547\n",
      "count_amostra: 7766900\n",
      "iteration:  77700 , total_loss:  21.209448941548665\n",
      "count_amostra: 7769900\n",
      "iteration:  77730 , total_loss:  20.9766419728597\n",
      "count_amostra: 7772900\n",
      "iteration:  77760 , total_loss:  21.210445467631022\n",
      "count_amostra: 7775900\n",
      "iteration:  77790 , total_loss:  21.16913757324219\n",
      "count_amostra: 7778900\n",
      "iteration:  77820 , total_loss:  21.070143254597983\n",
      "count_amostra: 7781900\n",
      "iteration:  77850 , total_loss:  20.3317014058431\n",
      "count_amostra: 7784900\n",
      "iteration:  77880 , total_loss:  20.997350056966145\n",
      "count_amostra: 7787900\n",
      "iteration:  77910 , total_loss:  20.94676628112793\n",
      "count_amostra: 7790900\n",
      "iteration:  77940 , total_loss:  20.947814559936525\n",
      "count_amostra: 7793900\n",
      "iteration:  77970 , total_loss:  20.80425230662028\n",
      "count_amostra: 7796900\n",
      "iteration:  78000 , total_loss:  20.742548370361327\n",
      "count_amostra: 7799900\n",
      "iteration:  78030 , total_loss:  20.699635950724282\n",
      "count_amostra: 7802900\n",
      "iteration:  78060 , total_loss:  21.120686467488607\n",
      "count_amostra: 7805900\n",
      "iteration:  78090 , total_loss:  21.519746653238933\n",
      "count_amostra: 7808900\n",
      "iteration:  78120 , total_loss:  21.335952059427896\n",
      "count_amostra: 7811900\n",
      "iteration:  78150 , total_loss:  21.026093864440917\n",
      "count_amostra: 7814900\n",
      "iteration:  78180 , total_loss:  20.925888379414875\n",
      "count_amostra: 7817900\n",
      "iteration:  78210 , total_loss:  20.80143985748291\n",
      "count_amostra: 7820900\n",
      "iteration:  78240 , total_loss:  20.955762545267742\n",
      "count_amostra: 7823900\n",
      "iteration:  78270 , total_loss:  20.868297576904297\n",
      "count_amostra: 7826900\n",
      "iteration:  78300 , total_loss:  20.86366647084554\n",
      "count_amostra: 7829900\n",
      "iteration:  78330 , total_loss:  20.723579915364585\n",
      "count_amostra: 7832900\n",
      "iteration:  78360 , total_loss:  21.051300366719563\n",
      "count_amostra: 7835900\n",
      "iteration:  78390 , total_loss:  20.680256589253744\n",
      "count_amostra: 7838900\n",
      "iteration:  78420 , total_loss:  20.728425979614258\n",
      "count_amostra: 7841900\n",
      "iteration:  78450 , total_loss:  20.966829172770183\n",
      "count_amostra: 7844900\n",
      "iteration:  78480 , total_loss:  20.532576751708984\n",
      "count_amostra: 7847900\n",
      "iteration:  78510 , total_loss:  21.000080871582032\n",
      "count_amostra: 7850900\n",
      "iteration:  78540 , total_loss:  20.93448740641276\n",
      "count_amostra: 7853900\n",
      "iteration:  78570 , total_loss:  21.232319577534994\n",
      "count_amostra: 7856900\n",
      "iteration:  78600 , total_loss:  20.927907498677573\n",
      "count_amostra: 7859900\n",
      "iteration:  78630 , total_loss:  21.104649035135903\n",
      "count_amostra: 7862900\n",
      "iteration:  78660 , total_loss:  20.831768290201822\n",
      "count_amostra: 7865900\n",
      "iteration:  78690 , total_loss:  20.801805814107258\n",
      "count_amostra: 7868900\n",
      "iteration:  78720 , total_loss:  20.66080118815104\n",
      "count_amostra: 7871900\n",
      "iteration:  78750 , total_loss:  21.067788060506185\n",
      "count_amostra: 7874900\n",
      "iteration:  78780 , total_loss:  20.822551409403484\n",
      "count_amostra: 7877900\n",
      "iteration:  78810 , total_loss:  20.838315836588542\n",
      "count_amostra: 7880900\n",
      "iteration:  78840 , total_loss:  20.961995760599773\n",
      "count_amostra: 7883900\n",
      "iteration:  78870 , total_loss:  20.677334149678547\n",
      "count_amostra: 7886900\n",
      "iteration:  78900 , total_loss:  20.99403082529704\n",
      "count_amostra: 7889900\n",
      "iteration:  78930 , total_loss:  20.69937063852946\n",
      "count_amostra: 7892900\n",
      "iteration:  78960 , total_loss:  21.376176198323567\n",
      "count_amostra: 7895900\n",
      "iteration:  78990 , total_loss:  20.982465298970542\n",
      "count_amostra: 7898900\n",
      "iteration:  79020 , total_loss:  21.053916104634602\n",
      "count_amostra: 7901900\n",
      "iteration:  79050 , total_loss:  20.981242370605468\n",
      "count_amostra: 7904900\n",
      "iteration:  79080 , total_loss:  21.25926481882731\n",
      "count_amostra: 7907900\n",
      "iteration:  79110 , total_loss:  21.198241806030275\n",
      "count_amostra: 7910900\n",
      "iteration:  79140 , total_loss:  21.24514980316162\n",
      "count_amostra: 7913900\n",
      "iteration:  79170 , total_loss:  20.44975496927897\n",
      "count_amostra: 7916900\n",
      "iteration:  79200 , total_loss:  21.24725882212321\n",
      "count_amostra: 7919900\n",
      "iteration:  79230 , total_loss:  20.842542966206867\n",
      "count_amostra: 7922900\n",
      "iteration:  79260 , total_loss:  20.584493827819824\n",
      "count_amostra: 7925900\n",
      "iteration:  79290 , total_loss:  20.967851320902508\n",
      "count_amostra: 7928900\n",
      "iteration:  79320 , total_loss:  20.789102617899577\n",
      "count_amostra: 7931900\n",
      "iteration:  79350 , total_loss:  21.056437238057455\n",
      "count_amostra: 7934900\n",
      "iteration:  79380 , total_loss:  20.785118293762206\n",
      "count_amostra: 7937900\n",
      "iteration:  79410 , total_loss:  21.356965446472167\n",
      "count_amostra: 7940900\n",
      "iteration:  79440 , total_loss:  21.419166056315103\n",
      "count_amostra: 7943900\n",
      "iteration:  79470 , total_loss:  20.97943967183431\n",
      "count_amostra: 7946900\n",
      "iteration:  79500 , total_loss:  20.974245643615724\n",
      "count_amostra: 7949900\n",
      "iteration:  79530 , total_loss:  21.499420356750488\n",
      "count_amostra: 7952900\n",
      "iteration:  79560 , total_loss:  21.132959938049318\n",
      "count_amostra: 7955900\n",
      "iteration:  79590 , total_loss:  20.728393109639487\n",
      "count_amostra: 7958900\n",
      "iteration:  79620 , total_loss:  20.77533524831136\n",
      "count_amostra: 7961900\n",
      "iteration:  79650 , total_loss:  21.013518969217937\n",
      "count_amostra: 7964900\n",
      "iteration:  79680 , total_loss:  20.879309590657552\n",
      "count_amostra: 7967900\n",
      "iteration:  79710 , total_loss:  20.8578125\n",
      "count_amostra: 7970900\n",
      "iteration:  79740 , total_loss:  21.240488243103027\n",
      "count_amostra: 7973900\n",
      "iteration:  79770 , total_loss:  20.81701596577962\n",
      "count_amostra: 7976900\n",
      "iteration:  79800 , total_loss:  20.941625340779623\n",
      "count_amostra: 7979900\n",
      "iteration:  79830 , total_loss:  21.078823471069335\n",
      "count_amostra: 7982900\n",
      "iteration:  79860 , total_loss:  20.79084892272949\n",
      "count_amostra: 7985900\n",
      "iteration:  79890 , total_loss:  20.95152867635091\n",
      "count_amostra: 7988900\n",
      "iteration:  79920 , total_loss:  21.28396962483724\n",
      "count_amostra: 7991900\n",
      "iteration:  79950 , total_loss:  21.25036710103353\n",
      "count_amostra: 7994900\n",
      "iteration:  79980 , total_loss:  21.297084935506184\n",
      "count_amostra: 7997900\n",
      "iteration:  80010 , total_loss:  21.14052766164144\n",
      "count_amostra: 8000900\n",
      "iteration:  80040 , total_loss:  21.30420290629069\n",
      "count_amostra: 8003900\n",
      "iteration:  80070 , total_loss:  20.691426595052082\n",
      "count_amostra: 8006900\n",
      "iteration:  80100 , total_loss:  21.088826243082682\n",
      "count_amostra: 8009900\n",
      "iteration:  80130 , total_loss:  20.99179611206055\n",
      "count_amostra: 8012900\n",
      "iteration:  80160 , total_loss:  21.210098203023275\n",
      "count_amostra: 8015900\n",
      "iteration:  80190 , total_loss:  20.418958155314126\n",
      "count_amostra: 8018900\n",
      "iteration:  80220 , total_loss:  20.712616157531738\n",
      "count_amostra: 8021900\n",
      "iteration:  80250 , total_loss:  20.96974728902181\n",
      "count_amostra: 8024900\n",
      "iteration:  80280 , total_loss:  21.01566047668457\n",
      "count_amostra: 8027900\n",
      "iteration:  80310 , total_loss:  21.573738479614256\n",
      "count_amostra: 8030900\n",
      "iteration:  80340 , total_loss:  20.933868980407716\n",
      "count_amostra: 8033900\n",
      "iteration:  80370 , total_loss:  20.909004147847494\n",
      "count_amostra: 8036900\n",
      "iteration:  80400 , total_loss:  21.19506416320801\n",
      "count_amostra: 8039900\n",
      "iteration:  80430 , total_loss:  20.94258232116699\n",
      "count_amostra: 8042900\n",
      "iteration:  80460 , total_loss:  20.873835881551106\n",
      "count_amostra: 8045900\n",
      "iteration:  80490 , total_loss:  20.832142957051595\n",
      "count_amostra: 8048900\n",
      "iteration:  80520 , total_loss:  20.899703534444175\n",
      "count_amostra: 8051900\n",
      "iteration:  80550 , total_loss:  20.915013567606607\n",
      "count_amostra: 8054900\n",
      "iteration:  80580 , total_loss:  21.2273255666097\n",
      "count_amostra: 8057900\n",
      "iteration:  80610 , total_loss:  21.2887238184611\n",
      "count_amostra: 8060900\n",
      "iteration:  80640 , total_loss:  20.585731569925944\n",
      "count_amostra: 8063900\n",
      "iteration:  80670 , total_loss:  21.266662216186525\n",
      "count_amostra: 8066900\n",
      "iteration:  80700 , total_loss:  20.652071062723795\n",
      "count_amostra: 8069900\n",
      "iteration:  80730 , total_loss:  21.105832672119142\n",
      "count_amostra: 8072900\n",
      "iteration:  80760 , total_loss:  21.405895233154297\n",
      "count_amostra: 8075900\n",
      "iteration:  80790 , total_loss:  21.246275011698405\n",
      "count_amostra: 8078900\n",
      "iteration:  80820 , total_loss:  20.85747922261556\n",
      "count_amostra: 8081900\n",
      "iteration:  80850 , total_loss:  20.80187028249105\n",
      "count_amostra: 8084900\n",
      "iteration:  80880 , total_loss:  20.926063791910806\n",
      "count_amostra: 8087900\n",
      "iteration:  80910 , total_loss:  20.705654017130534\n",
      "count_amostra: 8090900\n",
      "iteration:  80940 , total_loss:  20.74399579366048\n",
      "count_amostra: 8093900\n",
      "iteration:  80970 , total_loss:  21.292929140726724\n",
      "count_amostra: 8096900\n",
      "iteration:  81000 , total_loss:  20.478143056233723\n",
      "count_amostra: 8099900\n",
      "iteration:  81030 , total_loss:  21.268322563171388\n",
      "count_amostra: 8102900\n",
      "iteration:  81060 , total_loss:  20.650233777364097\n",
      "count_amostra: 8105900\n",
      "iteration:  81090 , total_loss:  21.477473640441893\n",
      "count_amostra: 8108900\n",
      "iteration:  81120 , total_loss:  20.85128835042318\n",
      "count_amostra: 8111900\n",
      "iteration:  81150 , total_loss:  20.480077171325682\n",
      "count_amostra: 8114900\n",
      "iteration:  81180 , total_loss:  21.229417737325033\n",
      "count_amostra: 8117900\n",
      "iteration:  81210 , total_loss:  21.04850985209147\n",
      "count_amostra: 8120900\n",
      "iteration:  81240 , total_loss:  20.39223658243815\n",
      "count_amostra: 8123900\n",
      "iteration:  81270 , total_loss:  20.82135435740153\n",
      "count_amostra: 8126900\n",
      "iteration:  81300 , total_loss:  21.10570945739746\n",
      "count_amostra: 8129900\n",
      "iteration:  81330 , total_loss:  21.04624932607015\n",
      "count_amostra: 8132900\n",
      "iteration:  81360 , total_loss:  21.18140869140625\n",
      "count_amostra: 8135900\n",
      "iteration:  81390 , total_loss:  20.632749875386555\n",
      "count_amostra: 8138900\n",
      "iteration:  81420 , total_loss:  20.919866180419923\n",
      "count_amostra: 8141900\n",
      "iteration:  81450 , total_loss:  21.063873545328775\n",
      "count_amostra: 8144900\n",
      "iteration:  81480 , total_loss:  20.878469785054524\n",
      "count_amostra: 8147900\n",
      "iteration:  81510 , total_loss:  20.525931866963706\n",
      "count_amostra: 8150900\n",
      "iteration:  81540 , total_loss:  21.113005956013996\n",
      "count_amostra: 8153900\n",
      "iteration:  81570 , total_loss:  20.96571133931478\n",
      "count_amostra: 8156900\n",
      "iteration:  81600 , total_loss:  20.95336125691732\n",
      "count_amostra: 8159900\n",
      "iteration:  81630 , total_loss:  20.640766906738282\n",
      "count_amostra: 8162900\n",
      "iteration:  81660 , total_loss:  21.072705904642742\n",
      "count_amostra: 8165900\n",
      "iteration:  81690 , total_loss:  21.484372711181642\n",
      "count_amostra: 8168900\n",
      "iteration:  81720 , total_loss:  20.9001251856486\n",
      "count_amostra: 8171900\n",
      "iteration:  81750 , total_loss:  20.73219051361084\n",
      "count_amostra: 8174900\n",
      "iteration:  81780 , total_loss:  20.83244946797689\n",
      "count_amostra: 8177900\n",
      "iteration:  81810 , total_loss:  20.967580032348632\n",
      "count_amostra: 8180900\n",
      "iteration:  81840 , total_loss:  20.533458455403647\n",
      "count_amostra: 8183900\n",
      "iteration:  81870 , total_loss:  20.4414826075236\n",
      "count_amostra: 8186900\n",
      "iteration:  81900 , total_loss:  20.810780461629232\n",
      "count_amostra: 8189900\n",
      "iteration:  81930 , total_loss:  20.934052403767904\n",
      "count_amostra: 8192900\n",
      "iteration:  81960 , total_loss:  20.871721903483074\n",
      "count_amostra: 8195900\n",
      "iteration:  81990 , total_loss:  20.741796493530273\n",
      "count_amostra: 8198900\n",
      "iteration:  82020 , total_loss:  21.473062070210776\n",
      "count_amostra: 8201900\n",
      "iteration:  82050 , total_loss:  21.013442357381184\n",
      "count_amostra: 8204900\n",
      "iteration:  82080 , total_loss:  21.326114590962728\n",
      "count_amostra: 8207900\n",
      "iteration:  82110 , total_loss:  21.196914672851562\n",
      "count_amostra: 8210900\n",
      "iteration:  82140 , total_loss:  20.958475240071614\n",
      "count_amostra: 8213900\n",
      "iteration:  82170 , total_loss:  20.654953320821125\n",
      "count_amostra: 8216900\n",
      "iteration:  82200 , total_loss:  20.81154270172119\n",
      "count_amostra: 8219900\n",
      "iteration:  82230 , total_loss:  20.480589612325033\n",
      "count_amostra: 8222900\n",
      "iteration:  82260 , total_loss:  20.789314142862956\n",
      "count_amostra: 8225900\n",
      "iteration:  82290 , total_loss:  20.615954717000324\n",
      "count_amostra: 8228900\n",
      "iteration:  82320 , total_loss:  20.65998229980469\n",
      "count_amostra: 8231900\n",
      "iteration:  82350 , total_loss:  20.961501121520996\n",
      "count_amostra: 8234900\n",
      "iteration:  82380 , total_loss:  20.901775614420572\n",
      "count_amostra: 8237900\n",
      "iteration:  82410 , total_loss:  21.102791786193848\n",
      "count_amostra: 8240900\n",
      "iteration:  82440 , total_loss:  20.965516789754233\n",
      "count_amostra: 8243900\n",
      "iteration:  82470 , total_loss:  21.045246632893882\n",
      "count_amostra: 8246900\n",
      "iteration:  82500 , total_loss:  20.83751417795817\n",
      "count_amostra: 8249900\n",
      "iteration:  82530 , total_loss:  21.330099932352702\n",
      "count_amostra: 8252900\n",
      "iteration:  82560 , total_loss:  20.758046595255532\n",
      "count_amostra: 8255900\n",
      "iteration:  82590 , total_loss:  21.195737393697105\n",
      "count_amostra: 8258900\n",
      "iteration:  82620 , total_loss:  20.753669102986652\n",
      "count_amostra: 8261900\n",
      "iteration:  82650 , total_loss:  21.040588251749675\n",
      "count_amostra: 8264900\n",
      "iteration:  82680 , total_loss:  21.141346613566082\n",
      "count_amostra: 8267900\n",
      "iteration:  82710 , total_loss:  20.870367749532065\n",
      "count_amostra: 8270900\n",
      "iteration:  82740 , total_loss:  20.955054346720377\n",
      "count_amostra: 8273900\n",
      "iteration:  82770 , total_loss:  20.834981409708657\n",
      "count_amostra: 8276900\n",
      "iteration:  82800 , total_loss:  21.32403195699056\n",
      "count_amostra: 8279900\n",
      "iteration:  82830 , total_loss:  21.038878122965496\n",
      "count_amostra: 8282900\n",
      "iteration:  82860 , total_loss:  20.869791094462077\n",
      "count_amostra: 8285900\n",
      "iteration:  82890 , total_loss:  21.05362860361735\n",
      "count_amostra: 8288900\n",
      "iteration:  82920 , total_loss:  21.228160730997722\n",
      "count_amostra: 8291900\n",
      "iteration:  82950 , total_loss:  20.91019369761149\n",
      "count_amostra: 8294900\n",
      "iteration:  82980 , total_loss:  20.75504347483317\n",
      "count_amostra: 8297900\n",
      "iteration:  83010 , total_loss:  20.71440798441569\n",
      "count_amostra: 8300900\n",
      "iteration:  83040 , total_loss:  20.381747245788574\n",
      "count_amostra: 8303900\n",
      "iteration:  83070 , total_loss:  20.90635363260905\n",
      "count_amostra: 8306900\n",
      "iteration:  83100 , total_loss:  21.083978017171223\n",
      "count_amostra: 8309900\n",
      "iteration:  83130 , total_loss:  20.968586349487303\n",
      "count_amostra: 8312900\n",
      "iteration:  83160 , total_loss:  20.843940607706706\n",
      "count_amostra: 8315900\n",
      "iteration:  83190 , total_loss:  21.065097173055012\n",
      "count_amostra: 8318900\n",
      "iteration:  83220 , total_loss:  21.016280682881675\n",
      "count_amostra: 8321900\n",
      "iteration:  83250 , total_loss:  21.301229159037273\n",
      "count_amostra: 8324900\n",
      "iteration:  83280 , total_loss:  21.091375414530436\n",
      "count_amostra: 8327900\n",
      "iteration:  83310 , total_loss:  20.85662752787272\n",
      "count_amostra: 8330900\n",
      "iteration:  83340 , total_loss:  21.14478282928467\n",
      "count_amostra: 8333900\n",
      "iteration:  83370 , total_loss:  21.006674830118815\n",
      "count_amostra: 8336900\n",
      "iteration:  83400 , total_loss:  20.660533459981284\n",
      "count_amostra: 8339900\n",
      "iteration:  83430 , total_loss:  21.04979763031006\n",
      "count_amostra: 8342900\n",
      "iteration:  83460 , total_loss:  20.516675631205242\n",
      "count_amostra: 8345900\n",
      "iteration:  83490 , total_loss:  20.966783332824708\n",
      "count_amostra: 8348900\n",
      "iteration:  83520 , total_loss:  20.84959748586019\n",
      "count_amostra: 8351900\n",
      "iteration:  83550 , total_loss:  21.24031613667806\n",
      "count_amostra: 8354900\n",
      "iteration:  83580 , total_loss:  20.720419947306315\n",
      "count_amostra: 8357900\n",
      "iteration:  83610 , total_loss:  21.012161382039388\n",
      "count_amostra: 8360900\n",
      "iteration:  83640 , total_loss:  20.698113759358723\n",
      "count_amostra: 8363900\n",
      "iteration:  83670 , total_loss:  21.123529624938964\n",
      "count_amostra: 8366900\n",
      "iteration:  83700 , total_loss:  21.102572695414224\n",
      "count_amostra: 8369900\n",
      "iteration:  83730 , total_loss:  20.62124341328939\n",
      "count_amostra: 8372900\n",
      "iteration:  83760 , total_loss:  20.633205604553222\n",
      "count_amostra: 8375900\n",
      "iteration:  83790 , total_loss:  20.771796035766602\n",
      "count_amostra: 8378900\n",
      "iteration:  83820 , total_loss:  20.971654574076336\n",
      "count_amostra: 8381900\n",
      "iteration:  83850 , total_loss:  21.064941469828288\n",
      "count_amostra: 8384900\n",
      "iteration:  83880 , total_loss:  20.555746205647786\n",
      "count_amostra: 8387900\n",
      "iteration:  83910 , total_loss:  21.19435183207194\n",
      "count_amostra: 8390900\n",
      "iteration:  83940 , total_loss:  20.719706217447918\n",
      "count_amostra: 8393900\n",
      "iteration:  83970 , total_loss:  20.643225288391115\n",
      "count_amostra: 8396900\n",
      "iteration:  84000 , total_loss:  20.87422440846761\n",
      "count_amostra: 8399900\n",
      "iteration:  84030 , total_loss:  20.847122383117675\n",
      "count_amostra: 8402900\n",
      "iteration:  84060 , total_loss:  20.811570421854654\n",
      "count_amostra: 8405900\n",
      "iteration:  84090 , total_loss:  21.076594670613606\n",
      "count_amostra: 8408900\n",
      "iteration:  84120 , total_loss:  20.55221519470215\n",
      "count_amostra: 8411900\n",
      "iteration:  84150 , total_loss:  21.13655465443929\n",
      "count_amostra: 8414900\n",
      "iteration:  84180 , total_loss:  20.814822260538737\n",
      "count_amostra: 8417900\n",
      "iteration:  84210 , total_loss:  21.01475772857666\n",
      "count_amostra: 8420900\n",
      "iteration:  84240 , total_loss:  21.156071027119953\n",
      "count_amostra: 8423900\n",
      "iteration:  84270 , total_loss:  20.981103897094727\n",
      "count_amostra: 8426900\n",
      "iteration:  84300 , total_loss:  20.846251233418783\n",
      "count_amostra: 8429900\n",
      "iteration:  84330 , total_loss:  21.158742141723632\n",
      "count_amostra: 8432900\n",
      "iteration:  84360 , total_loss:  21.39273993174235\n",
      "count_amostra: 8435900\n",
      "iteration:  84390 , total_loss:  20.82945524851481\n",
      "count_amostra: 8438900\n",
      "iteration:  84420 , total_loss:  21.13745797475179\n",
      "count_amostra: 8441900\n",
      "iteration:  84450 , total_loss:  20.93868408203125\n",
      "count_amostra: 8444900\n",
      "iteration:  84480 , total_loss:  21.001275062561035\n",
      "count_amostra: 8447900\n",
      "iteration:  84510 , total_loss:  20.94062328338623\n",
      "count_amostra: 8450900\n",
      "iteration:  84540 , total_loss:  21.193902842203777\n",
      "count_amostra: 8453900\n",
      "iteration:  84570 , total_loss:  20.88609161376953\n",
      "count_amostra: 8456900\n",
      "iteration:  84600 , total_loss:  21.078351402282713\n",
      "count_amostra: 8459900\n",
      "iteration:  84630 , total_loss:  20.91282107035319\n",
      "count_amostra: 8462900\n",
      "iteration:  84660 , total_loss:  21.009742164611815\n",
      "count_amostra: 8465900\n",
      "iteration:  84690 , total_loss:  21.2658203125\n",
      "count_amostra: 8468900\n",
      "iteration:  84720 , total_loss:  20.767584737141927\n",
      "count_amostra: 8471900\n",
      "iteration:  84750 , total_loss:  21.266605122884116\n",
      "count_amostra: 8474900\n",
      "iteration:  84780 , total_loss:  20.983395512898763\n",
      "count_amostra: 8477900\n",
      "iteration:  84810 , total_loss:  21.20550619761149\n",
      "count_amostra: 8480900\n",
      "iteration:  84840 , total_loss:  20.601032574971516\n",
      "count_amostra: 8483900\n",
      "iteration:  84870 , total_loss:  21.0642063776652\n",
      "count_amostra: 8486900\n",
      "iteration:  84900 , total_loss:  20.845691363016766\n",
      "count_amostra: 8489900\n",
      "iteration:  84930 , total_loss:  21.029125849405926\n",
      "count_amostra: 8492900\n",
      "iteration:  84960 , total_loss:  20.859356371561685\n",
      "count_amostra: 8495900\n",
      "iteration:  84990 , total_loss:  20.833836555480957\n",
      "count_amostra: 8498900\n",
      "iteration:  85020 , total_loss:  20.94840087890625\n",
      "count_amostra: 8501900\n",
      "iteration:  85050 , total_loss:  20.72752939860026\n",
      "count_amostra: 8504900\n",
      "iteration:  85080 , total_loss:  20.91048895517985\n",
      "count_amostra: 8507900\n",
      "iteration:  85110 , total_loss:  20.85460770924886\n",
      "count_amostra: 8510900\n",
      "iteration:  85140 , total_loss:  20.586493047078452\n",
      "count_amostra: 8513900\n",
      "iteration:  85170 , total_loss:  20.901448186238607\n",
      "count_amostra: 8516900\n",
      "iteration:  85200 , total_loss:  20.860304387410483\n",
      "count_amostra: 8519900\n",
      "iteration:  85230 , total_loss:  21.04162464141846\n",
      "count_amostra: 8522900\n",
      "iteration:  85260 , total_loss:  20.780072275797526\n",
      "count_amostra: 8525900\n",
      "iteration:  85290 , total_loss:  20.950684356689454\n",
      "count_amostra: 8528900\n",
      "iteration:  85320 , total_loss:  20.930627187093098\n",
      "count_amostra: 8531900\n",
      "iteration:  85350 , total_loss:  20.420903714497886\n",
      "count_amostra: 8534900\n",
      "iteration:  85380 , total_loss:  20.859270922342937\n",
      "count_amostra: 8537900\n",
      "iteration:  85410 , total_loss:  20.72056636810303\n",
      "count_amostra: 8540900\n",
      "iteration:  85440 , total_loss:  20.900879414876304\n",
      "count_amostra: 8543900\n",
      "iteration:  85470 , total_loss:  20.976450157165527\n",
      "count_amostra: 8546900\n",
      "iteration:  85500 , total_loss:  20.761561648050943\n",
      "count_amostra: 8549900\n",
      "iteration:  85530 , total_loss:  20.61619167327881\n",
      "count_amostra: 8552900\n",
      "iteration:  85560 , total_loss:  20.815806325276693\n",
      "count_amostra: 8555900\n",
      "iteration:  85590 , total_loss:  21.07469997406006\n",
      "count_amostra: 8558900\n",
      "iteration:  85620 , total_loss:  20.835579299926756\n",
      "count_amostra: 8561900\n",
      "iteration:  85650 , total_loss:  20.908317057291665\n",
      "count_amostra: 8564900\n",
      "iteration:  85680 , total_loss:  21.32683900197347\n",
      "count_amostra: 8567900\n",
      "iteration:  85710 , total_loss:  21.139735221862793\n",
      "count_amostra: 8570900\n",
      "iteration:  85740 , total_loss:  21.173793029785156\n",
      "count_amostra: 8573900\n",
      "iteration:  85770 , total_loss:  20.852515347798665\n",
      "count_amostra: 8576900\n",
      "iteration:  85800 , total_loss:  20.833868344624836\n",
      "count_amostra: 8579900\n",
      "iteration:  85830 , total_loss:  20.913814290364584\n",
      "count_amostra: 8582900\n",
      "iteration:  85860 , total_loss:  20.977731005350748\n",
      "count_amostra: 8585900\n",
      "iteration:  85890 , total_loss:  21.095513153076173\n",
      "count_amostra: 8588900\n",
      "iteration:  85920 , total_loss:  20.592138735453286\n",
      "count_amostra: 8591900\n",
      "iteration:  85950 , total_loss:  21.2769562403361\n",
      "count_amostra: 8594900\n",
      "iteration:  85980 , total_loss:  20.85160757700602\n",
      "count_amostra: 8597900\n",
      "iteration:  86010 , total_loss:  20.865157636006675\n",
      "count_amostra: 8600900\n",
      "iteration:  86040 , total_loss:  20.6626075108846\n",
      "count_amostra: 8603900\n",
      "iteration:  86070 , total_loss:  20.59277235666911\n",
      "count_amostra: 8606900\n",
      "iteration:  86100 , total_loss:  21.023016993204752\n",
      "count_amostra: 8609900\n",
      "iteration:  86130 , total_loss:  21.035620498657227\n",
      "count_amostra: 8612900\n",
      "iteration:  86160 , total_loss:  21.191173680623372\n",
      "count_amostra: 8615900\n",
      "iteration:  86190 , total_loss:  20.956779988606772\n",
      "count_amostra: 8618900\n",
      "iteration:  86220 , total_loss:  20.597724215189615\n",
      "count_amostra: 8621900\n",
      "iteration:  86250 , total_loss:  20.81948095957438\n",
      "count_amostra: 8624900\n",
      "iteration:  86280 , total_loss:  20.720706494649253\n",
      "count_amostra: 8627900\n",
      "iteration:  86310 , total_loss:  21.152156575520834\n",
      "count_amostra: 8630900\n",
      "iteration:  86340 , total_loss:  21.161476135253906\n",
      "count_amostra: 8633900\n",
      "iteration:  86370 , total_loss:  21.1277671178182\n",
      "count_amostra: 8636900\n",
      "iteration:  86400 , total_loss:  20.806490071614583\n",
      "count_amostra: 8639900\n",
      "iteration:  86430 , total_loss:  20.774990018208822\n",
      "count_amostra: 8642900\n",
      "iteration:  86460 , total_loss:  20.532721710205077\n",
      "count_amostra: 8645900\n",
      "iteration:  86490 , total_loss:  21.186680412292482\n",
      "count_amostra: 8648900\n",
      "iteration:  86520 , total_loss:  20.8298189163208\n",
      "count_amostra: 8651900\n",
      "iteration:  86550 , total_loss:  20.714843622843425\n",
      "count_amostra: 8654900\n",
      "iteration:  86580 , total_loss:  21.047311528523764\n",
      "count_amostra: 8657900\n",
      "iteration:  86610 , total_loss:  20.934943389892577\n",
      "count_amostra: 8660900\n",
      "iteration:  86640 , total_loss:  20.686288134256998\n",
      "count_amostra: 8663900\n",
      "iteration:  86670 , total_loss:  21.074485079447427\n",
      "count_amostra: 8666900\n",
      "iteration:  86700 , total_loss:  21.039532788594563\n",
      "count_amostra: 8669900\n",
      "iteration:  86730 , total_loss:  20.590891647338868\n",
      "count_amostra: 8672900\n",
      "iteration:  86760 , total_loss:  20.758355204264323\n",
      "count_amostra: 8675900\n",
      "iteration:  86790 , total_loss:  20.510858154296876\n",
      "count_amostra: 8678900\n",
      "iteration:  86820 , total_loss:  21.024989255269368\n",
      "count_amostra: 8681900\n",
      "iteration:  86850 , total_loss:  20.98358783721924\n",
      "count_amostra: 8684900\n",
      "iteration:  86880 , total_loss:  21.35224501291911\n",
      "count_amostra: 8687900\n",
      "iteration:  86910 , total_loss:  20.992306518554688\n",
      "count_amostra: 8690900\n",
      "iteration:  86940 , total_loss:  20.959111467997232\n",
      "count_amostra: 8693900\n",
      "iteration:  86970 , total_loss:  21.12451000213623\n",
      "count_amostra: 8696900\n",
      "iteration:  87000 , total_loss:  20.70526917775472\n",
      "count_amostra: 8699900\n",
      "iteration:  87030 , total_loss:  21.02151724497477\n",
      "count_amostra: 8702900\n",
      "iteration:  87060 , total_loss:  20.8589324315389\n",
      "count_amostra: 8705900\n",
      "iteration:  87090 , total_loss:  21.189713033040366\n",
      "count_amostra: 8708900\n",
      "iteration:  87120 , total_loss:  20.865757052103678\n",
      "count_amostra: 8711900\n",
      "iteration:  87150 , total_loss:  20.84356466929118\n",
      "count_amostra: 8714900\n",
      "iteration:  87180 , total_loss:  20.795000521341958\n",
      "count_amostra: 8717900\n",
      "iteration:  87210 , total_loss:  20.755463536580404\n",
      "count_amostra: 8720900\n",
      "iteration:  87240 , total_loss:  21.2617488861084\n",
      "count_amostra: 8723900\n",
      "iteration:  87270 , total_loss:  20.50426877339681\n",
      "count_amostra: 8726900\n",
      "iteration:  87300 , total_loss:  20.77763582865397\n",
      "count_amostra: 8729900\n",
      "iteration:  87330 , total_loss:  21.018458875020347\n",
      "count_amostra: 8732900\n",
      "iteration:  87360 , total_loss:  21.035749689737955\n",
      "count_amostra: 8735900\n",
      "iteration:  87390 , total_loss:  21.072769037882487\n",
      "count_amostra: 8738900\n",
      "iteration:  87420 , total_loss:  20.675603993733723\n",
      "count_amostra: 8741900\n",
      "iteration:  87450 , total_loss:  20.6069128036499\n",
      "count_amostra: 8744900\n",
      "iteration:  87480 , total_loss:  20.63217805226644\n",
      "count_amostra: 8747900\n",
      "iteration:  87510 , total_loss:  20.84127006530762\n",
      "count_amostra: 8750900\n",
      "iteration:  87540 , total_loss:  21.164454460144043\n",
      "count_amostra: 8753900\n",
      "iteration:  87570 , total_loss:  21.166755358378094\n",
      "count_amostra: 8756900\n",
      "iteration:  87600 , total_loss:  21.011630503336587\n",
      "count_amostra: 8759900\n",
      "iteration:  87630 , total_loss:  20.925241978963218\n",
      "count_amostra: 8762900\n",
      "iteration:  87660 , total_loss:  21.187168820699057\n",
      "count_amostra: 8765900\n",
      "iteration:  87690 , total_loss:  20.691634368896484\n",
      "count_amostra: 8768900\n",
      "iteration:  87720 , total_loss:  20.996292241414388\n",
      "count_amostra: 8771900\n",
      "iteration:  87750 , total_loss:  20.677030754089355\n",
      "count_amostra: 8774900\n",
      "iteration:  87780 , total_loss:  20.812630462646485\n",
      "count_amostra: 8777900\n",
      "iteration:  87810 , total_loss:  20.766182327270506\n",
      "count_amostra: 8780900\n",
      "iteration:  87840 , total_loss:  20.668009948730468\n",
      "count_amostra: 8783900\n",
      "iteration:  87870 , total_loss:  20.830857531229654\n",
      "count_amostra: 8786900\n",
      "iteration:  87900 , total_loss:  20.779316965738932\n",
      "count_amostra: 8789900\n",
      "iteration:  87930 , total_loss:  20.92537873586019\n",
      "count_amostra: 8792900\n",
      "iteration:  87960 , total_loss:  20.95396308898926\n",
      "count_amostra: 8795900\n",
      "iteration:  87990 , total_loss:  20.643340619405112\n",
      "count_amostra: 8798900\n",
      "iteration:  88020 , total_loss:  20.772331555684406\n",
      "count_amostra: 8801900\n",
      "iteration:  88050 , total_loss:  20.574595006306968\n",
      "count_amostra: 8804900\n",
      "iteration:  88080 , total_loss:  20.890542793273926\n",
      "count_amostra: 8807900\n",
      "iteration:  88110 , total_loss:  20.575837834676108\n",
      "count_amostra: 8810900\n",
      "iteration:  88140 , total_loss:  20.59464995066325\n",
      "count_amostra: 8813900\n",
      "iteration:  88170 , total_loss:  20.92280673980713\n",
      "count_amostra: 8816900\n",
      "iteration:  88200 , total_loss:  20.8816712697347\n",
      "count_amostra: 8819900\n",
      "iteration:  88230 , total_loss:  20.71839968363444\n",
      "count_amostra: 8822900\n",
      "iteration:  88260 , total_loss:  20.879595438639324\n",
      "count_amostra: 8825900\n",
      "iteration:  88290 , total_loss:  20.4188663482666\n",
      "count_amostra: 8828900\n",
      "iteration:  88320 , total_loss:  20.73535327911377\n",
      "count_amostra: 8831900\n",
      "iteration:  88350 , total_loss:  20.611763191223144\n",
      "count_amostra: 8834900\n",
      "iteration:  88380 , total_loss:  21.183707110087077\n",
      "count_amostra: 8837900\n",
      "iteration:  88410 , total_loss:  20.315887769063313\n",
      "count_amostra: 8840900\n",
      "iteration:  88440 , total_loss:  20.5432378133138\n",
      "count_amostra: 8843900\n",
      "iteration:  88470 , total_loss:  20.824057070414224\n",
      "count_amostra: 8846900\n",
      "iteration:  88500 , total_loss:  20.730160649617513\n",
      "count_amostra: 8849900\n",
      "iteration:  88530 , total_loss:  20.793299929300943\n",
      "count_amostra: 8852900\n",
      "iteration:  88560 , total_loss:  20.831643931070964\n",
      "count_amostra: 8855900\n",
      "iteration:  88590 , total_loss:  20.763486480712892\n",
      "count_amostra: 8858900\n",
      "iteration:  88620 , total_loss:  20.805345026652017\n",
      "count_amostra: 8861900\n",
      "iteration:  88650 , total_loss:  21.11253522237142\n",
      "count_amostra: 8864900\n",
      "iteration:  88680 , total_loss:  20.74124755859375\n",
      "count_amostra: 8867900\n",
      "iteration:  88710 , total_loss:  20.524153010050455\n",
      "count_amostra: 8870900\n",
      "iteration:  88740 , total_loss:  20.569317436218263\n",
      "count_amostra: 8873900\n",
      "iteration:  88770 , total_loss:  21.25020548502604\n",
      "count_amostra: 8876900\n",
      "iteration:  88800 , total_loss:  20.525339698791505\n",
      "count_amostra: 8879900\n",
      "iteration:  88830 , total_loss:  21.2377566019694\n",
      "count_amostra: 8882900\n",
      "iteration:  88860 , total_loss:  20.821407318115234\n",
      "count_amostra: 8885900\n",
      "iteration:  88890 , total_loss:  21.041709327697752\n",
      "count_amostra: 8888900\n",
      "iteration:  88920 , total_loss:  20.704064305623373\n",
      "count_amostra: 8891900\n",
      "iteration:  88950 , total_loss:  20.57844181060791\n",
      "count_amostra: 8894900\n",
      "iteration:  88980 , total_loss:  21.25089302062988\n",
      "count_amostra: 8897900\n",
      "iteration:  89010 , total_loss:  20.533453114827473\n",
      "count_amostra: 8900900\n",
      "iteration:  89040 , total_loss:  21.098150761922202\n",
      "count_amostra: 8903900\n",
      "iteration:  89070 , total_loss:  20.562366104125978\n",
      "count_amostra: 8906900\n",
      "iteration:  89100 , total_loss:  20.75323543548584\n",
      "count_amostra: 8909900\n",
      "iteration:  89130 , total_loss:  21.127981249491373\n",
      "count_amostra: 8912900\n",
      "iteration:  89160 , total_loss:  20.777095476786297\n",
      "count_amostra: 8915900\n",
      "iteration:  89190 , total_loss:  20.660029538472493\n",
      "count_amostra: 8918900\n",
      "iteration:  89220 , total_loss:  20.762822914123536\n",
      "count_amostra: 8921900\n",
      "iteration:  89250 , total_loss:  20.550694084167482\n",
      "count_amostra: 8924900\n",
      "iteration:  89280 , total_loss:  20.73201758066813\n",
      "count_amostra: 8927900\n",
      "iteration:  89310 , total_loss:  21.461125564575195\n",
      "count_amostra: 8930900\n",
      "iteration:  89340 , total_loss:  20.63651688893636\n",
      "count_amostra: 8933900\n",
      "iteration:  89370 , total_loss:  20.765311622619627\n",
      "count_amostra: 8936900\n",
      "iteration:  89400 , total_loss:  20.85572821299235\n",
      "count_amostra: 8939900\n",
      "iteration:  89430 , total_loss:  20.878167597452798\n",
      "count_amostra: 8942900\n",
      "iteration:  89460 , total_loss:  21.0210319519043\n",
      "count_amostra: 8945900\n",
      "iteration:  89490 , total_loss:  20.94499944051107\n",
      "count_amostra: 8948900\n",
      "iteration:  89520 , total_loss:  20.67216567993164\n",
      "count_amostra: 8951900\n",
      "iteration:  89550 , total_loss:  20.667305183410644\n",
      "count_amostra: 8954900\n",
      "iteration:  89580 , total_loss:  20.97703768412272\n",
      "count_amostra: 8957900\n",
      "iteration:  89610 , total_loss:  20.97607135772705\n",
      "count_amostra: 8960900\n",
      "iteration:  89640 , total_loss:  21.11581319173177\n",
      "count_amostra: 8963900\n",
      "iteration:  89670 , total_loss:  20.85296459197998\n",
      "count_amostra: 8966900\n",
      "iteration:  89700 , total_loss:  20.979087766011556\n",
      "count_amostra: 8969900\n",
      "iteration:  89730 , total_loss:  20.755116589864095\n",
      "count_amostra: 8972900\n",
      "iteration:  89760 , total_loss:  20.959459241231283\n",
      "count_amostra: 8975900\n",
      "iteration:  89790 , total_loss:  20.873163604736327\n",
      "count_amostra: 8978900\n",
      "iteration:  89820 , total_loss:  21.219206428527833\n",
      "count_amostra: 8981900\n",
      "iteration:  89850 , total_loss:  20.810437138875326\n",
      "count_amostra: 8984900\n",
      "iteration:  89880 , total_loss:  20.77204837799072\n",
      "count_amostra: 8987900\n",
      "iteration:  89910 , total_loss:  21.030860265096027\n",
      "count_amostra: 8990900\n",
      "iteration:  89940 , total_loss:  20.64767990112305\n",
      "count_amostra: 8993900\n",
      "iteration:  89970 , total_loss:  21.352824910481772\n",
      "count_amostra: 8996900\n",
      "iteration:  90000 , total_loss:  20.68241532643636\n",
      "count_amostra: 8999900\n",
      "iteration:  90030 , total_loss:  20.843014589945476\n",
      "count_amostra: 9002900\n",
      "iteration:  90060 , total_loss:  20.484910202026366\n",
      "count_amostra: 9005900\n",
      "iteration:  90090 , total_loss:  20.735163307189943\n",
      "count_amostra: 9008900\n",
      "iteration:  90120 , total_loss:  20.906757227579753\n",
      "count_amostra: 9011900\n",
      "iteration:  90150 , total_loss:  20.67555376688639\n",
      "count_amostra: 9014900\n",
      "iteration:  90180 , total_loss:  20.78227119445801\n",
      "count_amostra: 9017900\n",
      "iteration:  90210 , total_loss:  21.13343645731608\n",
      "count_amostra: 9020900\n",
      "iteration:  90240 , total_loss:  20.923867225646973\n",
      "count_amostra: 9023900\n",
      "iteration:  90270 , total_loss:  20.725150489807127\n",
      "count_amostra: 9026900\n",
      "iteration:  90300 , total_loss:  21.052627499898275\n",
      "count_amostra: 9029900\n",
      "iteration:  90330 , total_loss:  20.980508041381835\n",
      "count_amostra: 9032900\n",
      "iteration:  90360 , total_loss:  20.62293109893799\n",
      "count_amostra: 9035900\n",
      "iteration:  90390 , total_loss:  20.680585034688313\n",
      "count_amostra: 9038900\n",
      "iteration:  90420 , total_loss:  21.077143796284993\n",
      "count_amostra: 9041900\n",
      "iteration:  90450 , total_loss:  20.5794012705485\n",
      "count_amostra: 9044900\n",
      "iteration:  90480 , total_loss:  20.967311350504556\n",
      "count_amostra: 9047900\n",
      "iteration:  90510 , total_loss:  20.78661003112793\n",
      "count_amostra: 9050900\n",
      "iteration:  90540 , total_loss:  21.143763287862143\n",
      "count_amostra: 9053900\n",
      "iteration:  90570 , total_loss:  20.73871390024821\n",
      "count_amostra: 9056900\n",
      "iteration:  90600 , total_loss:  20.80720659891764\n",
      "count_amostra: 9059900\n",
      "iteration:  90630 , total_loss:  20.867412567138672\n",
      "count_amostra: 9062900\n",
      "iteration:  90660 , total_loss:  21.373724873860677\n",
      "count_amostra: 9065900\n",
      "iteration:  90690 , total_loss:  20.42013053894043\n",
      "count_amostra: 9068900\n",
      "iteration:  90720 , total_loss:  21.14682273864746\n",
      "count_amostra: 9071900\n",
      "iteration:  90750 , total_loss:  20.942835744222005\n",
      "count_amostra: 9074900\n",
      "iteration:  90780 , total_loss:  21.170670064290366\n",
      "count_amostra: 9077900\n",
      "iteration:  90810 , total_loss:  20.93498929341634\n",
      "count_amostra: 9080900\n",
      "iteration:  90840 , total_loss:  20.591799354553224\n",
      "count_amostra: 9083900\n",
      "iteration:  90870 , total_loss:  20.790302594502766\n",
      "count_amostra: 9086900\n",
      "iteration:  90900 , total_loss:  20.502831141153973\n",
      "count_amostra: 9089900\n",
      "iteration:  90930 , total_loss:  21.23284772237142\n",
      "count_amostra: 9092900\n",
      "iteration:  90960 , total_loss:  20.36772092183431\n",
      "count_amostra: 9095900\n",
      "iteration:  90990 , total_loss:  21.055129051208496\n",
      "count_amostra: 9098900\n",
      "iteration:  91020 , total_loss:  20.431697527567547\n",
      "count_amostra: 9101900\n",
      "iteration:  91050 , total_loss:  20.964905166625975\n",
      "count_amostra: 9104900\n",
      "iteration:  91080 , total_loss:  20.63996276855469\n",
      "count_amostra: 9107900\n",
      "iteration:  91110 , total_loss:  21.112208938598634\n",
      "count_amostra: 9110900\n",
      "iteration:  91140 , total_loss:  20.983945910135905\n",
      "count_amostra: 9113900\n",
      "iteration:  91170 , total_loss:  20.63745257059733\n",
      "count_amostra: 9116900\n",
      "iteration:  91200 , total_loss:  21.251945559183756\n",
      "count_amostra: 9119900\n",
      "iteration:  91230 , total_loss:  21.292953745524088\n",
      "count_amostra: 9122900\n",
      "iteration:  91260 , total_loss:  20.93462231953939\n",
      "count_amostra: 9125900\n",
      "iteration:  91290 , total_loss:  20.96186008453369\n",
      "count_amostra: 9128900\n",
      "iteration:  91320 , total_loss:  21.088358624776205\n",
      "count_amostra: 9131900\n",
      "iteration:  91350 , total_loss:  20.822777239481606\n",
      "count_amostra: 9134900\n",
      "iteration:  91380 , total_loss:  21.061680348714194\n",
      "count_amostra: 9137900\n",
      "iteration:  91410 , total_loss:  20.84332383473714\n",
      "count_amostra: 9140900\n",
      "iteration:  91440 , total_loss:  20.890865770975747\n",
      "count_amostra: 9143900\n",
      "iteration:  91470 , total_loss:  20.49949270884196\n",
      "count_amostra: 9146900\n",
      "iteration:  91500 , total_loss:  20.935290145874024\n",
      "count_amostra: 9149900\n",
      "iteration:  91530 , total_loss:  20.514509773254396\n",
      "count_amostra: 9152900\n",
      "iteration:  91560 , total_loss:  21.0545862197876\n",
      "count_amostra: 9155900\n",
      "iteration:  91590 , total_loss:  20.901898002624513\n",
      "count_amostra: 9158900\n",
      "iteration:  91620 , total_loss:  20.874225870768228\n",
      "count_amostra: 9161900\n",
      "iteration:  91650 , total_loss:  21.030492083231607\n",
      "count_amostra: 9164900\n",
      "iteration:  91680 , total_loss:  20.55808277130127\n",
      "count_amostra: 9167900\n",
      "iteration:  91710 , total_loss:  20.94486821492513\n",
      "count_amostra: 9170900\n",
      "iteration:  91740 , total_loss:  20.650168736775715\n",
      "count_amostra: 9173900\n",
      "iteration:  91770 , total_loss:  20.7924835840861\n",
      "count_amostra: 9176900\n",
      "iteration:  91800 , total_loss:  20.76364549001058\n",
      "count_amostra: 9179900\n",
      "iteration:  91830 , total_loss:  20.762637519836424\n",
      "count_amostra: 9182900\n",
      "iteration:  91860 , total_loss:  20.755563926696777\n",
      "count_amostra: 9185900\n",
      "iteration:  91890 , total_loss:  20.599982007344565\n",
      "count_amostra: 9188900\n",
      "iteration:  91920 , total_loss:  20.531225204467773\n",
      "count_amostra: 9191900\n",
      "iteration:  91950 , total_loss:  20.756818326314292\n",
      "count_amostra: 9194900\n",
      "iteration:  91980 , total_loss:  20.791650199890135\n",
      "count_amostra: 9197900\n",
      "iteration:  92010 , total_loss:  20.84264570871989\n",
      "count_amostra: 9200900\n",
      "iteration:  92040 , total_loss:  20.837576166788736\n",
      "count_amostra: 9203900\n",
      "iteration:  92070 , total_loss:  21.0581335067749\n",
      "count_amostra: 9206900\n",
      "iteration:  92100 , total_loss:  20.47490520477295\n",
      "count_amostra: 9209900\n",
      "iteration:  92130 , total_loss:  20.99506607055664\n",
      "count_amostra: 9212900\n",
      "iteration:  92160 , total_loss:  21.332963752746583\n",
      "count_amostra: 9215900\n",
      "iteration:  92190 , total_loss:  20.69356772104899\n",
      "count_amostra: 9218900\n",
      "iteration:  92220 , total_loss:  20.622416241963705\n",
      "count_amostra: 9221900\n",
      "iteration:  92250 , total_loss:  20.89288870493571\n",
      "count_amostra: 9224900\n",
      "iteration:  92280 , total_loss:  21.028178215026855\n",
      "count_amostra: 9227900\n",
      "iteration:  92310 , total_loss:  20.734876759847005\n",
      "count_amostra: 9230900\n",
      "iteration:  92340 , total_loss:  20.85112819671631\n",
      "count_amostra: 9233900\n",
      "iteration:  92370 , total_loss:  20.646960004170737\n",
      "count_amostra: 9236900\n",
      "iteration:  92400 , total_loss:  21.13563117980957\n",
      "count_amostra: 9239900\n",
      "iteration:  92430 , total_loss:  21.12839349110921\n",
      "count_amostra: 9242900\n",
      "iteration:  92460 , total_loss:  20.810820388793946\n",
      "count_amostra: 9245900\n",
      "iteration:  92490 , total_loss:  20.95381304423014\n",
      "count_amostra: 9248900\n",
      "iteration:  92520 , total_loss:  21.134677569071453\n",
      "count_amostra: 9251900\n",
      "iteration:  92550 , total_loss:  20.857782491048177\n",
      "count_amostra: 9254900\n",
      "iteration:  92580 , total_loss:  20.61418062845866\n",
      "count_amostra: 9257900\n",
      "iteration:  92610 , total_loss:  21.25136267344157\n",
      "count_amostra: 9260900\n",
      "iteration:  92640 , total_loss:  20.687335268656412\n",
      "count_amostra: 9263900\n",
      "iteration:  92670 , total_loss:  20.656941604614257\n",
      "count_amostra: 9266900\n",
      "iteration:  92700 , total_loss:  21.317565854390462\n",
      "count_amostra: 9269900\n",
      "iteration:  92730 , total_loss:  20.852075068155923\n",
      "count_amostra: 9272900\n",
      "iteration:  92760 , total_loss:  20.852995936075846\n",
      "count_amostra: 9275900\n",
      "iteration:  92790 , total_loss:  21.19425411224365\n",
      "count_amostra: 9278900\n",
      "iteration:  92820 , total_loss:  20.671660232543946\n",
      "count_amostra: 9281900\n",
      "iteration:  92850 , total_loss:  20.539837646484376\n",
      "count_amostra: 9284900\n",
      "iteration:  92880 , total_loss:  20.953801536560057\n",
      "count_amostra: 9287900\n",
      "iteration:  92910 , total_loss:  20.635116068522134\n",
      "count_amostra: 9290900\n",
      "iteration:  92940 , total_loss:  20.821267636617026\n",
      "count_amostra: 9293900\n",
      "iteration:  92970 , total_loss:  20.50089143117269\n",
      "count_amostra: 9296900\n",
      "iteration:  93000 , total_loss:  20.740921529134116\n",
      "count_amostra: 9299900\n",
      "iteration:  93030 , total_loss:  20.844145393371583\n",
      "count_amostra: 9302900\n",
      "iteration:  93060 , total_loss:  20.853266779581705\n",
      "count_amostra: 9305900\n",
      "iteration:  93090 , total_loss:  20.40180466969808\n",
      "count_amostra: 9308900\n",
      "iteration:  93120 , total_loss:  20.91288776397705\n",
      "count_amostra: 9311900\n",
      "iteration:  93150 , total_loss:  20.894980049133302\n",
      "count_amostra: 9314900\n",
      "iteration:  93180 , total_loss:  20.673364067077635\n",
      "count_amostra: 9317900\n",
      "iteration:  93210 , total_loss:  20.878021558125813\n",
      "count_amostra: 9320900\n",
      "iteration:  93240 , total_loss:  20.898408699035645\n",
      "count_amostra: 9323900\n",
      "iteration:  93270 , total_loss:  20.857895787556966\n",
      "count_amostra: 9326900\n",
      "iteration:  93300 , total_loss:  20.45799357096354\n",
      "count_amostra: 9329900\n",
      "iteration:  93330 , total_loss:  20.915997950236\n",
      "count_amostra: 9332900\n",
      "iteration:  93360 , total_loss:  20.58807455698649\n",
      "count_amostra: 9335900\n",
      "iteration:  93390 , total_loss:  20.748173459370932\n",
      "count_amostra: 9338900\n",
      "iteration:  93420 , total_loss:  20.533850606282552\n",
      "count_amostra: 9341900\n",
      "iteration:  93450 , total_loss:  20.840089480082195\n",
      "count_amostra: 9344900\n",
      "iteration:  93480 , total_loss:  20.752395502726237\n",
      "count_amostra: 9347900\n",
      "iteration:  93510 , total_loss:  20.62434787750244\n",
      "count_amostra: 9350900\n",
      "iteration:  93540 , total_loss:  20.616840235392253\n",
      "count_amostra: 9353900\n",
      "iteration:  93570 , total_loss:  20.860494105021157\n",
      "count_amostra: 9356900\n",
      "iteration:  93600 , total_loss:  20.907587877909343\n",
      "count_amostra: 9359900\n",
      "iteration:  93630 , total_loss:  20.510645612080893\n",
      "count_amostra: 9362900\n",
      "iteration:  93660 , total_loss:  20.707100741068523\n",
      "count_amostra: 9365900\n",
      "iteration:  93690 , total_loss:  21.152209981282553\n",
      "count_amostra: 9368900\n",
      "iteration:  93720 , total_loss:  20.896116892496746\n",
      "count_amostra: 9371900\n",
      "iteration:  93750 , total_loss:  20.492553265889487\n",
      "count_amostra: 9374900\n",
      "iteration:  93780 , total_loss:  20.90403486887614\n",
      "count_amostra: 9377900\n",
      "iteration:  93810 , total_loss:  21.258323605855306\n",
      "count_amostra: 9380900\n",
      "iteration:  93840 , total_loss:  20.972943178812663\n",
      "count_amostra: 9383900\n",
      "iteration:  93870 , total_loss:  20.861200714111327\n",
      "count_amostra: 9386900\n",
      "iteration:  93900 , total_loss:  20.30754680633545\n",
      "count_amostra: 9389900\n",
      "iteration:  93930 , total_loss:  20.6691130956014\n",
      "count_amostra: 9392900\n",
      "iteration:  93960 , total_loss:  20.89792283376058\n",
      "count_amostra: 9395900\n",
      "iteration:  93990 , total_loss:  20.550622431437173\n",
      "count_amostra: 9398900\n",
      "iteration:  94020 , total_loss:  21.049985122680663\n",
      "count_amostra: 9401900\n",
      "iteration:  94050 , total_loss:  20.828608767191568\n",
      "count_amostra: 9404900\n",
      "iteration:  94080 , total_loss:  20.81475149790446\n",
      "count_amostra: 9407900\n",
      "iteration:  94110 , total_loss:  20.34089552561442\n",
      "count_amostra: 9410900\n",
      "iteration:  94140 , total_loss:  20.848144022623696\n",
      "count_amostra: 9413900\n",
      "iteration:  94170 , total_loss:  20.877535120646158\n",
      "count_amostra: 9416900\n",
      "iteration:  94200 , total_loss:  20.725426737467448\n",
      "count_amostra: 9419900\n",
      "iteration:  94230 , total_loss:  20.62313969930013\n",
      "count_amostra: 9422900\n",
      "iteration:  94260 , total_loss:  20.916576957702638\n",
      "count_amostra: 9425900\n",
      "iteration:  94290 , total_loss:  20.642069753011068\n",
      "count_amostra: 9428900\n",
      "iteration:  94320 , total_loss:  20.73838799794515\n",
      "count_amostra: 9431900\n",
      "iteration:  94350 , total_loss:  20.761617851257324\n",
      "count_amostra: 9434900\n",
      "iteration:  94380 , total_loss:  20.53660233815511\n",
      "count_amostra: 9437900\n",
      "iteration:  94410 , total_loss:  20.74315814971924\n",
      "count_amostra: 9440900\n",
      "iteration:  94440 , total_loss:  20.62372563680013\n",
      "count_amostra: 9443900\n",
      "iteration:  94470 , total_loss:  20.76517823537191\n",
      "count_amostra: 9446900\n",
      "iteration:  94500 , total_loss:  20.755567932128905\n",
      "count_amostra: 9449900\n",
      "iteration:  94530 , total_loss:  20.39952869415283\n",
      "count_amostra: 9452900\n",
      "iteration:  94560 , total_loss:  20.77520268758138\n",
      "count_amostra: 9455900\n",
      "iteration:  94590 , total_loss:  20.950658734639486\n",
      "count_amostra: 9458900\n",
      "iteration:  94620 , total_loss:  20.85845527648926\n",
      "count_amostra: 9461900\n",
      "iteration:  94650 , total_loss:  20.724872016906737\n",
      "count_amostra: 9464900\n",
      "iteration:  94680 , total_loss:  20.930881881713866\n",
      "count_amostra: 9467900\n",
      "iteration:  94710 , total_loss:  20.668500010172526\n",
      "count_amostra: 9470900\n",
      "iteration:  94740 , total_loss:  20.78261884053548\n",
      "count_amostra: 9473900\n",
      "iteration:  94770 , total_loss:  20.52900791168213\n",
      "count_amostra: 9476900\n",
      "iteration:  94800 , total_loss:  20.891255633036295\n",
      "count_amostra: 9479900\n",
      "iteration:  94830 , total_loss:  20.440148035685223\n",
      "count_amostra: 9482900\n",
      "iteration:  94860 , total_loss:  20.57569999694824\n",
      "count_amostra: 9485900\n",
      "iteration:  94890 , total_loss:  20.847039222717285\n",
      "count_amostra: 9488900\n",
      "iteration:  94920 , total_loss:  21.195956802368165\n",
      "count_amostra: 9491900\n",
      "iteration:  94950 , total_loss:  20.419018173217772\n",
      "count_amostra: 9494900\n",
      "iteration:  94980 , total_loss:  20.93455956776937\n",
      "count_amostra: 9497900\n",
      "iteration:  95010 , total_loss:  20.287485376993814\n",
      "count_amostra: 9500900\n",
      "iteration:  95040 , total_loss:  20.797583770751952\n",
      "count_amostra: 9503900\n",
      "iteration:  95070 , total_loss:  20.74128621419271\n",
      "count_amostra: 9506900\n",
      "iteration:  95100 , total_loss:  21.138451131184897\n",
      "count_amostra: 9509900\n",
      "iteration:  95130 , total_loss:  20.68044548034668\n",
      "count_amostra: 9512900\n",
      "iteration:  95160 , total_loss:  20.60808499654134\n",
      "count_amostra: 9515900\n",
      "iteration:  95190 , total_loss:  21.170664978027343\n",
      "count_amostra: 9518900\n",
      "iteration:  95220 , total_loss:  21.276363945007326\n",
      "count_amostra: 9521900\n",
      "iteration:  95250 , total_loss:  20.879198010762533\n",
      "count_amostra: 9524900\n",
      "iteration:  95280 , total_loss:  20.60472100575765\n",
      "count_amostra: 9527900\n",
      "iteration:  95310 , total_loss:  20.891174697875975\n",
      "count_amostra: 9530900\n",
      "iteration:  95340 , total_loss:  20.80845724741618\n",
      "count_amostra: 9533900\n",
      "iteration:  95370 , total_loss:  20.753123219807943\n",
      "count_amostra: 9536900\n",
      "iteration:  95400 , total_loss:  20.95712401072184\n",
      "count_amostra: 9539900\n",
      "iteration:  95430 , total_loss:  20.495126914978027\n",
      "count_amostra: 9542900\n",
      "iteration:  95460 , total_loss:  20.286476771036785\n",
      "count_amostra: 9545900\n",
      "iteration:  95490 , total_loss:  20.693379847208657\n",
      "count_amostra: 9548900\n",
      "iteration:  95520 , total_loss:  20.963204765319823\n",
      "count_amostra: 9551900\n",
      "iteration:  95550 , total_loss:  20.452328236897788\n",
      "count_amostra: 9554900\n",
      "iteration:  95580 , total_loss:  20.738796615600585\n",
      "count_amostra: 9557900\n",
      "iteration:  95610 , total_loss:  20.860051155090332\n",
      "count_amostra: 9560900\n",
      "iteration:  95640 , total_loss:  20.65303325653076\n",
      "count_amostra: 9563900\n",
      "iteration:  95670 , total_loss:  20.41853001912435\n",
      "count_amostra: 9566900\n",
      "iteration:  95700 , total_loss:  20.628350003560385\n",
      "count_amostra: 9569900\n",
      "iteration:  95730 , total_loss:  20.533334159851073\n",
      "count_amostra: 9572900\n",
      "iteration:  95760 , total_loss:  20.798442840576172\n",
      "count_amostra: 9575900\n",
      "iteration:  95790 , total_loss:  20.979471842447918\n",
      "count_amostra: 9578900\n",
      "iteration:  95820 , total_loss:  20.755082448323567\n",
      "count_amostra: 9581900\n",
      "iteration:  95850 , total_loss:  20.901655451456705\n",
      "count_amostra: 9584900\n",
      "iteration:  95880 , total_loss:  20.7004976272583\n",
      "count_amostra: 9587900\n",
      "iteration:  95910 , total_loss:  20.833667119344074\n",
      "count_amostra: 9590900\n",
      "iteration:  95940 , total_loss:  20.76289202372233\n",
      "count_amostra: 9593900\n",
      "iteration:  95970 , total_loss:  20.853309377034506\n",
      "count_amostra: 9596900\n",
      "iteration:  96000 , total_loss:  20.756229654947916\n",
      "count_amostra: 9599900\n",
      "iteration:  96030 , total_loss:  20.584168179829916\n",
      "count_amostra: 9602900\n",
      "iteration:  96060 , total_loss:  20.94956544240316\n",
      "count_amostra: 9605900\n",
      "iteration:  96090 , total_loss:  20.750923919677735\n",
      "count_amostra: 9608900\n",
      "iteration:  96120 , total_loss:  20.839074007670085\n",
      "count_amostra: 9611900\n",
      "iteration:  96150 , total_loss:  21.104254722595215\n",
      "count_amostra: 9614900\n",
      "iteration:  96180 , total_loss:  20.9417174021403\n",
      "count_amostra: 9617900\n",
      "iteration:  96210 , total_loss:  20.758999252319335\n",
      "count_amostra: 9620900\n",
      "iteration:  96240 , total_loss:  20.746463330586753\n",
      "count_amostra: 9623900\n",
      "iteration:  96270 , total_loss:  20.80525646209717\n",
      "count_amostra: 9626900\n",
      "iteration:  96300 , total_loss:  21.015067036946615\n",
      "count_amostra: 9629900\n",
      "iteration:  96330 , total_loss:  21.378879165649415\n",
      "count_amostra: 9632900\n",
      "iteration:  96360 , total_loss:  20.89119249979655\n",
      "count_amostra: 9635900\n",
      "iteration:  96390 , total_loss:  20.784579149882\n",
      "count_amostra: 9638900\n",
      "iteration:  96420 , total_loss:  20.615193049112957\n",
      "count_amostra: 9641900\n",
      "iteration:  96450 , total_loss:  20.97841625213623\n",
      "count_amostra: 9644900\n",
      "iteration:  96480 , total_loss:  20.610514068603514\n",
      "count_amostra: 9647900\n",
      "iteration:  96510 , total_loss:  20.506042416890462\n",
      "count_amostra: 9650900\n",
      "iteration:  96540 , total_loss:  21.052880859375\n",
      "count_amostra: 9653900\n",
      "iteration:  96570 , total_loss:  20.68423366546631\n",
      "count_amostra: 9656900\n",
      "iteration:  96600 , total_loss:  20.734772173563638\n",
      "count_amostra: 9659900\n",
      "iteration:  96630 , total_loss:  20.656744702657065\n",
      "count_amostra: 9662900\n",
      "iteration:  96660 , total_loss:  20.63554547627767\n",
      "count_amostra: 9665900\n",
      "iteration:  96690 , total_loss:  20.91241086324056\n",
      "count_amostra: 9668900\n",
      "iteration:  96720 , total_loss:  20.463295809427898\n",
      "count_amostra: 9671900\n",
      "iteration:  96750 , total_loss:  20.677906163533528\n",
      "count_amostra: 9674900\n",
      "iteration:  96780 , total_loss:  21.046922620137533\n",
      "count_amostra: 9677900\n",
      "iteration:  96810 , total_loss:  20.890823554992675\n",
      "count_amostra: 9680900\n",
      "iteration:  96840 , total_loss:  20.716904830932616\n",
      "count_amostra: 9683900\n",
      "iteration:  96870 , total_loss:  21.394778760274253\n",
      "count_amostra: 9686900\n",
      "iteration:  96900 , total_loss:  20.599478848775227\n",
      "count_amostra: 9689900\n",
      "iteration:  96930 , total_loss:  20.698410097757975\n",
      "count_amostra: 9692900\n",
      "iteration:  96960 , total_loss:  20.479300944010415\n",
      "count_amostra: 9695900\n",
      "iteration:  96990 , total_loss:  20.713541348775227\n",
      "count_amostra: 9698900\n",
      "iteration:  97020 , total_loss:  20.952864583333334\n",
      "count_amostra: 9701900\n",
      "iteration:  97050 , total_loss:  20.950215021769207\n",
      "count_amostra: 9704900\n",
      "iteration:  97080 , total_loss:  20.684065691630046\n",
      "count_amostra: 9707900\n",
      "iteration:  97110 , total_loss:  20.78339614868164\n",
      "count_amostra: 9710900\n",
      "iteration:  97140 , total_loss:  21.110168520609538\n",
      "count_amostra: 9713900\n",
      "iteration:  97170 , total_loss:  20.874553934733072\n",
      "count_amostra: 9716900\n",
      "iteration:  97200 , total_loss:  20.616252009073893\n",
      "count_amostra: 9719900\n",
      "iteration:  97230 , total_loss:  20.77001336415609\n",
      "count_amostra: 9722900\n",
      "iteration:  97260 , total_loss:  20.698189353942873\n",
      "count_amostra: 9725900\n",
      "iteration:  97290 , total_loss:  20.632896169026694\n",
      "count_amostra: 9728900\n",
      "iteration:  97320 , total_loss:  20.903284136454264\n",
      "count_amostra: 9731900\n",
      "iteration:  97350 , total_loss:  20.8585568745931\n",
      "count_amostra: 9734900\n",
      "iteration:  97380 , total_loss:  20.88330707550049\n",
      "count_amostra: 9737900\n",
      "iteration:  97410 , total_loss:  20.89522031148275\n",
      "count_amostra: 9740900\n",
      "iteration:  97440 , total_loss:  20.816790453592937\n",
      "count_amostra: 9743900\n",
      "iteration:  97470 , total_loss:  20.562085596720376\n",
      "count_amostra: 9746900\n",
      "iteration:  97500 , total_loss:  20.503962580362955\n",
      "count_amostra: 9749900\n",
      "iteration:  97530 , total_loss:  20.830087916056314\n",
      "count_amostra: 9752900\n",
      "iteration:  97560 , total_loss:  21.046391105651857\n",
      "count_amostra: 9755900\n",
      "iteration:  97590 , total_loss:  20.777973238627116\n",
      "count_amostra: 9758900\n",
      "iteration:  97620 , total_loss:  20.271550051371257\n",
      "count_amostra: 9761900\n",
      "iteration:  97650 , total_loss:  20.94638131459554\n",
      "count_amostra: 9764900\n",
      "iteration:  97680 , total_loss:  20.69875939687093\n",
      "count_amostra: 9767900\n",
      "iteration:  97710 , total_loss:  21.075904273986815\n",
      "count_amostra: 9770900\n",
      "iteration:  97740 , total_loss:  20.93616428375244\n",
      "count_amostra: 9773900\n",
      "iteration:  97770 , total_loss:  20.64160353342692\n",
      "count_amostra: 9776900\n",
      "iteration:  97800 , total_loss:  20.714066886901854\n",
      "count_amostra: 9779900\n",
      "iteration:  97830 , total_loss:  21.01587200164795\n",
      "count_amostra: 9782900\n",
      "iteration:  97860 , total_loss:  20.662815030415853\n",
      "count_amostra: 9785900\n",
      "iteration:  97890 , total_loss:  20.97526798248291\n",
      "count_amostra: 9788900\n",
      "iteration:  97920 , total_loss:  20.90550734202067\n",
      "count_amostra: 9791900\n",
      "iteration:  97950 , total_loss:  21.044417762756346\n",
      "count_amostra: 9794900\n",
      "iteration:  97980 , total_loss:  20.601618258158364\n",
      "count_amostra: 9797900\n",
      "iteration:  98010 , total_loss:  20.945862261454263\n",
      "count_amostra: 9800900\n",
      "iteration:  98040 , total_loss:  20.782391039530435\n",
      "count_amostra: 9803900\n",
      "iteration:  98070 , total_loss:  20.99644686381022\n",
      "count_amostra: 9806900\n",
      "iteration:  98100 , total_loss:  20.590326182047527\n",
      "count_amostra: 9809900\n",
      "iteration:  98130 , total_loss:  20.876952616373696\n",
      "count_amostra: 9812900\n",
      "iteration:  98160 , total_loss:  20.809161949157716\n",
      "count_amostra: 9815900\n",
      "iteration:  98190 , total_loss:  20.986497815450033\n",
      "count_amostra: 9818900\n",
      "iteration:  98220 , total_loss:  21.126359939575195\n",
      "count_amostra: 9821900\n",
      "iteration:  98250 , total_loss:  20.840091896057128\n",
      "count_amostra: 9824900\n",
      "iteration:  98280 , total_loss:  20.75937385559082\n",
      "count_amostra: 9827900\n",
      "iteration:  98310 , total_loss:  20.791145896911623\n",
      "count_amostra: 9830900\n",
      "iteration:  98340 , total_loss:  20.840373865763347\n",
      "count_amostra: 9833900\n",
      "iteration:  98370 , total_loss:  20.743462880452473\n",
      "count_amostra: 9836900\n",
      "iteration:  98400 , total_loss:  20.964360109965007\n",
      "count_amostra: 9839900\n",
      "iteration:  98430 , total_loss:  20.486553637186685\n",
      "count_amostra: 9842900\n",
      "iteration:  98460 , total_loss:  20.81730728149414\n",
      "count_amostra: 9845900\n",
      "iteration:  98490 , total_loss:  20.810529200236\n",
      "count_amostra: 9848900\n",
      "iteration:  98520 , total_loss:  20.816276105244956\n",
      "count_amostra: 9851900\n",
      "iteration:  98550 , total_loss:  20.636501185099284\n",
      "count_amostra: 9854900\n",
      "iteration:  98580 , total_loss:  20.56830202738444\n",
      "count_amostra: 9857900\n",
      "iteration:  98610 , total_loss:  20.83001575469971\n",
      "count_amostra: 9860900\n",
      "iteration:  98640 , total_loss:  21.150794982910156\n",
      "count_amostra: 9863900\n",
      "iteration:  98670 , total_loss:  20.735791969299317\n",
      "count_amostra: 9866900\n",
      "iteration:  98700 , total_loss:  20.727474721272788\n",
      "count_amostra: 9869900\n",
      "iteration:  98730 , total_loss:  20.572598012288413\n",
      "count_amostra: 9872900\n",
      "iteration:  98760 , total_loss:  20.731661224365233\n",
      "count_amostra: 9875900\n",
      "iteration:  98790 , total_loss:  20.80855744679769\n",
      "count_amostra: 9878900\n",
      "iteration:  98820 , total_loss:  20.713935152689615\n",
      "count_amostra: 9881900\n",
      "iteration:  98850 , total_loss:  21.081457646687827\n",
      "count_amostra: 9884900\n",
      "iteration:  98880 , total_loss:  20.232001749674478\n",
      "count_amostra: 9887900\n",
      "iteration:  98910 , total_loss:  20.68644110361735\n",
      "count_amostra: 9890900\n",
      "iteration:  98940 , total_loss:  20.655035146077473\n",
      "count_amostra: 9893900\n",
      "iteration:  98970 , total_loss:  20.65601463317871\n",
      "count_amostra: 9896900\n",
      "iteration:  99000 , total_loss:  21.087338892618813\n",
      "count_amostra: 9899900\n",
      "iteration:  99030 , total_loss:  20.453696060180665\n",
      "count_amostra: 9902900\n",
      "iteration:  99060 , total_loss:  20.754007848103843\n",
      "count_amostra: 9905900\n",
      "iteration:  99090 , total_loss:  20.745107905069986\n",
      "count_amostra: 9908900\n",
      "iteration:  99120 , total_loss:  20.72157293955485\n",
      "count_amostra: 9911900\n",
      "iteration:  99150 , total_loss:  20.573054695129393\n",
      "count_amostra: 9914900\n",
      "iteration:  99180 , total_loss:  20.982061513264973\n",
      "count_amostra: 9917900\n",
      "iteration:  99210 , total_loss:  20.648579661051432\n",
      "count_amostra: 9920900\n",
      "iteration:  99240 , total_loss:  20.875102996826172\n",
      "count_amostra: 9923900\n",
      "iteration:  99270 , total_loss:  20.920381673177083\n",
      "count_amostra: 9926900\n",
      "iteration:  99300 , total_loss:  21.001458485921223\n",
      "count_amostra: 9929900\n",
      "iteration:  99330 , total_loss:  20.731431261698404\n",
      "count_amostra: 9932900\n",
      "iteration:  99360 , total_loss:  20.715636825561525\n",
      "count_amostra: 9935900\n",
      "iteration:  99390 , total_loss:  20.91695931752523\n",
      "count_amostra: 9938900\n",
      "iteration:  99420 , total_loss:  20.528716723124187\n",
      "count_amostra: 9941900\n",
      "iteration:  99450 , total_loss:  20.524424743652343\n",
      "count_amostra: 9944900\n",
      "iteration:  99480 , total_loss:  21.151554679870607\n",
      "count_amostra: 9947900\n",
      "iteration:  99510 , total_loss:  21.055485026041666\n",
      "count_amostra: 9950900\n",
      "iteration:  99540 , total_loss:  20.56120122273763\n",
      "count_amostra: 9953900\n",
      "iteration:  99570 , total_loss:  20.55158576965332\n",
      "count_amostra: 9956900\n",
      "iteration:  99600 , total_loss:  21.036867078145345\n",
      "count_amostra: 9959900\n",
      "iteration:  99630 , total_loss:  20.329964383443198\n",
      "count_amostra: 9962900\n",
      "iteration:  99660 , total_loss:  20.230519104003907\n",
      "count_amostra: 9965900\n",
      "iteration:  99690 , total_loss:  20.99360720316569\n",
      "count_amostra: 9968900\n",
      "iteration:  99720 , total_loss:  20.77005977630615\n",
      "count_amostra: 9971900\n",
      "iteration:  99750 , total_loss:  20.5891970316569\n",
      "count_amostra: 9974900\n",
      "iteration:  99780 , total_loss:  20.710993576049805\n",
      "count_amostra: 9977900\n",
      "iteration:  99810 , total_loss:  20.988354428609213\n",
      "count_amostra: 9980900\n",
      "iteration:  99840 , total_loss:  20.65935967763265\n",
      "count_amostra: 9983900\n",
      "iteration:  99870 , total_loss:  20.61711222330729\n",
      "count_amostra: 9986900\n",
      "iteration:  99900 , total_loss:  20.40953172047933\n",
      "count_amostra: 9989900\n",
      "iteration:  99930 , total_loss:  21.148061815897623\n",
      "count_amostra: 9992900\n",
      "iteration:  99960 , total_loss:  20.573779106140137\n",
      "count_amostra: 9995900\n",
      "iteration:  99990 , total_loss:  20.589848645528157\n",
      "count_amostra: 9998900\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "from einops import rearrange, pack, unpack\n",
    "\n",
    "# Only show the progress bar once on each machine.\n",
    "# progress_bar = tqdm(range(max_train_steps), disable=not accelerator.is_local_main_process)\n",
    "completed_steps = 0\n",
    "starting_epoch = 0\n",
    "\n",
    "# update the progress_bar if load from checkpoint\n",
    "total_loss=0\n",
    "# progress_bar.update(completed_steps)\n",
    "steps_log=30\n",
    "count_amostra=0\n",
    "num_train_epochs=1\n",
    "for epoch in range(starting_epoch, num_train_epochs):\n",
    "    if model_training=='modular':\n",
    "        train_dataloader.fill_bins()\n",
    "    model.train()\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        # with accelerator.accumulate(model):\n",
    "        results=model({'input':batch['input_ids'].cuda(),'domain':batch['domain'],'subdomain1':batch['subdomain1']}) \n",
    "\n",
    "        count_amostra+=int(len(batch['input_ids']))\n",
    "        loss = results.loss\n",
    "        # print(loss)\n",
    "        total_loss += loss.detach().float().cpu().numpy().item()\n",
    "        accelerator.backward(loss)\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()       \n",
    "        \n",
    "        if step%steps_log==0:\n",
    "            print('iteration: ',step,', total_loss: ',total_loss/steps_log)\n",
    "            print('count_amostra:',count_amostra)\n",
    "            total_loss=0\n",
    "    # Checks if the accelerator has performed an optimization step behind the scenes\n",
    "    if accelerator.sync_gradients:\n",
    "        # progress_bar.update(1)\n",
    "        completed_steps += 1\n",
    "\n",
    "    if completed_steps >= max_train_steps:\n",
    "        break\n",
    "\n",
    "    model.eval()\n",
    "    losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.discriminator,'models/'+model_path+'domgen3_3_electra_discriminator_16kvocab_10m.pt')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetuning model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Electra custom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "import torch\n",
    "import os\n",
    "from torch.utils.data import  DataLoader\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "    \n",
    "class ElectraClass(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ElectraClass, self).__init__()\n",
    "        self.l1 = model.discriminator._modules['0']\n",
    "        self.l2 = torch.nn.Dropout(0.3)\n",
    "        self.fl = torch.nn.Flatten()\n",
    "        self.l3 = torch.nn.Linear(768*(MODEL_MAX_SEQ_LEN-2), 3)\n",
    "    \n",
    "    def forward(self, ids):\n",
    "        output= self.l1(ids)\n",
    "        output = self.l2(output)\n",
    "        output = self.fl(output)\n",
    "        output = self.l3(output)\n",
    "        return output\n",
    "\n",
    "electra_model = ElectraClass()\n",
    "electra_model=electra_model.to('cuda')\n",
    "optimizer = torch.optim.Adam(params =  model.parameters(), lr=2e-5,weight_decay=0.01)\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "\n",
    "electra_model.train()\n",
    "\n",
    "def loss_fn(outputs, targets):\n",
    "    loss=torch.nn.CrossEntropyLoss()(outputs, targets)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "task='mrpc'\n",
    "dataset = load_dataset('glue', task, split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class TokenizedDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset,is_dom=False):\n",
    "        self.dataset = dataset\n",
    "        self.ls=[]\n",
    "        self.is_dom=is_dom\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        s=norm.normalize_str(self.dataset[\"sentence1\"][i])+'[SEP]'+norm.normalize_str(self.dataset[\"sentence2\"][i])\n",
    "        tokens = vocab.tokenize(s).tolist()        \n",
    "        \n",
    "        self.ls.append(i)\n",
    "        \n",
    "        #trucate\n",
    "        tokens=tokens[:MODEL_MAX_SEQ_LEN - 2]\n",
    "        l=len(tokens)\n",
    "        for j in range(l,MODEL_MAX_SEQ_LEN - 2):\n",
    "            tokens.append(0)\n",
    "        tokens=torch.Tensor(tokens).long()\n",
    "\n",
    "        label=self.dataset[\"label\"][i]\n",
    "\n",
    "        if self.is_dom:\n",
    "            p=tfidf_model.transform([self.dataset[\"sentence1\"][i]+self.dataset[\"sentence2\"][i]])\n",
    "            dom = kmeans.predict(\n",
    "                X=torch.from_numpy(p).to(device)\n",
    "            )\n",
    "            d={'input_ids':tokens,'label':label, 'domain':dom.item()}\n",
    "        else:\n",
    "            d={'input_ids':tokens,'label':label}\n",
    "        return d\n",
    "\n",
    "class HFTokenizedDataset(torch.utils.data.Dataset):\n",
    "    \"This wraps the dataset and tokenizes it, ready for the model\"\n",
    "\n",
    "    def __init__(self, dataset, tokenizer):\n",
    "        self.dataset = dataset\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        label=self.dataset[\"label\"][i]\n",
    "        inps=self.tokenizer.encode(\n",
    "            self.dataset[\"sentence1\"][i]+'[SEP]'+self.dataset[\"sentence2\"][i],\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            max_length=MODEL_MAX_SEQ_LEN - 2,\n",
    "            padding=\"max_length\",\n",
    "            return_special_tokens_mask=True,\n",
    "        )[0, ...]\n",
    "        return {'input_ids':inps,\n",
    "            'label':label}\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "if model_training=='bert':\n",
    "    tokenized_dataset = HFTokenizedDataset(dataset, tokenizer)\n",
    "else:\n",
    "    tokenized_dataset = TokenizedDataset(dataset)\n",
    "# train_dataloader=CustomDataloader(tokenized_dataset)\n",
    "train_dataloader = DataLoader(\n",
    "        tokenized_dataset, batch_size=DEVICE_BATCH_SIZE\n",
    "        # tokenized_dataset, batch_size=1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Iteration: 0, Loss:  0.01158959984779358\n",
      "Epoch: 1, Iteration: 0, Loss:  0.41333103120326997\n",
      "Epoch: 2, Iteration: 0, Loss:  0.38584744930267334\n"
     ]
    }
   ],
   "source": [
    "epochs=5\n",
    "running_loss=0\n",
    "for i in range(epochs):\n",
    "    for j,data in enumerate(train_dataloader):\n",
    "        if model_training=='modular':\n",
    "            ids = {'input':data['input_ids'].to('cuda', dtype = torch.long),'domain':0}\n",
    "            # ids = {'input':data['input_ids'].to('cuda', dtype = torch.long),'domain':data['domain']}\n",
    "        else:\n",
    "            ids = data['input_ids'].to('cuda', dtype = torch.long)\n",
    "        \n",
    "        targets = data['label'].to('cuda', dtype = torch.long)\n",
    "\n",
    "        outputs = electra_model(ids)\n",
    "        \n",
    "        loss = loss_fn(outputs,targets)\n",
    "        # loss=outputs.loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        running_loss+=loss.item()\n",
    "        if j%50==0:\n",
    "            print(f'Epoch: {i}, Iteration: {j}, Loss:  {running_loss/50}')\n",
    "            running_loss=0\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset('glue', task, split='validation')\n",
    "if model_training=='bert':\n",
    "    tokenized_dataset = HFTokenizedDataset(dataset, tokenizer)\n",
    "else:\n",
    "    # tokenized_dataset = TokenizedDataset(dataset)\n",
    "    tokenized_dataset = TokenizedDataset(dataset)\n",
    "\n",
    "eval_dataloader = DataLoader(\n",
    "        tokenized_dataset, batch_size=DEVICE_BATCH_SIZE\n",
    "        # tokenized_dataset, batch_size=1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "predictions=[]\n",
    "labels=[]\n",
    "c=0\n",
    "for _,data in enumerate(eval_dataloader):\n",
    "    if model_training=='modular':\n",
    "        ids = {'input':data['input_ids'].to('cuda', dtype = torch.long),'domain':0}\n",
    "        # ids = {'input':data['input_ids'].to('cuda', dtype = torch.long),'domain':data['domain']}\n",
    "    else:\n",
    "        ids = data['input_ids'].to('cuda', dtype = torch.long)\n",
    "    targets = data['label'].to('cuda', dtype = torch.long)\n",
    "    \n",
    "    outputs = electra_model(ids)\n",
    "    \n",
    "    outputs=torch.argmax(outputs, dim=1).cpu()\n",
    "    targets=targets.cpu()\n",
    "    \n",
    "    outputs=[int(o.item()) for o in outputs]\n",
    "    targets=[int(o.item()) for o in targets]\n",
    "\n",
    "    predictions+=outputs\n",
    "    labels+=targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.64      0.61       129\n",
      "           1       0.83      0.80      0.81       279\n",
      "\n",
      "    accuracy                           0.75       408\n",
      "   macro avg       0.71      0.72      0.71       408\n",
      "weighted avg       0.75      0.75      0.75       408\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(labels, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2 hierarquias 3_dom_3_subdom_electra_discriminator_16kvocab_10m\n",
    "\n",
    "precision    recall  f1-score   support\n",
    "\n",
    "           0       0.59      0.64      0.61       129\n",
    "           1       0.83      0.80      0.81       279\n",
    "\n",
    "    accuracy                           0.75       408\n",
    "   macro avg       0.71      0.72      0.71       408\n",
    "weighted avg       0.75      0.75      0.75       408\n",
    "#2 hierarquias 3_dom_3_subdom_electra_discriminator_16kvocab_1m\n",
    "    precision    recall  f1-score   support\n",
    "\n",
    "           0       0.50      0.42      0.46       129\n",
    "           1       0.75      0.81      0.78       279\n",
    "\n",
    "    accuracy                           0.69       408\n",
    "   macro avg       0.63      0.61      0.62       408\n",
    "weighted avg       0.67      0.69      0.68       408\n",
    "#primeira tentativa com  dominios no discriminador(1m?)\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.39      0.10      0.16       129\n",
    "           1       0.69      0.93      0.79       279\n",
    "\n",
    "    accuracy                           0.67       408\n",
    "   macro avg       0.54      0.51      0.48       408\n",
    "weighted avg       0.60      0.67      0.59       408\n",
    "## glue mrpc com 10_dom_electra_discriminator_16kvocab_10m\n",
    "                precision    recall  f1-score   support\n",
    "\n",
    "           0       0.61      0.57      0.59       129\n",
    "           1       0.81      0.83      0.82       279\n",
    "\n",
    "    accuracy                           0.75       408\n",
    "   macro avg       0.71      0.70      0.71       408\n",
    "weighted avg       0.75      0.75      0.75       408\n",
    "\n",
    "## glue mrpc com 3_dom_electra_discriminator_16kvocab_10m\n",
    "         precision    recall  f1-score   support\n",
    "\n",
    "           0       0.56      0.54      0.55       129\n",
    "           1       0.79      0.81      0.80       279\n",
    "\n",
    "    accuracy                           0.72       408\n",
    "   macro avg       0.68      0.67      0.68       408\n",
    "weighted avg       0.72      0.72      0.72       408\n",
    "\n",
    "## glue mrpc com electra_discriminator_16kvocab_10m\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.49      0.47      0.48       129\n",
    "           1       0.76      0.78      0.77       279\n",
    "\n",
    "    accuracy                           0.68       408\n",
    "   macro avg       0.63      0.62      0.62       408\n",
    "weighted avg       0.67      0.68      0.68       408\n",
    "\n",
    "## glue mrpc com electra com pretraining de 1m:\n",
    "            precision    recall  f1-score   support\n",
    "\n",
    "           0       0.47      0.48      0.47       129\n",
    "           1       0.76      0.75      0.75       279\n",
    "\n",
    "    accuracy                           0.66       408\n",
    "   macro avg       0.61      0.61      0.61       408\n",
    "weighted avg       0.66      0.66      0.66       408\n",
    "\n",
    "\n",
    "## glue mrpc com electra sem pretraining:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.50      0.23      0.32       129\n",
    "           1       0.72      0.89      0.79       279\n",
    "\n",
    "    accuracy                           0.68       408\n",
    "   macro avg       0.61      0.56      0.56       408\n",
    "weighted avg       0.65      0.68      0.64       408"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bert custom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import torch\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "class BERTClass(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BERTClass, self).__init__()\n",
    "        self.l1 = transformers.BertModel.from_pretrained('bert-base-cased')\n",
    "        self.l2 = torch.nn.Dropout(0.3)\n",
    "        self.l3 = torch.nn.Linear(768, 3)\n",
    "    \n",
    "    def forward(self, ids, mask, token_type_ids):\n",
    "        outs, output_1= self.l1(ids, attention_mask = mask, token_type_ids = token_type_ids, return_dict=False)\n",
    "        output_2 = self.l2(output_1)\n",
    "        output = self.l3(output_2)\n",
    "        return output\n",
    "    \n",
    "\n",
    "model = BERTClass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "import transformers\n",
    "import torch\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained('bert-base-cased', num_labels=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset glue (/home/kiki/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import  DataLoader\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "task='mrpc'\n",
    "dataset = load_dataset('glue', task, split='train')\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-cased', use_fast=True)\n",
    "# dataset = dataset.map(lambda e: tokenizer(e['sentence1'], truncation=True, padding='max_length'), batched=True)\n",
    "# dataset.set_format(type='torch', columns=['input_ids', 'token_type_ids', 'attention_mask', 'label']) #BERT\n",
    "# dataloader = torch.utils.data.DataLoader(dataset, batch_size=12)\n",
    "\n",
    "model=model.to('cuda')\n",
    "optimizer = torch.optim.Adam(params =  model.parameters(), lr=2e-5,weight_decay=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89502a00dc1f4a599875cfcf74516990",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3668 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def preprocess_function(examples):\n",
    "    if sentence2_key is None:\n",
    "        return tokenizer(examples[sentence1_key], truncation=True, padding='max_length')\n",
    "    return tokenizer(examples[sentence1_key], examples[sentence2_key], truncation=True, padding='max_length')\n",
    "sentence1_key,sentence2_key=(\"sentence1\", \"sentence2\")\n",
    "encoded_dataset = dataset.map(preprocess_function, batched=True)\n",
    "encoded_dataset.set_format(type='torch', columns=['input_ids', 'token_type_ids', 'attention_mask', 'label'])\n",
    "dataloader = torch.utils.data.DataLoader(encoded_dataset, batch_size=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "def loss_fn(outputs, targets):\n",
    "    # if task != \"stsb\":\n",
    "    #     outputs = torch.argmax(outputs, dim=1)\n",
    "    # else:\n",
    "    #     outputs = outputs[:, 0]\n",
    "    # loss=torch.nn.BCEWithLogitsLoss()(outputs,targets)\n",
    "    # loss=torch.nn.CrossEntropyLoss()(outputs,targets)\n",
    "    loss=torch.nn.CrossEntropyLoss()(outputs.view(-1,3), targets.view(-1))\n",
    "    # loss = Variable(loss, requires_grad = True)\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss:  0.007852643132209777\n",
      "Epoch: 0, Loss:  0.20191293969750404\n",
      "Epoch: 0, Loss:  0.16229299038648606\n",
      "Epoch: 0, Loss:  0.14223801746964454\n",
      "Epoch: 0, Loss:  0.1337541800737381\n",
      "Epoch: 0, Loss:  0.11794026836752891\n",
      "Epoch: 0, Loss:  0.10376800172030926\n",
      "Epoch: 1, Loss:  0.0131098260730505\n",
      "Epoch: 1, Loss:  0.10432511921972036\n",
      "Epoch: 1, Loss:  0.10928954795002938\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">23</span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">20 │   │   │   </span>running_loss=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">0</span>                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">21 │   │   </span>                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">22 │   │   </span>optimizer.zero_grad()                                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>23 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>loss.backward()                                                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">24 │   │   </span>optimizer.step()                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">25 </span>                                                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/kiki/miniconda3/envs/llama/lib/python3.9/site-packages/torch/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">_tensor.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">487</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">backward</span>    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 484 │   │   │   │   </span>create_graph=create_graph,                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 485 │   │   │   │   </span>inputs=inputs,                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 486 │   │   │   </span>)                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 487 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>torch.autograd.backward(                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 488 │   │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>, gradient, retain_graph, create_graph, inputs=inputs                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 489 │   │   </span>)                                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 490 </span>                                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/kiki/miniconda3/envs/llama/lib/python3.9/site-packages/torch/autograd/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">__init__.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">200</span> in   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">backward</span>                                                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">197 │   # The reason we repeat same the comment below is that</span>                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">198 │   # some Python versions print out the first line of a multi-line function</span>               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">199 │   # calls in the traceback and some print out the last line</span>                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>200 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>Variable._execution_engine.run_backward(  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># Calls into the C++ engine to run the bac</span>   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">201 │   │   </span>tensors, grad_tensors_, retain_graph, create_graph, inputs,                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">202 │   │   </span>allow_unreachable=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">True</span>, accumulate_grad=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">True</span>)  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># Calls into the C++ engine to ru</span>   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">203 </span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">KeyboardInterrupt</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92m<module>\u001b[0m:\u001b[94m23\u001b[0m                                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m20 \u001b[0m\u001b[2m│   │   │   \u001b[0mrunning_loss=\u001b[94m0\u001b[0m                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m21 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m22 \u001b[0m\u001b[2m│   │   \u001b[0moptimizer.zero_grad()                                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m23 \u001b[2m│   │   \u001b[0mloss.backward()                                                                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m24 \u001b[0m\u001b[2m│   │   \u001b[0moptimizer.step()                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m25 \u001b[0m                                                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/kiki/miniconda3/envs/llama/lib/python3.9/site-packages/torch/\u001b[0m\u001b[1;33m_tensor.py\u001b[0m:\u001b[94m487\u001b[0m in \u001b[92mbackward\u001b[0m    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 484 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mcreate_graph=create_graph,                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 485 \u001b[0m\u001b[2m│   │   │   │   \u001b[0minputs=inputs,                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 486 \u001b[0m\u001b[2m│   │   │   \u001b[0m)                                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 487 \u001b[2m│   │   \u001b[0mtorch.autograd.backward(                                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 488 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[96mself\u001b[0m, gradient, retain_graph, create_graph, inputs=inputs                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 489 \u001b[0m\u001b[2m│   │   \u001b[0m)                                                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 490 \u001b[0m                                                                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/kiki/miniconda3/envs/llama/lib/python3.9/site-packages/torch/autograd/\u001b[0m\u001b[1;33m__init__.py\u001b[0m:\u001b[94m200\u001b[0m in   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[92mbackward\u001b[0m                                                                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m197 \u001b[0m\u001b[2m│   \u001b[0m\u001b[2m# The reason we repeat same the comment below is that\u001b[0m                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m198 \u001b[0m\u001b[2m│   \u001b[0m\u001b[2m# some Python versions print out the first line of a multi-line function\u001b[0m               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m199 \u001b[0m\u001b[2m│   \u001b[0m\u001b[2m# calls in the traceback and some print out the last line\u001b[0m                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m200 \u001b[2m│   \u001b[0mVariable._execution_engine.run_backward(  \u001b[2m# Calls into the C++ engine to run the bac\u001b[0m   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m201 \u001b[0m\u001b[2m│   │   \u001b[0mtensors, grad_tensors_, retain_graph, create_graph, inputs,                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m202 \u001b[0m\u001b[2m│   │   \u001b[0mallow_unreachable=\u001b[94mTrue\u001b[0m, accumulate_grad=\u001b[94mTrue\u001b[0m)  \u001b[2m# Calls into the C++ engine to ru\u001b[0m   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m203 \u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mKeyboardInterrupt\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.train()\n",
    "epochs=5\n",
    "running_loss=0\n",
    "for i in range(epochs):\n",
    "    for _,data in enumerate(dataloader):\n",
    "        ids = data['input_ids'].to('cuda', dtype = torch.long)\n",
    "        mask = data['attention_mask'].to('cuda', dtype = torch.long)\n",
    "        token_type_ids = data['token_type_ids'].to('cuda', dtype = torch.long)\n",
    "        targets = data['label'].to('cuda', dtype = torch.long)\n",
    "\n",
    "        # outputs = model(ids, mask, token_type_ids)\n",
    "        outputs = model(ids, mask, token_type_ids)\n",
    "        loss = loss_fn(outputs.logits,targets)\n",
    "        # loss=outputs.loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        running_loss+=loss.item()\n",
    "        if _%50==0:\n",
    "            print(f'Epoch: {i}, Loss:  {running_loss/200}')\n",
    "            running_loss=0\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset glue (/home/kiki/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b703556b9b84f65b802c9d7269defea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/408 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset('glue', task, split='validation')\n",
    "# dataset = dataset.map(lambda e: tokenizer(e['sentence1'], truncation=True, padding='max_length'), batched=True)\n",
    "# dataset.set_format(type='torch', columns=['input_ids', 'token_type_ids', 'attention_mask', 'label'])\n",
    "# dataloader = torch.utils.data.DataLoader(dataset, batch_size=4)\n",
    "encoded_dataset = dataset.map(preprocess_function, batched=True)\n",
    "encoded_dataset.set_format(type='torch', columns=['input_ids', 'token_type_ids', 'attention_mask', 'label'])\n",
    "dataloader = torch.utils.data.DataLoader(encoded_dataset, batch_size=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "predictions=[]\n",
    "labels=[]\n",
    "c=0\n",
    "for _,data in enumerate(dataloader):\n",
    "    ids = data['input_ids'].to('cuda', dtype = torch.long)\n",
    "    mask = data['attention_mask'].to('cuda', dtype = torch.long)\n",
    "    token_type_ids = data['token_type_ids'].to('cuda', dtype = torch.long)\n",
    "    targets = data['label'].to('cuda', dtype = torch.long)\n",
    "\n",
    "    outputs = model(ids, mask, token_type_ids)\n",
    "    outputs = outputs.logits\n",
    "    \n",
    "    outputs=torch.argmax(outputs, dim=1).cpu()\n",
    "    targets=targets.cpu()\n",
    "    \n",
    "    outputs=[int(o.item()) for o in outputs]\n",
    "    targets=[int(o.item()) for o in targets]\n",
    "\n",
    "    predictions+=outputs\n",
    "    labels+=targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import load_metric\n",
    "# metric = load_metric('glue', 'mrpc')\n",
    "# final_score = metric.compute(predictions=predictions, references =labels )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.84      0.73       129\n",
      "           1       0.92      0.78      0.84       279\n",
      "\n",
      "    accuracy                           0.80       408\n",
      "   macro avg       0.78      0.81      0.79       408\n",
      "weighted avg       0.83      0.80      0.81       408\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(labels, predictions))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.30.0.dev0\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "GLUE_TASKS = [\"cola\", \"mnli\", \"mnli-mm\", \"mrpc\", \"qnli\", \"qqp\", \"rte\", \"sst2\", \"stsb\", \"wnli\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = \"mrpc\"\n",
    "model_checkpoint = \"bert-base-cased\"\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, load_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset glue (/home/kiki/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4aae4f248054178b9cbc635d19a2159",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_244174/1389288479.py:3: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = load_metric('glue', actual_task)\n"
     ]
    }
   ],
   "source": [
    "actual_task = \"mnli\" if task == \"mnli-mm\" else task\n",
    "dataset = load_dataset(\"glue\", actual_task)\n",
    "metric = load_metric('glue', actual_task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "    \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_to_keys = {\n",
    "    \"cola\": (\"sentence\", None),\n",
    "    \"mnli\": (\"premise\", \"hypothesis\"),\n",
    "    \"mnli-mm\": (\"premise\", \"hypothesis\"),\n",
    "    \"mrpc\": (\"sentence1\", \"sentence2\"),\n",
    "    \"qnli\": (\"question\", \"sentence\"),\n",
    "    \"qqp\": (\"question1\", \"question2\"),\n",
    "    \"rte\": (\"sentence1\", \"sentence2\"),\n",
    "    \"sst2\": (\"sentence\", None),\n",
    "    \"stsb\": (\"sentence1\", \"sentence2\"),\n",
    "    \"wnli\": (\"sentence1\", \"sentence2\"),\n",
    "}\n",
    "sentence1_key, sentence2_key = task_to_keys[task]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    if sentence2_key is None:\n",
    "        return tokenizer(examples[sentence1_key], truncation=True)\n",
    "    return tokenizer(examples[sentence1_key], examples[sentence2_key], truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "671e3a45668946a08773dcaea4506d5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3668 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0eb643aa13c4bb784b7d6f68f0e9ee1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/408 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e64158ab1568461a9d131c7d5f675c9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1725 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "encoded_dataset = dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "\n",
    "num_labels = 3 if task.startswith(\"mnli\") else 1 if task==\"stsb\" else 2\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_name = \"pearson\" if task == \"stsb\" else \"matthews_correlation\" if task == \"cola\" else \"accuracy\"\n",
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "\n",
    "args = TrainingArguments(\n",
    "    f\"{model_name}-finetuned-{task}\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    save_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=metric_name,\n",
    "    push_to_hub=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    if task != \"stsb\":\n",
    "        predictions = np.argmax(predictions, axis=1)\n",
    "    else:\n",
    "        predictions = predictions[:, 0]\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning https://huggingface.co/abcp4/bert-base-cased-finetuned-mrpc into local empty directory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "validation_key = \"validation_mismatched\" if task == \"mnli-mm\" else \"validation_matched\" if task == \"mnli\" else \"validation\"\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=encoded_dataset[\"train\"],\n",
    "    eval_dataset=encoded_dataset[validation_key],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57218c19fff94363bfe1478f7d6e2edc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.5459453463554382,\n",
       " 'eval_accuracy': 0.8578431372549019,\n",
       " 'eval_f1': 0.8993055555555555,\n",
       " 'eval_runtime': 0.4866,\n",
       " 'eval_samples_per_second': 838.425,\n",
       " 'eval_steps_per_second': 53.429,\n",
       " 'epoch': 5.0}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3470f0fd3b3a4641bde2110d54183b25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/230 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "preds=trainer.predict(encoded_dataset[\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset glue (/home/kiki/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b1438990ed844a992c43616097c4845",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/408 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset('glue', task, split='validation')\n",
    "dataset = dataset.map(lambda e: tokenizer(e['sentence1'], truncation=True, padding='max_length'), batched=True)\n",
    "dataset.set_format(type='torch', columns=['input_ids', 'token_type_ids', 'attention_mask', 'label'])\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "predictions=[]\n",
    "labels=[]\n",
    "c=0\n",
    "for _,data in enumerate(dataloader):\n",
    "    ids = data['input_ids'].to('cuda', dtype = torch.long)\n",
    "    mask = data['attention_mask'].to('cuda', dtype = torch.long)\n",
    "    token_type_ids = data['token_type_ids'].to('cuda', dtype = torch.long)\n",
    "    targets = data['label'].to('cuda', dtype = torch.long)\n",
    "\n",
    "    outputs = model(ids, mask, token_type_ids)\n",
    "    outputs = outputs.logits\n",
    "    \n",
    "    outputs=torch.argmax(outputs, dim=1).cpu()\n",
    "    targets=targets.cpu()\n",
    "    \n",
    "    outputs=[int(o.item()) for o in outputs]\n",
    "    targets=[int(o.item()) for o in targets]\n",
    "\n",
    "    predictions+=outputs\n",
    "    labels+=targets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 1, 1, 0])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions=np.argmax(preds.predictions, axis=1)\n",
    "labels=encoded_dataset[\"train\"]['label']\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.98      0.98      1194\n",
      "           1       0.99      1.00      0.99      2474\n",
      "\n",
      "    accuracy                           0.99      3668\n",
      "   macro avg       0.99      0.99      0.99      3668\n",
      "weighted avg       0.99      0.99      0.99      3668\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# from datasets import load_metric\n",
    "# metric = load_metric('glue', 'mrpc')\n",
    "# final_score = metric.compute(predictions=predictions, references =labels )\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(labels, predictions))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Torch model to HuggingFaceTransformers(optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Torch model to HuggingFaceTransformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from transformers import PreTrainedModel, PretrainedConfig\n",
    "from transformers import AutoModel, AutoConfig\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from typing import List, Optional, Tuple, Union\n",
    "from transformers.modeling_outputs import (\n",
    "    SequenceClassifierOutput,\n",
    ")\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyConfig(PretrainedConfig):\n",
    "    model_type = 'mymodel'\n",
    "    def __init__(self, important_param=42, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.important_param = important_param\n",
    "\n",
    "class MyModel(PreTrainedModel):\n",
    "    config_class = MyConfig\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.config = config\n",
    "        self.model = nn.Sequential(\n",
    "                          nn.Linear(3, self.config.important_param),\n",
    "                          nn.Sigmoid(),\n",
    "                          nn.Linear(self.config.important_param, 1),\n",
    "                          nn.Sigmoid()\n",
    "                          )\n",
    "    def forward(self, input):\n",
    "        return self.model(input) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = MyConfig(4)\n",
    "model = MyModel(config)\n",
    "model.save_pretrained('./models/electra')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MyModel(\n",
       "  (model): Sequential(\n",
       "    (0): Linear(in_features=3, out_features=4, bias=True)\n",
       "    (1): Sigmoid()\n",
       "    (2): Linear(in_features=4, out_features=1, bias=True)\n",
       "    (3): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "AutoConfig.register(\"mymodel\", MyConfig)\n",
    "AutoModel.register(MyConfig, MyModel)\n",
    "\n",
    "# new_model = MyModel.from_pretrained('./models/electra')\n",
    "new_model = AutoModel.from_pretrained('./models/electra')\n",
    "new_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_model.push_to_hub(\"mymodel-test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "\n",
    "class MySeqConfig(PretrainedConfig):\n",
    "    model_type = 'myseqmodel'\n",
    "    def __init__(self, important_param=42, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.important_param = important_param\n",
    "\n",
    "class MySequenceModel(PreTrainedModel):\n",
    "    config_class = MySeqConfig\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.config = config\n",
    "        self.model = nn.Sequential(\n",
    "                          nn.Linear(3, self.config.important_param),\n",
    "                          nn.Sigmoid(),\n",
    "                          nn.Linear(self.config.important_param, 1),\n",
    "                          nn.Sigmoid()\n",
    "                          )\n",
    "    def forward(self, input_ids: Optional[torch.Tensor] = None,\n",
    "                attention_mask: Optional[torch.Tensor] = None,\n",
    "                token_type_ids: Optional[torch.Tensor] = None,\n",
    "                position_ids: Optional[torch.Tensor] = None,\n",
    "                head_mask: Optional[torch.Tensor] = None,\n",
    "                inputs_embeds: Optional[torch.Tensor] = None,\n",
    "                labels: Optional[torch.Tensor] = None,\n",
    "                output_attentions: Optional[bool] = None,\n",
    "                output_hidden_states: Optional[bool] = None,\n",
    "                return_dict: Optional[bool] = None,)-> Union[Tuple[torch.Tensor], SequenceClassifierOutput]:\n",
    "        \n",
    "        print(input_ids.shape)\n",
    "        loss = Variable(torch.zeros(1).to('cuda'), requires_grad=True)\n",
    "\n",
    "        \n",
    "        return SequenceClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=torch.zeros((3)).to('cuda'),\n",
    "            hidden_states=torch.zeros(128).to('cuda'),\n",
    "            attentions=torch.zeros(128).to('cuda'),\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = MySeqConfig(4)\n",
    "model = MySequenceModel(config)\n",
    "model.save_pretrained('./models/electra')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AutoModelForSequenceClassification.register(MySeqConfig, MySequenceModel)\n",
    "AutoConfig.register(\"myseqmodel\", MySeqConfig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=AutoModelForSequenceClassification.from_pretrained('./models/electra')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "06/24/2023 21:48:38 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: False\n",
      "06/24/2023 21:48:38 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_backend=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=True,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=no,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=2e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=output_evals/runs/Jun24_21-48-38_kiki-System-Product-Name,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=500,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_type=linear,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=5.0,\n",
      "optim=adamw_hf,\n",
      "optim_args=None,\n",
      "output_dir=output_evals,\n",
      "overwrite_output_dir=False,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=8,\n",
      "per_device_train_batch_size=32,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=['tensorboard', 'wandb'],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=output_evals,\n",
      "save_on_each_node=False,\n",
      "save_safetensors=False,\n",
      "save_steps=500,\n",
      "save_strategy=no,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      "xpu_backend=None,\n",
      ")\n",
      "06/24/2023 21:48:39 - INFO - datasets.info - Loading Dataset Infos from /home/kiki/.cache/huggingface/modules/datasets_modules/datasets/glue/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad\n",
      "06/24/2023 21:48:39 - INFO - datasets.builder - Overwrite dataset info from restored data version.\n",
      "06/24/2023 21:48:39 - INFO - datasets.info - Loading Dataset info from /home/kiki/.cache/huggingface/datasets/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad\n",
      "06/24/2023 21:48:39 - WARNING - datasets.builder - Found cached dataset glue (/home/kiki/.cache/huggingface/datasets/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n",
      "06/24/2023 21:48:39 - INFO - datasets.info - Loading Dataset info from /home/kiki/.cache/huggingface/datasets/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad\n",
      "100%|███████████████████████████████████████████| 3/3 [00:00<00:00, 2040.36it/s]\n",
      "[INFO|configuration_utils.py:669] 2023-06-24 21:48:40,194 >> loading configuration file config.json from cache at /home/kiki/.cache/huggingface/hub/models--bert-base-cased/snapshots/5532cc56f74641d4bb33641f5c76a55d11f846e0/config.json\n",
      "[INFO|configuration_utils.py:725] 2023-06-24 21:48:40,200 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"finetuning_task\": \"cola\",\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.30.0.dev0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "[INFO|configuration_utils.py:669] 2023-06-24 21:48:40,399 >> loading configuration file config.json from cache at /home/kiki/.cache/huggingface/hub/models--bert-base-cased/snapshots/5532cc56f74641d4bb33641f5c76a55d11f846e0/config.json\n",
      "[INFO|configuration_utils.py:725] 2023-06-24 21:48:40,400 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.30.0.dev0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1810] 2023-06-24 21:48:40,401 >> loading file vocab.txt from cache at /home/kiki/.cache/huggingface/hub/models--bert-base-cased/snapshots/5532cc56f74641d4bb33641f5c76a55d11f846e0/vocab.txt\n",
      "[INFO|tokenization_utils_base.py:1810] 2023-06-24 21:48:40,401 >> loading file tokenizer.json from cache at /home/kiki/.cache/huggingface/hub/models--bert-base-cased/snapshots/5532cc56f74641d4bb33641f5c76a55d11f846e0/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:1810] 2023-06-24 21:48:40,401 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1810] 2023-06-24 21:48:40,401 >> loading file special_tokens_map.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1810] 2023-06-24 21:48:40,401 >> loading file tokenizer_config.json from cache at /home/kiki/.cache/huggingface/hub/models--bert-base-cased/snapshots/5532cc56f74641d4bb33641f5c76a55d11f846e0/tokenizer_config.json\n",
      "[INFO|configuration_utils.py:669] 2023-06-24 21:48:40,401 >> loading configuration file config.json from cache at /home/kiki/.cache/huggingface/hub/models--bert-base-cased/snapshots/5532cc56f74641d4bb33641f5c76a55d11f846e0/config.json\n",
      "[INFO|configuration_utils.py:725] 2023-06-24 21:48:40,402 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.30.0.dev0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:2535] 2023-06-24 21:48:40,435 >> loading weights file model.safetensors from cache at /home/kiki/.cache/huggingface/hub/models--bert-base-cased/snapshots/5532cc56f74641d4bb33641f5c76a55d11f846e0/model.safetensors\n",
      "[WARNING|modeling_utils.py:3229] 2023-06-24 21:48:40,813 >> Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "[WARNING|modeling_utils.py:3241] 2023-06-24 21:48:40,813 >> Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "06/24/2023 21:48:40 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/kiki/.cache/huggingface/datasets/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-76c8170a2a261ff8.arrow\n",
      "Running tokenizer on dataset:   0%|             | 0/1043 [00:00<?, ? examples/s]06/24/2023 21:48:40 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/kiki/.cache/huggingface/datasets/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-a2bf5301681fb373.arrow\n",
      "06/24/2023 21:48:40 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/kiki/.cache/huggingface/datasets/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-43a8c8283e9dbbef.arrow\n",
      "06/24/2023 21:48:40 - INFO - __main__ - Sample 1824 of the training set: {'sentence': 'I acknowledged that my father, he was tight as an owl.', 'label': 0, 'idx': 1824, 'input_ids': [101, 146, 8646, 1115, 1139, 1401, 117, 1119, 1108, 3600, 1112, 1126, 19976, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
      "06/24/2023 21:48:40 - INFO - __main__ - Sample 409 of the training set: {'sentence': 'For him to do that would be a mistake.', 'label': 1, 'idx': 409, 'input_ids': [101, 1370, 1140, 1106, 1202, 1115, 1156, 1129, 170, 6223, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
      "06/24/2023 21:48:40 - INFO - __main__ - Sample 4506 of the training set: {'sentence': 'Mary sang a song, but Lee never did.', 'label': 1, 'idx': 4506, 'input_ids': [101, 2090, 6407, 170, 1461, 117, 1133, 2499, 1309, 1225, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
      "[INFO|trainer.py:763] 2023-06-24 21:48:42,393 >> The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/home/kiki/miniconda3/envs/llama/lib/python3.9/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "[INFO|trainer.py:1812] 2023-06-24 21:48:42,397 >> ***** Running training *****\n",
      "[INFO|trainer.py:1813] 2023-06-24 21:48:42,397 >>   Num examples = 8,551\n",
      "[INFO|trainer.py:1814] 2023-06-24 21:48:42,397 >>   Num Epochs = 5\n",
      "[INFO|trainer.py:1815] 2023-06-24 21:48:42,397 >>   Instantaneous batch size per device = 32\n",
      "[INFO|trainer.py:1816] 2023-06-24 21:48:42,397 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "[INFO|trainer.py:1817] 2023-06-24 21:48:42,397 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1818] 2023-06-24 21:48:42,397 >>   Total optimization steps = 1,340\n",
      "[INFO|trainer.py:1819] 2023-06-24 21:48:42,398 >>   Number of trainable parameters = 108,311,810\n",
      "[INFO|integrations.py:720] 2023-06-24 21:48:42,402 >> Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mabcp4\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.4 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/mnt/e026e6cb-2abe-4ed4-bf51-0381c5a02c4b/Servidor/LM_Pretraining/Fast_Transformers/wandb/run-20230624_214843-jlxgfmk4\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mefficient-butterfly-34\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/abcp4/huggingface\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/abcp4/huggingface/runs/jlxgfmk4\u001b[0m\n",
      "{'loss': 0.4082, 'learning_rate': 1.2537313432835823e-05, 'epoch': 1.87}        \n",
      "{'loss': 0.1739, 'learning_rate': 5.074626865671642e-06, 'epoch': 3.73}         \n",
      "100%|██████████████████████████████████████▉| 1339/1340 [03:23<00:00,  6.52it/s][INFO|trainer.py:2085] 2023-06-24 21:52:09,773 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 207.3794, 'train_samples_per_second': 206.168, 'train_steps_per_second': 6.462, 'train_loss': 0.24341670933054455, 'epoch': 5.0}\n",
      "100%|███████████████████████████████████████| 1340/1340 [03:23<00:00,  6.59it/s]\n",
      "***** train metrics *****\n",
      "  epoch                    =        5.0\n",
      "  train_loss               =     0.2434\n",
      "  train_runtime            = 0:03:27.37\n",
      "  train_samples            =       8551\n",
      "  train_samples_per_second =    206.168\n",
      "  train_steps_per_second   =      6.462\n",
      "06/24/2023 21:52:09 - INFO - __main__ - *** Evaluate ***\n",
      "[INFO|trainer.py:763] 2023-06-24 21:52:09,779 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:3217] 2023-06-24 21:52:09,780 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:3219] 2023-06-24 21:52:09,780 >>   Num examples = 1043\n",
      "[INFO|trainer.py:3222] 2023-06-24 21:52:09,780 >>   Batch size = 8\n",
      "100%|█████████████████████████████████████████| 131/131 [00:01<00:00, 66.00it/s]\n",
      "***** eval metrics *****\n",
      "  epoch                     =        5.0\n",
      "  eval_loss                 =     0.6722\n",
      "  eval_matthews_correlation =     0.6082\n",
      "  eval_runtime              = 0:00:01.99\n",
      "  eval_samples              =       1043\n",
      "  eval_samples_per_second   =     522.76\n",
      "  eval_steps_per_second     =     65.658\n",
      "06/24/2023 21:52:11 - INFO - __main__ - *** Predict ***\n",
      "[INFO|trainer.py:763] 2023-06-24 21:52:11,781 >> The following columns in the test set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:3217] 2023-06-24 21:52:11,782 >> ***** Running Prediction *****\n",
      "[INFO|trainer.py:3219] 2023-06-24 21:52:11,782 >>   Num examples = 1063\n",
      "[INFO|trainer.py:3222] 2023-06-24 21:52:11,782 >>   Batch size = 8\n",
      "100%|█████████████████████████████████████████| 133/133 [00:02<00:00, 66.26it/s]\n",
      "06/24/2023 21:52:13 - INFO - __main__ - ***** Predict results cola *****\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                      eval/loss ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      eval/matthews_correlation ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   eval/runtime ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        eval/samples_per_second ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/steps_per_second ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    train/epoch ▁▅██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              train/global_step ▁▅██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/learning_rate █▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                     train/loss █▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               train/total_flos ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               train/train_loss ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/train_runtime ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train/train_samples_per_second ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   train/train_steps_per_second ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                      eval/loss 0.6722\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      eval/matthews_correlation 0.60817\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   eval/runtime 1.9952\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        eval/samples_per_second 522.76\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/steps_per_second 65.658\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    train/epoch 5.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              train/global_step 1340\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/learning_rate 1e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                     train/loss 0.1739\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               train/total_flos 2724443033823600.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               train/train_loss 0.24342\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/train_runtime 207.3794\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train/train_samples_per_second 206.168\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   train/train_steps_per_second 6.462\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mefficient-butterfly-34\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/abcp4/huggingface/runs/jlxgfmk4\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230624_214843-jlxgfmk4/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Glue\n",
    "!rm -r output_evals/runs\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "!python glue_eval.py --task_name cola --model_name_or_path bert-base-cased --tokenizer_name bert-base-cased --output_dir output_evals --do_train  --do_eval  --do_predict --max_seq_length 124 --per_device_train_batch_size 32 --learning_rate 2e-5 --lr_scheduler_type linear --num_train_epochs 5  --save_strategy no --seed 42 "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
