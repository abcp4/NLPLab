{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(3090) Para 128 tokens(Bert) e 32000 vocab: 1000 = 6 seg, 10000 = 1min, 100000 = 10min, 1m = 100min, 10m = 16h, 100m = 160h ou 6.6 dias\n",
    "#(3090) Para 84 tokens(tokenmonster salva 35%) e 24000 vocab(Bert): 1000 = 4.3s, 10000 = 43s, 100000 = 7.1m, 1m = 71min, 10m = 11.8h, 100m = 118h ou 5 dias\n",
    "#(4090) Para 84 tokens(tokenmonster salva 35%) e 24000 vocab(Electra): 1000 = 3.5s, 10000 = 35s, 100000 = 5.9m, 1m = 59min, 10m = 9.8h, 100m = 98h ou 4 dias\n",
    "#(4090) Para 84 tokens(tokenmonster salva 35%) e 16000 vocab (Electra): 1000 = 2.1s, 10000 = 21s, 100000 = 3.53m, 1m = 35.3min, 10m = 5.8h, 100m = 58h ou 2.4 dias\n",
    "\n",
    "#HIPOTÉTICO:\n",
    "#(4090) Para 24 tokens(tokenmonster salva 35%)e 24000 vocab(Electra): 1000 = , 10000 = , 100000 = 3.19m, 1m = 32min, 10m = 6.5h, 100m = 65h ou 2.7 dias\n",
    "\n",
    "#losses:\n",
    "#1m dataset, 24000 vocab electra, tokenmonster: iteration:  7800 , total_loss:  26.277032788594564\n",
    "#100k dataset, 16000 vocab electra, tokenmonster: iteration:  4980 , total_loss:  24.833836555480957\n",
    "#10m dataset, 16vocab electra , tokenmonster, 10 dom, iteration:  99990 , total_loss:  20.500934664408366\n",
    "\n",
    "\n",
    "#1m dataset, dom_electra_discriminator_16kvocab_1m  = 40min . Loss: 24.15\n",
    "#1m dataset, 3_3hierdom_electra_discriminator_16kvocab_1m  = 41min . Loss: 24.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.1+cu117\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "print(torch.__version__)\n",
    "\n",
    "import os\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "# !pip install datasets transformers tqdm magic_timer pandas tokenizers matplotlib pynvml\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path='dom9_lv1/'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: WANDB_START_METHOD=thread\n",
      "env: WANDB_PROJECT=pretraining_BERT_the_notebook\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/miu/miniconda3/envs/torch/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE_BATCH_SIZE = 20\n",
      "gradient_accumulation_steps = 102\n",
      "batch_size = 2040\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Resolving data files: 100%|██████████| 53/53 [00:00<00:00, 140.84it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%env WANDB_START_METHOD=thread\n",
    "%env WANDB_PROJECT=pretraining_BERT_the_notebook\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Iterator\n",
    "import time\n",
    "\n",
    "import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# import pynvml\n",
    "import torch\n",
    "# from magic_timer import MagicTimer\n",
    "from tokenizers import BertWordPieceTokenizer, Regex, normalizers\n",
    "from tqdm import tqdm\n",
    "from transformers import (\n",
    "    BertConfig,\n",
    "    BertForMaskedLM,\n",
    "    BertTokenizerFast,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "\n",
    "# Print hardware information\n",
    "# pynvml.nvmlInit()\n",
    "# handle = pynvml.nvmlDeviceGetHandleByIndex(0)\n",
    "# gpu_name = pynvml.nvmlDeviceGetName(handle)\n",
    "# gpu_mem = pynvml.nvmlDeviceGetMemoryInfo(handle).total / (1024**2)\n",
    "# print(f\"GPU: {gpu_name}, {gpu_mem} MiB\")\n",
    "# print(f\"{torch.cuda.is_available() = }\")\n",
    "model_training='modular'#'bert' , 'electra',; modular\n",
    "# LIMIT_DATASET = 2016 * 4  # keep small for development, set to None for full dataset\n",
    "\n",
    "\n",
    "LIMIT_DATASET = 10_000\n",
    "RANDOM_SEED = 42\n",
    "NUM_TOKENIZER_TRAINING_ITEMS = 1_000_000  # I made this up, but it seems reasonable\n",
    "if model_training=='bert':\n",
    "    VOCAB_SIZE = 32_768  # from Cramming\n",
    "    DEVICE_BATCH_SIZE = 100 # aprox 128, adjust to get near 100% gpu memory use\n",
    "    MODEL_MAX_SEQ_LEN = 128  # from Cramming\n",
    "else:\n",
    "    VOCAB_SIZE = 16_000  # tokenmonster\n",
    "    DEVICE_BATCH_SIZE = 20 # Token monster aguenta um batch size de (200-248)!! Geralmente melhora a qualidade do treino\n",
    "    MODEL_MAX_SEQ_LEN = 256  # token_monster\n",
    "\n",
    "MASK_ID=4\n",
    "PAD_ID=0\n",
    "\n",
    "gradient_accumulation_steps = 2048 // DEVICE_BATCH_SIZE  # roughly based on Cramming\n",
    "batch_size = DEVICE_BATCH_SIZE * gradient_accumulation_steps\n",
    "print(f\"{DEVICE_BATCH_SIZE = }\")\n",
    "print(f\"{gradient_accumulation_steps = }\")\n",
    "print(f\"{batch_size = }\")\n",
    "RUN_DIR = Path(\"data\") / f\"run_{time.strftime('%Y%m%d-%H%M%S')}\"\n",
    "CHECKPOINT_DIR = RUN_DIR / \"training_checkpoints\"\n",
    "MODEL_DIR = RUN_DIR / \"model\"\n",
    "TOKENIZER_PATH = RUN_DIR / \"tokenizer.json\"\n",
    "TRAINER_HISTORY_PATH = RUN_DIR / \"trainer_history.json\"\n",
    "\n",
    "RUN_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "dataset = datasets.load_dataset(\n",
    "    \"sradc/chunked-shuffled-wikipedia20220301en-bookcorpusopen\",\n",
    "    split=f\"train[:{LIMIT_DATASET}]\" if LIMIT_DATASET else \"train\",\n",
    "    revision=\"0e6fada2dd43136e4a3f637da41de2e596aee674\",\n",
    ")\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clusterize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_DOMAINS=9\n",
    "HIER_LEVEL=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !git clone https://github.com/kernelmachine/balanced-kmeans/\n",
    "# !cd balanced-kmeans && pip install --editable .\n",
    "# !pip install numba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from kmeans_pytorch import KMeans as BalancedKMeans\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_texts():\n",
    "    return[dataset[i]['text'] for i in range(len(dataset))]\n",
    "\n",
    "def number_normalizer(tokens):\n",
    "    \"\"\"Map all numeric tokens to a placeholder.\n",
    "\n",
    "    For many applications, tokens that begin with a number are not directly\n",
    "    useful, but the fact that such a token exists can be relevant.  By applying\n",
    "    this form of dimensionality reduction, some methods may perform better.\n",
    "    \"\"\"\n",
    "    return (\"#NUMBER\" if token[0].isdigit() else token for token in tokens)\n",
    "\n",
    "\n",
    "class NumberNormalizingVectorizer(TfidfVectorizer):\n",
    "    # this vectorizer replaces numbers with #NUMBER token\n",
    "    def build_tokenizer(self):\n",
    "        tokenize = super().build_tokenizer()\n",
    "        return lambda doc: list(number_normalizer(tokenize(doc)))\n",
    "\n",
    "\n",
    "def train_vectorizer(texts):\n",
    "    # english stopwords plus the #NUMBER dummy token\n",
    "    stop_words = list(text.ENGLISH_STOP_WORDS.union([\"#NUMBER\"]))\n",
    "\n",
    "    model = Pipeline([('tfidf', NumberNormalizingVectorizer(stop_words=stop_words)),\n",
    "                      ('svd', TruncatedSVD(n_components=100)),\n",
    "                      ('normalizer', Normalizer(copy=False))])\n",
    "\n",
    "    model.fit(tqdm(texts[:1000000]))\n",
    "    vecs = model.transform(tqdm(texts))\n",
    "    return model, vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10000 [00:00<?, ?it/s]/home/miu/miniconda3/envs/torch/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['number'] not in stop_words.\n",
      "  warnings.warn(\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 29854.06it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 28585.94it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(10000, 100)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts=get_texts()\n",
    "print(len(texts))\n",
    "tfidf_model, vecs=train_vectorizer(texts)\n",
    "vecs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "500it [02:01,  4.12it/s]\n",
      "500it [00:00, 19186.77it/s]\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "        device = torch.device('cuda')\n",
    "else:\n",
    "        device = torch.device('cpu')\n",
    "\n",
    "if HIER_LEVEL==1:\n",
    "        kmeans = BalancedKMeans(n_clusters=int(math.sqrt(N_DOMAINS)), device=device, balanced=True)\n",
    "else:\n",
    "        kmeans = BalancedKMeans(n_clusters=N_DOMAINS, device=device, balanced=True)\n",
    "\n",
    "bs=500\n",
    "batches = np.array_split(vecs, bs, axis=0)\n",
    "\n",
    "\n",
    "for i, batch in tqdm(enumerate(batches)):\n",
    "        kmeans.fit(torch.from_numpy(batch), iter_limit=20, online=True, tqdm_flag=False,iter_k=i)\n",
    "\n",
    "#LEVEL 0\n",
    "cluster_labels=[]\n",
    "for i, batch in tqdm(enumerate(batches)):\n",
    "    cluster_ids_y = kmeans.predict(\n",
    "        X=torch.from_numpy(batch).to(device),tqdm_flag=False\n",
    "    )\n",
    "    for c in cluster_ids_y:\n",
    "        cluster_labels.append(c.item())\n",
    "cluster_labels=np.asarray(cluster_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if HIER_LEVEL==1:\n",
    "    kmeans_level1=[]\n",
    "    cluster_labels_level1=[]\n",
    "    idxs_cluster_labels_level1=[]\n",
    "    for i in range(int(math.sqrt(N_DOMAINS))):\n",
    "        print('Sub domain:',i)\n",
    "        km=BalancedKMeans(n_clusters=int(math.sqrt(N_DOMAINS)), device=device, balanced=True)\n",
    "        idxs=np.where(cluster_labels==i)\n",
    "        batches_idxs = np.array_split(idxs[0], bs, axis=0)\n",
    "        \n",
    "        batches=[]\n",
    "        level0_idxs=[]\n",
    "        c=0\n",
    "        # for b in batches_idxs:\n",
    "        for k, b in tqdm(enumerate(batches_idxs)):\n",
    "            batch_data=[]\n",
    "            level0_batch_idxs=[]\n",
    "            for j in b:\n",
    "                batch_data.append(vecs[j])\n",
    "                level0_batch_idxs.append(j)\n",
    "                \n",
    "                \n",
    "            batch_data=np.asarray(batch_data)\n",
    "            batches.append(batch_data)\n",
    "            level0_idxs.append(level0_batch_idxs)\n",
    "            X=torch.from_numpy(batch_data).to(device)\n",
    "            km.fit(X, iter_limit=20, online=True, iter_k=c,tqdm_flag=False)\n",
    "            c+=1\n",
    "\n",
    "        subcluster_labels=[]\n",
    "        for k, batch in tqdm(enumerate(batches)):\n",
    "            cluster_ids_y = km.predict(\n",
    "                X=torch.from_numpy(batch).to(device),tqdm_flag=False\n",
    "            )\n",
    "            for z in cluster_ids_y:\n",
    "                subcluster_labels.append(z.item())\n",
    "            batch_idx=level0_idxs[k]\n",
    "            for o,j in enumerate(batch_idx):\n",
    "                idxs_cluster_labels_level1.append((i,j,subcluster_labels[o]))\n",
    "\n",
    "        cluster_labels_level1.append(subcluster_labels)\n",
    "\n",
    "        kmeans_level1.append(km)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([0, 1, 2, 3, 4, 5, 6, 7, 8]), array([4354,  633,  538,  754,  538,  506, 1056, 1313,  308]))\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import os\n",
    "\n",
    "if not os.path.exists('models'):\n",
    "    os.makedirs('models')\n",
    "if not os.path.exists('models/'+model_path):\n",
    "    os.makedirs('models/'+model_path)\n",
    "pickle.dump(tfidf_model,open('models/'+model_path+'tfidf_model.p','wb'))\n",
    "pickle.dump(kmeans,open('models/'+model_path+'/kmeans.p','wb'))\n",
    "if HIER_LEVEL==1:\n",
    "    pickle.dump(kmeans_level1,open('models/'+model_path+'/kmeans_level1.p','wb'))\n",
    "    pickle.dump(idxs_cluster_labels_level1,open('models/'+model_path+'/idxs_cluster_labels_level1.p','wb'))\n",
    "\n",
    "print(np.unique(cluster_labels, return_counts=True))\n",
    "# print(np.unique(np.asarray(cluster_labels_level1[0]), return_counts=True))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feeding samples to tokenizer: 100%|██████████| 10000/10000 [00:00<00:00, 37508.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# !pip install tokenmonster\n",
    "from random import shuffle\n",
    "\n",
    "tokenizer = BertWordPieceTokenizer()\n",
    "tokenizer._tokenizer.normalizer = normalizers.Sequence(\n",
    "    [\n",
    "        normalizers.Replace(Regex(\"(``|'')\"), '\"'),\n",
    "        normalizers.NFD(),\n",
    "        normalizers.Lowercase(),\n",
    "        normalizers.StripAccents(),\n",
    "        normalizers.Replace(Regex(\" {2,}\"), \" \"),\n",
    "        normalizers.Replace(Regex(r\"[^\\x00-\\x7F]+\"), \"\"),\n",
    "    ]\n",
    ")  # Normalizer based on, https://github.com/JonasGeiping/cramming/blob/50bd06a65a4cd4a3dd6ee9ecce1809e1a9085374/cramming/data/tokenizer_preparation.py#L52\n",
    "def tokenizer_training_data() -> Iterator[str]:\n",
    "    for i in tqdm(\n",
    "        range(min(NUM_TOKENIZER_TRAINING_ITEMS, len(dataset))),\n",
    "        desc=\"Feeding samples to tokenizer\",\n",
    "    ):\n",
    "        yield dataset[i][\"text\"]\n",
    "\n",
    "\n",
    "tokenizer.train_from_iterator(\n",
    "    tokenizer_training_data(),\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    min_frequency=2,\n",
    "    )\n",
    "tokenizer.save(str(TOKENIZER_PATH))\n",
    "tokenizer = BertTokenizerFast(tokenizer_file=str(TOKENIZER_PATH))\n",
    "# tokenizer.unk_token,tokenizer.unk_token_id,tokenizer.sep_token,tokenizer.sep_token_id,tokenizer.pad_token,tokenizer.pad_token_id,tokenizer.cls_token,tokenizer.cls_token_id,tokenizer.mask_token,tokenizer.mask_token_id\n",
    "\n",
    "class HFTokenizedDataset(torch.utils.data.Dataset):\n",
    "    \"This wraps the dataset and tokenizes it, ready for the model\"\n",
    "\n",
    "    def __init__(self, dataset, tokenizer):\n",
    "        self.dataset = dataset\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        inps= self.tokenizer.encode(\n",
    "            self.dataset[i][\"text\"],\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            max_length=MODEL_MAX_SEQ_LEN - 2,\n",
    "            padding=\"max_length\",\n",
    "            return_special_tokens_mask=True,\n",
    "        )[0, ...]\n",
    "        return {'input_ids':inps}\n",
    "\n",
    "class TokenizedDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def convert_tokens_to_ids(self,s):\n",
    "        s=norm.normalize_str(s)\n",
    "        tokens = vocab.tokenize(s)\n",
    "        tokens=tokens[:MODEL_MAX_SEQ_LEN - 2]\n",
    "        tokens=torch.Tensor(tokens).long()\n",
    "        return tokens\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        s=norm.normalize_str(self.dataset[i][\"text\"])\n",
    "        tokens = vocab.tokenize(s).tolist()\n",
    "        \n",
    "        #trucate\n",
    "        tokens=tokens[:MODEL_MAX_SEQ_LEN - 2]\n",
    "        l=len(tokens)\n",
    "        for j in range(l,MODEL_MAX_SEQ_LEN - 2):\n",
    "            tokens.append(0)\n",
    "\n",
    "        tokens=torch.Tensor(tokens)\n",
    "        if MODEL_MAX_SEQ_LEN - 2>l:\n",
    "            att_mask=np.concatenate((np.ones(l),np.zeros(MODEL_MAX_SEQ_LEN - 2-l)))\n",
    "        else:\n",
    "            att_mask=np.ones(tokens.shape[0])\n",
    "        # print(l,att_mask.shape[0],MODEL_MAX_SEQ_LEN - 2)\n",
    "        assert tokens.shape[0]==att_mask.shape[0]\n",
    "\n",
    "        input_ids=tokens\n",
    "\n",
    "        input_ids=torch.as_tensor(input_ids,dtype=torch.long)\n",
    "        # labels = torch.zeros(input_ids.shape)\n",
    "        # probability_matrix = torch.full(labels.shape, 0.15)\n",
    "        # masked_indices = torch.bernoulli(probability_matrix).bool()\n",
    "        # indices_replaced = torch.bernoulli(torch.full(labels.shape, 1.0)).bool() & masked_indices\n",
    "        # labels[indices_replaced] =1\n",
    "        # mask=torch.as_tensor(labels,dtype=torch.bool)\n",
    "        \n",
    "        d={'input_ids':input_ids, \n",
    "            # 'token_type_ids':token_type_ids, \n",
    "            # 'attention_mask':attention_mask,\n",
    "            # 'mask':mask\n",
    "            }\n",
    "        return d\n",
    "\n",
    "def texts2mlm(texts,domain,subdomain1=0):\n",
    "    input_ids=[]\n",
    "    # token_type_ids=[]\n",
    "    # attention_mask=[]\n",
    "    # data=[]\n",
    "    inputs=[]\n",
    "    # masks=[]\n",
    "    for t in texts:\n",
    "        s=norm.normalize_str(t[\"text\"])\n",
    "        tokens = vocab.tokenize(s).tolist()\n",
    "        \n",
    "        #trucate\n",
    "        tokens=tokens[:MODEL_MAX_SEQ_LEN - 2]\n",
    "        l=len(tokens)\n",
    "        for j in range(l,MODEL_MAX_SEQ_LEN - 2):\n",
    "            tokens.append(0)\n",
    "\n",
    "        tokens=torch.Tensor(tokens)\n",
    "        if MODEL_MAX_SEQ_LEN - 2>l:\n",
    "            att_mask=np.concatenate((np.ones(l),np.zeros(MODEL_MAX_SEQ_LEN - 2-l)))\n",
    "        else:\n",
    "            att_mask=np.ones(tokens.shape[0])\n",
    "        # print(l,att_mask.shape[0],MODEL_MAX_SEQ_LEN - 2)\n",
    "        assert tokens.shape[0]==att_mask.shape[0]\n",
    "\n",
    "        input_ids=tokens\n",
    "\n",
    "        input_ids=torch.as_tensor(input_ids,dtype=torch.long)\n",
    "        # labels = torch.zeros(input_ids.shape)\n",
    "        # probability_matrix = torch.full(labels.shape, 0.15)\n",
    "        # masked_indices = torch.bernoulli(probability_matrix).bool()\n",
    "        # indices_replaced = torch.bernoulli(torch.full(labels.shape, 1.0)).bool() & masked_indices\n",
    "        # labels[indices_replaced] =1\n",
    "        # mask=torch.as_tensor(labels,dtype=torch.bool)\n",
    "        inputs.append(input_ids)\n",
    "        # masks.append(mask)\n",
    "    \n",
    "    return {'input_ids':torch.stack(inputs),'domain':domain,'subdomain1':(domain*N_DOMAINS)+subdomain1}\n",
    "\n",
    "def divide_chunks(l, n):\n",
    "    return [l[i:i + n] for i in range(0, len(l), n)] \n",
    "\n",
    "class CustomDataloader(torch.utils.data.Dataset):\n",
    "    def __init__(self,dataset,cluster_labels,n_domains) -> None:\n",
    "        self.dataset=dataset\n",
    "        self.cluster_labels=cluster_labels\n",
    "        self.bin_dataset={}\n",
    "        self.domains=[i for i in range(n_domains)]\n",
    "        self.batch_ordering=[]\n",
    "        self.current_domain=0\n",
    "        self.bs=DEVICE_BATCH_SIZE\n",
    "        self.fill_bins()\n",
    "\n",
    "    def fill_bins(self):\n",
    "        self.bin_dataset={}\n",
    "        for i,c in enumerate(self.cluster_labels):\n",
    "            if c not in self.bin_dataset:\n",
    "                self.bin_dataset[c]=[i]\n",
    "            else:\n",
    "                self.bin_dataset[c].append(i)\n",
    "        n_batches=0\n",
    "        domains=[]\n",
    "        for i in range(len(self.domains)):\n",
    "            self.bin_dataset[i]=divide_chunks(self.bin_dataset[i],DEVICE_BATCH_SIZE)\n",
    "            for k in range(len(self.bin_dataset[i])):\n",
    "                domains.append((i,k))\n",
    "        shuffle(domains)\n",
    "        self.batch_ordering=domains\n",
    "\n",
    "    def __getitem__(self, i) -> torch.Tensor:\n",
    "        indexes=self.bin_dataset[self.batch_ordering[i][0]][self.batch_ordering[i][1]]\n",
    "        batch_data=[]\n",
    "        for j in indexes:\n",
    "            batch_data.append(self.dataset[j])\n",
    "        # print('batch_data:',batch_data)\n",
    "        batch_data=texts2mlm(batch_data,self.batch_ordering[i][0])\n",
    "        # batch_data = torch.from_numpy(a).long()\n",
    "        return batch_data\n",
    "\n",
    "    def __len__(self):\n",
    "        #colocar uma margem de erro pra baixo\n",
    "        return len(self.batch_ordering)\n",
    "\n",
    "\n",
    "class CustomDataloaderLevel1(torch.utils.data.Dataset):\n",
    "    def __init__(self,dataset,cluster_labels,idxs_cluster_labels_level1,n_domains,n_subdomains) -> None:\n",
    "        self.dataset=dataset\n",
    "        self.cluster_labels=cluster_labels\n",
    "        self.idxs_cluster_labels_level1=idxs_cluster_labels_level1\n",
    "        self.bin_dataset={}\n",
    "        self.domains=[i for i in range(n_domains)]\n",
    "        self.subdomains=[i for i in range(n_subdomains)]\n",
    "        self.batch_ordering=[]\n",
    "        self.current_domain=0\n",
    "        self.bs=DEVICE_BATCH_SIZE\n",
    "        self.fill_bins()\n",
    "\n",
    "    def fill_bins(self):\n",
    "        self.bin_dataset={}\n",
    "        # for i,c in enumerate(self.cluster_labels):\n",
    "        #     if c not in self.bin_dataset:\n",
    "        #         self.bin_dataset[c]=[i]\n",
    "        #     else:\n",
    "        #         self.bin_dataset[c].append(i)\n",
    "        for dom,txt_id,subdom in self.idxs_cluster_labels_level1:\n",
    "            id_dom_sub=str(dom)+'_'+str(subdom)#identifica o dominio e o subdominio\n",
    "            if  id_dom_sub not in self.bin_dataset:\n",
    "                self.bin_dataset[id_dom_sub]=[txt_id]\n",
    "            else:\n",
    "                self.bin_dataset[id_dom_sub].append(txt_id)\n",
    "\n",
    "        n_batches=0\n",
    "        data_hier_idxs=[]\n",
    "        for i in range(len(self.domains)):\n",
    "            for j in range(len(self.subdomains)):\n",
    "                id_dom_sub=str(i)+'_'+str(j)#identifica o dominio e o subdominio\n",
    "                #OBS: subdivide a lista de todos os ids do msm dom e subdom em batches. O bin dataset e um dicionario\n",
    "                #que mapeia dom e subdom -> lista de batches\n",
    "                self.bin_dataset[id_dom_sub]=divide_chunks(self.bin_dataset[id_dom_sub],DEVICE_BATCH_SIZE)\n",
    "                for k in range(len(self.bin_dataset[id_dom_sub])):\n",
    "                    data_hier_idxs.append((id_dom_sub,k))\n",
    "        shuffle(data_hier_idxs)\n",
    "        #batch ordering é um vetor de tuplas de 2 elementos. O primeiro é o dominio e subdominio desse batch\n",
    "        # O segundo elemento é a posicao do batch no bin dataset. \n",
    "        # Ex: ('0_0',0) se lê: Batch com dominio e subdominio 0, esse batch é o numero 0 na lista de batches do bin_dataset para esse dom e subdom.\n",
    "        self.batch_ordering=data_hier_idxs\n",
    "\n",
    "    def __getitem__(self, i) -> torch.Tensor:\n",
    "        #o bin dataset tem como chaves o dominio e subdominio, e o valor é uma lista de batches de msm dom e subdom\n",
    "        indexes=self.bin_dataset[self.batch_ordering[i][0]][self.batch_ordering[i][1]]\n",
    "        batch_data=[]\n",
    "        for j in indexes:\n",
    "            batch_data.append(self.dataset[int(j)])\n",
    "        # print('batch_data:',batch_data)\n",
    "        if len(self.subdomains)>0:\n",
    "            d0,d1=self.batch_ordering[i][0].split('_')\n",
    "            batch_data=texts2mlm(batch_data,int(d0),int(d1))\n",
    "        else:\n",
    "            batch_data=texts2mlm(batch_data,self.batch_ordering[i][0])\n",
    "        # batch_data = torch.from_numpy(a).long()\n",
    "        return batch_data\n",
    "\n",
    "    def __len__(self):\n",
    "        #colocar uma margem de erro pra baixo\n",
    "        return len(self.batch_ordering)\n",
    "\n",
    "def get_vocab():\n",
    "    #### TokenMonster BRRR!!!\n",
    "    import tokenmonster\n",
    "    vocab = tokenmonster.load(\"englishcode-16000-balanced-v1\")\n",
    "    # vocab = TokenMonster.load('tokenizers_monster/english-24000-capcode.vocab')\n",
    "\n",
    "    norm=normalizers.Sequence(\n",
    "        [\n",
    "            normalizers.Replace(Regex(\"(``|'')\"), '\"'),\n",
    "            normalizers.NFD(),\n",
    "            normalizers.Lowercase(),\n",
    "            normalizers.StripAccents(),\n",
    "            normalizers.Replace(Regex(\" {2,}\"), \" \"),\n",
    "            normalizers.Replace(Regex(r\"[^\\x00-\\x7F]+\"), \"\"),\n",
    "        ]\n",
    "    )\n",
    "    vocab.modify(\"[EOS]\")\n",
    "    vocab.modify(\"[UNK]\")\n",
    "    vocab.modify(\"[SEP]\")\n",
    "    vocab.modify(\"[PAD]\")\n",
    "    vocab.modify(\"[CLS]\")\n",
    "    vocab.modify(\"[MASK]\")\n",
    "    return norm,vocab\n",
    "norm,vocab=get_vocab()\n",
    "MASK_ID=vocab.tokenize(\"[MASK]\")[0]\n",
    "PAD_ID=vocab.tokenize(\"[PAD]\")[0]\n",
    "\n",
    "if model_training=='bert':\n",
    "    tokenized_dataset = HFTokenizedDataset(dataset, tokenizer)\n",
    "elif model_training=='electra':\n",
    "    tokenized_dataset = TokenizedDataset(dataset)\n",
    "\n",
    "# tokenizer.mask_token,tokenizer.convert_tokens_to_ids(tokenizer.mask_token)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Electra"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from functools import reduce\n",
    "from collections import namedtuple\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# constants\n",
    "\n",
    "Results = namedtuple('Results', [\n",
    "    'loss',\n",
    "    'mlm_loss',\n",
    "    'disc_loss',\n",
    "    'gen_acc',\n",
    "    'disc_acc',\n",
    "    'disc_labels',\n",
    "    'disc_predictions'\n",
    "])\n",
    "\n",
    "# helpers\n",
    "def log(t, eps=1e-9):\n",
    "    return torch.log(t + eps)\n",
    "\n",
    "def gumbel_noise(t):\n",
    "    noise = torch.zeros_like(t).uniform_(0, 1)\n",
    "    return -log(-log(noise))\n",
    "\n",
    "def gumbel_sample(t, temperature = 1.):\n",
    "    return ((t / temperature) + gumbel_noise(t)).argmax(dim=-1)\n",
    "\n",
    "def prob_mask_like(t, prob):\n",
    "    return torch.zeros_like(t).float().uniform_(0, 1) < prob\n",
    "\n",
    "def mask_with_tokens(t, token_ids):\n",
    "    init_no_mask = torch.full_like(t, False, dtype=torch.bool)\n",
    "    mask = reduce(lambda acc, el: acc | (t == el), token_ids, init_no_mask)\n",
    "    return mask\n",
    "\n",
    "def get_mask_subset_with_prob(mask, prob):\n",
    "    batch, seq_len, device = *mask.shape, mask.device\n",
    "    max_masked = math.ceil(prob * seq_len)\n",
    "\n",
    "    num_tokens = mask.sum(dim=-1, keepdim=True)\n",
    "    mask_excess = (mask.cumsum(dim=-1) > (num_tokens * prob).ceil())\n",
    "    mask_excess = mask_excess[:, :max_masked]\n",
    "\n",
    "    rand = torch.rand((batch, seq_len), device=device).masked_fill(~mask, -1e9)\n",
    "    _, sampled_indices = rand.topk(max_masked, dim=-1)\n",
    "    sampled_indices = (sampled_indices + 1).masked_fill_(mask_excess, 0)\n",
    "\n",
    "    new_mask = torch.zeros((batch, seq_len + 1), device=device)\n",
    "    new_mask.scatter_(-1, sampled_indices, 1)\n",
    "    return new_mask[:, 1:].bool()\n",
    "\n",
    "\n",
    "# hidden layer extractor class, for magically adding adapter to language model to be pretrained\n",
    "class HiddenLayerExtractor(nn.Module):\n",
    "    def __init__(self, net, layer = -2):\n",
    "        super().__init__()\n",
    "        self.net = net\n",
    "        self.layer = layer\n",
    "\n",
    "        self.hidden = None\n",
    "        self.hook_registered = False\n",
    "\n",
    "    def _find_layer(self):\n",
    "        if type(self.layer) == str:\n",
    "            modules = dict([*self.net.named_modules()])\n",
    "            return modules.get(self.layer, None)\n",
    "        elif type(self.layer) == int:\n",
    "            children = [*self.net.children()]\n",
    "            return children[self.layer]\n",
    "        return None\n",
    "\n",
    "    def _hook(self, _, __, output):\n",
    "        self.hidden = output\n",
    "\n",
    "    def _register_hook(self):\n",
    "        layer = self._find_layer()\n",
    "        assert layer is not None, f'hidden layer ({self.layer}) not found'\n",
    "        handle = layer.register_forward_hook(self._hook)\n",
    "        self.hook_registered = True\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.layer == -1:\n",
    "            return self.net(x['input'],x['domain'])\n",
    "\n",
    "        if not self.hook_registered:\n",
    "            self._register_hook()\n",
    "\n",
    "        _ = self.net(x['input'],x['domain'])\n",
    "        hidden = self.hidden\n",
    "        self.hidden = None\n",
    "        assert hidden is not None, f'hidden layer {self.layer} never emitted an output'\n",
    "        return hidden\n",
    "    \n",
    "class Electra(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        generator,\n",
    "        discriminator,\n",
    "        *,\n",
    "        num_tokens = None,\n",
    "        discr_dim = -1,\n",
    "        discr_layer = -1,\n",
    "        mask_prob = 0.15,\n",
    "        replace_prob = 0.85,\n",
    "        random_token_prob = 0.,\n",
    "        mask_token_id = 2,\n",
    "        pad_token_id = 0,\n",
    "        mask_ignore_token_ids = [],\n",
    "        disc_weight = 50.,\n",
    "        gen_weight = 1.,\n",
    "        temperature = 1.,\n",
    "        ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.generator = generator\n",
    "        self.discriminator = discriminator\n",
    "\n",
    "        if discr_dim > 0:\n",
    "            self.discriminator = nn.Sequential(\n",
    "                HiddenLayerExtractor(discriminator, layer = discr_layer),\n",
    "                nn.Linear(discr_dim, 1)\n",
    "            )\n",
    "\n",
    "        # mlm related probabilities\n",
    "        self.mask_prob = mask_prob\n",
    "        self.replace_prob = replace_prob\n",
    "\n",
    "        self.num_tokens = num_tokens\n",
    "        self.random_token_prob = random_token_prob\n",
    "\n",
    "        # token ids\n",
    "        self.pad_token_id = pad_token_id\n",
    "        self.mask_token_id = mask_token_id\n",
    "        self.mask_ignore_token_ids = set([*mask_ignore_token_ids, pad_token_id])\n",
    "\n",
    "        # sampling temperature\n",
    "        self.temperature = temperature\n",
    "\n",
    "        # loss weights\n",
    "        self.disc_weight = disc_weight\n",
    "        self.gen_weight = gen_weight\n",
    "\n",
    "\n",
    "    def forward(self, inputs, **kwargs):\n",
    "        input=inputs['input']\n",
    "        b, t = input.shape\n",
    "\n",
    "        replace_prob = prob_mask_like(input, self.replace_prob)\n",
    "\n",
    "        # do not mask [pad] tokens, or any other tokens in the tokens designated to be excluded ([cls], [sep])\n",
    "        # also do not include these special tokens in the tokens chosen at random\n",
    "        no_mask = mask_with_tokens(input, self.mask_ignore_token_ids)\n",
    "        mask = get_mask_subset_with_prob(~no_mask, self.mask_prob)\n",
    "\n",
    "        # get mask indices\n",
    "        mask_indices = torch.nonzero(mask, as_tuple=True)\n",
    "\n",
    "        # mask input with mask tokens with probability of `replace_prob` (keep tokens the same with probability 1 - replace_prob)\n",
    "        masked_input = input.clone().detach()\n",
    "\n",
    "        # set inverse of mask to padding tokens for labels\n",
    "        gen_labels = input.masked_fill(~mask, self.pad_token_id)\n",
    "\n",
    "        # clone the mask, for potential modification if random tokens are involved\n",
    "        # not to be mistakened for the mask above, which is for all tokens, whether not replaced nor replaced with random tokens\n",
    "        masking_mask = mask.clone()\n",
    "\n",
    "        # if random token probability > 0 for mlm\n",
    "        if self.random_token_prob > 0:\n",
    "            assert self.num_tokens is not None, 'Number of tokens (num_tokens) must be passed to Electra for randomizing tokens during masked language modeling'\n",
    "\n",
    "            random_token_prob = prob_mask_like(input, self.random_token_prob)\n",
    "            random_tokens = torch.randint(0, self.num_tokens, input.shape, device=input.device)\n",
    "            random_no_mask = mask_with_tokens(random_tokens, self.mask_ignore_token_ids)\n",
    "            random_token_prob &= ~random_no_mask\n",
    "            masked_input = torch.where(random_token_prob, random_tokens, masked_input)\n",
    "\n",
    "            # remove random token prob mask from masking mask\n",
    "            masking_mask = masking_mask & ~random_token_prob\n",
    "\n",
    "        # [mask] input\n",
    "        masked_input = masked_input.masked_fill(masking_mask * replace_prob, self.mask_token_id)\n",
    "\n",
    "        # get generator output and get mlm loss\n",
    "        logits = self.generator(masked_input, **kwargs)\n",
    "\n",
    "        mlm_loss = F.cross_entropy(\n",
    "            logits.transpose(1, 2),\n",
    "            gen_labels,\n",
    "            ignore_index = self.pad_token_id\n",
    "        )\n",
    "\n",
    "        # use mask from before to select logits that need sampling\n",
    "        sample_logits = logits[mask_indices]\n",
    "\n",
    "        # sample\n",
    "        sampled = gumbel_sample(sample_logits, temperature = self.temperature)\n",
    "\n",
    "        # scatter the sampled values back to the input\n",
    "        disc_input = input.clone()\n",
    "        disc_input[mask_indices] = sampled.detach()\n",
    "\n",
    "        # generate discriminator labels, with replaced as True and original as False\n",
    "        disc_labels = (input != disc_input).float().detach()\n",
    "\n",
    "        # get discriminator predictions of replaced / original\n",
    "        non_padded_indices = torch.nonzero(input != self.pad_token_id, as_tuple=True)\n",
    "\n",
    "        # get discriminator output and binary cross entropy loss\n",
    "        disc_logits = self.discriminator({'input':disc_input,'domain':inputs['domain']}, **kwargs)\n",
    "        disc_logits = disc_logits.reshape_as(disc_labels)\n",
    "\n",
    "        disc_loss = F.binary_cross_entropy_with_logits(\n",
    "            disc_logits[non_padded_indices],\n",
    "            disc_labels[non_padded_indices]\n",
    "        )\n",
    "\n",
    "        # gather metrics\n",
    "        with torch.no_grad():\n",
    "            gen_predictions = torch.argmax(logits, dim=-1)\n",
    "            disc_predictions = torch.round((torch.sign(disc_logits) + 1.0) * 0.5)\n",
    "            gen_acc = (gen_labels[mask] == gen_predictions[mask]).float().mean()\n",
    "            disc_acc = 0.5 * (disc_labels[mask] == disc_predictions[mask]).float().mean() + 0.5 * (disc_labels[~mask] == disc_predictions[~mask]).float().mean()\n",
    "\n",
    "        # return weighted sum of losses\n",
    "        return Results(self.gen_weight * mlm_loss + self.disc_weight * disc_loss, mlm_loss, disc_loss, gen_acc, disc_acc, disc_labels, disc_predictions)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-A100 GPU detected, using math or mem efficient attention if input tensor is on cuda\n"
     ]
    }
   ],
   "source": [
    "# pip install x_transformers reformer_pytorch accelerate einops\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "# from reformer_pytorch import ReformerLM\n",
    "from x_transformers import TransformerWrapper, Decoder,Encoder\n",
    "\n",
    "# (1) instantiate the generator and discriminator, making sure that the generator is roughly a quarter to a half of the size of the discriminator\n",
    "colt5_dict={\n",
    "            'generator':{'is_colt5_decoder':True,'light_ff_mult':0.5,'heavy_ff_mult':4,'num_heavy_tokens':300,\n",
    "                    'light_dim_head':64,'light_heads':8,'light_window_size':128,\n",
    "                    'heavy_window_size':128,'heavy_dim_head':64,'heavy_heads':8,\n",
    "                    'num_heavy_tokens_q':32,'num_heavy_tokens_kv':300,'num_routed_kv':2, \n",
    "                    'use_triton':True, 'use_flash_attn':True},\n",
    "            'discriminator':{'is_colt5_decoder':True,'light_ff_mult':0.5,'heavy_ff_mult':4,'num_heavy_tokens':300,\n",
    "                    'light_dim_head':64,'light_heads':8,'light_window_size':128,\n",
    "                    'heavy_window_size':128,'heavy_dim_head':64,'heavy_heads':8,\n",
    "                    'num_heavy_tokens_q':32,'num_heavy_tokens_kv':300,'num_routed_kv':2, \n",
    "                    'use_triton':True, 'use_flash_attn':True},     \n",
    "            }\n",
    "\n",
    "generator = TransformerWrapper(\n",
    "    num_tokens = VOCAB_SIZE+8,\n",
    "    max_seq_len = MODEL_MAX_SEQ_LEN,\n",
    "    attn_layers = Decoder(\n",
    "        dim = 256,\n",
    "        depth = 3,\n",
    "        heads = 3,\n",
    "        attn_flash = True,\n",
    "        # rel_pos_bias = True\n",
    "        colt5_dict=colt5_dict['generator'],\n",
    "        domains=N_DOMAINS,\n",
    "    ),\n",
    "    emb_dim=128,\n",
    "    )\n",
    "\n",
    "\n",
    "discriminator = TransformerWrapper(\n",
    "    num_tokens = VOCAB_SIZE+8,\n",
    "    max_seq_len = MODEL_MAX_SEQ_LEN,\n",
    "    attn_layers = Decoder(\n",
    "        dim = 768,\n",
    "        depth = 6,\n",
    "        heads = 6,\n",
    "        attn_flash = True,\n",
    "        # rel_pos_bias = True\n",
    "        colt5_dict=colt5_dict['discriminator'],\n",
    "        domains=0,\n",
    "    ),\n",
    "    emb_dim=128,\n",
    "    )\n",
    "#works with reformer!!!\n",
    "# generator.token_emb = discriminator.token_emb\n",
    "# generator.pos_emb = discriminator.pos_emb\n",
    "\n",
    "model = Electra(\n",
    "    generator,\n",
    "    discriminator,\n",
    "    discr_dim = 768,           # the embedding dimension of the discriminator\n",
    "    # discr_dim = 1024,           # the embedding dimension of the discriminator\n",
    "    # discr_layer = 'reformer',   # the layer name in the discriminator, whose output would be used for predicting token is still the same or replaced\n",
    "    discr_layer = 'attn_layers',   # the layer name in the discriminator, whose output would be used for predicting token is still the same or replaced\n",
    "    mask_token_id = MASK_ID,          # the token id reserved for masking\n",
    "    pad_token_id = PAD_ID,           # the token id for padding\n",
    "    mask_prob = 0.15,           # masking probability for masked language modeling\n",
    "    mask_ignore_token_ids = []  # ids of tokens to ignore for mask modeling ex. (cls, sep)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_ids are the indices corresponding to each token in the sentence.\n",
    "# attention_mask indicates whether a token should be attended to or not.\n",
    "# token_type_ids identifies which sequence a token belongs to when there is more than one sequence\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "if model_training=='modular':\n",
    "    if HIER_LEVEL==0:\n",
    "        train_dataloader = CustomDataloader(dataset,cluster_labels,N_DOMAINS)   \n",
    "    elif HIER_LEVEL==1:\n",
    "        #no dataloader os as var fr subdom sao do msm tamanho do dom. Exemplo: 3 dom e 3 subdom\n",
    "        #No modelo é diferente, temos uma primeira camada com 3 branches(dominio), e uma segunda camada com 9 branches(subdominios)\n",
    "        train_dataloader = CustomDataloaderLevel1(dataset,cluster_labels,idxs_cluster_labels_level1,int(math.sqrt(N_DOMAINS)),int(math.sqrt(N_DOMAINS)) ) \n",
    "else:\n",
    "    train_dataloader = DataLoader(\n",
    "            tokenized_dataset, shuffle=True, batch_size=DEVICE_BATCH_SIZE\n",
    "        )\n",
    "# Optimizer\n",
    "learning_rate=5e-5\n",
    "weight_decay=0\n",
    "\n",
    "gradient_accumulation_steps=1\n",
    "max_train_steps=None\n",
    "num_train_epochs=1\n",
    "lr_scheduler_type='linear'\n",
    "num_warmup_steps=0\n",
    "# Split weights in two groups, one with weight decay and the other not.\n",
    "no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "optimizer_grouped_parameters = [\n",
    "    {\n",
    "        \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "        \"weight_decay\": weight_decay,\n",
    "    },\n",
    "    {\n",
    "        \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "        \"weight_decay\": 0.0,\n",
    "    },\n",
    "]\n",
    "optimizer = torch.optim.AdamW(optimizer_grouped_parameters, lr=learning_rate)\n",
    "\n",
    "import math \n",
    "from transformers import (\n",
    "    get_scheduler,\n",
    ")\n",
    "# Scheduler and math around the number of training steps.\n",
    "overrode_max_train_steps = False\n",
    "num_update_steps_per_epoch = math.ceil(len(train_dataloader) / gradient_accumulation_steps)\n",
    "if max_train_steps is None:\n",
    "    max_train_steps = num_train_epochs * num_update_steps_per_epoch\n",
    "    overrode_max_train_steps = True\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=lr_scheduler_type,\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=num_warmup_steps * gradient_accumulation_steps,\n",
    "    num_training_steps=max_train_steps * gradient_accumulation_steps,\n",
    ")\n",
    "from accelerate import Accelerator\n",
    "\n",
    "accelerator = Accelerator()\n",
    "# Prepare everything with our `accelerator`.\n",
    "model, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(\n",
    "    model, optimizer, train_dataloader, lr_scheduler\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration:  0 , total_loss:  1.3085489908854167\n",
      "count_amostra: 20\n",
      "iteration:  30 , total_loss:  31.460942840576173\n",
      "count_amostra: 620\n",
      "iteration:  60 , total_loss:  27.598346138000487\n",
      "count_amostra: 1220\n",
      "iteration:  90 , total_loss:  26.552837562561034\n",
      "count_amostra: 1803\n",
      "iteration:  120 , total_loss:  24.93735866546631\n",
      "count_amostra: 2403\n",
      "iteration:  150 , total_loss:  22.770171674092612\n",
      "count_amostra: 2984\n",
      "iteration:  180 , total_loss:  22.4317720413208\n",
      "count_amostra: 3578\n",
      "iteration:  210 , total_loss:  22.319032351175945\n",
      "count_amostra: 4178\n",
      "iteration:  240 , total_loss:  22.14445514678955\n",
      "count_amostra: 4759\n",
      "iteration:  270 , total_loss:  22.231594530741372\n",
      "count_amostra: 5337\n",
      "iteration:  300 , total_loss:  22.07226022084554\n",
      "count_amostra: 5937\n",
      "iteration:  330 , total_loss:  21.902414258321127\n",
      "count_amostra: 6537\n",
      "iteration:  360 , total_loss:  21.679315694173177\n",
      "count_amostra: 7137\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/miu/Projects/NLP/LM_Pretraining/EfficientPretrain/electra_colt5_KB_domains.ipynb Cell 23\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/miu/Projects/NLP/LM_Pretraining/EfficientPretrain/electra_colt5_KB_domains.ipynb#X41sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m model\u001b[39m.\u001b[39mtrain()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/miu/Projects/NLP/LM_Pretraining/EfficientPretrain/electra_colt5_KB_domains.ipynb#X41sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39mfor\u001b[39;00m step, batch \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(train_dataloader):\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/miu/Projects/NLP/LM_Pretraining/EfficientPretrain/electra_colt5_KB_domains.ipynb#X41sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m     \u001b[39m# with accelerator.accumulate(model):\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/miu/Projects/NLP/LM_Pretraining/EfficientPretrain/electra_colt5_KB_domains.ipynb#X41sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m     results\u001b[39m=\u001b[39mmodel({\u001b[39m'\u001b[39;49m\u001b[39minput\u001b[39;49m\u001b[39m'\u001b[39;49m:batch[\u001b[39m'\u001b[39;49m\u001b[39minput_ids\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49mcuda(),\u001b[39m'\u001b[39;49m\u001b[39mdomain\u001b[39;49m\u001b[39m'\u001b[39;49m:batch[\u001b[39m'\u001b[39;49m\u001b[39mdomain\u001b[39;49m\u001b[39m'\u001b[39;49m],\u001b[39m'\u001b[39;49m\u001b[39msubdomain1\u001b[39;49m\u001b[39m'\u001b[39;49m:batch[\u001b[39m'\u001b[39;49m\u001b[39msubdomain1\u001b[39;49m\u001b[39m'\u001b[39;49m]}) \n\u001b[1;32m     <a href='vscode-notebook-cell:/home/miu/Projects/NLP/LM_Pretraining/EfficientPretrain/electra_colt5_KB_domains.ipynb#X41sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m     count_amostra\u001b[39m+\u001b[39m\u001b[39m=\u001b[39m\u001b[39mint\u001b[39m(\u001b[39mlen\u001b[39m(batch[\u001b[39m'\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m'\u001b[39m]))\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/miu/Projects/NLP/LM_Pretraining/EfficientPretrain/electra_colt5_KB_domains.ipynb#X41sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m     loss \u001b[39m=\u001b[39m results\u001b[39m.\u001b[39mloss\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/home/miu/Projects/NLP/LM_Pretraining/EfficientPretrain/electra_colt5_KB_domains.ipynb Cell 23\u001b[0m line \u001b[0;36m2\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/miu/Projects/NLP/LM_Pretraining/EfficientPretrain/electra_colt5_KB_domains.ipynb#X41sZmlsZQ%3D%3D?line=207'>208</a>\u001b[0m disc_labels \u001b[39m=\u001b[39m (\u001b[39minput\u001b[39m \u001b[39m!=\u001b[39m disc_input)\u001b[39m.\u001b[39mfloat()\u001b[39m.\u001b[39mdetach()\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/miu/Projects/NLP/LM_Pretraining/EfficientPretrain/electra_colt5_KB_domains.ipynb#X41sZmlsZQ%3D%3D?line=209'>210</a>\u001b[0m \u001b[39m# get discriminator predictions of replaced / original\u001b[39;00m\n\u001b[0;32m--> <a href='vscode-notebook-cell:/home/miu/Projects/NLP/LM_Pretraining/EfficientPretrain/electra_colt5_KB_domains.ipynb#X41sZmlsZQ%3D%3D?line=210'>211</a>\u001b[0m non_padded_indices \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mnonzero(\u001b[39minput\u001b[39;49m \u001b[39m!=\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpad_token_id, as_tuple\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/miu/Projects/NLP/LM_Pretraining/EfficientPretrain/electra_colt5_KB_domains.ipynb#X41sZmlsZQ%3D%3D?line=212'>213</a>\u001b[0m \u001b[39m# get discriminator output and binary cross entropy loss\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/miu/Projects/NLP/LM_Pretraining/EfficientPretrain/electra_colt5_KB_domains.ipynb#X41sZmlsZQ%3D%3D?line=213'>214</a>\u001b[0m disc_logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdiscriminator({\u001b[39m'\u001b[39m\u001b[39minput\u001b[39m\u001b[39m'\u001b[39m:disc_input,\u001b[39m'\u001b[39m\u001b[39mdomain\u001b[39m\u001b[39m'\u001b[39m:inputs[\u001b[39m'\u001b[39m\u001b[39mdomain\u001b[39m\u001b[39m'\u001b[39m]}, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "from einops import rearrange, pack, unpack\n",
    "\n",
    "# Only show the progress bar once on each machine.\n",
    "# progress_bar = tqdm(range(max_train_steps), disable=not accelerator.is_local_main_process)\n",
    "completed_steps = 0\n",
    "starting_epoch = 0\n",
    "\n",
    "# update the progress_bar if load from checkpoint\n",
    "total_loss=0\n",
    "# progress_bar.update(completed_steps)\n",
    "steps_log=30\n",
    "count_amostra=0\n",
    "num_train_epochs=1\n",
    "for epoch in range(starting_epoch, num_train_epochs):\n",
    "    if model_training=='modular':\n",
    "        train_dataloader.fill_bins()\n",
    "    model.train()\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        # with accelerator.accumulate(model):\n",
    "        results=model({'input':batch['input_ids'].cuda(),'domain':batch['domain'],'subdomain1':batch['subdomain1']}) \n",
    "\n",
    "        count_amostra+=int(len(batch['input_ids']))\n",
    "        loss = results.loss\n",
    "        # print(loss)\n",
    "        total_loss += loss.detach().float().cpu().numpy().item()\n",
    "        accelerator.backward(loss)\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()       \n",
    "        \n",
    "        if step%steps_log==0:\n",
    "            print('iteration: ',step,', total_loss: ',total_loss/steps_log)\n",
    "            print('count_amostra:',count_amostra)\n",
    "            total_loss=0\n",
    "    # Checks if the accelerator has performed an optimization step behind the scenes\n",
    "    if accelerator.sync_gradients:\n",
    "        # progress_bar.update(1)\n",
    "        completed_steps += 1\n",
    "\n",
    "    if completed_steps >= max_train_steps:\n",
    "        break\n",
    "\n",
    "    model.eval()\n",
    "    losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=2/0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.discriminator,'models/'+model_path+'domgen3_3_electra_discriminator_16kvocab_10m.pt')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetuning model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Electra custom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "import torch\n",
    "import os\n",
    "from torch.utils.data import  DataLoader\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "    \n",
    "class ElectraClass(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ElectraClass, self).__init__()\n",
    "        self.l1 = model.discriminator._modules['0']\n",
    "        self.l2 = torch.nn.Dropout(0.3)\n",
    "        self.fl = torch.nn.Flatten()\n",
    "        self.l3 = torch.nn.Linear(768*(MODEL_MAX_SEQ_LEN-2), 3)\n",
    "    \n",
    "    def forward(self, ids):\n",
    "        output= self.l1(ids)\n",
    "        output = self.l2(output)\n",
    "        output = self.fl(output)\n",
    "        output = self.l3(output)\n",
    "        return output\n",
    "\n",
    "electra_model = ElectraClass()\n",
    "electra_model=electra_model.to('cuda')\n",
    "optimizer = torch.optim.Adam(params =  model.parameters(), lr=2e-5,weight_decay=0.01)\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "\n",
    "electra_model.train()\n",
    "\n",
    "def loss_fn(outputs, targets):\n",
    "    loss=torch.nn.CrossEntropyLoss()(outputs, targets)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task='mrpc'\n",
    "dataset = load_dataset('glue', task, split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class TokenizedDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset,is_dom=False):\n",
    "        self.dataset = dataset\n",
    "        self.ls=[]\n",
    "        self.is_dom=is_dom\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        s=norm.normalize_str(self.dataset[\"sentence1\"][i])+'[SEP]'+norm.normalize_str(self.dataset[\"sentence2\"][i])\n",
    "        tokens = vocab.tokenize(s).tolist()        \n",
    "        \n",
    "        self.ls.append(i)\n",
    "        \n",
    "        #trucate\n",
    "        tokens=tokens[:MODEL_MAX_SEQ_LEN - 2]\n",
    "        l=len(tokens)\n",
    "        for j in range(l,MODEL_MAX_SEQ_LEN - 2):\n",
    "            tokens.append(0)\n",
    "        tokens=torch.Tensor(tokens).long()\n",
    "\n",
    "        label=self.dataset[\"label\"][i]\n",
    "\n",
    "        if self.is_dom:\n",
    "            p=tfidf_model.transform([self.dataset[\"sentence1\"][i]+self.dataset[\"sentence2\"][i]])\n",
    "            dom = kmeans.predict(\n",
    "                X=torch.from_numpy(p).to(device)\n",
    "            )\n",
    "            d={'input_ids':tokens,'label':label, 'domain':dom.item()}\n",
    "        else:\n",
    "            d={'input_ids':tokens,'label':label}\n",
    "        return d\n",
    "\n",
    "class HFTokenizedDataset(torch.utils.data.Dataset):\n",
    "    \"This wraps the dataset and tokenizes it, ready for the model\"\n",
    "\n",
    "    def __init__(self, dataset, tokenizer):\n",
    "        self.dataset = dataset\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        label=self.dataset[\"label\"][i]\n",
    "        inps=self.tokenizer.encode(\n",
    "            self.dataset[\"sentence1\"][i]+'[SEP]'+self.dataset[\"sentence2\"][i],\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            max_length=MODEL_MAX_SEQ_LEN - 2,\n",
    "            padding=\"max_length\",\n",
    "            return_special_tokens_mask=True,\n",
    "        )[0, ...]\n",
    "        return {'input_ids':inps,\n",
    "            'label':label}\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "if model_training=='bert':\n",
    "    tokenized_dataset = HFTokenizedDataset(dataset, tokenizer)\n",
    "else:\n",
    "    tokenized_dataset = TokenizedDataset(dataset)\n",
    "# train_dataloader=CustomDataloader(tokenized_dataset)\n",
    "train_dataloader = DataLoader(\n",
    "        tokenized_dataset, batch_size=DEVICE_BATCH_SIZE\n",
    "        # tokenized_dataset, batch_size=1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Iteration: 0, Loss:  0.01158959984779358\n",
      "Epoch: 1, Iteration: 0, Loss:  0.41333103120326997\n",
      "Epoch: 2, Iteration: 0, Loss:  0.38584744930267334\n"
     ]
    }
   ],
   "source": [
    "epochs=5\n",
    "running_loss=0\n",
    "for i in range(epochs):\n",
    "    for j,data in enumerate(train_dataloader):\n",
    "        if model_training=='modular':\n",
    "            ids = {'input':data['input_ids'].to('cuda', dtype = torch.long),'domain':0}\n",
    "            # ids = {'input':data['input_ids'].to('cuda', dtype = torch.long),'domain':data['domain']}\n",
    "        else:\n",
    "            ids = data['input_ids'].to('cuda', dtype = torch.long)\n",
    "        \n",
    "        targets = data['label'].to('cuda', dtype = torch.long)\n",
    "\n",
    "        outputs = electra_model(ids)\n",
    "        \n",
    "        loss = loss_fn(outputs,targets)\n",
    "        # loss=outputs.loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        running_loss+=loss.item()\n",
    "        if j%50==0:\n",
    "            print(f'Epoch: {i}, Iteration: {j}, Loss:  {running_loss/50}')\n",
    "            running_loss=0\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset('glue', task, split='validation')\n",
    "if model_training=='bert':\n",
    "    tokenized_dataset = HFTokenizedDataset(dataset, tokenizer)\n",
    "else:\n",
    "    # tokenized_dataset = TokenizedDataset(dataset)\n",
    "    tokenized_dataset = TokenizedDataset(dataset)\n",
    "\n",
    "eval_dataloader = DataLoader(\n",
    "        tokenized_dataset, batch_size=DEVICE_BATCH_SIZE\n",
    "        # tokenized_dataset, batch_size=1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "predictions=[]\n",
    "labels=[]\n",
    "c=0\n",
    "for _,data in enumerate(eval_dataloader):\n",
    "    if model_training=='modular':\n",
    "        ids = {'input':data['input_ids'].to('cuda', dtype = torch.long),'domain':0}\n",
    "        # ids = {'input':data['input_ids'].to('cuda', dtype = torch.long),'domain':data['domain']}\n",
    "    else:\n",
    "        ids = data['input_ids'].to('cuda', dtype = torch.long)\n",
    "    targets = data['label'].to('cuda', dtype = torch.long)\n",
    "    \n",
    "    outputs = electra_model(ids)\n",
    "    \n",
    "    outputs=torch.argmax(outputs, dim=1).cpu()\n",
    "    targets=targets.cpu()\n",
    "    \n",
    "    outputs=[int(o.item()) for o in outputs]\n",
    "    targets=[int(o.item()) for o in targets]\n",
    "\n",
    "    predictions+=outputs\n",
    "    labels+=targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.64      0.61       129\n",
      "           1       0.83      0.80      0.81       279\n",
      "\n",
      "    accuracy                           0.75       408\n",
      "   macro avg       0.71      0.72      0.71       408\n",
      "weighted avg       0.75      0.75      0.75       408\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(labels, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2 hierarquias 3_dom_3_subdom_electra_discriminator_16kvocab_10m\n",
    "\n",
    "precision    recall  f1-score   support\n",
    "\n",
    "           0       0.59      0.64      0.61       129\n",
    "           1       0.83      0.80      0.81       279\n",
    "\n",
    "    accuracy                           0.75       408\n",
    "   macro avg       0.71      0.72      0.71       408\n",
    "weighted avg       0.75      0.75      0.75       408\n",
    "#2 hierarquias 3_dom_3_subdom_electra_discriminator_16kvocab_1m\n",
    "    precision    recall  f1-score   support\n",
    "\n",
    "           0       0.50      0.42      0.46       129\n",
    "           1       0.75      0.81      0.78       279\n",
    "\n",
    "    accuracy                           0.69       408\n",
    "   macro avg       0.63      0.61      0.62       408\n",
    "weighted avg       0.67      0.69      0.68       408\n",
    "#primeira tentativa com  dominios no discriminador(1m?)\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.39      0.10      0.16       129\n",
    "           1       0.69      0.93      0.79       279\n",
    "\n",
    "    accuracy                           0.67       408\n",
    "   macro avg       0.54      0.51      0.48       408\n",
    "weighted avg       0.60      0.67      0.59       408\n",
    "## glue mrpc com 10_dom_electra_discriminator_16kvocab_10m\n",
    "                precision    recall  f1-score   support\n",
    "\n",
    "           0       0.61      0.57      0.59       129\n",
    "           1       0.81      0.83      0.82       279\n",
    "\n",
    "    accuracy                           0.75       408\n",
    "   macro avg       0.71      0.70      0.71       408\n",
    "weighted avg       0.75      0.75      0.75       408\n",
    "\n",
    "## glue mrpc com 3_dom_electra_discriminator_16kvocab_10m\n",
    "         precision    recall  f1-score   support\n",
    "\n",
    "           0       0.56      0.54      0.55       129\n",
    "           1       0.79      0.81      0.80       279\n",
    "\n",
    "    accuracy                           0.72       408\n",
    "   macro avg       0.68      0.67      0.68       408\n",
    "weighted avg       0.72      0.72      0.72       408\n",
    "\n",
    "## glue mrpc com electra_discriminator_16kvocab_10m\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.49      0.47      0.48       129\n",
    "           1       0.76      0.78      0.77       279\n",
    "\n",
    "    accuracy                           0.68       408\n",
    "   macro avg       0.63      0.62      0.62       408\n",
    "weighted avg       0.67      0.68      0.68       408\n",
    "\n",
    "## glue mrpc com electra com pretraining de 1m:\n",
    "            precision    recall  f1-score   support\n",
    "\n",
    "           0       0.47      0.48      0.47       129\n",
    "           1       0.76      0.75      0.75       279\n",
    "\n",
    "    accuracy                           0.66       408\n",
    "   macro avg       0.61      0.61      0.61       408\n",
    "weighted avg       0.66      0.66      0.66       408\n",
    "\n",
    "\n",
    "## glue mrpc com electra sem pretraining:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.50      0.23      0.32       129\n",
    "           1       0.72      0.89      0.79       279\n",
    "\n",
    "    accuracy                           0.68       408\n",
    "   macro avg       0.61      0.56      0.56       408\n",
    "weighted avg       0.65      0.68      0.64       408"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bert custom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import torch\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "class BERTClass(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BERTClass, self).__init__()\n",
    "        self.l1 = transformers.BertModel.from_pretrained('bert-base-cased')\n",
    "        self.l2 = torch.nn.Dropout(0.3)\n",
    "        self.l3 = torch.nn.Linear(768, 3)\n",
    "    \n",
    "    def forward(self, ids, mask, token_type_ids):\n",
    "        outs, output_1= self.l1(ids, attention_mask = mask, token_type_ids = token_type_ids, return_dict=False)\n",
    "        output_2 = self.l2(output_1)\n",
    "        output = self.l3(output_2)\n",
    "        return output\n",
    "    \n",
    "\n",
    "model = BERTClass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "import transformers\n",
    "import torch\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained('bert-base-cased', num_labels=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset glue (/home/kiki/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import  DataLoader\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "task='mrpc'\n",
    "dataset = load_dataset('glue', task, split='train')\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-cased', use_fast=True)\n",
    "# dataset = dataset.map(lambda e: tokenizer(e['sentence1'], truncation=True, padding='max_length'), batched=True)\n",
    "# dataset.set_format(type='torch', columns=['input_ids', 'token_type_ids', 'attention_mask', 'label']) #BERT\n",
    "# dataloader = torch.utils.data.DataLoader(dataset, batch_size=12)\n",
    "\n",
    "model=model.to('cuda')\n",
    "optimizer = torch.optim.Adam(params =  model.parameters(), lr=2e-5,weight_decay=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89502a00dc1f4a599875cfcf74516990",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3668 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def preprocess_function(examples):\n",
    "    if sentence2_key is None:\n",
    "        return tokenizer(examples[sentence1_key], truncation=True, padding='max_length')\n",
    "    return tokenizer(examples[sentence1_key], examples[sentence2_key], truncation=True, padding='max_length')\n",
    "sentence1_key,sentence2_key=(\"sentence1\", \"sentence2\")\n",
    "encoded_dataset = dataset.map(preprocess_function, batched=True)\n",
    "encoded_dataset.set_format(type='torch', columns=['input_ids', 'token_type_ids', 'attention_mask', 'label'])\n",
    "dataloader = torch.utils.data.DataLoader(encoded_dataset, batch_size=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "def loss_fn(outputs, targets):\n",
    "    # if task != \"stsb\":\n",
    "    #     outputs = torch.argmax(outputs, dim=1)\n",
    "    # else:\n",
    "    #     outputs = outputs[:, 0]\n",
    "    # loss=torch.nn.BCEWithLogitsLoss()(outputs,targets)\n",
    "    # loss=torch.nn.CrossEntropyLoss()(outputs,targets)\n",
    "    loss=torch.nn.CrossEntropyLoss()(outputs.view(-1,3), targets.view(-1))\n",
    "    # loss = Variable(loss, requires_grad = True)\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss:  0.007852643132209777\n",
      "Epoch: 0, Loss:  0.20191293969750404\n",
      "Epoch: 0, Loss:  0.16229299038648606\n",
      "Epoch: 0, Loss:  0.14223801746964454\n",
      "Epoch: 0, Loss:  0.1337541800737381\n",
      "Epoch: 0, Loss:  0.11794026836752891\n",
      "Epoch: 0, Loss:  0.10376800172030926\n",
      "Epoch: 1, Loss:  0.0131098260730505\n",
      "Epoch: 1, Loss:  0.10432511921972036\n",
      "Epoch: 1, Loss:  0.10928954795002938\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">23</span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">20 │   │   │   </span>running_loss=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">0</span>                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">21 │   │   </span>                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">22 │   │   </span>optimizer.zero_grad()                                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>23 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>loss.backward()                                                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">24 │   │   </span>optimizer.step()                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">25 </span>                                                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/kiki/miniconda3/envs/llama/lib/python3.9/site-packages/torch/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">_tensor.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">487</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">backward</span>    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 484 │   │   │   │   </span>create_graph=create_graph,                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 485 │   │   │   │   </span>inputs=inputs,                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 486 │   │   │   </span>)                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 487 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>torch.autograd.backward(                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 488 │   │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>, gradient, retain_graph, create_graph, inputs=inputs                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 489 │   │   </span>)                                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 490 </span>                                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/kiki/miniconda3/envs/llama/lib/python3.9/site-packages/torch/autograd/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">__init__.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">200</span> in   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">backward</span>                                                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">197 │   # The reason we repeat same the comment below is that</span>                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">198 │   # some Python versions print out the first line of a multi-line function</span>               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">199 │   # calls in the traceback and some print out the last line</span>                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>200 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>Variable._execution_engine.run_backward(  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># Calls into the C++ engine to run the bac</span>   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">201 │   │   </span>tensors, grad_tensors_, retain_graph, create_graph, inputs,                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">202 │   │   </span>allow_unreachable=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">True</span>, accumulate_grad=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">True</span>)  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># Calls into the C++ engine to ru</span>   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">203 </span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">KeyboardInterrupt</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92m<module>\u001b[0m:\u001b[94m23\u001b[0m                                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m20 \u001b[0m\u001b[2m│   │   │   \u001b[0mrunning_loss=\u001b[94m0\u001b[0m                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m21 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m22 \u001b[0m\u001b[2m│   │   \u001b[0moptimizer.zero_grad()                                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m23 \u001b[2m│   │   \u001b[0mloss.backward()                                                                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m24 \u001b[0m\u001b[2m│   │   \u001b[0moptimizer.step()                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m25 \u001b[0m                                                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/kiki/miniconda3/envs/llama/lib/python3.9/site-packages/torch/\u001b[0m\u001b[1;33m_tensor.py\u001b[0m:\u001b[94m487\u001b[0m in \u001b[92mbackward\u001b[0m    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 484 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mcreate_graph=create_graph,                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 485 \u001b[0m\u001b[2m│   │   │   │   \u001b[0minputs=inputs,                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 486 \u001b[0m\u001b[2m│   │   │   \u001b[0m)                                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 487 \u001b[2m│   │   \u001b[0mtorch.autograd.backward(                                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 488 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[96mself\u001b[0m, gradient, retain_graph, create_graph, inputs=inputs                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 489 \u001b[0m\u001b[2m│   │   \u001b[0m)                                                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 490 \u001b[0m                                                                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/kiki/miniconda3/envs/llama/lib/python3.9/site-packages/torch/autograd/\u001b[0m\u001b[1;33m__init__.py\u001b[0m:\u001b[94m200\u001b[0m in   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[92mbackward\u001b[0m                                                                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m197 \u001b[0m\u001b[2m│   \u001b[0m\u001b[2m# The reason we repeat same the comment below is that\u001b[0m                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m198 \u001b[0m\u001b[2m│   \u001b[0m\u001b[2m# some Python versions print out the first line of a multi-line function\u001b[0m               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m199 \u001b[0m\u001b[2m│   \u001b[0m\u001b[2m# calls in the traceback and some print out the last line\u001b[0m                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m200 \u001b[2m│   \u001b[0mVariable._execution_engine.run_backward(  \u001b[2m# Calls into the C++ engine to run the bac\u001b[0m   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m201 \u001b[0m\u001b[2m│   │   \u001b[0mtensors, grad_tensors_, retain_graph, create_graph, inputs,                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m202 \u001b[0m\u001b[2m│   │   \u001b[0mallow_unreachable=\u001b[94mTrue\u001b[0m, accumulate_grad=\u001b[94mTrue\u001b[0m)  \u001b[2m# Calls into the C++ engine to ru\u001b[0m   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m203 \u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mKeyboardInterrupt\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.train()\n",
    "epochs=5\n",
    "running_loss=0\n",
    "for i in range(epochs):\n",
    "    for _,data in enumerate(dataloader):\n",
    "        ids = data['input_ids'].to('cuda', dtype = torch.long)\n",
    "        mask = data['attention_mask'].to('cuda', dtype = torch.long)\n",
    "        token_type_ids = data['token_type_ids'].to('cuda', dtype = torch.long)\n",
    "        targets = data['label'].to('cuda', dtype = torch.long)\n",
    "\n",
    "        # outputs = model(ids, mask, token_type_ids)\n",
    "        outputs = model(ids, mask, token_type_ids)\n",
    "        loss = loss_fn(outputs.logits,targets)\n",
    "        # loss=outputs.loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        running_loss+=loss.item()\n",
    "        if _%50==0:\n",
    "            print(f'Epoch: {i}, Loss:  {running_loss/200}')\n",
    "            running_loss=0\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset glue (/home/kiki/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b703556b9b84f65b802c9d7269defea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/408 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset('glue', task, split='validation')\n",
    "# dataset = dataset.map(lambda e: tokenizer(e['sentence1'], truncation=True, padding='max_length'), batched=True)\n",
    "# dataset.set_format(type='torch', columns=['input_ids', 'token_type_ids', 'attention_mask', 'label'])\n",
    "# dataloader = torch.utils.data.DataLoader(dataset, batch_size=4)\n",
    "encoded_dataset = dataset.map(preprocess_function, batched=True)\n",
    "encoded_dataset.set_format(type='torch', columns=['input_ids', 'token_type_ids', 'attention_mask', 'label'])\n",
    "dataloader = torch.utils.data.DataLoader(encoded_dataset, batch_size=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "predictions=[]\n",
    "labels=[]\n",
    "c=0\n",
    "for _,data in enumerate(dataloader):\n",
    "    ids = data['input_ids'].to('cuda', dtype = torch.long)\n",
    "    mask = data['attention_mask'].to('cuda', dtype = torch.long)\n",
    "    token_type_ids = data['token_type_ids'].to('cuda', dtype = torch.long)\n",
    "    targets = data['label'].to('cuda', dtype = torch.long)\n",
    "\n",
    "    outputs = model(ids, mask, token_type_ids)\n",
    "    outputs = outputs.logits\n",
    "    \n",
    "    outputs=torch.argmax(outputs, dim=1).cpu()\n",
    "    targets=targets.cpu()\n",
    "    \n",
    "    outputs=[int(o.item()) for o in outputs]\n",
    "    targets=[int(o.item()) for o in targets]\n",
    "\n",
    "    predictions+=outputs\n",
    "    labels+=targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import load_metric\n",
    "# metric = load_metric('glue', 'mrpc')\n",
    "# final_score = metric.compute(predictions=predictions, references =labels )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.84      0.73       129\n",
      "           1       0.92      0.78      0.84       279\n",
      "\n",
      "    accuracy                           0.80       408\n",
      "   macro avg       0.78      0.81      0.79       408\n",
      "weighted avg       0.83      0.80      0.81       408\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(labels, predictions))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.30.0.dev0\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GLUE_TASKS = [\"cola\", \"mnli\", \"mnli-mm\", \"mrpc\", \"qnli\", \"qqp\", \"rte\", \"sst2\", \"stsb\", \"wnli\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = \"mrpc\"\n",
    "model_checkpoint = \"bert-base-cased\"\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, load_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset glue (/home/kiki/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4aae4f248054178b9cbc635d19a2159",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_244174/1389288479.py:3: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = load_metric('glue', actual_task)\n"
     ]
    }
   ],
   "source": [
    "actual_task = \"mnli\" if task == \"mnli-mm\" else task\n",
    "dataset = load_dataset(\"glue\", actual_task)\n",
    "metric = load_metric('glue', actual_task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "    \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_to_keys = {\n",
    "    \"cola\": (\"sentence\", None),\n",
    "    \"mnli\": (\"premise\", \"hypothesis\"),\n",
    "    \"mnli-mm\": (\"premise\", \"hypothesis\"),\n",
    "    \"mrpc\": (\"sentence1\", \"sentence2\"),\n",
    "    \"qnli\": (\"question\", \"sentence\"),\n",
    "    \"qqp\": (\"question1\", \"question2\"),\n",
    "    \"rte\": (\"sentence1\", \"sentence2\"),\n",
    "    \"sst2\": (\"sentence\", None),\n",
    "    \"stsb\": (\"sentence1\", \"sentence2\"),\n",
    "    \"wnli\": (\"sentence1\", \"sentence2\"),\n",
    "}\n",
    "sentence1_key, sentence2_key = task_to_keys[task]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    if sentence2_key is None:\n",
    "        return tokenizer(examples[sentence1_key], truncation=True)\n",
    "    return tokenizer(examples[sentence1_key], examples[sentence2_key], truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "671e3a45668946a08773dcaea4506d5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3668 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0eb643aa13c4bb784b7d6f68f0e9ee1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/408 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e64158ab1568461a9d131c7d5f675c9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1725 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "encoded_dataset = dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "\n",
    "num_labels = 3 if task.startswith(\"mnli\") else 1 if task==\"stsb\" else 2\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_name = \"pearson\" if task == \"stsb\" else \"matthews_correlation\" if task == \"cola\" else \"accuracy\"\n",
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "\n",
    "args = TrainingArguments(\n",
    "    f\"{model_name}-finetuned-{task}\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    save_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=metric_name,\n",
    "    push_to_hub=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    if task != \"stsb\":\n",
    "        predictions = np.argmax(predictions, axis=1)\n",
    "    else:\n",
    "        predictions = predictions[:, 0]\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning https://huggingface.co/abcp4/bert-base-cased-finetuned-mrpc into local empty directory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "validation_key = \"validation_mismatched\" if task == \"mnli-mm\" else \"validation_matched\" if task == \"mnli\" else \"validation\"\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=encoded_dataset[\"train\"],\n",
    "    eval_dataset=encoded_dataset[validation_key],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57218c19fff94363bfe1478f7d6e2edc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.5459453463554382,\n",
       " 'eval_accuracy': 0.8578431372549019,\n",
       " 'eval_f1': 0.8993055555555555,\n",
       " 'eval_runtime': 0.4866,\n",
       " 'eval_samples_per_second': 838.425,\n",
       " 'eval_steps_per_second': 53.429,\n",
       " 'epoch': 5.0}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3470f0fd3b3a4641bde2110d54183b25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/230 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "preds=trainer.predict(encoded_dataset[\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset glue (/home/kiki/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b1438990ed844a992c43616097c4845",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/408 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset('glue', task, split='validation')\n",
    "dataset = dataset.map(lambda e: tokenizer(e['sentence1'], truncation=True, padding='max_length'), batched=True)\n",
    "dataset.set_format(type='torch', columns=['input_ids', 'token_type_ids', 'attention_mask', 'label'])\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "predictions=[]\n",
    "labels=[]\n",
    "c=0\n",
    "for _,data in enumerate(dataloader):\n",
    "    ids = data['input_ids'].to('cuda', dtype = torch.long)\n",
    "    mask = data['attention_mask'].to('cuda', dtype = torch.long)\n",
    "    token_type_ids = data['token_type_ids'].to('cuda', dtype = torch.long)\n",
    "    targets = data['label'].to('cuda', dtype = torch.long)\n",
    "\n",
    "    outputs = model(ids, mask, token_type_ids)\n",
    "    outputs = outputs.logits\n",
    "    \n",
    "    outputs=torch.argmax(outputs, dim=1).cpu()\n",
    "    targets=targets.cpu()\n",
    "    \n",
    "    outputs=[int(o.item()) for o in outputs]\n",
    "    targets=[int(o.item()) for o in targets]\n",
    "\n",
    "    predictions+=outputs\n",
    "    labels+=targets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 1, 1, 0])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions=np.argmax(preds.predictions, axis=1)\n",
    "labels=encoded_dataset[\"train\"]['label']\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.98      0.98      1194\n",
      "           1       0.99      1.00      0.99      2474\n",
      "\n",
      "    accuracy                           0.99      3668\n",
      "   macro avg       0.99      0.99      0.99      3668\n",
      "weighted avg       0.99      0.99      0.99      3668\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# from datasets import load_metric\n",
    "# metric = load_metric('glue', 'mrpc')\n",
    "# final_score = metric.compute(predictions=predictions, references =labels )\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(labels, predictions))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Torch model to HuggingFaceTransformers(optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Torch model to HuggingFaceTransformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from transformers import PreTrainedModel, PretrainedConfig\n",
    "from transformers import AutoModel, AutoConfig\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from typing import List, Optional, Tuple, Union\n",
    "from transformers.modeling_outputs import (\n",
    "    SequenceClassifierOutput,\n",
    ")\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyConfig(PretrainedConfig):\n",
    "    model_type = 'mymodel'\n",
    "    def __init__(self, important_param=42, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.important_param = important_param\n",
    "\n",
    "class MyModel(PreTrainedModel):\n",
    "    config_class = MyConfig\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.config = config\n",
    "        self.model = nn.Sequential(\n",
    "                          nn.Linear(3, self.config.important_param),\n",
    "                          nn.Sigmoid(),\n",
    "                          nn.Linear(self.config.important_param, 1),\n",
    "                          nn.Sigmoid()\n",
    "                          )\n",
    "    def forward(self, input):\n",
    "        return self.model(input) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = MyConfig(4)\n",
    "model = MyModel(config)\n",
    "model.save_pretrained('./models/electra')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MyModel(\n",
       "  (model): Sequential(\n",
       "    (0): Linear(in_features=3, out_features=4, bias=True)\n",
       "    (1): Sigmoid()\n",
       "    (2): Linear(in_features=4, out_features=1, bias=True)\n",
       "    (3): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "AutoConfig.register(\"mymodel\", MyConfig)\n",
    "AutoModel.register(MyConfig, MyModel)\n",
    "\n",
    "# new_model = MyModel.from_pretrained('./models/electra')\n",
    "new_model = AutoModel.from_pretrained('./models/electra')\n",
    "new_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_model.push_to_hub(\"mymodel-test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "\n",
    "class MySeqConfig(PretrainedConfig):\n",
    "    model_type = 'myseqmodel'\n",
    "    def __init__(self, important_param=42, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.important_param = important_param\n",
    "\n",
    "class MySequenceModel(PreTrainedModel):\n",
    "    config_class = MySeqConfig\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.config = config\n",
    "        self.model = nn.Sequential(\n",
    "                          nn.Linear(3, self.config.important_param),\n",
    "                          nn.Sigmoid(),\n",
    "                          nn.Linear(self.config.important_param, 1),\n",
    "                          nn.Sigmoid()\n",
    "                          )\n",
    "    def forward(self, input_ids: Optional[torch.Tensor] = None,\n",
    "                attention_mask: Optional[torch.Tensor] = None,\n",
    "                token_type_ids: Optional[torch.Tensor] = None,\n",
    "                position_ids: Optional[torch.Tensor] = None,\n",
    "                head_mask: Optional[torch.Tensor] = None,\n",
    "                inputs_embeds: Optional[torch.Tensor] = None,\n",
    "                labels: Optional[torch.Tensor] = None,\n",
    "                output_attentions: Optional[bool] = None,\n",
    "                output_hidden_states: Optional[bool] = None,\n",
    "                return_dict: Optional[bool] = None,)-> Union[Tuple[torch.Tensor], SequenceClassifierOutput]:\n",
    "        \n",
    "        print(input_ids.shape)\n",
    "        loss = Variable(torch.zeros(1).to('cuda'), requires_grad=True)\n",
    "\n",
    "        \n",
    "        return SequenceClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=torch.zeros((3)).to('cuda'),\n",
    "            hidden_states=torch.zeros(128).to('cuda'),\n",
    "            attentions=torch.zeros(128).to('cuda'),\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = MySeqConfig(4)\n",
    "model = MySequenceModel(config)\n",
    "model.save_pretrained('./models/electra')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AutoModelForSequenceClassification.register(MySeqConfig, MySequenceModel)\n",
    "AutoConfig.register(\"myseqmodel\", MySeqConfig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=AutoModelForSequenceClassification.from_pretrained('./models/electra')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "06/24/2023 21:48:38 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: False\n",
      "06/24/2023 21:48:38 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_backend=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=True,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=no,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=2e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=output_evals/runs/Jun24_21-48-38_kiki-System-Product-Name,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=500,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_type=linear,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=5.0,\n",
      "optim=adamw_hf,\n",
      "optim_args=None,\n",
      "output_dir=output_evals,\n",
      "overwrite_output_dir=False,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=8,\n",
      "per_device_train_batch_size=32,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=['tensorboard', 'wandb'],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=output_evals,\n",
      "save_on_each_node=False,\n",
      "save_safetensors=False,\n",
      "save_steps=500,\n",
      "save_strategy=no,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      "xpu_backend=None,\n",
      ")\n",
      "06/24/2023 21:48:39 - INFO - datasets.info - Loading Dataset Infos from /home/kiki/.cache/huggingface/modules/datasets_modules/datasets/glue/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad\n",
      "06/24/2023 21:48:39 - INFO - datasets.builder - Overwrite dataset info from restored data version.\n",
      "06/24/2023 21:48:39 - INFO - datasets.info - Loading Dataset info from /home/kiki/.cache/huggingface/datasets/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad\n",
      "06/24/2023 21:48:39 - WARNING - datasets.builder - Found cached dataset glue (/home/kiki/.cache/huggingface/datasets/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n",
      "06/24/2023 21:48:39 - INFO - datasets.info - Loading Dataset info from /home/kiki/.cache/huggingface/datasets/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad\n",
      "100%|███████████████████████████████████████████| 3/3 [00:00<00:00, 2040.36it/s]\n",
      "[INFO|configuration_utils.py:669] 2023-06-24 21:48:40,194 >> loading configuration file config.json from cache at /home/kiki/.cache/huggingface/hub/models--bert-base-cased/snapshots/5532cc56f74641d4bb33641f5c76a55d11f846e0/config.json\n",
      "[INFO|configuration_utils.py:725] 2023-06-24 21:48:40,200 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"finetuning_task\": \"cola\",\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.30.0.dev0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "[INFO|configuration_utils.py:669] 2023-06-24 21:48:40,399 >> loading configuration file config.json from cache at /home/kiki/.cache/huggingface/hub/models--bert-base-cased/snapshots/5532cc56f74641d4bb33641f5c76a55d11f846e0/config.json\n",
      "[INFO|configuration_utils.py:725] 2023-06-24 21:48:40,400 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.30.0.dev0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1810] 2023-06-24 21:48:40,401 >> loading file vocab.txt from cache at /home/kiki/.cache/huggingface/hub/models--bert-base-cased/snapshots/5532cc56f74641d4bb33641f5c76a55d11f846e0/vocab.txt\n",
      "[INFO|tokenization_utils_base.py:1810] 2023-06-24 21:48:40,401 >> loading file tokenizer.json from cache at /home/kiki/.cache/huggingface/hub/models--bert-base-cased/snapshots/5532cc56f74641d4bb33641f5c76a55d11f846e0/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:1810] 2023-06-24 21:48:40,401 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1810] 2023-06-24 21:48:40,401 >> loading file special_tokens_map.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1810] 2023-06-24 21:48:40,401 >> loading file tokenizer_config.json from cache at /home/kiki/.cache/huggingface/hub/models--bert-base-cased/snapshots/5532cc56f74641d4bb33641f5c76a55d11f846e0/tokenizer_config.json\n",
      "[INFO|configuration_utils.py:669] 2023-06-24 21:48:40,401 >> loading configuration file config.json from cache at /home/kiki/.cache/huggingface/hub/models--bert-base-cased/snapshots/5532cc56f74641d4bb33641f5c76a55d11f846e0/config.json\n",
      "[INFO|configuration_utils.py:725] 2023-06-24 21:48:40,402 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.30.0.dev0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:2535] 2023-06-24 21:48:40,435 >> loading weights file model.safetensors from cache at /home/kiki/.cache/huggingface/hub/models--bert-base-cased/snapshots/5532cc56f74641d4bb33641f5c76a55d11f846e0/model.safetensors\n",
      "[WARNING|modeling_utils.py:3229] 2023-06-24 21:48:40,813 >> Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "[WARNING|modeling_utils.py:3241] 2023-06-24 21:48:40,813 >> Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "06/24/2023 21:48:40 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/kiki/.cache/huggingface/datasets/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-76c8170a2a261ff8.arrow\n",
      "Running tokenizer on dataset:   0%|             | 0/1043 [00:00<?, ? examples/s]06/24/2023 21:48:40 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/kiki/.cache/huggingface/datasets/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-a2bf5301681fb373.arrow\n",
      "06/24/2023 21:48:40 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/kiki/.cache/huggingface/datasets/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-43a8c8283e9dbbef.arrow\n",
      "06/24/2023 21:48:40 - INFO - __main__ - Sample 1824 of the training set: {'sentence': 'I acknowledged that my father, he was tight as an owl.', 'label': 0, 'idx': 1824, 'input_ids': [101, 146, 8646, 1115, 1139, 1401, 117, 1119, 1108, 3600, 1112, 1126, 19976, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
      "06/24/2023 21:48:40 - INFO - __main__ - Sample 409 of the training set: {'sentence': 'For him to do that would be a mistake.', 'label': 1, 'idx': 409, 'input_ids': [101, 1370, 1140, 1106, 1202, 1115, 1156, 1129, 170, 6223, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
      "06/24/2023 21:48:40 - INFO - __main__ - Sample 4506 of the training set: {'sentence': 'Mary sang a song, but Lee never did.', 'label': 1, 'idx': 4506, 'input_ids': [101, 2090, 6407, 170, 1461, 117, 1133, 2499, 1309, 1225, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
      "[INFO|trainer.py:763] 2023-06-24 21:48:42,393 >> The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/home/kiki/miniconda3/envs/llama/lib/python3.9/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "[INFO|trainer.py:1812] 2023-06-24 21:48:42,397 >> ***** Running training *****\n",
      "[INFO|trainer.py:1813] 2023-06-24 21:48:42,397 >>   Num examples = 8,551\n",
      "[INFO|trainer.py:1814] 2023-06-24 21:48:42,397 >>   Num Epochs = 5\n",
      "[INFO|trainer.py:1815] 2023-06-24 21:48:42,397 >>   Instantaneous batch size per device = 32\n",
      "[INFO|trainer.py:1816] 2023-06-24 21:48:42,397 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "[INFO|trainer.py:1817] 2023-06-24 21:48:42,397 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1818] 2023-06-24 21:48:42,397 >>   Total optimization steps = 1,340\n",
      "[INFO|trainer.py:1819] 2023-06-24 21:48:42,398 >>   Number of trainable parameters = 108,311,810\n",
      "[INFO|integrations.py:720] 2023-06-24 21:48:42,402 >> Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mabcp4\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.4 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/mnt/e026e6cb-2abe-4ed4-bf51-0381c5a02c4b/Servidor/LM_Pretraining/Fast_Transformers/wandb/run-20230624_214843-jlxgfmk4\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mefficient-butterfly-34\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/abcp4/huggingface\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/abcp4/huggingface/runs/jlxgfmk4\u001b[0m\n",
      "{'loss': 0.4082, 'learning_rate': 1.2537313432835823e-05, 'epoch': 1.87}        \n",
      "{'loss': 0.1739, 'learning_rate': 5.074626865671642e-06, 'epoch': 3.73}         \n",
      "100%|██████████████████████████████████████▉| 1339/1340 [03:23<00:00,  6.52it/s][INFO|trainer.py:2085] 2023-06-24 21:52:09,773 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 207.3794, 'train_samples_per_second': 206.168, 'train_steps_per_second': 6.462, 'train_loss': 0.24341670933054455, 'epoch': 5.0}\n",
      "100%|███████████████████████████████████████| 1340/1340 [03:23<00:00,  6.59it/s]\n",
      "***** train metrics *****\n",
      "  epoch                    =        5.0\n",
      "  train_loss               =     0.2434\n",
      "  train_runtime            = 0:03:27.37\n",
      "  train_samples            =       8551\n",
      "  train_samples_per_second =    206.168\n",
      "  train_steps_per_second   =      6.462\n",
      "06/24/2023 21:52:09 - INFO - __main__ - *** Evaluate ***\n",
      "[INFO|trainer.py:763] 2023-06-24 21:52:09,779 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:3217] 2023-06-24 21:52:09,780 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:3219] 2023-06-24 21:52:09,780 >>   Num examples = 1043\n",
      "[INFO|trainer.py:3222] 2023-06-24 21:52:09,780 >>   Batch size = 8\n",
      "100%|█████████████████████████████████████████| 131/131 [00:01<00:00, 66.00it/s]\n",
      "***** eval metrics *****\n",
      "  epoch                     =        5.0\n",
      "  eval_loss                 =     0.6722\n",
      "  eval_matthews_correlation =     0.6082\n",
      "  eval_runtime              = 0:00:01.99\n",
      "  eval_samples              =       1043\n",
      "  eval_samples_per_second   =     522.76\n",
      "  eval_steps_per_second     =     65.658\n",
      "06/24/2023 21:52:11 - INFO - __main__ - *** Predict ***\n",
      "[INFO|trainer.py:763] 2023-06-24 21:52:11,781 >> The following columns in the test set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:3217] 2023-06-24 21:52:11,782 >> ***** Running Prediction *****\n",
      "[INFO|trainer.py:3219] 2023-06-24 21:52:11,782 >>   Num examples = 1063\n",
      "[INFO|trainer.py:3222] 2023-06-24 21:52:11,782 >>   Batch size = 8\n",
      "100%|█████████████████████████████████████████| 133/133 [00:02<00:00, 66.26it/s]\n",
      "06/24/2023 21:52:13 - INFO - __main__ - ***** Predict results cola *****\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                      eval/loss ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      eval/matthews_correlation ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   eval/runtime ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        eval/samples_per_second ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/steps_per_second ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    train/epoch ▁▅██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              train/global_step ▁▅██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/learning_rate █▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                     train/loss █▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               train/total_flos ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               train/train_loss ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/train_runtime ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train/train_samples_per_second ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   train/train_steps_per_second ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                      eval/loss 0.6722\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      eval/matthews_correlation 0.60817\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   eval/runtime 1.9952\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        eval/samples_per_second 522.76\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/steps_per_second 65.658\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    train/epoch 5.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              train/global_step 1340\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/learning_rate 1e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                     train/loss 0.1739\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               train/total_flos 2724443033823600.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               train/train_loss 0.24342\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/train_runtime 207.3794\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train/train_samples_per_second 206.168\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   train/train_steps_per_second 6.462\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mefficient-butterfly-34\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/abcp4/huggingface/runs/jlxgfmk4\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230624_214843-jlxgfmk4/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Glue\n",
    "!rm -r output_evals/runs\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "!python glue_eval.py --task_name cola --model_name_or_path bert-base-cased --tokenizer_name bert-base-cased --output_dir output_evals --do_train  --do_eval  --do_predict --max_seq_length 124 --per_device_train_batch_size 32 --learning_rate 2e-5 --lr_scheduler_type linear --num_train_epochs 5  --save_strategy no --seed 42 "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
