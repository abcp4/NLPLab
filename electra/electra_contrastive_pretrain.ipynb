{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(3090) Para 128 tokens(Bert) e 32000 vocab: 1000 = 6 seg, 10000 = 1min, 100000 = 10min, 1m = 100min, 10m = 16h, 100m = 160h ou 6.6 dias\n",
    "#(3090) Para 84 tokens(tokenmonster salva 35%) e 24000 vocab(Bert): 1000 = 4.3s, 10000 = 43s, 100000 = 7.1m, 1m = 71min, 10m = 11.8h, 100m = 118h ou 5 dias\n",
    "#(4090) Para 84 tokens(tokenmonster salva 35%) e 24000 vocab(Electra): 1000 = 3.5s, 10000 = 35s, 100000 = 5.9m, 1m = 59min, 10m = 9.8h, 100m = 98h ou 4 dias\n",
    "#(4090) Para 84 tokens(tokenmonster salva 35%) e 16000 vocab (Electra): 1000 = 2.1s, 10000 = 21s, 100000 = 3.53m, 1m = 35.3min, 10m = 5.8h, 100m = 58h ou 2.4 dias\n",
    "\n",
    "#HIPOTÉTICO:\n",
    "#(4090) Para 24 tokens(tokenmonster salva 35%)e 24000 vocab(Electra): 1000 = , 10000 = , 100000 = 3.19m, 1m = 32min, 10m = 6.5h, 100m = 65h ou 2.7 dias\n",
    "\n",
    "#losses:\n",
    "#1m dataset, 24000 vocab electra, tokenmonster: iteration:  7800 , total_loss:  26.277032788594564\n",
    "#100k dataset, 16000 vocab electra, tokenmonster: iteration:  4980 , total_loss:  24.833836555480957\n",
    "#10m dataset, 16vocab electra , tokenmonster, 10 dom, iteration:  99990 , total_loss:  20.500934664408366\n",
    "\n",
    "\n",
    "#1m dataset, dom_electra_discriminator_16kvocab_1m  = 40min . Loss: 24.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.1+cu117\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "print(torch.__version__)\n",
    "\n",
    "import os\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "model_path='contrastive_model/'\n",
    "if not os.path.exists('models/'+model_path):\n",
    "    os.mkdir('models/'+model_path)\n",
    "# !pip install datasets transformers tqdm magic_timer pandas tokenizers matplotlib pynvml\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: WANDB_START_METHOD=thread\n",
      "env: WANDB_PROJECT=pretraining_BERT_the_notebook\n",
      "GPU: NVIDIA GeForce RTX 3090, 24576.0 MiB\n",
      "torch.cuda.is_available() = True\n",
      "DEVICE_BATCH_SIZE = 128\n",
      "gradient_accumulation_steps = 16\n",
      "batch_size = 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/home/kiki/.cache/huggingface/datasets/sradc___parquet/sradc--chunked-shuffled-wikipedia20220301en-bookcorpusopen-ff5cb88917a65ec5/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset in 1.7 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%env WANDB_START_METHOD=thread\n",
    "%env WANDB_PROJECT=pretraining_BERT_the_notebook\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Iterator\n",
    "import time\n",
    "\n",
    "import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pynvml\n",
    "import torch\n",
    "from magic_timer import MagicTimer\n",
    "from tokenizers import BertWordPieceTokenizer, Regex, normalizers\n",
    "from tqdm import tqdm\n",
    "from transformers import (\n",
    "    BertConfig,\n",
    "    BertForMaskedLM,\n",
    "    BertTokenizerFast,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "\n",
    "# Print hardware information\n",
    "pynvml.nvmlInit()\n",
    "handle = pynvml.nvmlDeviceGetHandleByIndex(0)\n",
    "gpu_name = pynvml.nvmlDeviceGetName(handle)\n",
    "gpu_mem = pynvml.nvmlDeviceGetMemoryInfo(handle).total / (1024**2)\n",
    "print(f\"GPU: {gpu_name}, {gpu_mem} MiB\")\n",
    "print(f\"{torch.cuda.is_available() = }\")\n",
    "model_training='electra'#'bert' , 'electra',; modular\n",
    "# LIMIT_DATASET = 2016 * 4  # keep small for development, set to None for full dataset\n",
    "\n",
    "\n",
    "LIMIT_DATASET = 10_000\n",
    "RANDOM_SEED = 42\n",
    "NUM_TOKENIZER_TRAINING_ITEMS = 1_000_000  # I made this up, but it seems reasonable\n",
    "if model_training=='bert':\n",
    "    VOCAB_SIZE = 32_768  # from Cramming\n",
    "    DEVICE_BATCH_SIZE = 100 # aprox 128, adjust to get near 100% gpu memory use\n",
    "    MODEL_MAX_SEQ_LEN = 128  # from Cramming\n",
    "else:\n",
    "    VOCAB_SIZE = 16_000  # tokenmonster\n",
    "    DEVICE_BATCH_SIZE = 128 # Token monster aguenta um batch size de (200-248)!! Geralmente melhora a qualidade do treino\n",
    "    MODEL_MAX_SEQ_LEN = 84  # token_monster\n",
    "\n",
    "N_DOMAINS=0\n",
    "HIER_LEVEL=0\n",
    "\n",
    "MASK_ID=4\n",
    "PAD_ID=0\n",
    "\n",
    "gradient_accumulation_steps = 2048 // DEVICE_BATCH_SIZE  # roughly based on Cramming\n",
    "batch_size = DEVICE_BATCH_SIZE * gradient_accumulation_steps\n",
    "print(f\"{DEVICE_BATCH_SIZE = }\")\n",
    "print(f\"{gradient_accumulation_steps = }\")\n",
    "print(f\"{batch_size = }\")\n",
    "RUN_DIR = Path(\"data\") / f\"run_{time.strftime('%Y%m%d-%H%M%S')}\"\n",
    "CHECKPOINT_DIR = RUN_DIR / \"training_checkpoints\"\n",
    "MODEL_DIR = RUN_DIR / \"model\"\n",
    "TOKENIZER_PATH = RUN_DIR / \"tokenizer.json\"\n",
    "TRAINER_HISTORY_PATH = RUN_DIR / \"trainer_history.json\"\n",
    "\n",
    "RUN_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "with MagicTimer() as timer:\n",
    "    dataset = datasets.load_dataset(\n",
    "        \"sradc/chunked-shuffled-wikipedia20220301en-bookcorpusopen\",\n",
    "        split=f\"train[:{LIMIT_DATASET}]\" if LIMIT_DATASET else \"train\",\n",
    "        revision=\"0e6fada2dd43136e4a3f637da41de2e596aee674\",\n",
    "    )\n",
    "print(f\"Loaded dataset in {timer}\")\n",
    "len(dataset)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feeding samples to tokenizer:   0%|          | 0/1000000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feeding samples to tokenizer: 100%|██████████| 1000000/1000000 [00:44<00:00, 22397.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Tokenizer trained in 57 seconds.\n"
     ]
    }
   ],
   "source": [
    "# !pip install tokenmonster\n",
    "from random import shuffle\n",
    "\n",
    "tokenizer = BertWordPieceTokenizer()\n",
    "tokenizer._tokenizer.normalizer = normalizers.Sequence(\n",
    "    [\n",
    "        normalizers.Replace(Regex(\"(``|'')\"), '\"'),\n",
    "        normalizers.NFD(),\n",
    "        normalizers.Lowercase(),\n",
    "        normalizers.StripAccents(),\n",
    "        normalizers.Replace(Regex(\" {2,}\"), \" \"),\n",
    "        normalizers.Replace(Regex(r\"[^\\x00-\\x7F]+\"), \"\"),\n",
    "    ]\n",
    ")  # Normalizer based on, https://github.com/JonasGeiping/cramming/blob/50bd06a65a4cd4a3dd6ee9ecce1809e1a9085374/cramming/data/tokenizer_preparation.py#L52\n",
    "def tokenizer_training_data() -> Iterator[str]:\n",
    "    for i in tqdm(\n",
    "        range(min(NUM_TOKENIZER_TRAINING_ITEMS, len(dataset))),\n",
    "        desc=\"Feeding samples to tokenizer\",\n",
    "    ):\n",
    "        yield dataset[i][\"text\"]\n",
    "\n",
    "\n",
    "with MagicTimer() as timer:\n",
    "    tokenizer.train_from_iterator(\n",
    "        tokenizer_training_data(),\n",
    "        vocab_size=VOCAB_SIZE,\n",
    "        min_frequency=2,\n",
    "    )\n",
    "print(f\"Tokenizer trained in {timer}.\")\n",
    "tokenizer.save(str(TOKENIZER_PATH))\n",
    "tokenizer = BertTokenizerFast(tokenizer_file=str(TOKENIZER_PATH))\n",
    "# tokenizer.unk_token,tokenizer.unk_token_id,tokenizer.sep_token,tokenizer.sep_token_id,tokenizer.pad_token,tokenizer.pad_token_id,tokenizer.cls_token,tokenizer.cls_token_id,tokenizer.mask_token,tokenizer.mask_token_id\n",
    "\n",
    "class HFTokenizedDataset(torch.utils.data.Dataset):\n",
    "    \"This wraps the dataset and tokenizes it, ready for the model\"\n",
    "\n",
    "    def __init__(self, dataset, tokenizer):\n",
    "        self.dataset = dataset\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        inps= self.tokenizer.encode(\n",
    "            self.dataset[i][\"text\"],\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            max_length=MODEL_MAX_SEQ_LEN - 2,\n",
    "            padding=\"max_length\",\n",
    "            return_special_tokens_mask=True,\n",
    "        )[0, ...]\n",
    "        return {'input_ids':inps}\n",
    "\n",
    "class TokenizedDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def convert_tokens_to_ids(self,s):\n",
    "        s=norm.normalize_str(s)\n",
    "        tokens = vocab.tokenize(s)\n",
    "        tokens=tokens[:MODEL_MAX_SEQ_LEN - 2]\n",
    "        tokens=torch.Tensor(tokens).long()\n",
    "        return tokens\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        s=norm.normalize_str(self.dataset[i][\"text\"])\n",
    "        tokens = vocab.tokenize(s).tolist()\n",
    "        \n",
    "        #trucate\n",
    "        tokens=tokens[:MODEL_MAX_SEQ_LEN - 2]\n",
    "        l=len(tokens)\n",
    "        for j in range(l,MODEL_MAX_SEQ_LEN - 2):\n",
    "            tokens.append(0)\n",
    "\n",
    "        tokens=torch.Tensor(tokens)\n",
    "        if MODEL_MAX_SEQ_LEN - 2>l:\n",
    "            att_mask=np.concatenate((np.ones(l),np.zeros(MODEL_MAX_SEQ_LEN - 2-l)))\n",
    "        else:\n",
    "            att_mask=np.ones(tokens.shape[0])\n",
    "        # print(l,att_mask.shape[0],MODEL_MAX_SEQ_LEN - 2)\n",
    "        assert tokens.shape[0]==att_mask.shape[0]\n",
    "\n",
    "        input_ids=tokens\n",
    "\n",
    "        input_ids=torch.as_tensor(input_ids,dtype=torch.long)\n",
    "        # labels = torch.zeros(input_ids.shape)\n",
    "        # probability_matrix = torch.full(labels.shape, 0.15)\n",
    "        # masked_indices = torch.bernoulli(probability_matrix).bool()\n",
    "        # indices_replaced = torch.bernoulli(torch.full(labels.shape, 1.0)).bool() & masked_indices\n",
    "        # labels[indices_replaced] =1\n",
    "        # mask=torch.as_tensor(labels,dtype=torch.bool)\n",
    "        \n",
    "        d={'input_ids':input_ids, \n",
    "            # 'token_type_ids':token_type_ids, \n",
    "            # 'attention_mask':attention_mask,\n",
    "            # 'mask':mask\n",
    "            'domain':0,\n",
    "            'subdomain1':0,\n",
    "            }\n",
    "        return d\n",
    "\n",
    "def texts2mlm(texts,domain,subdomain1=0):\n",
    "    input_ids=[]\n",
    "    # token_type_ids=[]\n",
    "    # attention_mask=[]\n",
    "    # data=[]\n",
    "    inputs=[]\n",
    "    # masks=[]\n",
    "    for t in texts:\n",
    "        s=norm.normalize_str(t[\"text\"])\n",
    "        tokens = vocab.tokenize(s).tolist()\n",
    "        \n",
    "        #trucate\n",
    "        tokens=tokens[:MODEL_MAX_SEQ_LEN - 2]\n",
    "        l=len(tokens)\n",
    "        for j in range(l,MODEL_MAX_SEQ_LEN - 2):\n",
    "            tokens.append(0)\n",
    "\n",
    "        tokens=torch.Tensor(tokens)\n",
    "        if MODEL_MAX_SEQ_LEN - 2>l:\n",
    "            att_mask=np.concatenate((np.ones(l),np.zeros(MODEL_MAX_SEQ_LEN - 2-l)))\n",
    "        else:\n",
    "            att_mask=np.ones(tokens.shape[0])\n",
    "        # print(l,att_mask.shape[0],MODEL_MAX_SEQ_LEN - 2)\n",
    "        assert tokens.shape[0]==att_mask.shape[0]\n",
    "\n",
    "        input_ids=tokens\n",
    "\n",
    "        input_ids=torch.as_tensor(input_ids,dtype=torch.long)\n",
    "        # labels = torch.zeros(input_ids.shape)\n",
    "        # probability_matrix = torch.full(labels.shape, 0.15)\n",
    "        # masked_indices = torch.bernoulli(probability_matrix).bool()\n",
    "        # indices_replaced = torch.bernoulli(torch.full(labels.shape, 1.0)).bool() & masked_indices\n",
    "        # labels[indices_replaced] =1\n",
    "        # mask=torch.as_tensor(labels,dtype=torch.bool)\n",
    "        inputs.append(input_ids)\n",
    "        # masks.append(mask)\n",
    "    \n",
    "    return {'input_ids':torch.stack(inputs),'domain':domain,'subdomain1':(domain*N_DOMAINS)+subdomain1}\n",
    "\n",
    "def divide_chunks(l, n):\n",
    "    return [l[i:i + n] for i in range(0, len(l), n)] \n",
    "\n",
    "class CustomDataloader(torch.utils.data.Dataset):\n",
    "    def __init__(self,dataset,cluster_labels,n_domains) -> None:\n",
    "        self.dataset=dataset\n",
    "        self.cluster_labels=cluster_labels\n",
    "        self.bin_dataset={}\n",
    "        self.domains=[i for i in range(n_domains)]\n",
    "        self.batch_ordering=[]\n",
    "        self.current_domain=0\n",
    "        self.bs=DEVICE_BATCH_SIZE\n",
    "        self.fill_bins()\n",
    "\n",
    "    def fill_bins(self):\n",
    "        self.bin_dataset={}\n",
    "        for i,c in enumerate(self.cluster_labels):\n",
    "            if c not in self.bin_dataset:\n",
    "                self.bin_dataset[c]=[i]\n",
    "            else:\n",
    "                self.bin_dataset[c].append(i)\n",
    "        n_batches=0\n",
    "        domains=[]\n",
    "        for i in range(len(self.domains)):\n",
    "            self.bin_dataset[i]=divide_chunks(self.bin_dataset[i],DEVICE_BATCH_SIZE)\n",
    "            for k in range(len(self.bin_dataset[i])):\n",
    "                domains.append((i,k))\n",
    "        shuffle(domains)\n",
    "        self.batch_ordering=domains\n",
    "\n",
    "    def __getitem__(self, i) -> torch.Tensor:\n",
    "        indexes=self.bin_dataset[self.batch_ordering[i][0]][self.batch_ordering[i][1]]\n",
    "        batch_data=[]\n",
    "        for j in indexes:\n",
    "            batch_data.append(self.dataset[j])\n",
    "        # print('batch_data:',batch_data)\n",
    "        batch_data=texts2mlm(batch_data,self.batch_ordering[i][0])\n",
    "        # batch_data = torch.from_numpy(a).long()\n",
    "        return batch_data\n",
    "\n",
    "    def __len__(self):\n",
    "        #colocar uma margem de erro pra baixo\n",
    "        return len(self.batch_ordering)\n",
    "\n",
    "\n",
    "class CustomDataloaderLevel1(torch.utils.data.Dataset):\n",
    "    def __init__(self,dataset,cluster_labels,idxs_cluster_labels_level1,n_domains,n_subdomains) -> None:\n",
    "        self.dataset=dataset\n",
    "        self.cluster_labels=cluster_labels\n",
    "        self.idxs_cluster_labels_level1=idxs_cluster_labels_level1\n",
    "        self.bin_dataset={}\n",
    "        self.domains=[i for i in range(n_domains)]\n",
    "        self.subdomains=[i for i in range(n_subdomains)]\n",
    "        self.batch_ordering=[]\n",
    "        self.current_domain=0\n",
    "        self.bs=DEVICE_BATCH_SIZE\n",
    "        self.fill_bins()\n",
    "\n",
    "    def fill_bins(self):\n",
    "        self.bin_dataset={}\n",
    "        # for i,c in enumerate(self.cluster_labels):\n",
    "        #     if c not in self.bin_dataset:\n",
    "        #         self.bin_dataset[c]=[i]\n",
    "        #     else:\n",
    "        #         self.bin_dataset[c].append(i)\n",
    "        for dom,txt_id,subdom in self.idxs_cluster_labels_level1:\n",
    "            id_dom_sub=str(dom)+'_'+str(subdom)#identifica o dominio e o subdominio\n",
    "            if  id_dom_sub not in self.bin_dataset:\n",
    "                self.bin_dataset[id_dom_sub]=[txt_id]\n",
    "            else:\n",
    "                self.bin_dataset[id_dom_sub].append(txt_id)\n",
    "\n",
    "        n_batches=0\n",
    "        data_hier_idxs=[]\n",
    "        for i in range(len(self.domains)):\n",
    "            for j in range(len(self.subdomains)):\n",
    "                id_dom_sub=str(i)+'_'+str(j)#identifica o dominio e o subdominio\n",
    "                #OBS: subdivide a lista de todos os ids do msm dom e subdom em batches. O bin dataset e um dicionario\n",
    "                #que mapeia dom e subdom -> lista de batches\n",
    "                self.bin_dataset[id_dom_sub]=divide_chunks(self.bin_dataset[id_dom_sub],DEVICE_BATCH_SIZE)\n",
    "                for k in range(len(self.bin_dataset[id_dom_sub])):\n",
    "                    data_hier_idxs.append((id_dom_sub,k))\n",
    "        shuffle(data_hier_idxs)\n",
    "        #batch ordering é um vetor de tuplas de 2 elementos. O primeiro é o dominio e subdominio desse batch\n",
    "        # O segundo elemento é a posicao do batch no bin dataset. \n",
    "        # Ex: ('0_0',0) se lê: Batch com dominio e subdominio 0, esse batch é o numero 0 na lista de batches do bin_dataset para esse dom e subdom.\n",
    "        self.batch_ordering=data_hier_idxs\n",
    "\n",
    "    def __getitem__(self, i) -> torch.Tensor:\n",
    "        #o bin dataset tem como chaves o dominio e subdominio, e o valor é uma lista de batches de msm dom e subdom\n",
    "        indexes=self.bin_dataset[self.batch_ordering[i][0]][self.batch_ordering[i][1]]\n",
    "        batch_data=[]\n",
    "        for j in indexes:\n",
    "            batch_data.append(self.dataset[int(j)])\n",
    "        # print('batch_data:',batch_data)\n",
    "        if len(self.subdomains)>0:\n",
    "            d0,d1=self.batch_ordering[i][0].split('_')\n",
    "            batch_data=texts2mlm(batch_data,int(d0),int(d1))\n",
    "        else:\n",
    "            batch_data=texts2mlm(batch_data,self.batch_ordering[i][0])\n",
    "        # batch_data = torch.from_numpy(a).long()\n",
    "        return batch_data\n",
    "\n",
    "    def __len__(self):\n",
    "        #colocar uma margem de erro pra baixo\n",
    "        return len(self.batch_ordering)\n",
    "\n",
    "def get_vocab():\n",
    "    #### TokenMonster BRRR!!!\n",
    "    import tokenmonster\n",
    "    vocab = tokenmonster.load(\"englishcode-16000-balanced-v1\")\n",
    "    # vocab = TokenMonster.load('tokenizers_monster/english-24000-capcode.vocab')\n",
    "\n",
    "    norm=normalizers.Sequence(\n",
    "        [\n",
    "            normalizers.Replace(Regex(\"(``|'')\"), '\"'),\n",
    "            normalizers.NFD(),\n",
    "            normalizers.Lowercase(),\n",
    "            normalizers.StripAccents(),\n",
    "            normalizers.Replace(Regex(\" {2,}\"), \" \"),\n",
    "            normalizers.Replace(Regex(r\"[^\\x00-\\x7F]+\"), \"\"),\n",
    "        ]\n",
    "    )\n",
    "    vocab.modify(\"[EOS]\")\n",
    "    vocab.modify(\"[UNK]\")\n",
    "    vocab.modify(\"[SEP]\")\n",
    "    vocab.modify(\"[PAD]\")\n",
    "    vocab.modify(\"[CLS]\")\n",
    "    vocab.modify(\"[MASK]\")\n",
    "    return norm,vocab\n",
    "norm,vocab=get_vocab()\n",
    "MASK_ID=vocab.tokenize(\"[MASK]\")[0]\n",
    "PAD_ID=vocab.tokenize(\"[PAD]\")[0]\n",
    "\n",
    "if model_training=='bert':\n",
    "    tokenized_dataset = HFTokenizedDataset(dataset, tokenizer)\n",
    "elif model_training=='electra':\n",
    "    tokenized_dataset = TokenizedDataset(dataset)\n",
    "\n",
    "# tokenizer.mask_token,tokenizer.convert_tokens_to_ids(tokenizer.mask_token)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Electra"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from functools import reduce\n",
    "from collections import namedtuple\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# constants\n",
    "\n",
    "Results = namedtuple('Results', [\n",
    "    'loss',\n",
    "    'mlm_loss',\n",
    "    'disc_loss',\n",
    "    'gen_acc',\n",
    "    'disc_acc',\n",
    "    'disc_labels',\n",
    "    'disc_predictions'\n",
    "])\n",
    "\n",
    "# helpers\n",
    "def log(t, eps=1e-9):\n",
    "    return torch.log(t + eps)\n",
    "\n",
    "def gumbel_noise(t):\n",
    "    noise = torch.zeros_like(t).uniform_(0, 1)\n",
    "    return -log(-log(noise))\n",
    "\n",
    "def gumbel_sample(t, temperature = 1.):\n",
    "    return ((t / temperature) + gumbel_noise(t)).argmax(dim=-1)\n",
    "\n",
    "def prob_mask_like(t, prob):\n",
    "    return torch.zeros_like(t).float().uniform_(0, 1) < prob\n",
    "\n",
    "def mask_with_tokens(t, token_ids):\n",
    "    init_no_mask = torch.full_like(t, False, dtype=torch.bool)\n",
    "    mask = reduce(lambda acc, el: acc | (t == el), token_ids, init_no_mask)\n",
    "    return mask\n",
    "\n",
    "def get_mask_subset_with_prob(mask, prob):\n",
    "    batch, seq_len, device = *mask.shape, mask.device\n",
    "    max_masked = math.ceil(prob * seq_len)\n",
    "\n",
    "    num_tokens = mask.sum(dim=-1, keepdim=True)\n",
    "    mask_excess = (mask.cumsum(dim=-1) > (num_tokens * prob).ceil())\n",
    "    mask_excess = mask_excess[:, :max_masked]\n",
    "\n",
    "    rand = torch.rand((batch, seq_len), device=device).masked_fill(~mask, -1e9)\n",
    "    _, sampled_indices = rand.topk(max_masked, dim=-1)\n",
    "    sampled_indices = (sampled_indices + 1).masked_fill_(mask_excess, 0)\n",
    "\n",
    "    new_mask = torch.zeros((batch, seq_len + 1), device=device)\n",
    "    new_mask.scatter_(-1, sampled_indices, 1)\n",
    "    return new_mask[:, 1:].bool()\n",
    "\n",
    "\n",
    "def device_as(t1, t2):\n",
    "   \"\"\"\n",
    "   Moves t1 to the device of t2\n",
    "   \"\"\"\n",
    "   return t1.to(t2.device)\n",
    "\n",
    "class ContrastiveLoss(nn.Module):\n",
    "    \n",
    "   \"\"\"\n",
    "   Vanilla Contrastive loss, also called InfoNceLoss as in SimCLR paper\n",
    "   \"\"\"\n",
    "   def __init__(self, batch_size, temperature=0.5):\n",
    "       super().__init__()\n",
    "       self.batch_size = batch_size\n",
    "       self.temperature = temperature\n",
    "       self.mask = (~torch.eye(batch_size * 2, batch_size * 2, dtype=bool)).float()\n",
    "\n",
    "   def calc_similarity_batch(self, a, b):\n",
    "       representations = torch.cat([a, b], dim=0)\n",
    "       return F.cosine_similarity(representations.unsqueeze(1), representations.unsqueeze(0), dim=2)\n",
    "\n",
    "   def forward(self, proj_1, proj_2):\n",
    "       \"\"\"\n",
    "       proj_1 and proj_2 are batched embeddings [batch, embedding_dim]\n",
    "       where corresponding indices are pairs\n",
    "       z_i, z_j in the SimCLR paper\n",
    "       \"\"\"\n",
    "       batch_size = proj_1.shape[0]\n",
    "       z_i = F.normalize(proj_1, p=2, dim=1)\n",
    "       z_j = F.normalize(proj_2, p=2, dim=1)\n",
    "\n",
    "       similarity_matrix = self.calc_similarity_batch(z_i, z_j)\n",
    "\n",
    "       sim_ij = torch.diag(similarity_matrix, batch_size)\n",
    "       sim_ji = torch.diag(similarity_matrix, -batch_size)\n",
    "\n",
    "       positives = torch.cat([sim_ij, sim_ji], dim=0)\n",
    "\n",
    "       nominator = torch.exp(positives / self.temperature)\n",
    "\n",
    "       denominator = device_as(self.mask, similarity_matrix) * torch.exp(similarity_matrix / self.temperature)\n",
    "\n",
    "       all_losses = -torch.log(nominator / torch.sum(denominator, dim=1))\n",
    "       loss = torch.sum(all_losses) / (2 * self.batch_size)\n",
    "       return loss\n",
    "\n",
    "class SimCLR_Loss(nn.Module):\n",
    "    def __init__(self, batch_size, temperature):\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.temperature = temperature\n",
    "\n",
    "        self.mask = self.mask_correlated_samples(batch_size)\n",
    "        self.criterion = nn.CrossEntropyLoss(reduction=\"sum\")\n",
    "        self.similarity_f = nn.CosineSimilarity(dim=2)\n",
    "\n",
    "    def mask_correlated_samples(self, batch_size):\n",
    "        N = 2 * batch_size\n",
    "        mask = torch.ones((N, N), dtype=bool)\n",
    "        mask = mask.fill_diagonal_(0)\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            mask[i, batch_size + i] = 0\n",
    "            mask[batch_size + i, i] = 0\n",
    "        return mask\n",
    "\n",
    "    def forward(self, z_i, z_j):\n",
    "\n",
    "        N = 2 * self.batch_size\n",
    "\n",
    "        z = torch.cat((z_i, z_j), dim=0)\n",
    "\n",
    "        sim = self.similarity_f(z.unsqueeze(1), z.unsqueeze(0)) / self.temperature\n",
    "\n",
    "        sim_i_j = torch.diag(sim, self.batch_size)\n",
    "        sim_j_i = torch.diag(sim, -self.batch_size)\n",
    "        \n",
    "        # We have 2N samples, but with Distributed training every GPU gets N examples too, resulting in: 2xNxN\n",
    "        positive_samples = torch.cat((sim_i_j, sim_j_i), dim=0).reshape(N, 1)\n",
    "        negative_samples = sim[self.mask].reshape(N, -1)\n",
    "        \n",
    "        #SIMCLR\n",
    "        labels = torch.from_numpy(np.array([0]*N)).reshape(-1).to(positive_samples.device).long() #.float()        \n",
    "        logits = torch.cat((positive_samples, negative_samples), dim=1)\n",
    "        loss = self.criterion(logits, labels)\n",
    "        loss /= N\n",
    "        \n",
    "        return loss\n",
    "\n",
    "# hidden layer extractor class, for magically adding adapter to language model to be pretrained\n",
    "class HiddenLayerExtractor(nn.Module):\n",
    "    def __init__(self, net, layer = -2):\n",
    "        super().__init__()\n",
    "        self.net = net\n",
    "        self.layer = layer\n",
    "\n",
    "        self.hidden = None\n",
    "        self.hook_registered = False\n",
    "\n",
    "    def _find_layer(self):\n",
    "        if type(self.layer) == str:\n",
    "            modules = dict([*self.net.named_modules()])\n",
    "            return modules.get(self.layer, None)\n",
    "        elif type(self.layer) == int:\n",
    "            children = [*self.net.children()]\n",
    "            return children[self.layer]\n",
    "        return None\n",
    "\n",
    "    def _hook(self, _, __, output):\n",
    "        self.hidden = output\n",
    "\n",
    "    def _register_hook(self):\n",
    "        layer = self._find_layer()\n",
    "        assert layer is not None, f'hidden layer ({self.layer}) not found'\n",
    "        handle = layer.register_forward_hook(self._hook)\n",
    "        self.hook_registered = True\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.layer == -1:\n",
    "            return self.net(x['input'],x['domain'])\n",
    "\n",
    "        if not self.hook_registered:\n",
    "            self._register_hook()\n",
    "\n",
    "        _ = self.net(x['input'],x['domain'])\n",
    "        hidden = self.hidden\n",
    "        self.hidden = None\n",
    "        assert hidden is not None, f'hidden layer {self.layer} never emitted an output'\n",
    "        return hidden\n",
    "    \n",
    "class Electra(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        generator,\n",
    "        discriminator,\n",
    "        *,\n",
    "        num_tokens = None,\n",
    "        discr_dim = -1,\n",
    "        discr_layer = -1,\n",
    "        mask_prob = 0.15,\n",
    "        replace_prob = 0.85,\n",
    "        random_token_prob = 0.,\n",
    "        mask_token_id = 2,\n",
    "        pad_token_id = 0,\n",
    "        mask_ignore_token_ids = [],\n",
    "        disc_weight = 50.,\n",
    "        gen_weight = 1.,\n",
    "        temperature = 1.,\n",
    "        ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.generator = generator\n",
    "        self.discriminator = discriminator\n",
    "\n",
    "        if discr_dim > 0:\n",
    "            self.discriminator = nn.Sequential(\n",
    "                HiddenLayerExtractor(discriminator, layer = discr_layer),\n",
    "                nn.Linear(discr_dim, 1)\n",
    "            )\n",
    "\n",
    "        # mlm related probabilities\n",
    "        self.mask_prob = mask_prob\n",
    "        self.replace_prob = replace_prob\n",
    "\n",
    "        self.num_tokens = num_tokens\n",
    "        self.random_token_prob = random_token_prob\n",
    "\n",
    "        # token ids\n",
    "        self.pad_token_id = pad_token_id\n",
    "        self.mask_token_id = mask_token_id\n",
    "        self.mask_ignore_token_ids = set([*mask_ignore_token_ids, pad_token_id])\n",
    "\n",
    "        # sampling temperature\n",
    "        self.temperature = temperature\n",
    "\n",
    "        # loss weights\n",
    "        self.disc_weight = disc_weight\n",
    "        self.gen_weight = gen_weight\n",
    "\n",
    "\n",
    "    def forward(self, inputs, **kwargs):\n",
    "        input=inputs['input']\n",
    "        b, t = input.shape\n",
    "\n",
    "        replace_prob = prob_mask_like(input, self.replace_prob)\n",
    "\n",
    "        # do not mask [pad] tokens, or any other tokens in the tokens designated to be excluded ([cls], [sep])\n",
    "        # also do not include these special tokens in the tokens chosen at random\n",
    "        no_mask = mask_with_tokens(input, self.mask_ignore_token_ids)\n",
    "        mask = get_mask_subset_with_prob(~no_mask, self.mask_prob)\n",
    "\n",
    "        # get mask indices\n",
    "        mask_indices = torch.nonzero(mask, as_tuple=True)\n",
    "\n",
    "        # mask input with mask tokens with probability of `replace_prob` (keep tokens the same with probability 1 - replace_prob)\n",
    "        masked_input = input.clone().detach()\n",
    "\n",
    "        # set inverse of mask to padding tokens for labels\n",
    "        gen_labels = input.masked_fill(~mask, self.pad_token_id)\n",
    "\n",
    "        # clone the mask, for potential modification if random tokens are involved\n",
    "        # not to be mistakened for the mask above, which is for all tokens, whether not replaced nor replaced with random tokens\n",
    "        masking_mask = mask.clone()\n",
    "\n",
    "        # if random token probability > 0 for mlm\n",
    "        if self.random_token_prob > 0:\n",
    "            assert self.num_tokens is not None, 'Number of tokens (num_tokens) must be passed to Electra for randomizing tokens during masked language modeling'\n",
    "\n",
    "            random_token_prob = prob_mask_like(input, self.random_token_prob)\n",
    "            random_tokens = torch.randint(0, self.num_tokens, input.shape, device=input.device)\n",
    "            random_no_mask = mask_with_tokens(random_tokens, self.mask_ignore_token_ids)\n",
    "            random_token_prob &= ~random_no_mask\n",
    "            masked_input = torch.where(random_token_prob, random_tokens, masked_input)\n",
    "\n",
    "            # remove random token prob mask from masking mask\n",
    "            masking_mask = masking_mask & ~random_token_prob\n",
    "\n",
    "        # [mask] input\n",
    "        masked_input = masked_input.masked_fill(masking_mask * replace_prob, self.mask_token_id)\n",
    "\n",
    "        # get generator output and get mlm loss\n",
    "        logits = self.generator(masked_input, **kwargs)\n",
    "\n",
    "        mlm_loss = F.cross_entropy(\n",
    "            logits.transpose(1, 2),\n",
    "            gen_labels,\n",
    "            ignore_index = self.pad_token_id\n",
    "        )\n",
    "\n",
    "        # use mask from before to select logits that need sampling\n",
    "        sample_logits = logits[mask_indices]\n",
    "\n",
    "        # sample\n",
    "        sampled = gumbel_sample(sample_logits, temperature = self.temperature)\n",
    "\n",
    "        # scatter the sampled values back to the input\n",
    "        disc_input = input.clone()\n",
    "        disc_input[mask_indices] = sampled.detach()\n",
    "\n",
    "        # generate discriminator labels, with replaced as True and original as False\n",
    "        disc_labels = (input != disc_input).float().detach()\n",
    "\n",
    "        # get discriminator predictions of replaced / original\n",
    "        non_padded_indices = torch.nonzero(input != self.pad_token_id, as_tuple=True)\n",
    "\n",
    "        # get discriminator output and binary cross entropy loss\n",
    "        disc_logits = self.discriminator({'input':disc_input,'domain':inputs['domain']}, **kwargs)\n",
    "        disc_logits = disc_logits.reshape_as(disc_labels)\n",
    "\n",
    "        disc_loss = F.binary_cross_entropy_with_logits(\n",
    "            disc_logits[non_padded_indices],\n",
    "            disc_labels[non_padded_indices]\n",
    "        )\n",
    "\n",
    "        # gather metrics\n",
    "        with torch.no_grad():\n",
    "            gen_predictions = torch.argmax(logits, dim=-1)\n",
    "            disc_predictions = torch.round((torch.sign(disc_logits) + 1.0) * 0.5)\n",
    "            gen_acc = (gen_labels[mask] == gen_predictions[mask]).float().mean()\n",
    "            disc_acc = 0.5 * (disc_labels[mask] == disc_predictions[mask]).float().mean() + 0.5 * (disc_labels[~mask] == disc_predictions[~mask]).float().mean()\n",
    "\n",
    "        # return weighted sum of losses\n",
    "        return Results(self.gen_weight * mlm_loss + self.disc_weight * disc_loss, mlm_loss, disc_loss, gen_acc, disc_acc, disc_labels, disc_predictions)\n",
    "    \n",
    "\n",
    "class ElectraConstrative(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        generator,\n",
    "        discriminator,\n",
    "        *,\n",
    "        num_tokens = None,\n",
    "        discr_dim = -1,\n",
    "        discr_layer = -1,\n",
    "        mask_prob = 0.15,\n",
    "        replace_prob = 0.85,\n",
    "        random_token_prob = 0.,\n",
    "        mask_token_id = 2,\n",
    "        pad_token_id = 0,\n",
    "        mask_ignore_token_ids = [],\n",
    "        disc_weight = 50.,\n",
    "        gen_weight = 1.,\n",
    "        temperature = 1.,\n",
    "        ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.generator = generator\n",
    "        self.discriminator = discriminator\n",
    "\n",
    "        if discr_dim > 0:\n",
    "            self.discriminator = nn.Sequential(\n",
    "                HiddenLayerExtractor(discriminator, layer = discr_layer),\n",
    "                nn.Linear(discr_dim, 1)\n",
    "            )\n",
    "\n",
    "        # mlm related probabilities\n",
    "        self.mask_prob = mask_prob\n",
    "        self.replace_prob = replace_prob\n",
    "\n",
    "        self.num_tokens = num_tokens\n",
    "        self.random_token_prob = random_token_prob\n",
    "\n",
    "        # token ids\n",
    "        self.pad_token_id = pad_token_id\n",
    "        self.mask_token_id = mask_token_id\n",
    "        self.mask_ignore_token_ids = set([*mask_ignore_token_ids, pad_token_id])\n",
    "\n",
    "        # sampling temperature\n",
    "        self.temperature = temperature\n",
    "\n",
    "        # loss weights\n",
    "        self.disc_weight = disc_weight\n",
    "        self.gen_weight = gen_weight\n",
    "\n",
    "        # self.const_loss = SimCLR_Loss(batch_size=DEVICE_BATCH_SIZE, temperature=0.1)\n",
    "        self.const_loss = ContrastiveLoss(batch_size=DEVICE_BATCH_SIZE, temperature=0.1)\n",
    "\n",
    "\n",
    "    def forward(self, inputs, **kwargs):\n",
    "        input=inputs['input']\n",
    "        b, t = input.shape\n",
    "\n",
    "        replace_prob = prob_mask_like(input, self.replace_prob)\n",
    "\n",
    "        # do not mask [pad] tokens, or any other tokens in the tokens designated to be excluded ([cls], [sep])\n",
    "        # also do not include these special tokens in the tokens chosen at random\n",
    "        no_mask = mask_with_tokens(input, self.mask_ignore_token_ids)\n",
    "        mask = get_mask_subset_with_prob(~no_mask, self.mask_prob)\n",
    "\n",
    "        # get mask indices\n",
    "        mask_indices = torch.nonzero(mask, as_tuple=True)\n",
    "\n",
    "        # mask input with mask tokens with probability of `replace_prob` (keep tokens the same with probability 1 - replace_prob)\n",
    "        masked_input = input.clone().detach()\n",
    "\n",
    "        # set inverse of mask to padding tokens for labels\n",
    "        gen_labels = input.masked_fill(~mask, self.pad_token_id)\n",
    "\n",
    "        # clone the mask, for potential modification if random tokens are involved\n",
    "        # not to be mistakened for the mask above, which is for all tokens, whether not replaced nor replaced with random tokens\n",
    "        masking_mask = mask.clone()\n",
    "\n",
    "        # if random token probability > 0 for mlm\n",
    "        if self.random_token_prob > 0:\n",
    "            assert self.num_tokens is not None, 'Number of tokens (num_tokens) must be passed to Electra for randomizing tokens during masked language modeling'\n",
    "\n",
    "            random_token_prob = prob_mask_like(input, self.random_token_prob)\n",
    "            random_tokens = torch.randint(0, self.num_tokens, input.shape, device=input.device)\n",
    "            random_no_mask = mask_with_tokens(random_tokens, self.mask_ignore_token_ids)\n",
    "            random_token_prob &= ~random_no_mask\n",
    "            masked_input = torch.where(random_token_prob, random_tokens, masked_input)\n",
    "\n",
    "            # remove random token prob mask from masking mask\n",
    "            masking_mask = masking_mask & ~random_token_prob\n",
    "\n",
    "        # [mask] input\n",
    "        masked_input = masked_input.masked_fill(masking_mask * replace_prob, self.mask_token_id)\n",
    "\n",
    "        # get generator output and get mlm loss\n",
    "        logits = self.generator(masked_input, **kwargs)\n",
    "\n",
    "        mlm_loss = F.cross_entropy(\n",
    "            logits.transpose(1, 2),\n",
    "            gen_labels,\n",
    "            ignore_index = self.pad_token_id\n",
    "        )\n",
    "\n",
    "        # use mask from before to select logits that need sampling\n",
    "        sample_logits = logits[mask_indices]\n",
    "\n",
    "        # sample\n",
    "        sampled = gumbel_sample(sample_logits, temperature = self.temperature)\n",
    "\n",
    "        # scatter the sampled values back to the input\n",
    "        disc_input = input.clone()\n",
    "        disc_input[mask_indices] = sampled.detach()\n",
    "\n",
    "        # generate discriminator labels, with replaced as True and original as False\n",
    "        disc_labels = (input != disc_input).float().detach()\n",
    "\n",
    "        # get discriminator predictions of replaced / original\n",
    "        non_padded_indices = torch.nonzero(input != self.pad_token_id, as_tuple=True)\n",
    "\n",
    "        \n",
    "        #Contrastive Learning\n",
    "\n",
    "        #get generator input/output embeddings from discriminator \n",
    "        # embeddings_1 = self.discriminator[0]({'input':input,'domain':inputs['domain']}, **kwargs)\n",
    "        # embeddings_2 = self.discriminator[0]({'input':disc_input,'domain':inputs['domain']}, **kwargs)\n",
    "\n",
    "        #simcse: both embeddings are from the same sentence(output from generator), with dropout as noise\n",
    "        embeddings_1 = self.discriminator[0]({'input':disc_input,'domain':inputs['domain']}, **kwargs)\n",
    "        embeddings_2 = self.discriminator[0]({'input':disc_input,'domain':inputs['domain']}, **kwargs)\n",
    "\n",
    "        #average embeddings\n",
    "        sent_embeddings_1 = torch.mean(embeddings_1, dim=1)\n",
    "        sent_embeddings_2 = torch.mean(embeddings_2, dim=1)\n",
    "        \n",
    "        constrative_loss = self.const_loss.forward(sent_embeddings_1,sent_embeddings_2)\n",
    "        # print(sent_embeddings_1)\n",
    "        # print(sent_embeddings_2)\n",
    "        # print(constrative_loss)\n",
    "        # a=2/0\n",
    "\n",
    "        # get discriminator output and binary cross entropy loss\n",
    "        disc_logits = self.discriminator[1](embeddings_2)\n",
    "        disc_logits = disc_logits.reshape_as(disc_labels)\n",
    "\n",
    "        disc_loss = F.binary_cross_entropy_with_logits(\n",
    "            disc_logits[non_padded_indices],\n",
    "            disc_labels[non_padded_indices]\n",
    "        )\n",
    "\n",
    "        disc_loss+=disc_loss+constrative_loss\n",
    "\n",
    "\n",
    "        # gather metrics\n",
    "        with torch.no_grad():\n",
    "            gen_predictions = torch.argmax(logits, dim=-1)\n",
    "            disc_predictions = torch.round((torch.sign(disc_logits) + 1.0) * 0.5)\n",
    "            gen_acc = (gen_labels[mask] == gen_predictions[mask]).float().mean()\n",
    "            disc_acc = 0.5 * (disc_labels[mask] == disc_predictions[mask]).float().mean() + 0.5 * (disc_labels[~mask] == disc_predictions[~mask]).float().mean()\n",
    "\n",
    "        # return weighted sum of losses\n",
    "        return Results(self.gen_weight * mlm_loss + self.disc_weight * disc_loss, mlm_loss, disc_loss, gen_acc, disc_acc, disc_labels, disc_predictions)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-A100 GPU detected, using math or mem efficient attention if input tensor is on cuda\n"
     ]
    }
   ],
   "source": [
    "# pip install x_transformers reformer_pytorch accelerate einops\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from reformer_pytorch import ReformerLM\n",
    "from x_transformers import TransformerWrapper, Decoder,Encoder\n",
    "\n",
    "# (1) instantiate the generator and discriminator, making sure that the generator is roughly a quarter to a half of the size of the discriminator\n",
    "\n",
    "\n",
    "generator = TransformerWrapper(\n",
    "    num_tokens = VOCAB_SIZE+8,\n",
    "    max_seq_len = MODEL_MAX_SEQ_LEN,\n",
    "    # emb_dropout = 0.1,\n",
    "    attn_layers = Decoder(\n",
    "        dim = 256,\n",
    "        depth = 12,\n",
    "        heads = 4,\n",
    "        attn_flash = True,\n",
    "        domains=int(math.sqrt(N_DOMAINS)),\n",
    "        subdomains1=N_DOMAINS,\n",
    "        # rel_pos_bias = True \n",
    "    )\n",
    ")\n",
    "\n",
    "discriminator = TransformerWrapper(\n",
    "    num_tokens = VOCAB_SIZE+8,\n",
    "    max_seq_len = MODEL_MAX_SEQ_LEN,\n",
    "    # emb_dropout = 0.1,\n",
    "    attn_layers = Decoder(\n",
    "        dim = 768,\n",
    "        depth = 12,\n",
    "        heads = 12,\n",
    "        attn_flash = True,\n",
    "        domains=0,\n",
    "        layer_dropout=0.1,\n",
    "        cross_attn_tokens_dropout=0.1,\n",
    "        # rel_pos_bias = True \n",
    "    )\n",
    ")\n",
    "\n",
    "#works with reformer!!!\n",
    "# generator.token_emb = discriminator.token_emb\n",
    "# generator.pos_emb = discriminator.pos_emb\n",
    "\n",
    "model = ElectraConstrative(\n",
    "    generator,\n",
    "    discriminator,\n",
    "    temperature=0.05,\n",
    "    discr_dim = 768,           # the embedding dimension of the discriminator\n",
    "    # discr_dim = 1024,           # the embedding dimension of the discriminator\n",
    "    # discr_layer = 'reformer',   # the layer name in the discriminator, whose output would be used for predicting token is still the same or replaced\n",
    "    discr_layer = 'attn_layers',   # the layer name in the discriminator, whose output would be used for predicting token is still the same or replaced\n",
    "    mask_token_id = MASK_ID,          # the token id reserved for masking\n",
    "    pad_token_id = PAD_ID,           # the token id for padding\n",
    "    mask_prob = 0.15,           # masking probability for masked language modeling\n",
    "    mask_ignore_token_ids = []  # ids of tokens to ignore for mask modeling ex. (cls, sep)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_ids are the indices corresponding to each token in the sentence.\n",
    "# attention_mask indicates whether a token should be attended to or not.\n",
    "# token_type_ids identifies which sequence a token belongs to when there is more than one sequence\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "if model_training=='modular':\n",
    "    if HIER_LEVEL==0:\n",
    "        train_dataloader = CustomDataloader(dataset,cluster_labels,N_DOMAINS)   \n",
    "    elif HIER_LEVEL==1:\n",
    "        #no dataloader os as var fr subdom sao do msm tamanho do dom. Exemplo: 3 dom e 3 subdom\n",
    "        #No modelo é diferente, temos uma primeira camada com 3 branches(dominio), e uma segunda camada com 9 branches(subdominios)\n",
    "        train_dataloader = CustomDataloaderLevel1(dataset,cluster_labels,idxs_cluster_labels_level1,int(math.sqrt(N_DOMAINS)),int(math.sqrt(N_DOMAINS)) ) \n",
    "else:\n",
    "    train_dataloader = DataLoader(\n",
    "            tokenized_dataset, shuffle=True, batch_size=DEVICE_BATCH_SIZE\n",
    "        )\n",
    "# Optimizer\n",
    "learning_rate=5e-5\n",
    "weight_decay=0\n",
    "\n",
    "gradient_accumulation_steps=1\n",
    "max_train_steps=None\n",
    "num_train_epochs=1\n",
    "lr_scheduler_type='linear'\n",
    "num_warmup_steps=0\n",
    "# Split weights in two groups, one with weight decay and the other not.\n",
    "no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "optimizer_grouped_parameters = [\n",
    "    {\n",
    "        \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "        \"weight_decay\": weight_decay,\n",
    "    },\n",
    "    {\n",
    "        \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "        \"weight_decay\": 0.0,\n",
    "    },\n",
    "]\n",
    "optimizer = torch.optim.AdamW(optimizer_grouped_parameters, lr=learning_rate)\n",
    "\n",
    "import math \n",
    "from transformers import (\n",
    "    get_scheduler,\n",
    ")\n",
    "# Scheduler and math around the number of training steps.\n",
    "overrode_max_train_steps = False\n",
    "num_update_steps_per_epoch = math.ceil(len(train_dataloader) / gradient_accumulation_steps)\n",
    "if max_train_steps is None:\n",
    "    max_train_steps = num_train_epochs * num_update_steps_per_epoch\n",
    "    overrode_max_train_steps = True\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=lr_scheduler_type,\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=num_warmup_steps * gradient_accumulation_steps,\n",
    "    num_training_steps=max_train_steps * gradient_accumulation_steps,\n",
    ")\n",
    "from accelerate import Accelerator\n",
    "\n",
    "accelerator = Accelerator()\n",
    "# Prepare everything with our `accelerator`.\n",
    "model, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(\n",
    "    model, optimizer, train_dataloader, lr_scheduler\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration:  0 , total_loss:  6.992696634928385\n",
      "iteration:  30 , total_loss:  285.9026407877604\n",
      "iteration:  60 , total_loss:  197.3063757578532\n",
      "iteration:  90 , total_loss:  124.7024553934733\n",
      "iteration:  120 , total_loss:  87.41490745544434\n",
      "iteration:  150 , total_loss:  85.36539370218912\n",
      "iteration:  180 , total_loss:  112.19822909037272\n",
      "iteration:  210 , total_loss:  101.4834103902181\n",
      "iteration:  240 , total_loss:  115.21927909851074\n",
      "iteration:  270 , total_loss:  91.0132874806722\n",
      "iteration:  300 , total_loss:  77.12401606241862\n",
      "iteration:  330 , total_loss:  76.62908732096354\n",
      "iteration:  360 , total_loss:  106.94939333597819\n",
      "iteration:  390 , total_loss:  76.78578592936198\n",
      "iteration:  420 , total_loss:  72.05549405415853\n",
      "iteration:  450 , total_loss:  55.4007319132487\n",
      "iteration:  480 , total_loss:  66.08126742045084\n",
      "iteration:  510 , total_loss:  61.2308967590332\n",
      "iteration:  540 , total_loss:  66.16464856465657\n",
      "iteration:  570 , total_loss:  61.01326192220052\n",
      "iteration:  600 , total_loss:  52.43672828674316\n",
      "iteration:  630 , total_loss:  52.657021077473956\n",
      "iteration:  660 , total_loss:  56.67269910176595\n",
      "iteration:  690 , total_loss:  56.98252538045247\n",
      "iteration:  720 , total_loss:  59.75203692118327\n",
      "iteration:  750 , total_loss:  56.09091771443685\n",
      "iteration:  780 , total_loss:  54.89822947184245\n",
      "iteration:  810 , total_loss:  49.134005864461265\n",
      "iteration:  840 , total_loss:  44.657118479410805\n",
      "iteration:  870 , total_loss:  43.471679814656575\n",
      "iteration:  900 , total_loss:  42.94793306986491\n",
      "iteration:  930 , total_loss:  46.36582145690918\n",
      "iteration:  960 , total_loss:  41.103019587198894\n",
      "iteration:  990 , total_loss:  46.50038922627767\n",
      "iteration:  1020 , total_loss:  60.74208030700684\n",
      "iteration:  1050 , total_loss:  53.705189259847\n",
      "iteration:  1080 , total_loss:  45.960693740844725\n",
      "iteration:  1110 , total_loss:  42.3288215637207\n",
      "iteration:  1140 , total_loss:  45.74256833394369\n",
      "iteration:  1170 , total_loss:  42.04435666402181\n",
      "iteration:  1200 , total_loss:  40.02268905639649\n",
      "iteration:  1230 , total_loss:  37.49656677246094\n",
      "iteration:  1260 , total_loss:  36.1348627726237\n",
      "iteration:  1290 , total_loss:  43.218881607055664\n",
      "iteration:  1320 , total_loss:  41.19584083557129\n",
      "iteration:  1350 , total_loss:  37.696403884887694\n",
      "iteration:  1380 , total_loss:  40.61891530354818\n",
      "iteration:  1410 , total_loss:  40.158824412028\n",
      "iteration:  1440 , total_loss:  38.84816500345866\n",
      "iteration:  1470 , total_loss:  35.32530530293783\n",
      "iteration:  1500 , total_loss:  37.12871685028076\n",
      "iteration:  1530 , total_loss:  37.24557711283366\n",
      "iteration:  1560 , total_loss:  36.77489032745361\n",
      "iteration:  1590 , total_loss:  32.33810323079427\n",
      "iteration:  1620 , total_loss:  36.243229420979816\n",
      "iteration:  1650 , total_loss:  33.12804209391276\n",
      "iteration:  1680 , total_loss:  34.20424480438233\n",
      "iteration:  1710 , total_loss:  33.966273880004884\n",
      "iteration:  1740 , total_loss:  33.05730514526367\n",
      "iteration:  1770 , total_loss:  33.72576020558675\n",
      "iteration:  1800 , total_loss:  32.78413651784261\n",
      "iteration:  1830 , total_loss:  32.941134452819824\n",
      "iteration:  1860 , total_loss:  36.63003209431966\n",
      "iteration:  1890 , total_loss:  32.814028930664065\n",
      "iteration:  1920 , total_loss:  40.82333838144938\n",
      "iteration:  1950 , total_loss:  33.495743942260745\n",
      "iteration:  1980 , total_loss:  34.539015706380205\n",
      "iteration:  2010 , total_loss:  32.29132766723633\n",
      "iteration:  2040 , total_loss:  32.428925132751466\n",
      "iteration:  2070 , total_loss:  33.12254085540771\n",
      "iteration:  2100 , total_loss:  31.76685447692871\n",
      "iteration:  2130 , total_loss:  31.390328025817873\n",
      "iteration:  2160 , total_loss:  31.97630043029785\n",
      "iteration:  2190 , total_loss:  31.379870796203612\n",
      "iteration:  2220 , total_loss:  31.256570307413735\n",
      "iteration:  2250 , total_loss:  33.31663544972738\n",
      "iteration:  2280 , total_loss:  31.992798360188804\n",
      "iteration:  2310 , total_loss:  32.434478187561034\n",
      "iteration:  2340 , total_loss:  31.386499214172364\n",
      "iteration:  2370 , total_loss:  30.31981881459554\n",
      "iteration:  2400 , total_loss:  30.11642869313558\n",
      "iteration:  2430 , total_loss:  40.89165700276693\n",
      "iteration:  2460 , total_loss:  41.07758356730143\n",
      "iteration:  2490 , total_loss:  33.55357360839844\n",
      "iteration:  2520 , total_loss:  34.33009885152181\n",
      "iteration:  2550 , total_loss:  33.06127452850342\n",
      "iteration:  2580 , total_loss:  32.358317883809406\n",
      "iteration:  2610 , total_loss:  33.578935623168945\n",
      "iteration:  2640 , total_loss:  31.82133541107178\n",
      "iteration:  2670 , total_loss:  32.88237705230713\n",
      "iteration:  2700 , total_loss:  30.927232297261558\n",
      "iteration:  2730 , total_loss:  31.533657264709472\n",
      "iteration:  2760 , total_loss:  31.660184224446613\n",
      "iteration:  2790 , total_loss:  32.80040353139241\n",
      "iteration:  2820 , total_loss:  31.606965764363608\n",
      "iteration:  2850 , total_loss:  31.11500212351481\n",
      "iteration:  2880 , total_loss:  31.273215166727702\n",
      "iteration:  2910 , total_loss:  30.663791211446128\n",
      "iteration:  2940 , total_loss:  31.03274586995443\n",
      "iteration:  2970 , total_loss:  31.486771901448567\n",
      "iteration:  3000 , total_loss:  30.95570748647054\n",
      "iteration:  3030 , total_loss:  31.580281893412273\n",
      "iteration:  3060 , total_loss:  31.38961861928304\n",
      "iteration:  3090 , total_loss:  31.11682523091634\n",
      "iteration:  3120 , total_loss:  31.084961700439454\n",
      "iteration:  3150 , total_loss:  30.64840965270996\n",
      "iteration:  3180 , total_loss:  37.472337532043454\n",
      "iteration:  3210 , total_loss:  31.94774278004964\n",
      "iteration:  3240 , total_loss:  31.534596761067707\n",
      "iteration:  3270 , total_loss:  30.83950513203939\n",
      "iteration:  3300 , total_loss:  30.809532610575356\n",
      "iteration:  3330 , total_loss:  31.96426124572754\n",
      "iteration:  3360 , total_loss:  31.03254852294922\n",
      "iteration:  3390 , total_loss:  30.778957303365072\n",
      "iteration:  3420 , total_loss:  30.77686767578125\n",
      "iteration:  3450 , total_loss:  30.30345942179362\n",
      "iteration:  3480 , total_loss:  30.072163327534994\n",
      "iteration:  3510 , total_loss:  29.970778020222983\n",
      "iteration:  3540 , total_loss:  30.414681180318198\n",
      "iteration:  3570 , total_loss:  30.644656499226887\n",
      "iteration:  3600 , total_loss:  29.911645253499348\n",
      "iteration:  3630 , total_loss:  30.04933541615804\n",
      "iteration:  3660 , total_loss:  29.947120666503906\n",
      "iteration:  3690 , total_loss:  30.17222811381022\n",
      "iteration:  3720 , total_loss:  29.957166481018067\n",
      "iteration:  3750 , total_loss:  29.792430623372397\n",
      "iteration:  3780 , total_loss:  29.73464469909668\n",
      "iteration:  3810 , total_loss:  29.51332619984945\n",
      "iteration:  3840 , total_loss:  29.80093371073405\n",
      "iteration:  3870 , total_loss:  30.228432401021323\n",
      "iteration:  3900 , total_loss:  43.08405984242757\n",
      "iteration:  3930 , total_loss:  31.200719134012857\n",
      "iteration:  3960 , total_loss:  30.24198366800944\n",
      "iteration:  3990 , total_loss:  30.847269948323568\n",
      "iteration:  4020 , total_loss:  30.220347849527993\n",
      "iteration:  4050 , total_loss:  30.178540229797363\n",
      "iteration:  4080 , total_loss:  30.15886255900065\n",
      "iteration:  4110 , total_loss:  29.874735895792643\n",
      "iteration:  4140 , total_loss:  30.461423301696776\n",
      "iteration:  4170 , total_loss:  30.10984141031901\n",
      "iteration:  4200 , total_loss:  30.055611928304035\n",
      "iteration:  4230 , total_loss:  29.898382822672527\n",
      "iteration:  4260 , total_loss:  29.88795846303304\n",
      "iteration:  4290 , total_loss:  29.743010584513346\n",
      "iteration:  4320 , total_loss:  29.742535082499185\n",
      "iteration:  4350 , total_loss:  30.10956745147705\n",
      "iteration:  4380 , total_loss:  29.8760124206543\n",
      "iteration:  4410 , total_loss:  29.77973810831706\n",
      "iteration:  4440 , total_loss:  29.70789451599121\n",
      "iteration:  4470 , total_loss:  29.66565055847168\n",
      "iteration:  4500 , total_loss:  44.57015736897787\n",
      "iteration:  4530 , total_loss:  31.141091092427573\n",
      "iteration:  4560 , total_loss:  31.056417147318523\n",
      "iteration:  4590 , total_loss:  31.086813481648765\n",
      "iteration:  4620 , total_loss:  30.767264111836752\n",
      "iteration:  4650 , total_loss:  30.85625801086426\n",
      "iteration:  4680 , total_loss:  36.728739166259764\n",
      "iteration:  4710 , total_loss:  30.879314295450847\n",
      "iteration:  4740 , total_loss:  30.234486071268716\n",
      "iteration:  4770 , total_loss:  30.55798282623291\n",
      "iteration:  4800 , total_loss:  30.69434248606364\n",
      "iteration:  4830 , total_loss:  30.614652887980142\n",
      "iteration:  4860 , total_loss:  33.38842404683431\n",
      "iteration:  4890 , total_loss:  30.31974894205729\n",
      "iteration:  4920 , total_loss:  29.96967093149821\n",
      "iteration:  4950 , total_loss:  30.264492797851563\n",
      "iteration:  4980 , total_loss:  29.649049377441408\n",
      "iteration:  5010 , total_loss:  30.215671157836915\n",
      "iteration:  5040 , total_loss:  34.291305987040204\n",
      "iteration:  5070 , total_loss:  30.577968978881835\n",
      "iteration:  5100 , total_loss:  30.500736872355144\n",
      "iteration:  5130 , total_loss:  30.449120712280273\n",
      "iteration:  5160 , total_loss:  30.624552090962727\n",
      "iteration:  5190 , total_loss:  29.555536651611327\n",
      "iteration:  5220 , total_loss:  29.781846936543783\n",
      "iteration:  5250 , total_loss:  29.803946431477865\n",
      "iteration:  5280 , total_loss:  29.702017402648927\n",
      "iteration:  5310 , total_loss:  32.83262920379639\n",
      "iteration:  5340 , total_loss:  31.29898363749186\n",
      "iteration:  5370 , total_loss:  43.904496637980145\n",
      "iteration:  5400 , total_loss:  33.64811967213949\n",
      "iteration:  5430 , total_loss:  31.187236976623534\n",
      "iteration:  5460 , total_loss:  30.665579795837402\n",
      "iteration:  5490 , total_loss:  29.891891098022462\n",
      "iteration:  5520 , total_loss:  29.84277712504069\n",
      "iteration:  5550 , total_loss:  30.043443234761558\n",
      "iteration:  5580 , total_loss:  29.85094025929769\n",
      "iteration:  5610 , total_loss:  29.699652353922527\n",
      "iteration:  5640 , total_loss:  29.653183619181316\n",
      "iteration:  5670 , total_loss:  29.717945035298666\n",
      "iteration:  5700 , total_loss:  29.39875774383545\n",
      "iteration:  5730 , total_loss:  29.310755856831868\n",
      "iteration:  5760 , total_loss:  29.206300036112467\n",
      "iteration:  5790 , total_loss:  28.88755219777425\n",
      "iteration:  5820 , total_loss:  29.302563858032226\n",
      "iteration:  5850 , total_loss:  29.349131139119468\n",
      "iteration:  5880 , total_loss:  29.124851417541503\n",
      "iteration:  5910 , total_loss:  29.481272188822427\n",
      "iteration:  5940 , total_loss:  29.05037612915039\n",
      "iteration:  5970 , total_loss:  28.97491308848063\n",
      "iteration:  6000 , total_loss:  29.024835268656414\n",
      "iteration:  6030 , total_loss:  29.220775159200034\n",
      "iteration:  6060 , total_loss:  29.289028803507488\n",
      "iteration:  6090 , total_loss:  29.607617823282876\n",
      "iteration:  6120 , total_loss:  29.012023735046387\n",
      "iteration:  6150 , total_loss:  28.769438870747884\n",
      "iteration:  6180 , total_loss:  29.4594087600708\n",
      "iteration:  6210 , total_loss:  29.185982767740885\n",
      "iteration:  6240 , total_loss:  28.83142833709717\n",
      "iteration:  6270 , total_loss:  29.270907974243165\n",
      "iteration:  6300 , total_loss:  29.131537246704102\n",
      "iteration:  6330 , total_loss:  28.679868443806967\n",
      "iteration:  6360 , total_loss:  29.06055145263672\n",
      "iteration:  6390 , total_loss:  28.585754585266113\n",
      "iteration:  6420 , total_loss:  28.51763661702474\n",
      "iteration:  6450 , total_loss:  29.14080677032471\n",
      "iteration:  6480 , total_loss:  28.583293151855468\n",
      "iteration:  6510 , total_loss:  28.542390060424804\n",
      "iteration:  6540 , total_loss:  28.780887095133462\n",
      "iteration:  6570 , total_loss:  28.397516695658364\n",
      "iteration:  6600 , total_loss:  28.410501289367676\n",
      "iteration:  6630 , total_loss:  29.02621701558431\n",
      "iteration:  6660 , total_loss:  28.470209693908693\n",
      "iteration:  6690 , total_loss:  28.932708676656087\n",
      "iteration:  6720 , total_loss:  29.08331807454427\n",
      "iteration:  6750 , total_loss:  28.83008429209391\n",
      "iteration:  6780 , total_loss:  28.82275168100993\n",
      "iteration:  6810 , total_loss:  28.79444694519043\n",
      "iteration:  6840 , total_loss:  28.615730985005698\n",
      "iteration:  6870 , total_loss:  28.82173417409261\n",
      "iteration:  6900 , total_loss:  28.68840363820394\n",
      "iteration:  6930 , total_loss:  28.843251927693686\n",
      "iteration:  6960 , total_loss:  28.534498278299967\n",
      "iteration:  6990 , total_loss:  28.90488274892171\n",
      "iteration:  7020 , total_loss:  29.061655616760255\n",
      "iteration:  7050 , total_loss:  28.598051325480142\n",
      "iteration:  7080 , total_loss:  28.99393304189046\n",
      "iteration:  7110 , total_loss:  29.933892059326173\n",
      "iteration:  7140 , total_loss:  28.43667367299398\n",
      "iteration:  7170 , total_loss:  28.57080014546712\n",
      "iteration:  7200 , total_loss:  28.43320960998535\n",
      "iteration:  7230 , total_loss:  28.732357343037922\n",
      "iteration:  7260 , total_loss:  29.45591862996419\n",
      "iteration:  7290 , total_loss:  28.432559458414712\n",
      "iteration:  7320 , total_loss:  28.250343386332194\n",
      "iteration:  7350 , total_loss:  28.42365442911784\n",
      "iteration:  7380 , total_loss:  28.70597858428955\n",
      "iteration:  7410 , total_loss:  28.25081221262614\n",
      "iteration:  7440 , total_loss:  35.707004674275716\n",
      "iteration:  7470 , total_loss:  28.63139820098877\n",
      "iteration:  7500 , total_loss:  28.660115687052407\n",
      "iteration:  7530 , total_loss:  28.256946182250978\n",
      "iteration:  7560 , total_loss:  28.224739456176756\n",
      "iteration:  7590 , total_loss:  28.75145034790039\n",
      "iteration:  7620 , total_loss:  29.136144383748373\n",
      "iteration:  7650 , total_loss:  28.368654123942058\n",
      "iteration:  7680 , total_loss:  28.20989335378011\n",
      "iteration:  7710 , total_loss:  28.245241165161133\n",
      "iteration:  7740 , total_loss:  28.196144739786785\n",
      "iteration:  7770 , total_loss:  28.5888978322347\n",
      "iteration:  7800 , total_loss:  28.43318576812744\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "from einops import rearrange, pack, unpack\n",
    "\n",
    "# Only show the progress bar once on each machine.\n",
    "# progress_bar = tqdm(range(max_train_steps), disable=not accelerator.is_local_main_process)\n",
    "completed_steps = 0\n",
    "starting_epoch = 0\n",
    "\n",
    "# update the progress_bar if load from checkpoint\n",
    "total_loss=0\n",
    "# progress_bar.update(completed_steps)\n",
    "steps_log=30\n",
    "count_amostra=0\n",
    "num_train_epochs=1\n",
    "for epoch in range(starting_epoch, num_train_epochs):\n",
    "    if model_training=='modular':\n",
    "        train_dataloader.fill_bins()\n",
    "    model.train()\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        # with accelerator.accumulate(model):\n",
    "        #test if batch is in the right format\n",
    "        if len(batch['input_ids'])!=DEVICE_BATCH_SIZE:\n",
    "            break\n",
    "         \n",
    "        results=model({'input':batch['input_ids'].cuda(),'domain':batch['domain'],'subdomain1':batch['subdomain1']}) \n",
    "        # results=model({'input':batch['input_ids'].cuda()}) \n",
    "\n",
    "        # count_amostra+=int(len(batch['input_ids']))\n",
    "        loss = results.loss\n",
    "        total_loss += loss.detach().float().cpu().numpy().item()\n",
    "        accelerator.backward(loss)\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()       \n",
    "        \n",
    "        if step%steps_log==0:\n",
    "            print('iteration: ',step,', total_loss: ',total_loss/steps_log)\n",
    "            # print('count_amostra:',count_amostra)\n",
    "            total_loss=0\n",
    "    # Checks if the accelerator has performed an optimization step behind the scenes\n",
    "    if accelerator.sync_gradients:\n",
    "        # progress_bar.update(1)\n",
    "        completed_steps += 1\n",
    "\n",
    "    if completed_steps >= max_train_steps:\n",
    "        break\n",
    "\n",
    "    model.eval()\n",
    "    losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.discriminator,'models/contrastive_model/discriminator_16kvocab_1m.pt')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetuning model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Electra custom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "import torch\n",
    "import os\n",
    "from torch.utils.data import  DataLoader\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "    \n",
    "class ElectraClass(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ElectraClass, self).__init__()\n",
    "        self.l1 = model.discriminator._modules['0']\n",
    "        self.l2 = torch.nn.Dropout(0.3)\n",
    "        self.fl = torch.nn.Flatten()\n",
    "        self.l3 = torch.nn.Linear(768*(MODEL_MAX_SEQ_LEN-2), 3)\n",
    "    \n",
    "    def forward(self, ids):\n",
    "        output= self.l1(ids)\n",
    "        output = self.l2(output)\n",
    "        output = self.fl(output)\n",
    "        output = self.l3(output)\n",
    "        return output\n",
    "\n",
    "electra_model = ElectraClass()\n",
    "electra_model=electra_model.to('cuda')\n",
    "optimizer = torch.optim.Adam(params =  model.parameters(), lr=2e-5,weight_decay=0.01)\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "\n",
    "electra_model.train()\n",
    "\n",
    "def loss_fn(outputs, targets):\n",
    "    loss=torch.nn.CrossEntropyLoss()(outputs, targets)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "task='mrpc'\n",
    "dataset = load_dataset('glue', task, split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class TokenizedDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset,is_dom=False):\n",
    "        self.dataset = dataset\n",
    "        self.ls=[]\n",
    "        self.is_dom=is_dom\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        s=norm.normalize_str(self.dataset[\"sentence1\"][i])+'[SEP]'+norm.normalize_str(self.dataset[\"sentence2\"][i])\n",
    "        tokens = vocab.tokenize(s).tolist()        \n",
    "        \n",
    "        self.ls.append(i)\n",
    "        \n",
    "        #trucate\n",
    "        tokens=tokens[:MODEL_MAX_SEQ_LEN - 2]\n",
    "        l=len(tokens)\n",
    "        for j in range(l,MODEL_MAX_SEQ_LEN - 2):\n",
    "            tokens.append(0)\n",
    "        tokens=torch.Tensor(tokens).long()\n",
    "\n",
    "        label=self.dataset[\"label\"][i]\n",
    "\n",
    "        if self.is_dom:\n",
    "            p=tfidf_model.transform([self.dataset[\"sentence1\"][i]+self.dataset[\"sentence2\"][i]])\n",
    "            dom = kmeans.predict(\n",
    "                X=torch.from_numpy(p).to(device)\n",
    "            )\n",
    "            d={'input_ids':tokens,'label':label, 'domain':dom.item()}\n",
    "        else:\n",
    "            d={'input_ids':tokens,'label':label,'domain':0}\n",
    "        return d\n",
    "\n",
    "class HFTokenizedDataset(torch.utils.data.Dataset):\n",
    "    \"This wraps the dataset and tokenizes it, ready for the model\"\n",
    "\n",
    "    def __init__(self, dataset, tokenizer):\n",
    "        self.dataset = dataset\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        label=self.dataset[\"label\"][i]\n",
    "        inps=self.tokenizer.encode(\n",
    "            self.dataset[\"sentence1\"][i]+'[SEP]'+self.dataset[\"sentence2\"][i],\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            max_length=MODEL_MAX_SEQ_LEN - 2,\n",
    "            padding=\"max_length\",\n",
    "            return_special_tokens_mask=True,\n",
    "        )[0, ...]\n",
    "        return {'input_ids':inps,\n",
    "            'label':label}\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "if model_training=='bert':\n",
    "    tokenized_dataset = HFTokenizedDataset(dataset, tokenizer)\n",
    "else:\n",
    "    tokenized_dataset = TokenizedDataset(dataset)\n",
    "# train_dataloader=CustomDataloader(tokenized_dataset)\n",
    "train_dataloader = DataLoader(\n",
    "        tokenized_dataset, batch_size=DEVICE_BATCH_SIZE\n",
    "        # tokenized_dataset, batch_size=1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Iteration: 0, Loss:  0.02487355947494507\n",
      "Epoch: 1, Iteration: 0, Loss:  0.41004410028457644\n"
     ]
    }
   ],
   "source": [
    "epochs=2\n",
    "running_loss=0\n",
    "for i in range(epochs):\n",
    "    for j,data in enumerate(train_dataloader):\n",
    "        if model_training=='bert':\n",
    "            ids = data['input_ids'].to('cuda', dtype = torch.long)\n",
    "        else:\n",
    "            ids = {'input':data['input_ids'].to('cuda', dtype = torch.long),'domain':0}\n",
    "            # ids = {'input':data['input_ids'].to('cuda', dtype = torch.long),'domain':data['domain']}\n",
    "            \n",
    "        \n",
    "        targets = data['label'].to('cuda', dtype = torch.long)\n",
    "\n",
    "        outputs = electra_model(ids)\n",
    "        \n",
    "        loss = loss_fn(outputs,targets)\n",
    "        # loss=outputs.loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        running_loss+=loss.item()\n",
    "        if j%50==0:\n",
    "            print(f'Epoch: {i}, Iteration: {j}, Loss:  {running_loss/50}')\n",
    "            running_loss=0\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset('glue', task, split='validation')\n",
    "if model_training=='bert':\n",
    "    tokenized_dataset = HFTokenizedDataset(dataset, tokenizer)\n",
    "else:\n",
    "    # tokenized_dataset = TokenizedDataset(dataset)\n",
    "    tokenized_dataset = TokenizedDataset(dataset)\n",
    "\n",
    "eval_dataloader = DataLoader(\n",
    "        tokenized_dataset, batch_size=DEVICE_BATCH_SIZE\n",
    "        # tokenized_dataset, batch_size=1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "predictions=[]\n",
    "labels=[]\n",
    "c=0\n",
    "for _,data in enumerate(eval_dataloader):\n",
    "    if model_training=='bert':\n",
    "        ids = data['input_ids'].to('cuda', dtype = torch.long)\n",
    "    else:\n",
    "        ids = {'input':data['input_ids'].to('cuda', dtype = torch.long),'domain':0}\n",
    "        # ids = {'input':data['input_ids'].to('cuda', dtype = torch.long),'domain':data['domain']}\n",
    "        \n",
    "    targets = data['label'].to('cuda', dtype = torch.long)\n",
    "    \n",
    "    outputs = electra_model(ids)\n",
    "    \n",
    "    outputs=torch.argmax(outputs, dim=1).cpu()\n",
    "    targets=targets.cpu()\n",
    "    \n",
    "    outputs=[int(o.item()) for o in outputs]\n",
    "    targets=[int(o.item()) for o in targets]\n",
    "\n",
    "    predictions+=outputs\n",
    "    labels+=targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.08      0.13       129\n",
      "           1       0.69      0.96      0.81       279\n",
      "\n",
      "    accuracy                           0.68       408\n",
      "   macro avg       0.60      0.52      0.47       408\n",
      "weighted avg       0.63      0.68      0.59       408\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(labels, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.44      0.25      0.32       129\n",
      "           1       0.71      0.86      0.78       279\n",
      "\n",
      "    accuracy                           0.66       408\n",
      "   macro avg       0.58      0.55      0.55       408\n",
      "weighted avg       0.63      0.66      0.63       408\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## glue mrpc com 3_dom_electra_discriminator_16kvocab_10m\n",
    "                precision    recall  f1-score   support\n",
    "\n",
    "           0       0.61      0.57      0.59       129\n",
    "           1       0.81      0.83      0.82       279\n",
    "\n",
    "    accuracy                           0.75       408\n",
    "   macro avg       0.71      0.70      0.71       408\n",
    "weighted avg       0.75      0.75      0.75       408\n",
    "\n",
    "## glue mrpc com 3_dom_electra_discriminator_16kvocab_10m\n",
    "         precision    recall  f1-score   support\n",
    "\n",
    "           0       0.56      0.54      0.55       129\n",
    "           1       0.79      0.81      0.80       279\n",
    "\n",
    "    accuracy                           0.72       408\n",
    "   macro avg       0.68      0.67      0.68       408\n",
    "weighted avg       0.72      0.72      0.72       408\n",
    "\n",
    "## glue mrpc com electra_discriminator_16kvocab_10m\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.49      0.47      0.48       129\n",
    "           1       0.76      0.78      0.77       279\n",
    "\n",
    "    accuracy                           0.68       408\n",
    "   macro avg       0.63      0.62      0.62       408\n",
    "weighted avg       0.67      0.68      0.68       408\n",
    "\n",
    "## glue mrpc com electra com pretraining de 1m:\n",
    "            precision    recall  f1-score   support\n",
    "\n",
    "           0       0.47      0.48      0.47       129\n",
    "           1       0.76      0.75      0.75       279\n",
    "\n",
    "    accuracy                           0.66       408\n",
    "   macro avg       0.61      0.61      0.61       408\n",
    "weighted avg       0.66      0.66      0.66       408\n",
    "\n",
    "\n",
    "## glue mrpc com electra sem pretraining:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.50      0.23      0.32       129\n",
    "           1       0.72      0.89      0.79       279\n",
    "\n",
    "    accuracy                           0.68       408\n",
    "   macro avg       0.61      0.56      0.56       408\n",
    "weighted avg       0.65      0.68      0.64       408"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bert custom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import torch\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "class BERTClass(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BERTClass, self).__init__()\n",
    "        self.l1 = transformers.BertModel.from_pretrained('bert-base-cased')\n",
    "        self.l2 = torch.nn.Dropout(0.3)\n",
    "        self.l3 = torch.nn.Linear(768, 3)\n",
    "    \n",
    "    def forward(self, ids, mask, token_type_ids):\n",
    "        outs, output_1= self.l1(ids, attention_mask = mask, token_type_ids = token_type_ids, return_dict=False)\n",
    "        output_2 = self.l2(output_1)\n",
    "        output = self.l3(output_2)\n",
    "        return output\n",
    "    \n",
    "\n",
    "model = BERTClass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "import transformers\n",
    "import torch\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained('bert-base-cased', num_labels=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset glue (/home/kiki/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import  DataLoader\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "task='mrpc'\n",
    "dataset = load_dataset('glue', task, split='train')\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-cased', use_fast=True)\n",
    "# dataset = dataset.map(lambda e: tokenizer(e['sentence1'], truncation=True, padding='max_length'), batched=True)\n",
    "# dataset.set_format(type='torch', columns=['input_ids', 'token_type_ids', 'attention_mask', 'label']) #BERT\n",
    "# dataloader = torch.utils.data.DataLoader(dataset, batch_size=12)\n",
    "\n",
    "model=model.to('cuda')\n",
    "optimizer = torch.optim.Adam(params =  model.parameters(), lr=2e-5,weight_decay=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89502a00dc1f4a599875cfcf74516990",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3668 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def preprocess_function(examples):\n",
    "    if sentence2_key is None:\n",
    "        return tokenizer(examples[sentence1_key], truncation=True, padding='max_length')\n",
    "    return tokenizer(examples[sentence1_key], examples[sentence2_key], truncation=True, padding='max_length')\n",
    "sentence1_key,sentence2_key=(\"sentence1\", \"sentence2\")\n",
    "encoded_dataset = dataset.map(preprocess_function, batched=True)\n",
    "encoded_dataset.set_format(type='torch', columns=['input_ids', 'token_type_ids', 'attention_mask', 'label'])\n",
    "dataloader = torch.utils.data.DataLoader(encoded_dataset, batch_size=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "def loss_fn(outputs, targets):\n",
    "    # if task != \"stsb\":\n",
    "    #     outputs = torch.argmax(outputs, dim=1)\n",
    "    # else:\n",
    "    #     outputs = outputs[:, 0]\n",
    "    # loss=torch.nn.BCEWithLogitsLoss()(outputs,targets)\n",
    "    # loss=torch.nn.CrossEntropyLoss()(outputs,targets)\n",
    "    loss=torch.nn.CrossEntropyLoss()(outputs.view(-1,3), targets.view(-1))\n",
    "    # loss = Variable(loss, requires_grad = True)\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss:  0.007852643132209777\n",
      "Epoch: 0, Loss:  0.20191293969750404\n",
      "Epoch: 0, Loss:  0.16229299038648606\n",
      "Epoch: 0, Loss:  0.14223801746964454\n",
      "Epoch: 0, Loss:  0.1337541800737381\n",
      "Epoch: 0, Loss:  0.11794026836752891\n",
      "Epoch: 0, Loss:  0.10376800172030926\n",
      "Epoch: 1, Loss:  0.0131098260730505\n",
      "Epoch: 1, Loss:  0.10432511921972036\n",
      "Epoch: 1, Loss:  0.10928954795002938\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">23</span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">20 │   │   │   </span>running_loss=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">0</span>                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">21 │   │   </span>                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">22 │   │   </span>optimizer.zero_grad()                                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>23 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>loss.backward()                                                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">24 │   │   </span>optimizer.step()                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">25 </span>                                                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/kiki/miniconda3/envs/llama/lib/python3.9/site-packages/torch/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">_tensor.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">487</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">backward</span>    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 484 │   │   │   │   </span>create_graph=create_graph,                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 485 │   │   │   │   </span>inputs=inputs,                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 486 │   │   │   </span>)                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 487 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>torch.autograd.backward(                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 488 │   │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>, gradient, retain_graph, create_graph, inputs=inputs                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 489 │   │   </span>)                                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 490 </span>                                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/kiki/miniconda3/envs/llama/lib/python3.9/site-packages/torch/autograd/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">__init__.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">200</span> in   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">backward</span>                                                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">197 │   # The reason we repeat same the comment below is that</span>                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">198 │   # some Python versions print out the first line of a multi-line function</span>               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">199 │   # calls in the traceback and some print out the last line</span>                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>200 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>Variable._execution_engine.run_backward(  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># Calls into the C++ engine to run the bac</span>   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">201 │   │   </span>tensors, grad_tensors_, retain_graph, create_graph, inputs,                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">202 │   │   </span>allow_unreachable=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">True</span>, accumulate_grad=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">True</span>)  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># Calls into the C++ engine to ru</span>   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">203 </span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">KeyboardInterrupt</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92m<module>\u001b[0m:\u001b[94m23\u001b[0m                                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m20 \u001b[0m\u001b[2m│   │   │   \u001b[0mrunning_loss=\u001b[94m0\u001b[0m                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m21 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m22 \u001b[0m\u001b[2m│   │   \u001b[0moptimizer.zero_grad()                                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m23 \u001b[2m│   │   \u001b[0mloss.backward()                                                                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m24 \u001b[0m\u001b[2m│   │   \u001b[0moptimizer.step()                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m25 \u001b[0m                                                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/kiki/miniconda3/envs/llama/lib/python3.9/site-packages/torch/\u001b[0m\u001b[1;33m_tensor.py\u001b[0m:\u001b[94m487\u001b[0m in \u001b[92mbackward\u001b[0m    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 484 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mcreate_graph=create_graph,                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 485 \u001b[0m\u001b[2m│   │   │   │   \u001b[0minputs=inputs,                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 486 \u001b[0m\u001b[2m│   │   │   \u001b[0m)                                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 487 \u001b[2m│   │   \u001b[0mtorch.autograd.backward(                                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 488 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[96mself\u001b[0m, gradient, retain_graph, create_graph, inputs=inputs                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 489 \u001b[0m\u001b[2m│   │   \u001b[0m)                                                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 490 \u001b[0m                                                                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/kiki/miniconda3/envs/llama/lib/python3.9/site-packages/torch/autograd/\u001b[0m\u001b[1;33m__init__.py\u001b[0m:\u001b[94m200\u001b[0m in   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[92mbackward\u001b[0m                                                                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m197 \u001b[0m\u001b[2m│   \u001b[0m\u001b[2m# The reason we repeat same the comment below is that\u001b[0m                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m198 \u001b[0m\u001b[2m│   \u001b[0m\u001b[2m# some Python versions print out the first line of a multi-line function\u001b[0m               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m199 \u001b[0m\u001b[2m│   \u001b[0m\u001b[2m# calls in the traceback and some print out the last line\u001b[0m                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m200 \u001b[2m│   \u001b[0mVariable._execution_engine.run_backward(  \u001b[2m# Calls into the C++ engine to run the bac\u001b[0m   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m201 \u001b[0m\u001b[2m│   │   \u001b[0mtensors, grad_tensors_, retain_graph, create_graph, inputs,                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m202 \u001b[0m\u001b[2m│   │   \u001b[0mallow_unreachable=\u001b[94mTrue\u001b[0m, accumulate_grad=\u001b[94mTrue\u001b[0m)  \u001b[2m# Calls into the C++ engine to ru\u001b[0m   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m203 \u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mKeyboardInterrupt\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.train()\n",
    "epochs=5\n",
    "running_loss=0\n",
    "for i in range(epochs):\n",
    "    for _,data in enumerate(dataloader):\n",
    "        ids = data['input_ids'].to('cuda', dtype = torch.long)\n",
    "        mask = data['attention_mask'].to('cuda', dtype = torch.long)\n",
    "        token_type_ids = data['token_type_ids'].to('cuda', dtype = torch.long)\n",
    "        targets = data['label'].to('cuda', dtype = torch.long)\n",
    "\n",
    "        # outputs = model(ids, mask, token_type_ids)\n",
    "        outputs = model(ids, mask, token_type_ids)\n",
    "        loss = loss_fn(outputs.logits,targets)\n",
    "        # loss=outputs.loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        running_loss+=loss.item()\n",
    "        if _%50==0:\n",
    "            print(f'Epoch: {i}, Loss:  {running_loss/200}')\n",
    "            running_loss=0\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset glue (/home/kiki/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b703556b9b84f65b802c9d7269defea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/408 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset('glue', task, split='validation')\n",
    "# dataset = dataset.map(lambda e: tokenizer(e['sentence1'], truncation=True, padding='max_length'), batched=True)\n",
    "# dataset.set_format(type='torch', columns=['input_ids', 'token_type_ids', 'attention_mask', 'label'])\n",
    "# dataloader = torch.utils.data.DataLoader(dataset, batch_size=4)\n",
    "encoded_dataset = dataset.map(preprocess_function, batched=True)\n",
    "encoded_dataset.set_format(type='torch', columns=['input_ids', 'token_type_ids', 'attention_mask', 'label'])\n",
    "dataloader = torch.utils.data.DataLoader(encoded_dataset, batch_size=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "predictions=[]\n",
    "labels=[]\n",
    "c=0\n",
    "for _,data in enumerate(dataloader):\n",
    "    ids = data['input_ids'].to('cuda', dtype = torch.long)\n",
    "    mask = data['attention_mask'].to('cuda', dtype = torch.long)\n",
    "    token_type_ids = data['token_type_ids'].to('cuda', dtype = torch.long)\n",
    "    targets = data['label'].to('cuda', dtype = torch.long)\n",
    "\n",
    "    outputs = model(ids, mask, token_type_ids)\n",
    "    outputs = outputs.logits\n",
    "    \n",
    "    outputs=torch.argmax(outputs, dim=1).cpu()\n",
    "    targets=targets.cpu()\n",
    "    \n",
    "    outputs=[int(o.item()) for o in outputs]\n",
    "    targets=[int(o.item()) for o in targets]\n",
    "\n",
    "    predictions+=outputs\n",
    "    labels+=targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import load_metric\n",
    "# metric = load_metric('glue', 'mrpc')\n",
    "# final_score = metric.compute(predictions=predictions, references =labels )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.84      0.73       129\n",
      "           1       0.92      0.78      0.84       279\n",
      "\n",
      "    accuracy                           0.80       408\n",
      "   macro avg       0.78      0.81      0.79       408\n",
      "weighted avg       0.83      0.80      0.81       408\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(labels, predictions))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.30.0.dev0\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "GLUE_TASKS = [\"cola\", \"mnli\", \"mnli-mm\", \"mrpc\", \"qnli\", \"qqp\", \"rte\", \"sst2\", \"stsb\", \"wnli\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = \"mrpc\"\n",
    "model_checkpoint = \"bert-base-cased\"\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, load_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset glue (/home/kiki/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4aae4f248054178b9cbc635d19a2159",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_244174/1389288479.py:3: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = load_metric('glue', actual_task)\n"
     ]
    }
   ],
   "source": [
    "actual_task = \"mnli\" if task == \"mnli-mm\" else task\n",
    "dataset = load_dataset(\"glue\", actual_task)\n",
    "metric = load_metric('glue', actual_task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "    \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_to_keys = {\n",
    "    \"cola\": (\"sentence\", None),\n",
    "    \"mnli\": (\"premise\", \"hypothesis\"),\n",
    "    \"mnli-mm\": (\"premise\", \"hypothesis\"),\n",
    "    \"mrpc\": (\"sentence1\", \"sentence2\"),\n",
    "    \"qnli\": (\"question\", \"sentence\"),\n",
    "    \"qqp\": (\"question1\", \"question2\"),\n",
    "    \"rte\": (\"sentence1\", \"sentence2\"),\n",
    "    \"sst2\": (\"sentence\", None),\n",
    "    \"stsb\": (\"sentence1\", \"sentence2\"),\n",
    "    \"wnli\": (\"sentence1\", \"sentence2\"),\n",
    "}\n",
    "sentence1_key, sentence2_key = task_to_keys[task]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    if sentence2_key is None:\n",
    "        return tokenizer(examples[sentence1_key], truncation=True)\n",
    "    return tokenizer(examples[sentence1_key], examples[sentence2_key], truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "671e3a45668946a08773dcaea4506d5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3668 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0eb643aa13c4bb784b7d6f68f0e9ee1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/408 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e64158ab1568461a9d131c7d5f675c9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1725 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "encoded_dataset = dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "\n",
    "num_labels = 3 if task.startswith(\"mnli\") else 1 if task==\"stsb\" else 2\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_name = \"pearson\" if task == \"stsb\" else \"matthews_correlation\" if task == \"cola\" else \"accuracy\"\n",
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "\n",
    "args = TrainingArguments(\n",
    "    f\"{model_name}-finetuned-{task}\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    save_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=metric_name,\n",
    "    push_to_hub=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    if task != \"stsb\":\n",
    "        predictions = np.argmax(predictions, axis=1)\n",
    "    else:\n",
    "        predictions = predictions[:, 0]\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning https://huggingface.co/abcp4/bert-base-cased-finetuned-mrpc into local empty directory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "validation_key = \"validation_mismatched\" if task == \"mnli-mm\" else \"validation_matched\" if task == \"mnli\" else \"validation\"\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=encoded_dataset[\"train\"],\n",
    "    eval_dataset=encoded_dataset[validation_key],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57218c19fff94363bfe1478f7d6e2edc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.5459453463554382,\n",
       " 'eval_accuracy': 0.8578431372549019,\n",
       " 'eval_f1': 0.8993055555555555,\n",
       " 'eval_runtime': 0.4866,\n",
       " 'eval_samples_per_second': 838.425,\n",
       " 'eval_steps_per_second': 53.429,\n",
       " 'epoch': 5.0}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3470f0fd3b3a4641bde2110d54183b25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/230 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "preds=trainer.predict(encoded_dataset[\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset glue (/home/kiki/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b1438990ed844a992c43616097c4845",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/408 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset('glue', task, split='validation')\n",
    "dataset = dataset.map(lambda e: tokenizer(e['sentence1'], truncation=True, padding='max_length'), batched=True)\n",
    "dataset.set_format(type='torch', columns=['input_ids', 'token_type_ids', 'attention_mask', 'label'])\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "predictions=[]\n",
    "labels=[]\n",
    "c=0\n",
    "for _,data in enumerate(dataloader):\n",
    "    ids = data['input_ids'].to('cuda', dtype = torch.long)\n",
    "    mask = data['attention_mask'].to('cuda', dtype = torch.long)\n",
    "    token_type_ids = data['token_type_ids'].to('cuda', dtype = torch.long)\n",
    "    targets = data['label'].to('cuda', dtype = torch.long)\n",
    "\n",
    "    outputs = model(ids, mask, token_type_ids)\n",
    "    outputs = outputs.logits\n",
    "    \n",
    "    outputs=torch.argmax(outputs, dim=1).cpu()\n",
    "    targets=targets.cpu()\n",
    "    \n",
    "    outputs=[int(o.item()) for o in outputs]\n",
    "    targets=[int(o.item()) for o in targets]\n",
    "\n",
    "    predictions+=outputs\n",
    "    labels+=targets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 1, 1, 0])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions=np.argmax(preds.predictions, axis=1)\n",
    "labels=encoded_dataset[\"train\"]['label']\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.98      0.98      1194\n",
      "           1       0.99      1.00      0.99      2474\n",
      "\n",
      "    accuracy                           0.99      3668\n",
      "   macro avg       0.99      0.99      0.99      3668\n",
      "weighted avg       0.99      0.99      0.99      3668\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# from datasets import load_metric\n",
    "# metric = load_metric('glue', 'mrpc')\n",
    "# final_score = metric.compute(predictions=predictions, references =labels )\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(labels, predictions))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Torch model to HuggingFaceTransformers(optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Torch model to HuggingFaceTransformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from transformers import PreTrainedModel, PretrainedConfig\n",
    "from transformers import AutoModel, AutoConfig\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from typing import List, Optional, Tuple, Union\n",
    "from transformers.modeling_outputs import (\n",
    "    SequenceClassifierOutput,\n",
    ")\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyConfig(PretrainedConfig):\n",
    "    model_type = 'mymodel'\n",
    "    def __init__(self, important_param=42, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.important_param = important_param\n",
    "\n",
    "class MyModel(PreTrainedModel):\n",
    "    config_class = MyConfig\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.config = config\n",
    "        self.model = nn.Sequential(\n",
    "                          nn.Linear(3, self.config.important_param),\n",
    "                          nn.Sigmoid(),\n",
    "                          nn.Linear(self.config.important_param, 1),\n",
    "                          nn.Sigmoid()\n",
    "                          )\n",
    "    def forward(self, input):\n",
    "        return self.model(input) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = MyConfig(4)\n",
    "model = MyModel(config)\n",
    "model.save_pretrained('./models/electra')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MyModel(\n",
       "  (model): Sequential(\n",
       "    (0): Linear(in_features=3, out_features=4, bias=True)\n",
       "    (1): Sigmoid()\n",
       "    (2): Linear(in_features=4, out_features=1, bias=True)\n",
       "    (3): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "AutoConfig.register(\"mymodel\", MyConfig)\n",
    "AutoModel.register(MyConfig, MyModel)\n",
    "\n",
    "# new_model = MyModel.from_pretrained('./models/electra')\n",
    "new_model = AutoModel.from_pretrained('./models/electra')\n",
    "new_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_model.push_to_hub(\"mymodel-test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "\n",
    "class MySeqConfig(PretrainedConfig):\n",
    "    model_type = 'myseqmodel'\n",
    "    def __init__(self, important_param=42, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.important_param = important_param\n",
    "\n",
    "class MySequenceModel(PreTrainedModel):\n",
    "    config_class = MySeqConfig\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.config = config\n",
    "        self.model = nn.Sequential(\n",
    "                          nn.Linear(3, self.config.important_param),\n",
    "                          nn.Sigmoid(),\n",
    "                          nn.Linear(self.config.important_param, 1),\n",
    "                          nn.Sigmoid()\n",
    "                          )\n",
    "    def forward(self, input_ids: Optional[torch.Tensor] = None,\n",
    "                attention_mask: Optional[torch.Tensor] = None,\n",
    "                token_type_ids: Optional[torch.Tensor] = None,\n",
    "                position_ids: Optional[torch.Tensor] = None,\n",
    "                head_mask: Optional[torch.Tensor] = None,\n",
    "                inputs_embeds: Optional[torch.Tensor] = None,\n",
    "                labels: Optional[torch.Tensor] = None,\n",
    "                output_attentions: Optional[bool] = None,\n",
    "                output_hidden_states: Optional[bool] = None,\n",
    "                return_dict: Optional[bool] = None,)-> Union[Tuple[torch.Tensor], SequenceClassifierOutput]:\n",
    "        \n",
    "        print(input_ids.shape)\n",
    "        loss = Variable(torch.zeros(1).to('cuda'), requires_grad=True)\n",
    "\n",
    "        \n",
    "        return SequenceClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=torch.zeros((3)).to('cuda'),\n",
    "            hidden_states=torch.zeros(128).to('cuda'),\n",
    "            attentions=torch.zeros(128).to('cuda'),\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = MySeqConfig(4)\n",
    "model = MySequenceModel(config)\n",
    "model.save_pretrained('./models/electra')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AutoModelForSequenceClassification.register(MySeqConfig, MySequenceModel)\n",
    "AutoConfig.register(\"myseqmodel\", MySeqConfig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=AutoModelForSequenceClassification.from_pretrained('./models/electra')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "06/24/2023 21:48:38 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: False\n",
      "06/24/2023 21:48:38 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_backend=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=True,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=no,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=2e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=output_evals/runs/Jun24_21-48-38_kiki-System-Product-Name,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=500,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_type=linear,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=5.0,\n",
      "optim=adamw_hf,\n",
      "optim_args=None,\n",
      "output_dir=output_evals,\n",
      "overwrite_output_dir=False,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=8,\n",
      "per_device_train_batch_size=32,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=['tensorboard', 'wandb'],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=output_evals,\n",
      "save_on_each_node=False,\n",
      "save_safetensors=False,\n",
      "save_steps=500,\n",
      "save_strategy=no,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      "xpu_backend=None,\n",
      ")\n",
      "06/24/2023 21:48:39 - INFO - datasets.info - Loading Dataset Infos from /home/kiki/.cache/huggingface/modules/datasets_modules/datasets/glue/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad\n",
      "06/24/2023 21:48:39 - INFO - datasets.builder - Overwrite dataset info from restored data version.\n",
      "06/24/2023 21:48:39 - INFO - datasets.info - Loading Dataset info from /home/kiki/.cache/huggingface/datasets/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad\n",
      "06/24/2023 21:48:39 - WARNING - datasets.builder - Found cached dataset glue (/home/kiki/.cache/huggingface/datasets/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n",
      "06/24/2023 21:48:39 - INFO - datasets.info - Loading Dataset info from /home/kiki/.cache/huggingface/datasets/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad\n",
      "100%|███████████████████████████████████████████| 3/3 [00:00<00:00, 2040.36it/s]\n",
      "[INFO|configuration_utils.py:669] 2023-06-24 21:48:40,194 >> loading configuration file config.json from cache at /home/kiki/.cache/huggingface/hub/models--bert-base-cased/snapshots/5532cc56f74641d4bb33641f5c76a55d11f846e0/config.json\n",
      "[INFO|configuration_utils.py:725] 2023-06-24 21:48:40,200 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"finetuning_task\": \"cola\",\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.30.0.dev0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "[INFO|configuration_utils.py:669] 2023-06-24 21:48:40,399 >> loading configuration file config.json from cache at /home/kiki/.cache/huggingface/hub/models--bert-base-cased/snapshots/5532cc56f74641d4bb33641f5c76a55d11f846e0/config.json\n",
      "[INFO|configuration_utils.py:725] 2023-06-24 21:48:40,400 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.30.0.dev0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1810] 2023-06-24 21:48:40,401 >> loading file vocab.txt from cache at /home/kiki/.cache/huggingface/hub/models--bert-base-cased/snapshots/5532cc56f74641d4bb33641f5c76a55d11f846e0/vocab.txt\n",
      "[INFO|tokenization_utils_base.py:1810] 2023-06-24 21:48:40,401 >> loading file tokenizer.json from cache at /home/kiki/.cache/huggingface/hub/models--bert-base-cased/snapshots/5532cc56f74641d4bb33641f5c76a55d11f846e0/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:1810] 2023-06-24 21:48:40,401 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1810] 2023-06-24 21:48:40,401 >> loading file special_tokens_map.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1810] 2023-06-24 21:48:40,401 >> loading file tokenizer_config.json from cache at /home/kiki/.cache/huggingface/hub/models--bert-base-cased/snapshots/5532cc56f74641d4bb33641f5c76a55d11f846e0/tokenizer_config.json\n",
      "[INFO|configuration_utils.py:669] 2023-06-24 21:48:40,401 >> loading configuration file config.json from cache at /home/kiki/.cache/huggingface/hub/models--bert-base-cased/snapshots/5532cc56f74641d4bb33641f5c76a55d11f846e0/config.json\n",
      "[INFO|configuration_utils.py:725] 2023-06-24 21:48:40,402 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.30.0.dev0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:2535] 2023-06-24 21:48:40,435 >> loading weights file model.safetensors from cache at /home/kiki/.cache/huggingface/hub/models--bert-base-cased/snapshots/5532cc56f74641d4bb33641f5c76a55d11f846e0/model.safetensors\n",
      "[WARNING|modeling_utils.py:3229] 2023-06-24 21:48:40,813 >> Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "[WARNING|modeling_utils.py:3241] 2023-06-24 21:48:40,813 >> Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "06/24/2023 21:48:40 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/kiki/.cache/huggingface/datasets/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-76c8170a2a261ff8.arrow\n",
      "Running tokenizer on dataset:   0%|             | 0/1043 [00:00<?, ? examples/s]06/24/2023 21:48:40 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/kiki/.cache/huggingface/datasets/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-a2bf5301681fb373.arrow\n",
      "06/24/2023 21:48:40 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/kiki/.cache/huggingface/datasets/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-43a8c8283e9dbbef.arrow\n",
      "06/24/2023 21:48:40 - INFO - __main__ - Sample 1824 of the training set: {'sentence': 'I acknowledged that my father, he was tight as an owl.', 'label': 0, 'idx': 1824, 'input_ids': [101, 146, 8646, 1115, 1139, 1401, 117, 1119, 1108, 3600, 1112, 1126, 19976, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
      "06/24/2023 21:48:40 - INFO - __main__ - Sample 409 of the training set: {'sentence': 'For him to do that would be a mistake.', 'label': 1, 'idx': 409, 'input_ids': [101, 1370, 1140, 1106, 1202, 1115, 1156, 1129, 170, 6223, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
      "06/24/2023 21:48:40 - INFO - __main__ - Sample 4506 of the training set: {'sentence': 'Mary sang a song, but Lee never did.', 'label': 1, 'idx': 4506, 'input_ids': [101, 2090, 6407, 170, 1461, 117, 1133, 2499, 1309, 1225, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
      "[INFO|trainer.py:763] 2023-06-24 21:48:42,393 >> The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/home/kiki/miniconda3/envs/llama/lib/python3.9/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "[INFO|trainer.py:1812] 2023-06-24 21:48:42,397 >> ***** Running training *****\n",
      "[INFO|trainer.py:1813] 2023-06-24 21:48:42,397 >>   Num examples = 8,551\n",
      "[INFO|trainer.py:1814] 2023-06-24 21:48:42,397 >>   Num Epochs = 5\n",
      "[INFO|trainer.py:1815] 2023-06-24 21:48:42,397 >>   Instantaneous batch size per device = 32\n",
      "[INFO|trainer.py:1816] 2023-06-24 21:48:42,397 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "[INFO|trainer.py:1817] 2023-06-24 21:48:42,397 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1818] 2023-06-24 21:48:42,397 >>   Total optimization steps = 1,340\n",
      "[INFO|trainer.py:1819] 2023-06-24 21:48:42,398 >>   Number of trainable parameters = 108,311,810\n",
      "[INFO|integrations.py:720] 2023-06-24 21:48:42,402 >> Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mabcp4\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.4 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/mnt/e026e6cb-2abe-4ed4-bf51-0381c5a02c4b/Servidor/LM_Pretraining/Fast_Transformers/wandb/run-20230624_214843-jlxgfmk4\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mefficient-butterfly-34\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/abcp4/huggingface\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/abcp4/huggingface/runs/jlxgfmk4\u001b[0m\n",
      "{'loss': 0.4082, 'learning_rate': 1.2537313432835823e-05, 'epoch': 1.87}        \n",
      "{'loss': 0.1739, 'learning_rate': 5.074626865671642e-06, 'epoch': 3.73}         \n",
      "100%|██████████████████████████████████████▉| 1339/1340 [03:23<00:00,  6.52it/s][INFO|trainer.py:2085] 2023-06-24 21:52:09,773 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 207.3794, 'train_samples_per_second': 206.168, 'train_steps_per_second': 6.462, 'train_loss': 0.24341670933054455, 'epoch': 5.0}\n",
      "100%|███████████████████████████████████████| 1340/1340 [03:23<00:00,  6.59it/s]\n",
      "***** train metrics *****\n",
      "  epoch                    =        5.0\n",
      "  train_loss               =     0.2434\n",
      "  train_runtime            = 0:03:27.37\n",
      "  train_samples            =       8551\n",
      "  train_samples_per_second =    206.168\n",
      "  train_steps_per_second   =      6.462\n",
      "06/24/2023 21:52:09 - INFO - __main__ - *** Evaluate ***\n",
      "[INFO|trainer.py:763] 2023-06-24 21:52:09,779 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:3217] 2023-06-24 21:52:09,780 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:3219] 2023-06-24 21:52:09,780 >>   Num examples = 1043\n",
      "[INFO|trainer.py:3222] 2023-06-24 21:52:09,780 >>   Batch size = 8\n",
      "100%|█████████████████████████████████████████| 131/131 [00:01<00:00, 66.00it/s]\n",
      "***** eval metrics *****\n",
      "  epoch                     =        5.0\n",
      "  eval_loss                 =     0.6722\n",
      "  eval_matthews_correlation =     0.6082\n",
      "  eval_runtime              = 0:00:01.99\n",
      "  eval_samples              =       1043\n",
      "  eval_samples_per_second   =     522.76\n",
      "  eval_steps_per_second     =     65.658\n",
      "06/24/2023 21:52:11 - INFO - __main__ - *** Predict ***\n",
      "[INFO|trainer.py:763] 2023-06-24 21:52:11,781 >> The following columns in the test set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:3217] 2023-06-24 21:52:11,782 >> ***** Running Prediction *****\n",
      "[INFO|trainer.py:3219] 2023-06-24 21:52:11,782 >>   Num examples = 1063\n",
      "[INFO|trainer.py:3222] 2023-06-24 21:52:11,782 >>   Batch size = 8\n",
      "100%|█████████████████████████████████████████| 133/133 [00:02<00:00, 66.26it/s]\n",
      "06/24/2023 21:52:13 - INFO - __main__ - ***** Predict results cola *****\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                      eval/loss ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      eval/matthews_correlation ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   eval/runtime ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        eval/samples_per_second ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/steps_per_second ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    train/epoch ▁▅██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              train/global_step ▁▅██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/learning_rate █▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                     train/loss █▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               train/total_flos ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               train/train_loss ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/train_runtime ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train/train_samples_per_second ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   train/train_steps_per_second ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                      eval/loss 0.6722\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      eval/matthews_correlation 0.60817\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   eval/runtime 1.9952\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        eval/samples_per_second 522.76\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/steps_per_second 65.658\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    train/epoch 5.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              train/global_step 1340\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/learning_rate 1e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                     train/loss 0.1739\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               train/total_flos 2724443033823600.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               train/train_loss 0.24342\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/train_runtime 207.3794\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train/train_samples_per_second 206.168\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   train/train_steps_per_second 6.462\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mefficient-butterfly-34\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/abcp4/huggingface/runs/jlxgfmk4\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230624_214843-jlxgfmk4/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Glue\n",
    "!rm -r output_evals/runs\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "!python glue_eval.py --task_name cola --model_name_or_path bert-base-cased --tokenizer_name bert-base-cased --output_dir output_evals --do_train  --do_eval  --do_predict --max_seq_length 124 --per_device_train_batch_size 32 --learning_rate 2e-5 --lr_scheduler_type linear --num_train_epochs 5  --save_strategy no --seed 42 "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
