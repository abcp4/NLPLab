{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.1+cu117\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "print(torch.__version__)\n",
    "\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install cchardet chardet datasets transformers tqdm magic_timer pandas tokenizers matplotlib pynvml tokenmonster\n",
    "# !conda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia -y"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: WANDB_START_METHOD=thread\n",
      "env: WANDB_PROJECT=pretraining_BERT_the_notebook\n",
      "GPU: NVIDIA GeForce RTX 3090, 24576.0 MiB\n",
      "torch.cuda.is_available() = True\n",
      "DEVICE_BATCH_SIZE = 200\n",
      "gradient_accumulation_steps = 10\n",
      "batch_size = 2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/home/kiki/.cache/huggingface/datasets/sradc___parquet/sradc--chunked-shuffled-wikipedia20220301en-bookcorpusopen-ff5cb88917a65ec5/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset in 1.7 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%env WANDB_START_METHOD=thread\n",
    "%env WANDB_PROJECT=pretraining_BERT_the_notebook\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Iterator\n",
    "import time\n",
    "\n",
    "import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pynvml\n",
    "import torch\n",
    "from magic_timer import MagicTimer\n",
    "from tokenizers import BertWordPieceTokenizer, Regex, normalizers\n",
    "from tqdm import tqdm\n",
    "from transformers import (\n",
    "    BertConfig,\n",
    "    BertForMaskedLM,\n",
    "    BertTokenizerFast,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "\n",
    "# Print hardware information\n",
    "pynvml.nvmlInit()\n",
    "handle = pynvml.nvmlDeviceGetHandleByIndex(0)\n",
    "gpu_name = pynvml.nvmlDeviceGetName(handle)\n",
    "gpu_mem = pynvml.nvmlDeviceGetMemoryInfo(handle).total / (1024**2)\n",
    "print(f\"GPU: {gpu_name}, {gpu_mem} MiB\")\n",
    "print(f\"{torch.cuda.is_available() = }\")\n",
    "model_training='tokemonster'#'bert' , 'tokemonster\n",
    "# LIMIT_DATASET = 2016 * 4  # keep small for development, set to None for full dataset\n",
    "#(3090) Para 128 tokens(Bert) e 32000 vocab: 1000 = 6 seg, 10000 = 1min, 100000 = 10min, 1m = 100min, 10m = 16h, 100m = 160h ou 6.6 dias\n",
    "#(3090) Para 84 tokens(tokenmonster salva 35%) e 24000 vocab(Bert): 1000 = 4.3s, 10000 = 43s, 100000 = 7.1m, 1m = 71min, 10m = 11.8h, 100m = 118h ou 5 dias\n",
    "#(4090) Para 84 tokens(tokenmonster salva 35%) e 24000 vocab(Electra): 1000 = 3.5s, 10000 = 35s, 100000 = 5.9m, 1m = 59min, 10m = 9.8h, 100m = 98h ou 4 dias\n",
    "#(4090) Para 84 tokens(tokenmonster salva 35%) e 16000 vocab (Electra): 1000 = 2.1s, 10000 = 21s, 100000 = 3.53m, 1m = 35.3min, 10m = 5.8h, 100m = 58h ou 2.4 dias\n",
    "\n",
    "\n",
    "#HIPOTÉTICO:\n",
    "#(4090) Para 24 tokens(tokenmonster salva 35%)(Electra): 1000 = , 10000 = , 100000 = 3.19m, 1m = 32min, 10m = 6.5h, 100m = 65h ou 2.7 dias\n",
    "\n",
    "#losses:\n",
    "#1m dataset, electra, tokenmonster: iteration:  7800 , total_loss:  26.277032788594564\n",
    "#10m dataset, 84 tokens(tokenmonster salva 35%) e 16000 vocab (Electra) iteration:  49980 , total_loss:  20.8733185450236. 339min total\n",
    "\n",
    "\n",
    "LIMIT_DATASET = 10_000\n",
    "RANDOM_SEED = 42\n",
    "NUM_TOKENIZER_TRAINING_ITEMS = 1_000_000  # I made this up, but it seems reasonable\n",
    "if model_training=='bert':\n",
    "    VOCAB_SIZE = 32_768  # from Cramming\n",
    "    DEVICE_BATCH_SIZE = 100 # aprox 128, adjust to get near 100% gpu memory use\n",
    "    MODEL_MAX_SEQ_LEN = 128  # from Cramming\n",
    "else:\n",
    "    VOCAB_SIZE = 16_000  # tokenmonster\n",
    "    DEVICE_BATCH_SIZE = 200 # Token monster aguenta um batch size de (200-248)!! Geralmente melhora a qualidade do treino\n",
    "    MODEL_MAX_SEQ_LEN = 84  # token_monster\n",
    "MASK_ID=4\n",
    "PAD_ID=0\n",
    "gradient_accumulation_steps = 2048 // DEVICE_BATCH_SIZE  # roughly based on Cramming\n",
    "batch_size = DEVICE_BATCH_SIZE * gradient_accumulation_steps\n",
    "print(f\"{DEVICE_BATCH_SIZE = }\")\n",
    "print(f\"{gradient_accumulation_steps = }\")\n",
    "print(f\"{batch_size = }\")\n",
    "RUN_DIR = Path(\"data\") / f\"run_{time.strftime('%Y%m%d-%H%M%S')}\"\n",
    "CHECKPOINT_DIR = RUN_DIR / \"training_checkpoints\"\n",
    "MODEL_DIR = RUN_DIR / \"model\"\n",
    "TOKENIZER_PATH = RUN_DIR / \"tokenizer.json\"\n",
    "TRAINER_HISTORY_PATH = RUN_DIR / \"trainer_history.json\"\n",
    "\n",
    "RUN_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "with MagicTimer() as timer:\n",
    "    dataset = datasets.load_dataset(\n",
    "        \"sradc/chunked-shuffled-wikipedia20220301en-bookcorpusopen\",\n",
    "        split=f\"train[:{LIMIT_DATASET}]\" if LIMIT_DATASET else \"train\",\n",
    "        revision=\"0e6fada2dd43136e4a3f637da41de2e596aee674\",\n",
    "    )\n",
    "print(f\"Loaded dataset in {timer}\")\n",
    "len(dataset)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feeding samples to tokenizer:  23%|██▎       | 2304/10000 [00:00<00:00, 21556.18it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feeding samples to tokenizer: 100%|██████████| 10000/10000 [00:00<00:00, 17984.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Tokenizer trained in 1.4 seconds.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('[MASK]', 4)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = BertWordPieceTokenizer()\n",
    "tokenizer._tokenizer.normalizer = normalizers.Sequence(\n",
    "    [\n",
    "        normalizers.Replace(Regex(\"(``|'')\"), '\"'),\n",
    "        normalizers.NFD(),\n",
    "        normalizers.Lowercase(),\n",
    "        normalizers.StripAccents(),\n",
    "        normalizers.Replace(Regex(\" {2,}\"), \" \"),\n",
    "        normalizers.Replace(Regex(r\"[^\\x00-\\x7F]+\"), \"\"),\n",
    "    ]\n",
    ")  # Normalizer based on, https://github.com/JonasGeiping/cramming/blob/50bd06a65a4cd4a3dd6ee9ecce1809e1a9085374/cramming/data/tokenizer_preparation.py#L52\n",
    "def tokenizer_training_data() -> Iterator[str]:\n",
    "    for i in tqdm(\n",
    "        range(min(NUM_TOKENIZER_TRAINING_ITEMS, len(dataset))),\n",
    "        desc=\"Feeding samples to tokenizer\",\n",
    "    ):\n",
    "        yield dataset[i][\"text\"]\n",
    "\n",
    "\n",
    "with MagicTimer() as timer:\n",
    "    tokenizer.train_from_iterator(\n",
    "        tokenizer_training_data(),\n",
    "        vocab_size=VOCAB_SIZE,\n",
    "        min_frequency=2,\n",
    "    )\n",
    "print(f\"Tokenizer trained in {timer}.\")\n",
    "tokenizer.save(str(TOKENIZER_PATH))\n",
    "tokenizer = BertTokenizerFast(tokenizer_file=str(TOKENIZER_PATH))\n",
    "tokenizer.unk_token,tokenizer.unk_token_id,tokenizer.sep_token,tokenizer.sep_token_id,tokenizer.pad_token,tokenizer.pad_token_id,tokenizer.cls_token,tokenizer.cls_token_id,tokenizer.mask_token,tokenizer.mask_token_id\n",
    "\n",
    "class HFTokenizedDataset(torch.utils.data.Dataset):\n",
    "    \"This wraps the dataset and tokenizes it, ready for the model\"\n",
    "\n",
    "    def __init__(self, dataset, tokenizer):\n",
    "        self.dataset = dataset\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        inps= self.tokenizer.encode(\n",
    "            self.dataset[i][\"text\"],\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            max_length=MODEL_MAX_SEQ_LEN - 2,\n",
    "            padding=\"max_length\",\n",
    "            return_special_tokens_mask=True,\n",
    "        )[0, ...]\n",
    "        return {'input_ids':inps}\n",
    "\n",
    "class TokenizedDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def convert_tokens_to_ids(self,s):\n",
    "        s=norm.normalize_str(s)\n",
    "        tokens = vocab.tokenize(s).tolist()\n",
    "        tokens=tokens[:MODEL_MAX_SEQ_LEN - 2]\n",
    "        tokens=torch.Tensor(tokens).long()\n",
    "        return tokens\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        s=norm.normalize_str(self.dataset[i][\"text\"])\n",
    "        tokens = vocab.tokenize(s).tolist()\n",
    "        \n",
    "        #trucate\n",
    "        tokens=tokens[:MODEL_MAX_SEQ_LEN - 2]\n",
    "        l=len(tokens)\n",
    "        for j in range(l,MODEL_MAX_SEQ_LEN - 2):\n",
    "            tokens.append(0)\n",
    "        tokens=torch.Tensor(tokens)\n",
    "        if MODEL_MAX_SEQ_LEN - 2>l:\n",
    "            att_mask=np.concatenate((np.ones(l),np.zeros(MODEL_MAX_SEQ_LEN - 2-l)))\n",
    "        else:\n",
    "            att_mask=np.ones(tokens.shape[0])\n",
    "        # print(l,att_mask.shape[0],MODEL_MAX_SEQ_LEN - 2)\n",
    "        assert tokens.shape[0]==att_mask.shape[0]\n",
    "\n",
    "        input_ids=tokens\n",
    "        # token_type_ids=torch.Tensor(np.zeros(tokens.shape[0]))\n",
    "        # attention_mask=torch.Tensor(att_mask)\n",
    "\n",
    "        input_ids=torch.as_tensor(input_ids,dtype=torch.long)\n",
    "        # token_type_ids=torch.as_tensor(token_type_ids,dtype=torch.long)\n",
    "        # attention_mask=torch.as_tensor(attention_mask,dtype=torch.long)\n",
    "\n",
    "        # module_id=torch.Tensor(np.zeros(tokens.shape[0])).long()\n",
    "        module_id=0\n",
    "        labels = torch.zeros(input_ids.shape)\n",
    "        probability_matrix = torch.full(labels.shape, 0.15)\n",
    "        masked_indices = torch.bernoulli(probability_matrix).bool()\n",
    "        indices_replaced = torch.bernoulli(torch.full(labels.shape, 1.0)).bool() & masked_indices\n",
    "        labels[indices_replaced] =1\n",
    "        mask=torch.as_tensor(labels,dtype=torch.bool)\n",
    "        \n",
    "        d={'input_ids':input_ids, \n",
    "            # 'token_type_ids':token_type_ids, \n",
    "            # 'attention_mask':attention_mask,\n",
    "            'mask':mask}\n",
    "        return d\n",
    "\n",
    "\n",
    "def get_vocab():\n",
    "    #### TokenMonster BRRR!!!\n",
    "    import tokenmonster\n",
    "    vocab = tokenmonster.load(\"englishcode-16000-balanced-v1\")\n",
    "\n",
    "    norm=normalizers.Sequence(\n",
    "        [\n",
    "            normalizers.Replace(Regex(\"(``|'')\"), '\"'),\n",
    "            normalizers.NFD(),\n",
    "            normalizers.Lowercase(),\n",
    "            normalizers.StripAccents(),\n",
    "            normalizers.Replace(Regex(\" {2,}\"), \" \"),\n",
    "            normalizers.Replace(Regex(r\"[^\\x00-\\x7F]+\"), \"\"),\n",
    "        ]\n",
    "    )\n",
    "    vocab.modify(\"[EOS]\")\n",
    "    vocab.modify(\"[UNK]\")\n",
    "    vocab.modify(\"[SEP]\")\n",
    "    vocab.modify(\"[PAD]\")\n",
    "    vocab.modify(\"[CLS]\")\n",
    "    vocab.modify(\"[MASK]\")\n",
    "    # ('[UNK]', 1, '[SEP]', 3, '[PAD]', 0, '[CLS]', 2, '[MASK]', 4)\n",
    "    # repl_words=vocab.id2word[:5]\n",
    "    # repl_words\n",
    "    # del vocab.word2id[b'']\n",
    "    # del vocab.word2id[b'\\x01']\n",
    "    # del vocab.word2id[b'\\x02']\n",
    "    # del vocab.word2id[b'\\x03']\n",
    "    # del vocab.word2id[b'\\x04']\n",
    "\n",
    "    # vocab.word2id[b'[UNK]']=1\n",
    "    # vocab.word2id[b'[SEP]']=3\n",
    "    # vocab.word2id[b'[PAD]']=0\n",
    "    # vocab.word2id[b'[CLS]']=2\n",
    "    # vocab.word2id[b'[MASK]']=4\n",
    "\n",
    "    # vocab.word2id[b'']=len(vocab.word2id)+1\n",
    "    # vocab.word2id[b'\\x01']=len(vocab.word2id)+2\n",
    "    # vocab.word2id[b'\\x02']=len(vocab.word2id)+3\n",
    "    # vocab.word2id[b'\\x03']=len(vocab.word2id)+4\n",
    "    # vocab.word2id[b'\\x04']=len(vocab.word2id)+5\n",
    "\n",
    "    # vocab.id2word[0]=b'[UNK]'\n",
    "    # vocab.id2word[1]=b'[SEP]'\n",
    "    # vocab.id2word[2]=b'[PAD]'\n",
    "    # vocab.id2word[3]=b'[CLS]'\n",
    "    # vocab.id2word[4]=b'[MASK]'\n",
    "\n",
    "    # vocab.id2word.append(b'')\n",
    "    # vocab.id2word.append(b'\\x01')\n",
    "    # vocab.id2word.append(b'\\x02')\n",
    "    # vocab.id2word.append(b'\\x03')\n",
    "    # vocab.id2word.append(b'\\x04')\n",
    "    return norm,vocab\n",
    "norm,vocab=get_vocab()\n",
    "MASK_ID=vocab.tokenize(\"[MASK]\")[0]\n",
    "PAD_ID=vocab.tokenize(\"[PAD]\")[0]\n",
    "if model_training=='bert':\n",
    "    tokenized_dataset = HFTokenizedDataset(dataset, tokenizer)\n",
    "else:\n",
    "    tokenized_dataset = TokenizedDataset(dataset)\n",
    "\n",
    "\n",
    "tokenizer.mask_token,tokenizer.convert_tokens_to_ids(tokenizer.mask_token)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Electra"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from functools import reduce\n",
    "from collections import namedtuple\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# constants\n",
    "\n",
    "Results = namedtuple('Results', [\n",
    "    'loss',\n",
    "    'mlm_loss',\n",
    "    'disc_loss',\n",
    "    'gen_acc',\n",
    "    'disc_acc',\n",
    "    'disc_labels',\n",
    "    'disc_predictions'\n",
    "])\n",
    "\n",
    "# helpers\n",
    "def log(t, eps=1e-9):\n",
    "    return torch.log(t + eps)\n",
    "\n",
    "def gumbel_noise(t):\n",
    "    noise = torch.zeros_like(t).uniform_(0, 1)\n",
    "    return -log(-log(noise))\n",
    "\n",
    "def gumbel_sample(t, temperature = 1.):\n",
    "    return ((t / temperature) + gumbel_noise(t)).argmax(dim=-1)\n",
    "\n",
    "def prob_mask_like(t, prob):\n",
    "    return torch.zeros_like(t).float().uniform_(0, 1) < prob\n",
    "\n",
    "def mask_with_tokens(t, token_ids):\n",
    "    init_no_mask = torch.full_like(t, False, dtype=torch.bool)\n",
    "    mask = reduce(lambda acc, el: acc | (t == el), token_ids, init_no_mask)\n",
    "    return mask\n",
    "\n",
    "def get_mask_subset_with_prob(mask, prob):\n",
    "    batch, seq_len, device = *mask.shape, mask.device\n",
    "    max_masked = math.ceil(prob * seq_len)\n",
    "\n",
    "    num_tokens = mask.sum(dim=-1, keepdim=True)\n",
    "    mask_excess = (mask.cumsum(dim=-1) > (num_tokens * prob).ceil())\n",
    "    mask_excess = mask_excess[:, :max_masked]\n",
    "\n",
    "    rand = torch.rand((batch, seq_len), device=device).masked_fill(~mask, -1e9)\n",
    "    _, sampled_indices = rand.topk(max_masked, dim=-1)\n",
    "    sampled_indices = (sampled_indices + 1).masked_fill_(mask_excess, 0)\n",
    "\n",
    "    new_mask = torch.zeros((batch, seq_len + 1), device=device)\n",
    "    new_mask.scatter_(-1, sampled_indices, 1)\n",
    "    return new_mask[:, 1:].bool()\n",
    "\n",
    "\n",
    "# hidden layer extractor class, for magically adding adapter to language model to be pretrained\n",
    "class HiddenLayerExtractor(nn.Module):\n",
    "    def __init__(self, net, layer = -2):\n",
    "        super().__init__()\n",
    "        self.net = net\n",
    "        self.layer = layer\n",
    "\n",
    "        self.hidden = None\n",
    "        self.hook_registered = False\n",
    "\n",
    "    def _find_layer(self):\n",
    "        if type(self.layer) == str:\n",
    "            modules = dict([*self.net.named_modules()])\n",
    "            return modules.get(self.layer, None)\n",
    "        elif type(self.layer) == int:\n",
    "            children = [*self.net.children()]\n",
    "            return children[self.layer]\n",
    "        return None\n",
    "\n",
    "    def _hook(self, _, __, output):\n",
    "        self.hidden = output\n",
    "\n",
    "    def _register_hook(self):\n",
    "        layer = self._find_layer()\n",
    "        assert layer is not None, f'hidden layer ({self.layer}) not found'\n",
    "        handle = layer.register_forward_hook(self._hook)\n",
    "        self.hook_registered = True\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.layer == -1:\n",
    "            return self.net(x)\n",
    "\n",
    "        if not self.hook_registered:\n",
    "            self._register_hook()\n",
    "\n",
    "        _ = self.net(x)\n",
    "        hidden = self.hidden\n",
    "        self.hidden = None\n",
    "        assert hidden is not None, f'hidden layer {self.layer} never emitted an output'\n",
    "        return hidden\n",
    "    \n",
    "class Electra(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        generator,\n",
    "        discriminator,\n",
    "        teacher,\n",
    "        *,\n",
    "        num_tokens = None,\n",
    "        discr_dim = -1,\n",
    "        discr_layer = -1,\n",
    "        mask_prob = 0.15,\n",
    "        replace_prob = 0.85,\n",
    "        random_token_prob = 0.,\n",
    "        mask_token_id = 2,\n",
    "        pad_token_id = 0,\n",
    "        mask_ignore_token_ids = [],\n",
    "        disc_weight = 50.,\n",
    "        gen_weight = 1.,\n",
    "        temperature = 1.):\n",
    "        super().__init__()\n",
    "\n",
    "        self.generator = generator\n",
    "        self.discriminator = discriminator\n",
    "        self.teacher = teacher\n",
    "\n",
    "        if discr_dim > 0:\n",
    "            self.discriminator = nn.Sequential(\n",
    "                HiddenLayerExtractor(discriminator, layer = discr_layer),\n",
    "                nn.Linear(discr_dim, 1)\n",
    "            )\n",
    "\n",
    "        # mlm related probabilities\n",
    "        self.mask_prob = mask_prob\n",
    "        self.replace_prob = replace_prob\n",
    "\n",
    "        self.num_tokens = num_tokens\n",
    "        self.random_token_prob = random_token_prob\n",
    "\n",
    "        # token ids\n",
    "        self.pad_token_id = pad_token_id\n",
    "        self.mask_token_id = mask_token_id\n",
    "        self.mask_ignore_token_ids = set([*mask_ignore_token_ids, pad_token_id])\n",
    "\n",
    "        # sampling temperature\n",
    "        self.temperature = temperature\n",
    "\n",
    "        # loss weights\n",
    "        self.disc_weight = disc_weight\n",
    "        self.gen_weight = gen_weight\n",
    "\n",
    "\n",
    "    def forward(self, input, **kwargs):\n",
    "        b, t = input.shape\n",
    "\n",
    "        replace_prob = prob_mask_like(input, self.replace_prob)\n",
    "\n",
    "        # do not mask [pad] tokens, or any other tokens in the tokens designated to be excluded ([cls], [sep])\n",
    "        # also do not include these special tokens in the tokens chosen at random\n",
    "        no_mask = mask_with_tokens(input, self.mask_ignore_token_ids)\n",
    "        mask = get_mask_subset_with_prob(~no_mask, self.mask_prob)\n",
    "\n",
    "        # get mask indices\n",
    "        mask_indices = torch.nonzero(mask, as_tuple=True)\n",
    "\n",
    "        # mask input with mask tokens with probability of `replace_prob` (keep tokens the same with probability 1 - replace_prob)\n",
    "        masked_input = input.clone().detach()\n",
    "\n",
    "        # set inverse of mask to padding tokens for labels\n",
    "        gen_labels = input.masked_fill(~mask, self.pad_token_id)\n",
    "\n",
    "        # clone the mask, for potential modification if random tokens are involved\n",
    "        # not to be mistakened for the mask above, which is for all tokens, whether not replaced nor replaced with random tokens\n",
    "        masking_mask = mask.clone()\n",
    "\n",
    "        # if random token probability > 0 for mlm\n",
    "        if self.random_token_prob > 0:\n",
    "            assert self.num_tokens is not None, 'Number of tokens (num_tokens) must be passed to Electra for randomizing tokens during masked language modeling'\n",
    "\n",
    "            random_token_prob = prob_mask_like(input, self.random_token_prob)\n",
    "            random_tokens = torch.randint(0, self.num_tokens, input.shape, device=input.device)\n",
    "            random_no_mask = mask_with_tokens(random_tokens, self.mask_ignore_token_ids)\n",
    "            random_token_prob &= ~random_no_mask\n",
    "            masked_input = torch.where(random_token_prob, random_tokens, masked_input)\n",
    "\n",
    "            # remove random token prob mask from masking mask\n",
    "            masking_mask = masking_mask & ~random_token_prob\n",
    "\n",
    "        # [mask] input\n",
    "        masked_input = masked_input.masked_fill(masking_mask * replace_prob, self.mask_token_id)\n",
    "\n",
    "        # get generator output and get mlm loss\n",
    "        logits = self.generator(masked_input, **kwargs)\n",
    "\n",
    "        mlm_loss = F.cross_entropy(\n",
    "            logits.transpose(1, 2),\n",
    "            gen_labels,\n",
    "            ignore_index = self.pad_token_id\n",
    "        )\n",
    "\n",
    "        # use mask from before to select logits that need sampling\n",
    "        sample_logits = logits[mask_indices]\n",
    "\n",
    "        # sample\n",
    "        sampled = gumbel_sample(sample_logits, temperature = self.temperature)\n",
    "\n",
    "        # scatter the sampled values back to the input\n",
    "        disc_input = input.clone()\n",
    "        disc_input[mask_indices] = sampled.detach()\n",
    "\n",
    "        # generate discriminator labels, with replaced as True and original as False\n",
    "        disc_labels = (input != disc_input).float().detach()\n",
    "\n",
    "        # get discriminator predictions of replaced / original\n",
    "        non_padded_indices = torch.nonzero(input != self.pad_token_id, as_tuple=True)\n",
    "\n",
    "        # get discriminator output and binary cross entropy loss\n",
    "        disc_logits = self.discriminator(disc_input, **kwargs)\n",
    "        disc_logits = disc_logits.reshape_as(disc_labels)\n",
    "\n",
    "        disc_loss = F.binary_cross_entropy_with_logits(\n",
    "            disc_logits[non_padded_indices],\n",
    "            disc_labels[non_padded_indices]\n",
    "        )\n",
    "\n",
    "        # gather metrics\n",
    "        with torch.no_grad():\n",
    "            gen_predictions = torch.argmax(logits, dim=-1)\n",
    "            disc_predictions = torch.round((torch.sign(disc_logits) + 1.0) * 0.5)\n",
    "            gen_acc = (gen_labels[mask] == gen_predictions[mask]).float().mean()\n",
    "            disc_acc = 0.5 * (disc_labels[mask] == disc_predictions[mask]).float().mean() + 0.5 * (disc_labels[~mask] == disc_predictions[~mask]).float().mean()\n",
    "\n",
    "        # return weighted sum of losses\n",
    "        return Results(self.gen_weight * mlm_loss + self.disc_weight * disc_loss, mlm_loss, disc_loss, gen_acc, disc_acc, disc_labels, disc_predictions)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install x_transformers reformer_pytorch accelerate einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-A100 GPU detected, using math or mem efficient attention if input tensor is on cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from reformer_pytorch import ReformerLM\n",
    "from x_transformers import TransformerWrapper, Decoder,Encoder\n",
    "\n",
    "# (1) instantiate the generator and discriminator, making sure that the generator is roughly a quarter to a half of the size of the discriminator\n",
    "\n",
    "\n",
    "generator = TransformerWrapper(\n",
    "    num_tokens = VOCAB_SIZE+8,\n",
    "    max_seq_len = MODEL_MAX_SEQ_LEN,\n",
    "    attn_layers = Decoder(\n",
    "        dim = 256,\n",
    "        depth = 12,\n",
    "        heads = 4,\n",
    "        attn_flash = True,\n",
    "        # rel_pos_bias = True \n",
    "    )\n",
    ")\n",
    "\n",
    "teacher = TransformerWrapper(\n",
    "    num_tokens = VOCAB_SIZE+8,\n",
    "    max_seq_len = MODEL_MAX_SEQ_LEN,\n",
    "    attn_layers = Decoder(\n",
    "        dim = 256,\n",
    "        depth = 12,\n",
    "        heads = 4,\n",
    "        attn_flash = True,\n",
    "        # rel_pos_bias = True \n",
    "    )\n",
    ")\n",
    "\n",
    "discriminator = TransformerWrapper(\n",
    "    num_tokens = VOCAB_SIZE+8,\n",
    "    max_seq_len = MODEL_MAX_SEQ_LEN,\n",
    "    attn_layers = Decoder(\n",
    "        dim = 768,\n",
    "        depth = 12,\n",
    "        heads = 12,\n",
    "        attn_flash = True,\n",
    "        # rel_pos_bias = True \n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "model = Electra(\n",
    "    generator,\n",
    "    discriminator,\n",
    "    teacher,\n",
    "    discr_dim = 768,           # the embedding dimension of the discriminator\n",
    "    # discr_layer = 'reformer',   # the layer name in the discriminator, whose output would be used for predicting token is still the same or replaced\n",
    "    discr_layer = 'attn_layers',   # the layer name in the discriminator, whose output would be used for predicting token is still the same or replaced\n",
    "    mask_token_id = MASK_ID,          # the token id reserved for masking\n",
    "    pad_token_id = PAD_ID,           # the token id for padding\n",
    "    mask_prob = 0.15,           # masking probability for masked language modeling\n",
    "    mask_ignore_token_ids = []  # ids of tokens to ignore for mask modeling ex. (cls, sep)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_ids are the indices corresponding to each token in the sentence.\n",
    "# attention_mask indicates whether a token should be attended to or not.\n",
    "# token_type_ids identifies which sequence a token belongs to when there is more than one sequence\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "train_dataloader = DataLoader(\n",
    "        tokenized_dataset, shuffle=True, batch_size=DEVICE_BATCH_SIZE\n",
    "    )\n",
    "# Optimizer\n",
    "learning_rate=5e-5\n",
    "weight_decay=0\n",
    "\n",
    "gradient_accumulation_steps=1\n",
    "max_train_steps=None\n",
    "num_train_epochs=1\n",
    "lr_scheduler_type='linear'\n",
    "num_warmup_steps=0\n",
    "# Split weights in two groups, one with weight decay and the other not.\n",
    "no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "optimizer_grouped_parameters = [\n",
    "    {\n",
    "        \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "        \"weight_decay\": weight_decay,\n",
    "    },\n",
    "    {\n",
    "        \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "        \"weight_decay\": 0.0,\n",
    "    },\n",
    "]\n",
    "optimizer = torch.optim.AdamW(optimizer_grouped_parameters, lr=learning_rate)\n",
    "\n",
    "import math \n",
    "from transformers import (\n",
    "    get_scheduler,\n",
    ")\n",
    "# Scheduler and math around the number of training steps.\n",
    "overrode_max_train_steps = False\n",
    "num_update_steps_per_epoch = math.ceil(len(train_dataloader) / gradient_accumulation_steps)\n",
    "if max_train_steps is None:\n",
    "    max_train_steps = num_train_epochs * num_update_steps_per_epoch\n",
    "    overrode_max_train_steps = True\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=lr_scheduler_type,\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=num_warmup_steps * gradient_accumulation_steps,\n",
    "    num_training_steps=max_train_steps * gradient_accumulation_steps,\n",
    ")\n",
    "from accelerate import Accelerator\n",
    "\n",
    "accelerator = Accelerator()\n",
    "# Prepare everything with our `accelerator`.\n",
    "model, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(\n",
    "    model, optimizer, train_dataloader, lr_scheduler\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration:  0 , total_loss:  1.3024195353190104\n",
      "count_amostra: 200\n",
      "iteration:  30 , total_loss:  37.32218316396077\n",
      "count_amostra: 6200\n",
      "iteration:  60 , total_loss:  30.727979278564455\n",
      "count_amostra: 12200\n",
      "iteration:  90 , total_loss:  30.368733088175457\n",
      "count_amostra: 18200\n",
      "iteration:  120 , total_loss:  30.114696057637534\n",
      "count_amostra: 24200\n",
      "iteration:  150 , total_loss:  29.87586498260498\n",
      "count_amostra: 30200\n",
      "iteration:  180 , total_loss:  29.658064524332683\n",
      "count_amostra: 36200\n",
      "iteration:  210 , total_loss:  29.45107390085856\n",
      "count_amostra: 42200\n",
      "iteration:  240 , total_loss:  29.29213104248047\n",
      "count_amostra: 48200\n",
      "iteration:  270 , total_loss:  29.20277016957601\n",
      "count_amostra: 54200\n",
      "iteration:  300 , total_loss:  29.135904630025227\n",
      "count_amostra: 60200\n",
      "iteration:  330 , total_loss:  29.104577191670735\n",
      "count_amostra: 66200\n",
      "iteration:  360 , total_loss:  29.052973365783693\n",
      "count_amostra: 72200\n",
      "iteration:  390 , total_loss:  28.95620543162028\n",
      "count_amostra: 78200\n",
      "iteration:  420 , total_loss:  28.929990196228026\n",
      "count_amostra: 84200\n",
      "iteration:  450 , total_loss:  28.92066682179769\n",
      "count_amostra: 90200\n",
      "iteration:  480 , total_loss:  28.86379273732503\n",
      "count_amostra: 96200\n",
      "iteration:  510 , total_loss:  28.80905507405599\n",
      "count_amostra: 102200\n",
      "iteration:  540 , total_loss:  28.736702537536623\n",
      "count_amostra: 108200\n",
      "iteration:  570 , total_loss:  28.68769423166911\n",
      "count_amostra: 114200\n",
      "iteration:  600 , total_loss:  28.628900019327798\n",
      "count_amostra: 120200\n",
      "iteration:  630 , total_loss:  28.582971382141114\n",
      "count_amostra: 126200\n",
      "iteration:  660 , total_loss:  28.565596707661946\n",
      "count_amostra: 132200\n",
      "iteration:  690 , total_loss:  28.487178738911947\n",
      "count_amostra: 138200\n",
      "iteration:  720 , total_loss:  28.46884104410807\n",
      "count_amostra: 144200\n",
      "iteration:  750 , total_loss:  28.388462511698403\n",
      "count_amostra: 150200\n",
      "iteration:  780 , total_loss:  28.334489822387695\n",
      "count_amostra: 156200\n",
      "iteration:  810 , total_loss:  28.264921442667642\n",
      "count_amostra: 162200\n",
      "iteration:  840 , total_loss:  28.216868082682293\n",
      "count_amostra: 168200\n",
      "iteration:  870 , total_loss:  28.1544095993042\n",
      "count_amostra: 174200\n",
      "iteration:  900 , total_loss:  28.100372505187988\n",
      "count_amostra: 180200\n",
      "iteration:  930 , total_loss:  28.05932985941569\n",
      "count_amostra: 186200\n",
      "iteration:  960 , total_loss:  28.0530548731486\n",
      "count_amostra: 192200\n",
      "iteration:  990 , total_loss:  27.932428932189943\n",
      "count_amostra: 198200\n",
      "iteration:  1020 , total_loss:  27.852119890848794\n",
      "count_amostra: 204200\n",
      "iteration:  1050 , total_loss:  27.869802729288736\n",
      "count_amostra: 210200\n",
      "iteration:  1080 , total_loss:  27.850279235839842\n",
      "count_amostra: 216200\n",
      "iteration:  1110 , total_loss:  27.7928108215332\n",
      "count_amostra: 222200\n",
      "iteration:  1140 , total_loss:  27.747148704528808\n",
      "count_amostra: 228200\n",
      "iteration:  1170 , total_loss:  27.712524477640788\n",
      "count_amostra: 234200\n",
      "iteration:  1200 , total_loss:  27.686176300048828\n",
      "count_amostra: 240200\n",
      "iteration:  1230 , total_loss:  27.652083778381346\n",
      "count_amostra: 246200\n",
      "iteration:  1260 , total_loss:  27.603722445170085\n",
      "count_amostra: 252200\n",
      "iteration:  1290 , total_loss:  27.63289318084717\n",
      "count_amostra: 258200\n",
      "iteration:  1320 , total_loss:  27.596427090962727\n",
      "count_amostra: 264200\n",
      "iteration:  1350 , total_loss:  27.55080140431722\n",
      "count_amostra: 270200\n",
      "iteration:  1380 , total_loss:  27.52835890452067\n",
      "count_amostra: 276200\n",
      "iteration:  1410 , total_loss:  27.459033203125\n",
      "count_amostra: 282200\n",
      "iteration:  1440 , total_loss:  27.47919069925944\n",
      "count_amostra: 288200\n",
      "iteration:  1470 , total_loss:  27.390868949890137\n",
      "count_amostra: 294200\n",
      "iteration:  1500 , total_loss:  27.41135679880778\n",
      "count_amostra: 300200\n",
      "iteration:  1530 , total_loss:  27.34641761779785\n",
      "count_amostra: 306200\n",
      "iteration:  1560 , total_loss:  27.32999261220296\n",
      "count_amostra: 312200\n",
      "iteration:  1590 , total_loss:  27.3179651260376\n",
      "count_amostra: 318200\n",
      "iteration:  1620 , total_loss:  27.200008583068847\n",
      "count_amostra: 324200\n",
      "iteration:  1650 , total_loss:  27.277825927734376\n",
      "count_amostra: 330200\n",
      "iteration:  1680 , total_loss:  27.207225227355956\n",
      "count_amostra: 336200\n",
      "iteration:  1710 , total_loss:  27.19172541300456\n",
      "count_amostra: 342200\n",
      "iteration:  1740 , total_loss:  27.150277519226073\n",
      "count_amostra: 348200\n",
      "iteration:  1770 , total_loss:  27.076463190714517\n",
      "count_amostra: 354200\n",
      "iteration:  1800 , total_loss:  27.00948975880941\n",
      "count_amostra: 360200\n",
      "iteration:  1830 , total_loss:  27.002360153198243\n",
      "count_amostra: 366200\n",
      "iteration:  1860 , total_loss:  26.987119928995767\n",
      "count_amostra: 372200\n",
      "iteration:  1890 , total_loss:  26.975978660583497\n",
      "count_amostra: 378200\n",
      "iteration:  1920 , total_loss:  26.880319849650064\n",
      "count_amostra: 384200\n",
      "iteration:  1950 , total_loss:  26.850170771280926\n",
      "count_amostra: 390200\n",
      "iteration:  1980 , total_loss:  26.770587158203124\n",
      "count_amostra: 396200\n",
      "iteration:  2010 , total_loss:  26.69279499053955\n",
      "count_amostra: 402200\n",
      "iteration:  2040 , total_loss:  26.652049128214518\n",
      "count_amostra: 408200\n",
      "iteration:  2070 , total_loss:  26.557310231526692\n",
      "count_amostra: 414200\n",
      "iteration:  2100 , total_loss:  26.489923604329427\n",
      "count_amostra: 420200\n",
      "iteration:  2130 , total_loss:  26.437201754252115\n",
      "count_amostra: 426200\n",
      "iteration:  2160 , total_loss:  26.44807186126709\n",
      "count_amostra: 432200\n",
      "iteration:  2190 , total_loss:  26.386232821146645\n",
      "count_amostra: 438200\n",
      "iteration:  2220 , total_loss:  26.24193064371745\n",
      "count_amostra: 444200\n",
      "iteration:  2250 , total_loss:  26.193091201782227\n",
      "count_amostra: 450200\n",
      "iteration:  2280 , total_loss:  26.086643600463866\n",
      "count_amostra: 456200\n",
      "iteration:  2310 , total_loss:  26.04658711751302\n",
      "count_amostra: 462200\n",
      "iteration:  2340 , total_loss:  26.044544665018716\n",
      "count_amostra: 468200\n",
      "iteration:  2370 , total_loss:  26.00792948404948\n",
      "count_amostra: 474200\n",
      "iteration:  2400 , total_loss:  25.923599688212075\n",
      "count_amostra: 480200\n",
      "iteration:  2430 , total_loss:  25.820995585123697\n",
      "count_amostra: 486200\n",
      "iteration:  2460 , total_loss:  25.83982213338216\n",
      "count_amostra: 492200\n",
      "iteration:  2490 , total_loss:  25.751530838012695\n",
      "count_amostra: 498200\n",
      "iteration:  2520 , total_loss:  25.649939092000327\n",
      "count_amostra: 504200\n",
      "iteration:  2550 , total_loss:  25.684587542215983\n",
      "count_amostra: 510200\n",
      "iteration:  2580 , total_loss:  25.533026377360027\n",
      "count_amostra: 516200\n",
      "iteration:  2610 , total_loss:  25.603793144226074\n",
      "count_amostra: 522200\n",
      "iteration:  2640 , total_loss:  25.495342191060384\n",
      "count_amostra: 528200\n",
      "iteration:  2670 , total_loss:  25.544228426615398\n",
      "count_amostra: 534200\n",
      "iteration:  2700 , total_loss:  25.46683006286621\n",
      "count_amostra: 540200\n",
      "iteration:  2730 , total_loss:  25.32946434020996\n",
      "count_amostra: 546200\n",
      "iteration:  2760 , total_loss:  25.323251978556314\n",
      "count_amostra: 552200\n",
      "iteration:  2790 , total_loss:  25.317038790384927\n",
      "count_amostra: 558200\n",
      "iteration:  2820 , total_loss:  25.385084279378255\n",
      "count_amostra: 564200\n",
      "iteration:  2850 , total_loss:  25.278358459472656\n",
      "count_amostra: 570200\n",
      "iteration:  2880 , total_loss:  25.209202575683594\n",
      "count_amostra: 576200\n",
      "iteration:  2910 , total_loss:  25.180811754862468\n",
      "count_amostra: 582200\n",
      "iteration:  2940 , total_loss:  25.118218231201173\n",
      "count_amostra: 588200\n",
      "iteration:  2970 , total_loss:  25.14486853281657\n",
      "count_amostra: 594200\n",
      "iteration:  3000 , total_loss:  25.03113905588786\n",
      "count_amostra: 600200\n",
      "iteration:  3030 , total_loss:  25.06401278177897\n",
      "count_amostra: 606200\n",
      "iteration:  3060 , total_loss:  25.051449457804363\n",
      "count_amostra: 612200\n",
      "iteration:  3090 , total_loss:  25.01235548655192\n",
      "count_amostra: 618200\n",
      "iteration:  3120 , total_loss:  25.063288180033364\n",
      "count_amostra: 624200\n",
      "iteration:  3150 , total_loss:  24.998380343119305\n",
      "count_amostra: 630200\n",
      "iteration:  3180 , total_loss:  24.943325678507488\n",
      "count_amostra: 636200\n",
      "iteration:  3210 , total_loss:  24.984814325968426\n",
      "count_amostra: 642200\n",
      "iteration:  3240 , total_loss:  24.9304074605306\n",
      "count_amostra: 648200\n",
      "iteration:  3270 , total_loss:  24.893740971883137\n",
      "count_amostra: 654200\n",
      "iteration:  3300 , total_loss:  24.964223353068032\n",
      "count_amostra: 660200\n",
      "iteration:  3330 , total_loss:  24.89692300160726\n",
      "count_amostra: 666200\n",
      "iteration:  3360 , total_loss:  24.903004964192707\n",
      "count_amostra: 672200\n",
      "iteration:  3390 , total_loss:  24.87404397328695\n",
      "count_amostra: 678200\n",
      "iteration:  3420 , total_loss:  24.75404930114746\n",
      "count_amostra: 684200\n",
      "iteration:  3450 , total_loss:  24.822771517435708\n",
      "count_amostra: 690200\n",
      "iteration:  3480 , total_loss:  24.80795580546061\n",
      "count_amostra: 696200\n",
      "iteration:  3510 , total_loss:  24.769543011983234\n",
      "count_amostra: 702200\n",
      "iteration:  3540 , total_loss:  24.795761362711588\n",
      "count_amostra: 708200\n",
      "iteration:  3570 , total_loss:  24.766866175333657\n",
      "count_amostra: 714200\n",
      "iteration:  3600 , total_loss:  24.772153663635255\n",
      "count_amostra: 720200\n",
      "iteration:  3630 , total_loss:  24.736094665527343\n",
      "count_amostra: 726200\n",
      "iteration:  3660 , total_loss:  24.7430632909139\n",
      "count_amostra: 732200\n",
      "iteration:  3690 , total_loss:  24.694792238871255\n",
      "count_amostra: 738200\n",
      "iteration:  3720 , total_loss:  24.679961585998534\n",
      "count_amostra: 744200\n",
      "iteration:  3750 , total_loss:  24.76645482381185\n",
      "count_amostra: 750200\n",
      "iteration:  3780 , total_loss:  24.732465680440267\n",
      "count_amostra: 756200\n",
      "iteration:  3810 , total_loss:  24.6979647954305\n",
      "count_amostra: 762200\n",
      "iteration:  3840 , total_loss:  24.682735888163247\n",
      "count_amostra: 768200\n",
      "iteration:  3870 , total_loss:  24.625495020548502\n",
      "count_amostra: 774200\n",
      "iteration:  3900 , total_loss:  24.62864767710368\n",
      "count_amostra: 780200\n",
      "iteration:  3930 , total_loss:  24.587626393636068\n",
      "count_amostra: 786200\n",
      "iteration:  3960 , total_loss:  24.64768295288086\n",
      "count_amostra: 792200\n",
      "iteration:  3990 , total_loss:  24.71041342417399\n",
      "count_amostra: 798200\n",
      "iteration:  4020 , total_loss:  24.616481018066406\n",
      "count_amostra: 804200\n",
      "iteration:  4050 , total_loss:  24.660499064127603\n",
      "count_amostra: 810200\n",
      "iteration:  4080 , total_loss:  24.534398206075032\n",
      "count_amostra: 816200\n",
      "iteration:  4110 , total_loss:  24.560730934143066\n",
      "count_amostra: 822200\n",
      "iteration:  4140 , total_loss:  24.590504201253257\n",
      "count_amostra: 828200\n",
      "iteration:  4170 , total_loss:  24.55551681518555\n",
      "count_amostra: 834200\n",
      "iteration:  4200 , total_loss:  24.527773348490395\n",
      "count_amostra: 840200\n",
      "iteration:  4230 , total_loss:  24.594767888387043\n",
      "count_amostra: 846200\n",
      "iteration:  4260 , total_loss:  24.518433316548666\n",
      "count_amostra: 852200\n",
      "iteration:  4290 , total_loss:  24.571083895365398\n",
      "count_amostra: 858200\n",
      "iteration:  4320 , total_loss:  24.600889078776042\n",
      "count_amostra: 864200\n",
      "iteration:  4350 , total_loss:  24.43493429819743\n",
      "count_amostra: 870200\n",
      "iteration:  4380 , total_loss:  24.463630358378094\n",
      "count_amostra: 876200\n",
      "iteration:  4410 , total_loss:  24.465277735392252\n",
      "count_amostra: 882200\n",
      "iteration:  4440 , total_loss:  24.49222602844238\n",
      "count_amostra: 888200\n",
      "iteration:  4470 , total_loss:  24.448767852783202\n",
      "count_amostra: 894200\n",
      "iteration:  4500 , total_loss:  24.472281710306802\n",
      "count_amostra: 900200\n",
      "iteration:  4530 , total_loss:  24.44266300201416\n",
      "count_amostra: 906200\n",
      "iteration:  4560 , total_loss:  24.524937057495116\n",
      "count_amostra: 912200\n",
      "iteration:  4590 , total_loss:  24.44805997212728\n",
      "count_amostra: 918200\n",
      "iteration:  4620 , total_loss:  24.367743810017902\n",
      "count_amostra: 924200\n",
      "iteration:  4650 , total_loss:  24.49956703186035\n",
      "count_amostra: 930200\n",
      "iteration:  4680 , total_loss:  24.40307839711507\n",
      "count_amostra: 936200\n",
      "iteration:  4710 , total_loss:  24.396963119506836\n",
      "count_amostra: 942200\n",
      "iteration:  4740 , total_loss:  24.364143816630044\n",
      "count_amostra: 948200\n",
      "iteration:  4770 , total_loss:  24.344661903381347\n",
      "count_amostra: 954200\n",
      "iteration:  4800 , total_loss:  24.3795565923055\n",
      "count_amostra: 960200\n",
      "iteration:  4830 , total_loss:  24.355326461791993\n",
      "count_amostra: 966200\n",
      "iteration:  4860 , total_loss:  24.359014320373536\n",
      "count_amostra: 972200\n",
      "iteration:  4890 , total_loss:  24.397886975606283\n",
      "count_amostra: 978200\n",
      "iteration:  4920 , total_loss:  24.34513053894043\n",
      "count_amostra: 984200\n",
      "iteration:  4950 , total_loss:  24.338418261210123\n",
      "count_amostra: 990200\n",
      "iteration:  4980 , total_loss:  24.33968931833903\n",
      "count_amostra: 996200\n",
      "iteration:  5010 , total_loss:  24.319810104370116\n",
      "count_amostra: 1002200\n",
      "iteration:  5040 , total_loss:  24.291506576538087\n",
      "count_amostra: 1008200\n",
      "iteration:  5070 , total_loss:  24.350197537740073\n",
      "count_amostra: 1014200\n",
      "iteration:  5100 , total_loss:  24.33142000834147\n",
      "count_amostra: 1020200\n",
      "iteration:  5130 , total_loss:  24.272923024495444\n",
      "count_amostra: 1026200\n",
      "iteration:  5160 , total_loss:  24.236409250895182\n",
      "count_amostra: 1032200\n",
      "iteration:  5190 , total_loss:  24.259842745463054\n",
      "count_amostra: 1038200\n",
      "iteration:  5220 , total_loss:  24.31591993967692\n",
      "count_amostra: 1044200\n",
      "iteration:  5250 , total_loss:  24.330285835266114\n",
      "count_amostra: 1050200\n",
      "iteration:  5280 , total_loss:  24.2046023050944\n",
      "count_amostra: 1056200\n",
      "iteration:  5310 , total_loss:  24.169354820251463\n",
      "count_amostra: 1062200\n",
      "iteration:  5340 , total_loss:  24.21937656402588\n",
      "count_amostra: 1068200\n",
      "iteration:  5370 , total_loss:  24.171071751912436\n",
      "count_amostra: 1074200\n",
      "iteration:  5400 , total_loss:  24.24481093088786\n",
      "count_amostra: 1080200\n",
      "iteration:  5430 , total_loss:  24.12484073638916\n",
      "count_amostra: 1086200\n",
      "iteration:  5460 , total_loss:  24.165197054545086\n",
      "count_amostra: 1092200\n",
      "iteration:  5490 , total_loss:  24.13311284383138\n",
      "count_amostra: 1098200\n",
      "iteration:  5520 , total_loss:  24.116692606608073\n",
      "count_amostra: 1104200\n",
      "iteration:  5550 , total_loss:  24.196871121724445\n",
      "count_amostra: 1110200\n",
      "iteration:  5580 , total_loss:  24.09423573811849\n",
      "count_amostra: 1116200\n",
      "iteration:  5610 , total_loss:  24.047692171732585\n",
      "count_amostra: 1122200\n",
      "iteration:  5640 , total_loss:  24.18294684092204\n",
      "count_amostra: 1128200\n",
      "iteration:  5670 , total_loss:  24.170275497436524\n",
      "count_amostra: 1134200\n",
      "iteration:  5700 , total_loss:  24.162228965759276\n",
      "count_amostra: 1140200\n",
      "iteration:  5730 , total_loss:  24.159944597880045\n",
      "count_amostra: 1146200\n",
      "iteration:  5760 , total_loss:  24.08878453572591\n",
      "count_amostra: 1152200\n",
      "iteration:  5790 , total_loss:  24.048011016845702\n",
      "count_amostra: 1158200\n",
      "iteration:  5820 , total_loss:  24.10339787801107\n",
      "count_amostra: 1164200\n",
      "iteration:  5850 , total_loss:  24.086875343322752\n",
      "count_amostra: 1170200\n",
      "iteration:  5880 , total_loss:  24.10647989908854\n",
      "count_amostra: 1176200\n",
      "iteration:  5910 , total_loss:  24.08130601247152\n",
      "count_amostra: 1182200\n",
      "iteration:  5940 , total_loss:  24.06743818918864\n",
      "count_amostra: 1188200\n",
      "iteration:  5970 , total_loss:  23.985470708211263\n",
      "count_amostra: 1194200\n",
      "iteration:  6000 , total_loss:  23.96372833251953\n",
      "count_amostra: 1200200\n",
      "iteration:  6030 , total_loss:  24.004066721598306\n",
      "count_amostra: 1206200\n",
      "iteration:  6060 , total_loss:  24.01032320658366\n",
      "count_amostra: 1212200\n",
      "iteration:  6090 , total_loss:  23.992572657267253\n",
      "count_amostra: 1218200\n",
      "iteration:  6120 , total_loss:  24.00693391164144\n",
      "count_amostra: 1224200\n",
      "iteration:  6150 , total_loss:  23.89302438100179\n",
      "count_amostra: 1230200\n",
      "iteration:  6180 , total_loss:  23.91500587463379\n",
      "count_amostra: 1236200\n",
      "iteration:  6210 , total_loss:  23.90733985900879\n",
      "count_amostra: 1242200\n",
      "iteration:  6240 , total_loss:  23.992286745707194\n",
      "count_amostra: 1248200\n",
      "iteration:  6270 , total_loss:  24.007227643330893\n",
      "count_amostra: 1254200\n",
      "iteration:  6300 , total_loss:  23.906472651163735\n",
      "count_amostra: 1260200\n",
      "iteration:  6330 , total_loss:  23.89921080271403\n",
      "count_amostra: 1266200\n",
      "iteration:  6360 , total_loss:  23.890692965189615\n",
      "count_amostra: 1272200\n",
      "iteration:  6390 , total_loss:  23.992274157206218\n",
      "count_amostra: 1278200\n",
      "iteration:  6420 , total_loss:  23.951194953918456\n",
      "count_amostra: 1284200\n",
      "iteration:  6450 , total_loss:  23.898098373413085\n",
      "count_amostra: 1290200\n",
      "iteration:  6480 , total_loss:  23.877624066670737\n",
      "count_amostra: 1296200\n",
      "iteration:  6510 , total_loss:  23.886262702941895\n",
      "count_amostra: 1302200\n",
      "iteration:  6540 , total_loss:  23.86642417907715\n",
      "count_amostra: 1308200\n",
      "iteration:  6570 , total_loss:  23.83353131612142\n",
      "count_amostra: 1314200\n",
      "iteration:  6600 , total_loss:  23.81912784576416\n",
      "count_amostra: 1320200\n",
      "iteration:  6630 , total_loss:  23.863351122538248\n",
      "count_amostra: 1326200\n",
      "iteration:  6660 , total_loss:  23.861350313822427\n",
      "count_amostra: 1332200\n",
      "iteration:  6690 , total_loss:  23.9004119237264\n",
      "count_amostra: 1338200\n",
      "iteration:  6720 , total_loss:  23.793857192993165\n",
      "count_amostra: 1344200\n",
      "iteration:  6750 , total_loss:  23.8884583791097\n",
      "count_amostra: 1350200\n",
      "iteration:  6780 , total_loss:  23.75458189646403\n",
      "count_amostra: 1356200\n",
      "iteration:  6810 , total_loss:  23.76533177693685\n",
      "count_amostra: 1362200\n",
      "iteration:  6840 , total_loss:  23.836883735656738\n",
      "count_amostra: 1368200\n",
      "iteration:  6870 , total_loss:  23.811466407775878\n",
      "count_amostra: 1374200\n",
      "iteration:  6900 , total_loss:  23.847612444559733\n",
      "count_amostra: 1380200\n",
      "iteration:  6930 , total_loss:  23.791803805033364\n",
      "count_amostra: 1386200\n",
      "iteration:  6960 , total_loss:  23.793547185262046\n",
      "count_amostra: 1392200\n",
      "iteration:  6990 , total_loss:  23.70821043650309\n",
      "count_amostra: 1398200\n",
      "iteration:  7020 , total_loss:  23.75830980936686\n",
      "count_amostra: 1404200\n",
      "iteration:  7050 , total_loss:  23.72436580657959\n",
      "count_amostra: 1410200\n",
      "iteration:  7080 , total_loss:  23.68675003051758\n",
      "count_amostra: 1416200\n",
      "iteration:  7110 , total_loss:  23.704813957214355\n",
      "count_amostra: 1422200\n",
      "iteration:  7140 , total_loss:  23.77806339263916\n",
      "count_amostra: 1428200\n",
      "iteration:  7170 , total_loss:  23.666627502441408\n",
      "count_amostra: 1434200\n",
      "iteration:  7200 , total_loss:  23.70755271911621\n",
      "count_amostra: 1440200\n",
      "iteration:  7230 , total_loss:  23.64183667500814\n",
      "count_amostra: 1446200\n",
      "iteration:  7260 , total_loss:  23.690746370951334\n",
      "count_amostra: 1452200\n",
      "iteration:  7290 , total_loss:  23.62987537384033\n",
      "count_amostra: 1458200\n",
      "iteration:  7320 , total_loss:  23.675991757710776\n",
      "count_amostra: 1464200\n",
      "iteration:  7350 , total_loss:  23.638867314656576\n",
      "count_amostra: 1470200\n",
      "iteration:  7380 , total_loss:  23.619344202677407\n",
      "count_amostra: 1476200\n",
      "iteration:  7410 , total_loss:  23.543138631184895\n",
      "count_amostra: 1482200\n",
      "iteration:  7440 , total_loss:  23.579280026753743\n",
      "count_amostra: 1488200\n",
      "iteration:  7470 , total_loss:  23.525090726216636\n",
      "count_amostra: 1494200\n",
      "iteration:  7500 , total_loss:  23.5246332804362\n",
      "count_amostra: 1500200\n",
      "iteration:  7530 , total_loss:  23.67901744842529\n",
      "count_amostra: 1506200\n",
      "iteration:  7560 , total_loss:  23.527286465962728\n",
      "count_amostra: 1512200\n",
      "iteration:  7590 , total_loss:  23.53254165649414\n",
      "count_amostra: 1518200\n",
      "iteration:  7620 , total_loss:  23.58756732940674\n",
      "count_amostra: 1524200\n",
      "iteration:  7650 , total_loss:  23.550887807210287\n",
      "count_amostra: 1530200\n",
      "iteration:  7680 , total_loss:  23.535502624511718\n",
      "count_amostra: 1536200\n",
      "iteration:  7710 , total_loss:  23.4250306447347\n",
      "count_amostra: 1542200\n",
      "iteration:  7740 , total_loss:  23.524524688720703\n",
      "count_amostra: 1548200\n",
      "iteration:  7770 , total_loss:  23.52141399383545\n",
      "count_amostra: 1554200\n",
      "iteration:  7800 , total_loss:  23.442016665140788\n",
      "count_amostra: 1560200\n",
      "iteration:  7830 , total_loss:  23.379391606648763\n",
      "count_amostra: 1566200\n",
      "iteration:  7860 , total_loss:  23.40002021789551\n",
      "count_amostra: 1572200\n",
      "iteration:  7890 , total_loss:  23.43461259206136\n",
      "count_amostra: 1578200\n",
      "iteration:  7920 , total_loss:  23.43927084604899\n",
      "count_amostra: 1584200\n",
      "iteration:  7950 , total_loss:  23.395263989766438\n",
      "count_amostra: 1590200\n",
      "iteration:  7980 , total_loss:  23.391620508829753\n",
      "count_amostra: 1596200\n",
      "iteration:  8010 , total_loss:  23.420307286580403\n",
      "count_amostra: 1602200\n",
      "iteration:  8040 , total_loss:  23.38190924326579\n",
      "count_amostra: 1608200\n",
      "iteration:  8070 , total_loss:  23.442788378397623\n",
      "count_amostra: 1614200\n",
      "iteration:  8100 , total_loss:  23.422430801391602\n",
      "count_amostra: 1620200\n",
      "iteration:  8130 , total_loss:  23.312147903442384\n",
      "count_amostra: 1626200\n",
      "iteration:  8160 , total_loss:  23.400335820515952\n",
      "count_amostra: 1632200\n",
      "iteration:  8190 , total_loss:  23.323559443155926\n",
      "count_amostra: 1638200\n",
      "iteration:  8220 , total_loss:  23.333426729838052\n",
      "count_amostra: 1644200\n",
      "iteration:  8250 , total_loss:  23.30172481536865\n",
      "count_amostra: 1650200\n",
      "iteration:  8280 , total_loss:  23.284056599934896\n",
      "count_amostra: 1656200\n",
      "iteration:  8310 , total_loss:  23.33058713277181\n",
      "count_amostra: 1662200\n",
      "iteration:  8340 , total_loss:  23.24685147603353\n",
      "count_amostra: 1668200\n",
      "iteration:  8370 , total_loss:  23.284418805440268\n",
      "count_amostra: 1674200\n",
      "iteration:  8400 , total_loss:  23.30444202423096\n",
      "count_amostra: 1680200\n",
      "iteration:  8430 , total_loss:  23.31693655649821\n",
      "count_amostra: 1686200\n",
      "iteration:  8460 , total_loss:  23.25069777170817\n",
      "count_amostra: 1692200\n",
      "iteration:  8490 , total_loss:  23.28541488647461\n",
      "count_amostra: 1698200\n",
      "iteration:  8520 , total_loss:  23.20102202097575\n",
      "count_amostra: 1704200\n",
      "iteration:  8550 , total_loss:  23.206813112894693\n",
      "count_amostra: 1710200\n",
      "iteration:  8580 , total_loss:  23.20613981882731\n",
      "count_amostra: 1716200\n",
      "iteration:  8610 , total_loss:  23.295973014831542\n",
      "count_amostra: 1722200\n",
      "iteration:  8640 , total_loss:  23.23375898996989\n",
      "count_amostra: 1728200\n",
      "iteration:  8670 , total_loss:  23.21870822906494\n",
      "count_amostra: 1734200\n",
      "iteration:  8700 , total_loss:  23.256203397115073\n",
      "count_amostra: 1740200\n",
      "iteration:  8730 , total_loss:  23.19635715484619\n",
      "count_amostra: 1746200\n",
      "iteration:  8760 , total_loss:  23.120252545674642\n",
      "count_amostra: 1752200\n",
      "iteration:  8790 , total_loss:  23.180636405944824\n",
      "count_amostra: 1758200\n",
      "iteration:  8820 , total_loss:  23.27187182108561\n",
      "count_amostra: 1764200\n",
      "iteration:  8850 , total_loss:  23.147528902689615\n",
      "count_amostra: 1770200\n",
      "iteration:  8880 , total_loss:  23.1841339747111\n",
      "count_amostra: 1776200\n",
      "iteration:  8910 , total_loss:  23.145561726888022\n",
      "count_amostra: 1782200\n",
      "iteration:  8940 , total_loss:  23.135045941670736\n",
      "count_amostra: 1788200\n",
      "iteration:  8970 , total_loss:  23.180274073282877\n",
      "count_amostra: 1794200\n",
      "iteration:  9000 , total_loss:  23.08455607096354\n",
      "count_amostra: 1800200\n",
      "iteration:  9030 , total_loss:  23.123044395446776\n",
      "count_amostra: 1806200\n",
      "iteration:  9060 , total_loss:  23.057468922932944\n",
      "count_amostra: 1812200\n",
      "iteration:  9090 , total_loss:  23.113281059265137\n",
      "count_amostra: 1818200\n",
      "iteration:  9120 , total_loss:  23.060115559895834\n",
      "count_amostra: 1824200\n",
      "iteration:  9150 , total_loss:  23.0640287399292\n",
      "count_amostra: 1830200\n",
      "iteration:  9180 , total_loss:  23.129306093851724\n",
      "count_amostra: 1836200\n",
      "iteration:  9210 , total_loss:  23.06330509185791\n",
      "count_amostra: 1842200\n",
      "iteration:  9240 , total_loss:  23.096552912394205\n",
      "count_amostra: 1848200\n",
      "iteration:  9270 , total_loss:  23.057985242207845\n",
      "count_amostra: 1854200\n",
      "iteration:  9300 , total_loss:  22.98996206919352\n",
      "count_amostra: 1860200\n",
      "iteration:  9330 , total_loss:  23.139701461791994\n",
      "count_amostra: 1866200\n",
      "iteration:  9360 , total_loss:  23.094012959798178\n",
      "count_amostra: 1872200\n",
      "iteration:  9390 , total_loss:  23.050258509318034\n",
      "count_amostra: 1878200\n",
      "iteration:  9420 , total_loss:  23.05312099456787\n",
      "count_amostra: 1884200\n",
      "iteration:  9450 , total_loss:  23.04102897644043\n",
      "count_amostra: 1890200\n",
      "iteration:  9480 , total_loss:  23.10109977722168\n",
      "count_amostra: 1896200\n",
      "iteration:  9510 , total_loss:  22.99881223042806\n",
      "count_amostra: 1902200\n",
      "iteration:  9540 , total_loss:  23.028533299763996\n",
      "count_amostra: 1908200\n",
      "iteration:  9570 , total_loss:  22.951572100321453\n",
      "count_amostra: 1914200\n",
      "iteration:  9600 , total_loss:  23.02297732035319\n",
      "count_amostra: 1920200\n",
      "iteration:  9630 , total_loss:  23.084417215983073\n",
      "count_amostra: 1926200\n",
      "iteration:  9660 , total_loss:  23.025718625386556\n",
      "count_amostra: 1932200\n",
      "iteration:  9690 , total_loss:  22.981771087646486\n",
      "count_amostra: 1938200\n",
      "iteration:  9720 , total_loss:  23.046632067362467\n",
      "count_amostra: 1944200\n",
      "iteration:  9750 , total_loss:  22.949313163757324\n",
      "count_amostra: 1950200\n",
      "iteration:  9780 , total_loss:  23.078353627522787\n",
      "count_amostra: 1956200\n",
      "iteration:  9810 , total_loss:  22.957508087158203\n",
      "count_amostra: 1962200\n",
      "iteration:  9840 , total_loss:  22.956118011474608\n",
      "count_amostra: 1968200\n",
      "iteration:  9870 , total_loss:  22.98212299346924\n",
      "count_amostra: 1974200\n",
      "iteration:  9900 , total_loss:  22.97422701517741\n",
      "count_amostra: 1980200\n",
      "iteration:  9930 , total_loss:  23.032361920674642\n",
      "count_amostra: 1986200\n",
      "iteration:  9960 , total_loss:  22.953248596191408\n",
      "count_amostra: 1992200\n",
      "iteration:  9990 , total_loss:  22.922870445251466\n",
      "count_amostra: 1998200\n",
      "iteration:  10020 , total_loss:  22.925425910949706\n",
      "count_amostra: 2004200\n",
      "iteration:  10050 , total_loss:  22.946713829040526\n",
      "count_amostra: 2010200\n",
      "iteration:  10080 , total_loss:  22.87968985239665\n",
      "count_amostra: 2016200\n",
      "iteration:  10110 , total_loss:  22.95627644856771\n",
      "count_amostra: 2022200\n",
      "iteration:  10140 , total_loss:  22.857000096639\n",
      "count_amostra: 2028200\n",
      "iteration:  10170 , total_loss:  22.918879127502443\n",
      "count_amostra: 2034200\n",
      "iteration:  10200 , total_loss:  22.851607259114584\n",
      "count_amostra: 2040200\n",
      "iteration:  10230 , total_loss:  22.876915613810223\n",
      "count_amostra: 2046200\n",
      "iteration:  10260 , total_loss:  22.836845525105794\n",
      "count_amostra: 2052200\n",
      "iteration:  10290 , total_loss:  22.913599332173664\n",
      "count_amostra: 2058200\n",
      "iteration:  10320 , total_loss:  22.840817133585613\n",
      "count_amostra: 2064200\n",
      "iteration:  10350 , total_loss:  22.965705362955728\n",
      "count_amostra: 2070200\n",
      "iteration:  10380 , total_loss:  22.90777702331543\n",
      "count_amostra: 2076200\n",
      "iteration:  10410 , total_loss:  22.868660418192544\n",
      "count_amostra: 2082200\n",
      "iteration:  10440 , total_loss:  22.904865773518882\n",
      "count_amostra: 2088200\n",
      "iteration:  10470 , total_loss:  22.766542434692383\n",
      "count_amostra: 2094200\n",
      "iteration:  10500 , total_loss:  22.81709098815918\n",
      "count_amostra: 2100200\n",
      "iteration:  10530 , total_loss:  22.913964335123698\n",
      "count_amostra: 2106200\n",
      "iteration:  10560 , total_loss:  22.838587506612143\n",
      "count_amostra: 2112200\n",
      "iteration:  10590 , total_loss:  22.774738121032716\n",
      "count_amostra: 2118200\n",
      "iteration:  10620 , total_loss:  22.760543696085612\n",
      "count_amostra: 2124200\n",
      "iteration:  10650 , total_loss:  22.820552126566568\n",
      "count_amostra: 2130200\n",
      "iteration:  10680 , total_loss:  22.79388287862142\n",
      "count_amostra: 2136200\n",
      "iteration:  10710 , total_loss:  22.81959457397461\n",
      "count_amostra: 2142200\n",
      "iteration:  10740 , total_loss:  22.838307698567707\n",
      "count_amostra: 2148200\n",
      "iteration:  10770 , total_loss:  22.823860867818198\n",
      "count_amostra: 2154200\n",
      "iteration:  10800 , total_loss:  22.831432596842447\n",
      "count_amostra: 2160200\n",
      "iteration:  10830 , total_loss:  22.861317761739095\n",
      "count_amostra: 2166200\n",
      "iteration:  10860 , total_loss:  22.743390846252442\n",
      "count_amostra: 2172200\n",
      "iteration:  10890 , total_loss:  22.745958518981933\n",
      "count_amostra: 2178200\n",
      "iteration:  10920 , total_loss:  22.83606719970703\n",
      "count_amostra: 2184200\n",
      "iteration:  10950 , total_loss:  22.74531675974528\n",
      "count_amostra: 2190200\n",
      "iteration:  10980 , total_loss:  22.786700948079428\n",
      "count_amostra: 2196200\n",
      "iteration:  11010 , total_loss:  22.79929599761963\n",
      "count_amostra: 2202200\n",
      "iteration:  11040 , total_loss:  22.8617301940918\n",
      "count_amostra: 2208200\n",
      "iteration:  11070 , total_loss:  22.796360842386882\n",
      "count_amostra: 2214200\n",
      "iteration:  11100 , total_loss:  22.879302660624187\n",
      "count_amostra: 2220200\n",
      "iteration:  11130 , total_loss:  22.83984400431315\n",
      "count_amostra: 2226200\n",
      "iteration:  11160 , total_loss:  22.730218378702798\n",
      "count_amostra: 2232200\n",
      "iteration:  11190 , total_loss:  22.823546600341796\n",
      "count_amostra: 2238200\n",
      "iteration:  11220 , total_loss:  22.734485626220703\n",
      "count_amostra: 2244200\n",
      "iteration:  11250 , total_loss:  22.73568204243978\n",
      "count_amostra: 2250200\n",
      "iteration:  11280 , total_loss:  22.697254435221353\n",
      "count_amostra: 2256200\n",
      "iteration:  11310 , total_loss:  22.75758628845215\n",
      "count_amostra: 2262200\n",
      "iteration:  11340 , total_loss:  22.765190505981444\n",
      "count_amostra: 2268200\n",
      "iteration:  11370 , total_loss:  22.750121625264487\n",
      "count_amostra: 2274200\n",
      "iteration:  11400 , total_loss:  22.767316500345867\n",
      "count_amostra: 2280200\n",
      "iteration:  11430 , total_loss:  22.65852108001709\n",
      "count_amostra: 2286200\n",
      "iteration:  11460 , total_loss:  22.80632317860921\n",
      "count_amostra: 2292200\n",
      "iteration:  11490 , total_loss:  22.665082486470542\n",
      "count_amostra: 2298200\n",
      "iteration:  11520 , total_loss:  22.667818450927733\n",
      "count_amostra: 2304200\n",
      "iteration:  11550 , total_loss:  22.636083793640136\n",
      "count_amostra: 2310200\n",
      "iteration:  11580 , total_loss:  22.68156255086263\n",
      "count_amostra: 2316200\n",
      "iteration:  11610 , total_loss:  22.666669273376463\n",
      "count_amostra: 2322200\n",
      "iteration:  11640 , total_loss:  22.65339781443278\n",
      "count_amostra: 2328200\n",
      "iteration:  11670 , total_loss:  22.63025074005127\n",
      "count_amostra: 2334200\n",
      "iteration:  11700 , total_loss:  22.728206634521484\n",
      "count_amostra: 2340200\n",
      "iteration:  11730 , total_loss:  22.657787132263184\n",
      "count_amostra: 2346200\n",
      "iteration:  11760 , total_loss:  22.617452239990236\n",
      "count_amostra: 2352200\n",
      "iteration:  11790 , total_loss:  22.688881810506185\n",
      "count_amostra: 2358200\n",
      "iteration:  11820 , total_loss:  22.66374619801839\n",
      "count_amostra: 2364200\n",
      "iteration:  11850 , total_loss:  22.653296852111815\n",
      "count_amostra: 2370200\n",
      "iteration:  11880 , total_loss:  22.531720670064292\n",
      "count_amostra: 2376200\n",
      "iteration:  11910 , total_loss:  22.671294848124187\n",
      "count_amostra: 2382200\n",
      "iteration:  11940 , total_loss:  22.627370834350586\n",
      "count_amostra: 2388200\n",
      "iteration:  11970 , total_loss:  22.62162227630615\n",
      "count_amostra: 2394200\n",
      "iteration:  12000 , total_loss:  22.694986661275227\n",
      "count_amostra: 2400200\n",
      "iteration:  12030 , total_loss:  22.653290049235025\n",
      "count_amostra: 2406200\n",
      "iteration:  12060 , total_loss:  22.658195050557456\n",
      "count_amostra: 2412200\n",
      "iteration:  12090 , total_loss:  22.663623809814453\n",
      "count_amostra: 2418200\n",
      "iteration:  12120 , total_loss:  22.622276051839194\n",
      "count_amostra: 2424200\n",
      "iteration:  12150 , total_loss:  22.686259714762368\n",
      "count_amostra: 2430200\n",
      "iteration:  12180 , total_loss:  22.674012438456217\n",
      "count_amostra: 2436200\n",
      "iteration:  12210 , total_loss:  22.671077728271484\n",
      "count_amostra: 2442200\n",
      "iteration:  12240 , total_loss:  22.687685139973958\n",
      "count_amostra: 2448200\n",
      "iteration:  12270 , total_loss:  22.607762654622395\n",
      "count_amostra: 2454200\n",
      "iteration:  12300 , total_loss:  22.607113138834634\n",
      "count_amostra: 2460200\n",
      "iteration:  12330 , total_loss:  22.492464192708333\n",
      "count_amostra: 2466200\n",
      "iteration:  12360 , total_loss:  22.604712295532227\n",
      "count_amostra: 2472200\n",
      "iteration:  12390 , total_loss:  22.628387769063313\n",
      "count_amostra: 2478200\n",
      "iteration:  12420 , total_loss:  22.578230794270834\n",
      "count_amostra: 2484200\n",
      "iteration:  12450 , total_loss:  22.59887809753418\n",
      "count_amostra: 2490200\n",
      "iteration:  12480 , total_loss:  22.671947542826334\n",
      "count_amostra: 2496200\n",
      "iteration:  12510 , total_loss:  22.631575202941896\n",
      "count_amostra: 2502200\n",
      "iteration:  12540 , total_loss:  22.6124018351237\n",
      "count_amostra: 2508200\n",
      "iteration:  12570 , total_loss:  22.591154098510742\n",
      "count_amostra: 2514200\n",
      "iteration:  12600 , total_loss:  22.567549324035646\n",
      "count_amostra: 2520200\n",
      "iteration:  12630 , total_loss:  22.635952313741047\n",
      "count_amostra: 2526200\n",
      "iteration:  12660 , total_loss:  22.611727460225424\n",
      "count_amostra: 2532200\n",
      "iteration:  12690 , total_loss:  22.55280278523763\n",
      "count_amostra: 2538200\n",
      "iteration:  12720 , total_loss:  22.560588328043618\n",
      "count_amostra: 2544200\n",
      "iteration:  12750 , total_loss:  22.560370127360027\n",
      "count_amostra: 2550200\n",
      "iteration:  12780 , total_loss:  22.54268309275309\n",
      "count_amostra: 2556200\n",
      "iteration:  12810 , total_loss:  22.409572982788085\n",
      "count_amostra: 2562200\n",
      "iteration:  12840 , total_loss:  22.511434936523436\n",
      "count_amostra: 2568200\n",
      "iteration:  12870 , total_loss:  22.53216749827067\n",
      "count_amostra: 2574200\n",
      "iteration:  12900 , total_loss:  22.521785100301106\n",
      "count_amostra: 2580200\n",
      "iteration:  12930 , total_loss:  22.48766606648763\n",
      "count_amostra: 2586200\n",
      "iteration:  12960 , total_loss:  22.51030044555664\n",
      "count_amostra: 2592200\n",
      "iteration:  12990 , total_loss:  22.538868522644044\n",
      "count_amostra: 2598200\n",
      "iteration:  13020 , total_loss:  22.50362122853597\n",
      "count_amostra: 2604200\n",
      "iteration:  13050 , total_loss:  22.554856808980308\n",
      "count_amostra: 2610200\n",
      "iteration:  13080 , total_loss:  22.413335355122886\n",
      "count_amostra: 2616200\n",
      "iteration:  13110 , total_loss:  22.5513053894043\n",
      "count_amostra: 2622200\n",
      "iteration:  13140 , total_loss:  22.47981332143148\n",
      "count_amostra: 2628200\n",
      "iteration:  13170 , total_loss:  22.480086263020834\n",
      "count_amostra: 2634200\n",
      "iteration:  13200 , total_loss:  22.500946235656738\n",
      "count_amostra: 2640200\n",
      "iteration:  13230 , total_loss:  22.54237283070882\n",
      "count_amostra: 2646200\n",
      "iteration:  13260 , total_loss:  22.512888526916505\n",
      "count_amostra: 2652200\n",
      "iteration:  13290 , total_loss:  22.4902286529541\n",
      "count_amostra: 2658200\n",
      "iteration:  13320 , total_loss:  22.502401415507\n",
      "count_amostra: 2664200\n",
      "iteration:  13350 , total_loss:  22.54347292582194\n",
      "count_amostra: 2670200\n",
      "iteration:  13380 , total_loss:  22.576815032958983\n",
      "count_amostra: 2676200\n",
      "iteration:  13410 , total_loss:  22.482349268595378\n",
      "count_amostra: 2682200\n",
      "iteration:  13440 , total_loss:  22.43755226135254\n",
      "count_amostra: 2688200\n",
      "iteration:  13470 , total_loss:  22.479637400309244\n",
      "count_amostra: 2694200\n",
      "iteration:  13500 , total_loss:  22.47880942026774\n",
      "count_amostra: 2700200\n",
      "iteration:  13530 , total_loss:  22.512103780110678\n",
      "count_amostra: 2706200\n",
      "iteration:  13560 , total_loss:  22.543126996358236\n",
      "count_amostra: 2712200\n",
      "iteration:  13590 , total_loss:  22.400554529825847\n",
      "count_amostra: 2718200\n",
      "iteration:  13620 , total_loss:  22.40822067260742\n",
      "count_amostra: 2724200\n",
      "iteration:  13650 , total_loss:  22.452136739095053\n",
      "count_amostra: 2730200\n",
      "iteration:  13680 , total_loss:  22.559638341267902\n",
      "count_amostra: 2736200\n",
      "iteration:  13710 , total_loss:  22.400876553853355\n",
      "count_amostra: 2742200\n",
      "iteration:  13740 , total_loss:  22.393097368876138\n",
      "count_amostra: 2748200\n",
      "iteration:  13770 , total_loss:  22.432128397623696\n",
      "count_amostra: 2754200\n",
      "iteration:  13800 , total_loss:  22.50656941731771\n",
      "count_amostra: 2760200\n",
      "iteration:  13830 , total_loss:  22.372545433044433\n",
      "count_amostra: 2766200\n",
      "iteration:  13860 , total_loss:  22.470135243733726\n",
      "count_amostra: 2772200\n",
      "iteration:  13890 , total_loss:  22.338676261901856\n",
      "count_amostra: 2778200\n",
      "iteration:  13920 , total_loss:  22.476776123046875\n",
      "count_amostra: 2784200\n",
      "iteration:  13950 , total_loss:  22.42948843638102\n",
      "count_amostra: 2790200\n",
      "iteration:  13980 , total_loss:  22.46004556020101\n",
      "count_amostra: 2796200\n",
      "iteration:  14010 , total_loss:  22.390706380208332\n",
      "count_amostra: 2802200\n",
      "iteration:  14040 , total_loss:  22.409775479634604\n",
      "count_amostra: 2808200\n",
      "iteration:  14070 , total_loss:  22.289774640401205\n",
      "count_amostra: 2814200\n",
      "iteration:  14100 , total_loss:  22.353450775146484\n",
      "count_amostra: 2820200\n",
      "iteration:  14130 , total_loss:  22.346946461995444\n",
      "count_amostra: 2826200\n",
      "iteration:  14160 , total_loss:  22.407733790079753\n",
      "count_amostra: 2832200\n",
      "iteration:  14190 , total_loss:  22.440009053548177\n",
      "count_amostra: 2838200\n",
      "iteration:  14220 , total_loss:  22.27968578338623\n",
      "count_amostra: 2844200\n",
      "iteration:  14250 , total_loss:  22.461789258321126\n",
      "count_amostra: 2850200\n",
      "iteration:  14280 , total_loss:  22.355979092915852\n",
      "count_amostra: 2856200\n",
      "iteration:  14310 , total_loss:  22.359242566426595\n",
      "count_amostra: 2862200\n",
      "iteration:  14340 , total_loss:  22.352912267049152\n",
      "count_amostra: 2868200\n",
      "iteration:  14370 , total_loss:  22.38576291402181\n",
      "count_amostra: 2874200\n",
      "iteration:  14400 , total_loss:  22.388706080118816\n",
      "count_amostra: 2880200\n",
      "iteration:  14430 , total_loss:  22.410256004333498\n",
      "count_amostra: 2886200\n",
      "iteration:  14460 , total_loss:  22.477943738301594\n",
      "count_amostra: 2892200\n",
      "iteration:  14490 , total_loss:  22.319084231058756\n",
      "count_amostra: 2898200\n",
      "iteration:  14520 , total_loss:  22.429996490478516\n",
      "count_amostra: 2904200\n",
      "iteration:  14550 , total_loss:  22.34708677927653\n",
      "count_amostra: 2910200\n",
      "iteration:  14580 , total_loss:  22.38122425079346\n",
      "count_amostra: 2916200\n",
      "iteration:  14610 , total_loss:  22.3229164759318\n",
      "count_amostra: 2922200\n",
      "iteration:  14640 , total_loss:  22.36640993754069\n",
      "count_amostra: 2928200\n",
      "iteration:  14670 , total_loss:  22.33852424621582\n",
      "count_amostra: 2934200\n",
      "iteration:  14700 , total_loss:  22.329025522867838\n",
      "count_amostra: 2940200\n",
      "iteration:  14730 , total_loss:  22.401992670694987\n",
      "count_amostra: 2946200\n",
      "iteration:  14760 , total_loss:  22.31679547627767\n",
      "count_amostra: 2952200\n",
      "iteration:  14790 , total_loss:  22.36371415456136\n",
      "count_amostra: 2958200\n",
      "iteration:  14820 , total_loss:  22.3619566599528\n",
      "count_amostra: 2964200\n",
      "iteration:  14850 , total_loss:  22.346530405680337\n",
      "count_amostra: 2970200\n",
      "iteration:  14880 , total_loss:  22.36852067311605\n",
      "count_amostra: 2976200\n",
      "iteration:  14910 , total_loss:  22.38961664835612\n",
      "count_amostra: 2982200\n",
      "iteration:  14940 , total_loss:  22.363302167256673\n",
      "count_amostra: 2988200\n",
      "iteration:  14970 , total_loss:  22.27027359008789\n",
      "count_amostra: 2994200\n",
      "iteration:  15000 , total_loss:  22.341795349121092\n",
      "count_amostra: 3000200\n",
      "iteration:  15030 , total_loss:  22.32650629679362\n",
      "count_amostra: 3006200\n",
      "iteration:  15060 , total_loss:  22.29745724995931\n",
      "count_amostra: 3012200\n",
      "iteration:  15090 , total_loss:  22.341264152526854\n",
      "count_amostra: 3018200\n",
      "iteration:  15120 , total_loss:  22.26931711832682\n",
      "count_amostra: 3024200\n",
      "iteration:  15150 , total_loss:  22.288229751586915\n",
      "count_amostra: 3030200\n",
      "iteration:  15180 , total_loss:  22.338979212443032\n",
      "count_amostra: 3036200\n",
      "iteration:  15210 , total_loss:  22.413288752237957\n",
      "count_amostra: 3042200\n",
      "iteration:  15240 , total_loss:  22.31356601715088\n",
      "count_amostra: 3048200\n",
      "iteration:  15270 , total_loss:  22.3408992767334\n",
      "count_amostra: 3054200\n",
      "iteration:  15300 , total_loss:  22.356797472635904\n",
      "count_amostra: 3060200\n",
      "iteration:  15330 , total_loss:  22.23007386525472\n",
      "count_amostra: 3066200\n",
      "iteration:  15360 , total_loss:  22.34529863993327\n",
      "count_amostra: 3072200\n",
      "iteration:  15390 , total_loss:  22.27641353607178\n",
      "count_amostra: 3078200\n",
      "iteration:  15420 , total_loss:  22.408387756347658\n",
      "count_amostra: 3084200\n",
      "iteration:  15450 , total_loss:  22.272381591796876\n",
      "count_amostra: 3090200\n",
      "iteration:  15480 , total_loss:  22.253180249532065\n",
      "count_amostra: 3096200\n",
      "iteration:  15510 , total_loss:  22.33030382792155\n",
      "count_amostra: 3102200\n",
      "iteration:  15540 , total_loss:  22.249755414326987\n",
      "count_amostra: 3108200\n",
      "iteration:  15570 , total_loss:  22.290410868326823\n",
      "count_amostra: 3114200\n",
      "iteration:  15600 , total_loss:  22.28170534769694\n",
      "count_amostra: 3120200\n",
      "iteration:  15630 , total_loss:  22.263049825032553\n",
      "count_amostra: 3126200\n",
      "iteration:  15660 , total_loss:  22.229437573750815\n",
      "count_amostra: 3132200\n",
      "iteration:  15690 , total_loss:  22.27546068827311\n",
      "count_amostra: 3138200\n",
      "iteration:  15720 , total_loss:  22.242525355021158\n",
      "count_amostra: 3144200\n",
      "iteration:  15750 , total_loss:  22.251930872599285\n",
      "count_amostra: 3150200\n",
      "iteration:  15780 , total_loss:  22.268751525878905\n",
      "count_amostra: 3156200\n",
      "iteration:  15810 , total_loss:  22.256911849975587\n",
      "count_amostra: 3162200\n",
      "iteration:  15840 , total_loss:  22.236056836446128\n",
      "count_amostra: 3168200\n",
      "iteration:  15870 , total_loss:  22.26145871480306\n",
      "count_amostra: 3174200\n",
      "iteration:  15900 , total_loss:  22.30470148722331\n",
      "count_amostra: 3180200\n",
      "iteration:  15930 , total_loss:  22.1847988764445\n",
      "count_amostra: 3186200\n",
      "iteration:  15960 , total_loss:  22.314474296569824\n",
      "count_amostra: 3192200\n",
      "iteration:  15990 , total_loss:  22.249211311340332\n",
      "count_amostra: 3198200\n",
      "iteration:  16020 , total_loss:  22.30330899556478\n",
      "count_amostra: 3204200\n",
      "iteration:  16050 , total_loss:  22.297545687357584\n",
      "count_amostra: 3210200\n",
      "iteration:  16080 , total_loss:  22.193598175048827\n",
      "count_amostra: 3216200\n",
      "iteration:  16110 , total_loss:  22.2663880666097\n",
      "count_amostra: 3222200\n",
      "iteration:  16140 , total_loss:  22.260777727762857\n",
      "count_amostra: 3228200\n",
      "iteration:  16170 , total_loss:  22.233581988016763\n",
      "count_amostra: 3234200\n",
      "iteration:  16200 , total_loss:  22.219155248006185\n",
      "count_amostra: 3240200\n",
      "iteration:  16230 , total_loss:  22.192344601949056\n",
      "count_amostra: 3246200\n",
      "iteration:  16260 , total_loss:  22.200526237487793\n",
      "count_amostra: 3252200\n",
      "iteration:  16290 , total_loss:  22.287673950195312\n",
      "count_amostra: 3258200\n",
      "iteration:  16320 , total_loss:  22.240095456441242\n",
      "count_amostra: 3264200\n",
      "iteration:  16350 , total_loss:  22.267147254943847\n",
      "count_amostra: 3270200\n",
      "iteration:  16380 , total_loss:  22.155876795450848\n",
      "count_amostra: 3276200\n",
      "iteration:  16410 , total_loss:  22.14703114827474\n",
      "count_amostra: 3282200\n",
      "iteration:  16440 , total_loss:  22.23942279815674\n",
      "count_amostra: 3288200\n",
      "iteration:  16470 , total_loss:  22.210361607869466\n",
      "count_amostra: 3294200\n",
      "iteration:  16500 , total_loss:  22.172352409362794\n",
      "count_amostra: 3300200\n",
      "iteration:  16530 , total_loss:  22.148889605204264\n",
      "count_amostra: 3306200\n",
      "iteration:  16560 , total_loss:  22.203601773579916\n",
      "count_amostra: 3312200\n",
      "iteration:  16590 , total_loss:  22.21748294830322\n",
      "count_amostra: 3318200\n",
      "iteration:  16620 , total_loss:  22.13650442759196\n",
      "count_amostra: 3324200\n",
      "iteration:  16650 , total_loss:  22.185738817850748\n",
      "count_amostra: 3330200\n",
      "iteration:  16680 , total_loss:  22.243009503682455\n",
      "count_amostra: 3336200\n",
      "iteration:  16710 , total_loss:  22.275948270161948\n",
      "count_amostra: 3342200\n",
      "iteration:  16740 , total_loss:  22.207489077250163\n",
      "count_amostra: 3348200\n",
      "iteration:  16770 , total_loss:  22.16466032663981\n",
      "count_amostra: 3354200\n",
      "iteration:  16800 , total_loss:  22.266151173909506\n",
      "count_amostra: 3360200\n",
      "iteration:  16830 , total_loss:  22.19429416656494\n",
      "count_amostra: 3366200\n",
      "iteration:  16860 , total_loss:  22.178910128275554\n",
      "count_amostra: 3372200\n",
      "iteration:  16890 , total_loss:  22.166140365600587\n",
      "count_amostra: 3378200\n",
      "iteration:  16920 , total_loss:  22.147034962972004\n",
      "count_amostra: 3384200\n",
      "iteration:  16950 , total_loss:  22.1896546681722\n",
      "count_amostra: 3390200\n",
      "iteration:  16980 , total_loss:  22.149008178710936\n",
      "count_amostra: 3396200\n",
      "iteration:  17010 , total_loss:  22.15670394897461\n",
      "count_amostra: 3402200\n",
      "iteration:  17040 , total_loss:  22.19071496327718\n",
      "count_amostra: 3408200\n",
      "iteration:  17070 , total_loss:  22.06309045155843\n",
      "count_amostra: 3414200\n",
      "iteration:  17100 , total_loss:  22.171029345194498\n",
      "count_amostra: 3420200\n",
      "iteration:  17130 , total_loss:  22.21113452911377\n",
      "count_amostra: 3426200\n",
      "iteration:  17160 , total_loss:  22.133372624715168\n",
      "count_amostra: 3432200\n",
      "iteration:  17190 , total_loss:  22.186164347330728\n",
      "count_amostra: 3438200\n",
      "iteration:  17220 , total_loss:  22.187151273091633\n",
      "count_amostra: 3444200\n",
      "iteration:  17250 , total_loss:  22.113915506998698\n",
      "count_amostra: 3450200\n",
      "iteration:  17280 , total_loss:  22.129158973693848\n",
      "count_amostra: 3456200\n",
      "iteration:  17310 , total_loss:  22.233964284261067\n",
      "count_amostra: 3462200\n",
      "iteration:  17340 , total_loss:  22.134561856587727\n",
      "count_amostra: 3468200\n",
      "iteration:  17370 , total_loss:  22.146613756815594\n",
      "count_amostra: 3474200\n",
      "iteration:  17400 , total_loss:  22.174243799845378\n",
      "count_amostra: 3480200\n",
      "iteration:  17430 , total_loss:  22.054306348164875\n",
      "count_amostra: 3486200\n",
      "iteration:  17460 , total_loss:  22.0773801167806\n",
      "count_amostra: 3492200\n",
      "iteration:  17490 , total_loss:  22.18701591491699\n",
      "count_amostra: 3498200\n",
      "iteration:  17520 , total_loss:  22.234839820861815\n",
      "count_amostra: 3504200\n",
      "iteration:  17550 , total_loss:  22.04945971171061\n",
      "count_amostra: 3510200\n",
      "iteration:  17580 , total_loss:  22.19307518005371\n",
      "count_amostra: 3516200\n",
      "iteration:  17610 , total_loss:  22.127573267618814\n",
      "count_amostra: 3522200\n",
      "iteration:  17640 , total_loss:  22.18499215443929\n",
      "count_amostra: 3528200\n",
      "iteration:  17670 , total_loss:  22.204540888468426\n",
      "count_amostra: 3534200\n",
      "iteration:  17700 , total_loss:  22.086955070495605\n",
      "count_amostra: 3540200\n",
      "iteration:  17730 , total_loss:  22.16435209910075\n",
      "count_amostra: 3546200\n",
      "iteration:  17760 , total_loss:  22.0976157506307\n",
      "count_amostra: 3552200\n",
      "iteration:  17790 , total_loss:  22.079207293192546\n",
      "count_amostra: 3558200\n",
      "iteration:  17820 , total_loss:  22.139154815673827\n",
      "count_amostra: 3564200\n",
      "iteration:  17850 , total_loss:  22.173722775777183\n",
      "count_amostra: 3570200\n",
      "iteration:  17880 , total_loss:  22.132533454895018\n",
      "count_amostra: 3576200\n",
      "iteration:  17910 , total_loss:  22.08384895324707\n",
      "count_amostra: 3582200\n",
      "iteration:  17940 , total_loss:  22.126979446411134\n",
      "count_amostra: 3588200\n",
      "iteration:  17970 , total_loss:  22.05546474456787\n",
      "count_amostra: 3594200\n",
      "iteration:  18000 , total_loss:  22.050500615437826\n",
      "count_amostra: 3600200\n",
      "iteration:  18030 , total_loss:  22.028748003641763\n",
      "count_amostra: 3606200\n",
      "iteration:  18060 , total_loss:  22.161190605163576\n",
      "count_amostra: 3612200\n",
      "iteration:  18090 , total_loss:  22.03126023610433\n",
      "count_amostra: 3618200\n",
      "iteration:  18120 , total_loss:  21.98816204071045\n",
      "count_amostra: 3624200\n",
      "iteration:  18150 , total_loss:  22.06226177215576\n",
      "count_amostra: 3630200\n",
      "iteration:  18180 , total_loss:  22.02715867360433\n",
      "count_amostra: 3636200\n",
      "iteration:  18210 , total_loss:  22.132480557759603\n",
      "count_amostra: 3642200\n",
      "iteration:  18240 , total_loss:  22.070141983032226\n",
      "count_amostra: 3648200\n",
      "iteration:  18270 , total_loss:  22.055632718404134\n",
      "count_amostra: 3654200\n",
      "iteration:  18300 , total_loss:  21.990821011861165\n",
      "count_amostra: 3660200\n",
      "iteration:  18330 , total_loss:  22.030438550313313\n",
      "count_amostra: 3666200\n",
      "iteration:  18360 , total_loss:  22.13524398803711\n",
      "count_amostra: 3672200\n",
      "iteration:  18390 , total_loss:  21.920075352986654\n",
      "count_amostra: 3678200\n",
      "iteration:  18420 , total_loss:  22.112690989176432\n",
      "count_amostra: 3684200\n",
      "iteration:  18450 , total_loss:  21.996367772420246\n",
      "count_amostra: 3690200\n",
      "iteration:  18480 , total_loss:  22.028381729125975\n",
      "count_amostra: 3696200\n",
      "iteration:  18510 , total_loss:  22.07012087504069\n",
      "count_amostra: 3702200\n",
      "iteration:  18540 , total_loss:  21.968383407592775\n",
      "count_amostra: 3708200\n",
      "iteration:  18570 , total_loss:  22.086530431111655\n",
      "count_amostra: 3714200\n",
      "iteration:  18600 , total_loss:  22.035349146525064\n",
      "count_amostra: 3720200\n",
      "iteration:  18630 , total_loss:  22.06038080851237\n",
      "count_amostra: 3726200\n",
      "iteration:  18660 , total_loss:  22.03362331390381\n",
      "count_amostra: 3732200\n",
      "iteration:  18690 , total_loss:  21.98867384592692\n",
      "count_amostra: 3738200\n",
      "iteration:  18720 , total_loss:  22.025823911031086\n",
      "count_amostra: 3744200\n",
      "iteration:  18750 , total_loss:  22.068778419494627\n",
      "count_amostra: 3750200\n",
      "iteration:  18780 , total_loss:  21.984021186828613\n",
      "count_amostra: 3756200\n",
      "iteration:  18810 , total_loss:  21.961473147074383\n",
      "count_amostra: 3762200\n",
      "iteration:  18840 , total_loss:  22.070439338684082\n",
      "count_amostra: 3768200\n",
      "iteration:  18870 , total_loss:  22.016931851704914\n",
      "count_amostra: 3774200\n",
      "iteration:  18900 , total_loss:  22.105453364054362\n",
      "count_amostra: 3780200\n",
      "iteration:  18930 , total_loss:  22.105435180664063\n",
      "count_amostra: 3786200\n",
      "iteration:  18960 , total_loss:  22.068914540608723\n",
      "count_amostra: 3792200\n",
      "iteration:  18990 , total_loss:  22.029526392618816\n",
      "count_amostra: 3798200\n",
      "iteration:  19020 , total_loss:  22.040315183003745\n",
      "count_amostra: 3804200\n",
      "iteration:  19050 , total_loss:  21.99593512217204\n",
      "count_amostra: 3810200\n",
      "iteration:  19080 , total_loss:  22.00355364481608\n",
      "count_amostra: 3816200\n",
      "iteration:  19110 , total_loss:  21.957029596964517\n",
      "count_amostra: 3822200\n",
      "iteration:  19140 , total_loss:  22.04514980316162\n",
      "count_amostra: 3828200\n",
      "iteration:  19170 , total_loss:  21.95618241628011\n",
      "count_amostra: 3834200\n",
      "iteration:  19200 , total_loss:  22.02840741475423\n",
      "count_amostra: 3840200\n",
      "iteration:  19230 , total_loss:  21.92513999938965\n",
      "count_amostra: 3846200\n",
      "iteration:  19260 , total_loss:  22.07712065378825\n",
      "count_amostra: 3852200\n",
      "iteration:  19290 , total_loss:  22.052077611287434\n",
      "count_amostra: 3858200\n",
      "iteration:  19320 , total_loss:  22.016348139444986\n",
      "count_amostra: 3864200\n",
      "iteration:  19350 , total_loss:  21.981227048238118\n",
      "count_amostra: 3870200\n",
      "iteration:  19380 , total_loss:  22.000133641560872\n",
      "count_amostra: 3876200\n",
      "iteration:  19410 , total_loss:  21.989083544413248\n",
      "count_amostra: 3882200\n",
      "iteration:  19440 , total_loss:  22.047788047790526\n",
      "count_amostra: 3888200\n",
      "iteration:  19470 , total_loss:  21.955221875508627\n",
      "count_amostra: 3894200\n",
      "iteration:  19500 , total_loss:  21.983606147766114\n",
      "count_amostra: 3900200\n",
      "iteration:  19530 , total_loss:  22.020344289143882\n",
      "count_amostra: 3906200\n",
      "iteration:  19560 , total_loss:  21.95816771189372\n",
      "count_amostra: 3912200\n",
      "iteration:  19590 , total_loss:  21.980289777119953\n",
      "count_amostra: 3918200\n",
      "iteration:  19620 , total_loss:  21.95248203277588\n",
      "count_amostra: 3924200\n",
      "iteration:  19650 , total_loss:  22.013638242085776\n",
      "count_amostra: 3930200\n",
      "iteration:  19680 , total_loss:  21.933378092447917\n",
      "count_amostra: 3936200\n",
      "iteration:  19710 , total_loss:  21.987812296549478\n",
      "count_amostra: 3942200\n",
      "iteration:  19740 , total_loss:  21.894482040405272\n",
      "count_amostra: 3948200\n",
      "iteration:  19770 , total_loss:  21.898008155822755\n",
      "count_amostra: 3954200\n",
      "iteration:  19800 , total_loss:  21.986948013305664\n",
      "count_amostra: 3960200\n",
      "iteration:  19830 , total_loss:  21.916144943237306\n",
      "count_amostra: 3966200\n",
      "iteration:  19860 , total_loss:  21.89352347056071\n",
      "count_amostra: 3972200\n",
      "iteration:  19890 , total_loss:  22.009355354309083\n",
      "count_amostra: 3978200\n",
      "iteration:  19920 , total_loss:  21.996344566345215\n",
      "count_amostra: 3984200\n",
      "iteration:  19950 , total_loss:  21.91842886606852\n",
      "count_amostra: 3990200\n",
      "iteration:  19980 , total_loss:  21.85520362854004\n",
      "count_amostra: 3996200\n",
      "iteration:  20010 , total_loss:  21.952171579996744\n",
      "count_amostra: 4002200\n",
      "iteration:  20040 , total_loss:  21.94214433034261\n",
      "count_amostra: 4008200\n",
      "iteration:  20070 , total_loss:  22.009015210469563\n",
      "count_amostra: 4014200\n",
      "iteration:  20100 , total_loss:  21.993111864725748\n",
      "count_amostra: 4020200\n",
      "iteration:  20130 , total_loss:  21.81888573964437\n",
      "count_amostra: 4026200\n",
      "iteration:  20160 , total_loss:  21.9429266611735\n",
      "count_amostra: 4032200\n",
      "iteration:  20190 , total_loss:  21.9254612604777\n",
      "count_amostra: 4038200\n",
      "iteration:  20220 , total_loss:  21.98418083190918\n",
      "count_amostra: 4044200\n",
      "iteration:  20250 , total_loss:  22.002081235249836\n",
      "count_amostra: 4050200\n",
      "iteration:  20280 , total_loss:  21.863429832458497\n",
      "count_amostra: 4056200\n",
      "iteration:  20310 , total_loss:  21.83830674489339\n",
      "count_amostra: 4062200\n",
      "iteration:  20340 , total_loss:  21.993004735310873\n",
      "count_amostra: 4068200\n",
      "iteration:  20370 , total_loss:  21.82649803161621\n",
      "count_amostra: 4074200\n",
      "iteration:  20400 , total_loss:  21.875685946146646\n",
      "count_amostra: 4080200\n",
      "iteration:  20430 , total_loss:  21.870146369934083\n",
      "count_amostra: 4086200\n",
      "iteration:  20460 , total_loss:  21.838118235270183\n",
      "count_amostra: 4092200\n",
      "iteration:  20490 , total_loss:  21.877354113260903\n",
      "count_amostra: 4098200\n",
      "iteration:  20520 , total_loss:  21.95869363149007\n",
      "count_amostra: 4104200\n",
      "iteration:  20550 , total_loss:  21.956617228190105\n",
      "count_amostra: 4110200\n",
      "iteration:  20580 , total_loss:  21.986447525024413\n",
      "count_amostra: 4116200\n",
      "iteration:  20610 , total_loss:  21.921353149414063\n",
      "count_amostra: 4122200\n",
      "iteration:  20640 , total_loss:  21.884377225240073\n",
      "count_amostra: 4128200\n",
      "iteration:  20670 , total_loss:  21.864901860555012\n",
      "count_amostra: 4134200\n",
      "iteration:  20700 , total_loss:  21.933347829182942\n",
      "count_amostra: 4140200\n",
      "iteration:  20730 , total_loss:  21.965963872273765\n",
      "count_amostra: 4146200\n",
      "iteration:  20760 , total_loss:  21.946967124938965\n",
      "count_amostra: 4152200\n",
      "iteration:  20790 , total_loss:  21.821155675252278\n",
      "count_amostra: 4158200\n",
      "iteration:  20820 , total_loss:  21.933686192830404\n",
      "count_amostra: 4164200\n",
      "iteration:  20850 , total_loss:  21.988669141133627\n",
      "count_amostra: 4170200\n",
      "iteration:  20880 , total_loss:  21.847009023030598\n",
      "count_amostra: 4176200\n",
      "iteration:  20910 , total_loss:  21.918639437357584\n",
      "count_amostra: 4182200\n",
      "iteration:  20940 , total_loss:  21.85973523457845\n",
      "count_amostra: 4188200\n",
      "iteration:  20970 , total_loss:  21.955639012654622\n",
      "count_amostra: 4194200\n",
      "iteration:  21000 , total_loss:  21.947072982788086\n",
      "count_amostra: 4200200\n",
      "iteration:  21030 , total_loss:  21.865143140157063\n",
      "count_amostra: 4206200\n",
      "iteration:  21060 , total_loss:  21.85910504659017\n",
      "count_amostra: 4212200\n",
      "iteration:  21090 , total_loss:  21.897012519836426\n",
      "count_amostra: 4218200\n",
      "iteration:  21120 , total_loss:  21.79882443745931\n",
      "count_amostra: 4224200\n",
      "iteration:  21150 , total_loss:  21.90714562733968\n",
      "count_amostra: 4230200\n",
      "iteration:  21180 , total_loss:  21.92124843597412\n",
      "count_amostra: 4236200\n",
      "iteration:  21210 , total_loss:  21.864682579040526\n",
      "count_amostra: 4242200\n",
      "iteration:  21240 , total_loss:  21.9143861134847\n",
      "count_amostra: 4248200\n",
      "iteration:  21270 , total_loss:  21.89489599863688\n",
      "count_amostra: 4254200\n",
      "iteration:  21300 , total_loss:  21.774371337890624\n",
      "count_amostra: 4260200\n",
      "iteration:  21330 , total_loss:  21.79622027079264\n",
      "count_amostra: 4266200\n",
      "iteration:  21360 , total_loss:  21.888024838765464\n",
      "count_amostra: 4272200\n",
      "iteration:  21390 , total_loss:  21.878124936421713\n",
      "count_amostra: 4278200\n",
      "iteration:  21420 , total_loss:  21.80995318094889\n",
      "count_amostra: 4284200\n",
      "iteration:  21450 , total_loss:  21.897321764628092\n",
      "count_amostra: 4290200\n",
      "iteration:  21480 , total_loss:  21.8071751276652\n",
      "count_amostra: 4296200\n",
      "iteration:  21510 , total_loss:  21.912268129984536\n",
      "count_amostra: 4302200\n",
      "iteration:  21540 , total_loss:  21.852479934692383\n",
      "count_amostra: 4308200\n",
      "iteration:  21570 , total_loss:  21.811355272928875\n",
      "count_amostra: 4314200\n",
      "iteration:  21600 , total_loss:  21.868976656595866\n",
      "count_amostra: 4320200\n",
      "iteration:  21630 , total_loss:  21.854457028706868\n",
      "count_amostra: 4326200\n",
      "iteration:  21660 , total_loss:  21.808456420898438\n",
      "count_amostra: 4332200\n",
      "iteration:  21690 , total_loss:  21.85810966491699\n",
      "count_amostra: 4338200\n",
      "iteration:  21720 , total_loss:  21.858793640136717\n",
      "count_amostra: 4344200\n",
      "iteration:  21750 , total_loss:  21.83162155151367\n",
      "count_amostra: 4350200\n",
      "iteration:  21780 , total_loss:  21.81030387878418\n",
      "count_amostra: 4356200\n",
      "iteration:  21810 , total_loss:  21.803334363301595\n",
      "count_amostra: 4362200\n",
      "iteration:  21840 , total_loss:  21.84549700419108\n",
      "count_amostra: 4368200\n",
      "iteration:  21870 , total_loss:  21.822999827067058\n",
      "count_amostra: 4374200\n",
      "iteration:  21900 , total_loss:  21.86358299255371\n",
      "count_amostra: 4380200\n",
      "iteration:  21930 , total_loss:  21.81004015604655\n",
      "count_amostra: 4386200\n",
      "iteration:  21960 , total_loss:  21.805507723490397\n",
      "count_amostra: 4392200\n",
      "iteration:  21990 , total_loss:  21.936452929178873\n",
      "count_amostra: 4398200\n",
      "iteration:  22020 , total_loss:  21.802195103963218\n",
      "count_amostra: 4404200\n",
      "iteration:  22050 , total_loss:  21.843609619140626\n",
      "count_amostra: 4410200\n",
      "iteration:  22080 , total_loss:  21.799875704447427\n",
      "count_amostra: 4416200\n",
      "iteration:  22110 , total_loss:  21.728637822469075\n",
      "count_amostra: 4422200\n",
      "iteration:  22140 , total_loss:  21.739994239807128\n",
      "count_amostra: 4428200\n",
      "iteration:  22170 , total_loss:  21.784020868937173\n",
      "count_amostra: 4434200\n",
      "iteration:  22200 , total_loss:  21.755633799235024\n",
      "count_amostra: 4440200\n",
      "iteration:  22230 , total_loss:  21.764983876546225\n",
      "count_amostra: 4446200\n",
      "iteration:  22260 , total_loss:  21.94811725616455\n",
      "count_amostra: 4452200\n",
      "iteration:  22290 , total_loss:  21.825771776835122\n",
      "count_amostra: 4458200\n",
      "iteration:  22320 , total_loss:  21.81509698232015\n",
      "count_amostra: 4464200\n",
      "iteration:  22350 , total_loss:  21.751403363545737\n",
      "count_amostra: 4470200\n",
      "iteration:  22380 , total_loss:  21.756071535746255\n",
      "count_amostra: 4476200\n",
      "iteration:  22410 , total_loss:  21.793514315287272\n",
      "count_amostra: 4482200\n",
      "iteration:  22440 , total_loss:  21.79435017903646\n",
      "count_amostra: 4488200\n",
      "iteration:  22470 , total_loss:  21.78805637359619\n",
      "count_amostra: 4494200\n",
      "iteration:  22500 , total_loss:  21.770853360493977\n",
      "count_amostra: 4500200\n",
      "iteration:  22530 , total_loss:  21.744164085388185\n",
      "count_amostra: 4506200\n",
      "iteration:  22560 , total_loss:  21.80592130025228\n",
      "count_amostra: 4512200\n",
      "iteration:  22590 , total_loss:  21.74609349568685\n",
      "count_amostra: 4518200\n",
      "iteration:  22620 , total_loss:  21.831359354654946\n",
      "count_amostra: 4524200\n",
      "iteration:  22650 , total_loss:  21.749540837605796\n",
      "count_amostra: 4530200\n",
      "iteration:  22680 , total_loss:  21.822918128967284\n",
      "count_amostra: 4536200\n",
      "iteration:  22710 , total_loss:  21.834309005737303\n",
      "count_amostra: 4542200\n",
      "iteration:  22740 , total_loss:  21.731732749938963\n",
      "count_amostra: 4548200\n",
      "iteration:  22770 , total_loss:  21.747355397542318\n",
      "count_amostra: 4554200\n",
      "iteration:  22800 , total_loss:  21.811237653096516\n",
      "count_amostra: 4560200\n",
      "iteration:  22830 , total_loss:  21.748536936442058\n",
      "count_amostra: 4566200\n",
      "iteration:  22860 , total_loss:  21.762690162658693\n",
      "count_amostra: 4572200\n",
      "iteration:  22890 , total_loss:  21.76706994374593\n",
      "count_amostra: 4578200\n",
      "iteration:  22920 , total_loss:  21.77266654968262\n",
      "count_amostra: 4584200\n",
      "iteration:  22950 , total_loss:  21.670490646362303\n",
      "count_amostra: 4590200\n",
      "iteration:  22980 , total_loss:  21.805112902323405\n",
      "count_amostra: 4596200\n",
      "iteration:  23010 , total_loss:  21.803205680847167\n",
      "count_amostra: 4602200\n",
      "iteration:  23040 , total_loss:  21.750071843465168\n",
      "count_amostra: 4608200\n",
      "iteration:  23070 , total_loss:  21.702515029907225\n",
      "count_amostra: 4614200\n",
      "iteration:  23100 , total_loss:  21.752973620096842\n",
      "count_amostra: 4620200\n",
      "iteration:  23130 , total_loss:  21.74320863087972\n",
      "count_amostra: 4626200\n",
      "iteration:  23160 , total_loss:  21.80808022816976\n",
      "count_amostra: 4632200\n",
      "iteration:  23190 , total_loss:  21.72014923095703\n",
      "count_amostra: 4638200\n",
      "iteration:  23220 , total_loss:  21.797630055745444\n",
      "count_amostra: 4644200\n",
      "iteration:  23250 , total_loss:  21.70526860555013\n",
      "count_amostra: 4650200\n",
      "iteration:  23280 , total_loss:  21.77064914703369\n",
      "count_amostra: 4656200\n",
      "iteration:  23310 , total_loss:  21.78216985066732\n",
      "count_amostra: 4662200\n",
      "iteration:  23340 , total_loss:  21.757052421569824\n",
      "count_amostra: 4668200\n",
      "iteration:  23370 , total_loss:  21.748089853922526\n",
      "count_amostra: 4674200\n",
      "iteration:  23400 , total_loss:  21.74147103627523\n",
      "count_amostra: 4680200\n",
      "iteration:  23430 , total_loss:  21.728222211201984\n",
      "count_amostra: 4686200\n",
      "iteration:  23460 , total_loss:  21.784690602620444\n",
      "count_amostra: 4692200\n",
      "iteration:  23490 , total_loss:  21.83779296875\n",
      "count_amostra: 4698200\n",
      "iteration:  23520 , total_loss:  21.752095222473145\n",
      "count_amostra: 4704200\n",
      "iteration:  23550 , total_loss:  21.709953117370606\n",
      "count_amostra: 4710200\n",
      "iteration:  23580 , total_loss:  21.754822731018066\n",
      "count_amostra: 4716200\n",
      "iteration:  23610 , total_loss:  21.723374112447104\n",
      "count_amostra: 4722200\n",
      "iteration:  23640 , total_loss:  21.74613920847575\n",
      "count_amostra: 4728200\n",
      "iteration:  23670 , total_loss:  21.78389771779378\n",
      "count_amostra: 4734200\n",
      "iteration:  23700 , total_loss:  21.731141344706217\n",
      "count_amostra: 4740200\n",
      "iteration:  23730 , total_loss:  21.773617553710938\n",
      "count_amostra: 4746200\n",
      "iteration:  23760 , total_loss:  21.767193921407063\n",
      "count_amostra: 4752200\n",
      "iteration:  23790 , total_loss:  21.72862517038981\n",
      "count_amostra: 4758200\n",
      "iteration:  23820 , total_loss:  21.71430015563965\n",
      "count_amostra: 4764200\n",
      "iteration:  23850 , total_loss:  21.70457515716553\n",
      "count_amostra: 4770200\n",
      "iteration:  23880 , total_loss:  21.717145029703776\n",
      "count_amostra: 4776200\n",
      "iteration:  23910 , total_loss:  21.63119799296061\n",
      "count_amostra: 4782200\n",
      "iteration:  23940 , total_loss:  21.700626182556153\n",
      "count_amostra: 4788200\n",
      "iteration:  23970 , total_loss:  21.70554402669271\n",
      "count_amostra: 4794200\n",
      "iteration:  24000 , total_loss:  21.676791191101074\n",
      "count_amostra: 4800200\n",
      "iteration:  24030 , total_loss:  21.752640151977538\n",
      "count_amostra: 4806200\n",
      "iteration:  24060 , total_loss:  21.6127072652181\n",
      "count_amostra: 4812200\n",
      "iteration:  24090 , total_loss:  21.651696968078614\n",
      "count_amostra: 4818200\n",
      "iteration:  24120 , total_loss:  21.749580001831056\n",
      "count_amostra: 4824200\n",
      "iteration:  24150 , total_loss:  21.68801294962565\n",
      "count_amostra: 4830200\n",
      "iteration:  24180 , total_loss:  21.66459986368815\n",
      "count_amostra: 4836200\n",
      "iteration:  24210 , total_loss:  21.695350710550944\n",
      "count_amostra: 4842200\n",
      "iteration:  24240 , total_loss:  21.6931941986084\n",
      "count_amostra: 4848200\n",
      "iteration:  24270 , total_loss:  21.71876220703125\n",
      "count_amostra: 4854200\n",
      "iteration:  24300 , total_loss:  21.713644981384277\n",
      "count_amostra: 4860200\n",
      "iteration:  24330 , total_loss:  21.6229913075765\n",
      "count_amostra: 4866200\n",
      "iteration:  24360 , total_loss:  21.668335660298666\n",
      "count_amostra: 4872200\n",
      "iteration:  24390 , total_loss:  21.704183514912923\n",
      "count_amostra: 4878200\n",
      "iteration:  24420 , total_loss:  21.744244194030763\n",
      "count_amostra: 4884200\n",
      "iteration:  24450 , total_loss:  21.776408767700197\n",
      "count_amostra: 4890200\n",
      "iteration:  24480 , total_loss:  21.694991302490234\n",
      "count_amostra: 4896200\n",
      "iteration:  24510 , total_loss:  21.73981901804606\n",
      "count_amostra: 4902200\n",
      "iteration:  24540 , total_loss:  21.65098787943522\n",
      "count_amostra: 4908200\n",
      "iteration:  24570 , total_loss:  21.69478448232015\n",
      "count_amostra: 4914200\n",
      "iteration:  24600 , total_loss:  21.792570559183755\n",
      "count_amostra: 4920200\n",
      "iteration:  24630 , total_loss:  21.722033882141112\n",
      "count_amostra: 4926200\n",
      "iteration:  24660 , total_loss:  21.55961380004883\n",
      "count_amostra: 4932200\n",
      "iteration:  24690 , total_loss:  21.722697575887043\n",
      "count_amostra: 4938200\n",
      "iteration:  24720 , total_loss:  21.62370096842448\n",
      "count_amostra: 4944200\n",
      "iteration:  24750 , total_loss:  21.700124168395995\n",
      "count_amostra: 4950200\n",
      "iteration:  24780 , total_loss:  21.664812342325845\n",
      "count_amostra: 4956200\n",
      "iteration:  24810 , total_loss:  21.708724848429362\n",
      "count_amostra: 4962200\n",
      "iteration:  24840 , total_loss:  21.778992462158204\n",
      "count_amostra: 4968200\n",
      "iteration:  24870 , total_loss:  21.66626033782959\n",
      "count_amostra: 4974200\n",
      "iteration:  24900 , total_loss:  21.666387875874836\n",
      "count_amostra: 4980200\n",
      "iteration:  24930 , total_loss:  21.695572916666666\n",
      "count_amostra: 4986200\n",
      "iteration:  24960 , total_loss:  21.562826029459636\n",
      "count_amostra: 4992200\n",
      "iteration:  24990 , total_loss:  21.670913441975912\n",
      "count_amostra: 4998200\n",
      "iteration:  25020 , total_loss:  21.71679967244466\n",
      "count_amostra: 5004200\n",
      "iteration:  25050 , total_loss:  21.668655776977538\n",
      "count_amostra: 5010200\n",
      "iteration:  25080 , total_loss:  21.620271237691245\n",
      "count_amostra: 5016200\n",
      "iteration:  25110 , total_loss:  21.690778414408367\n",
      "count_amostra: 5022200\n",
      "iteration:  25140 , total_loss:  21.602161407470703\n",
      "count_amostra: 5028200\n",
      "iteration:  25170 , total_loss:  21.726321919759116\n",
      "count_amostra: 5034200\n",
      "iteration:  25200 , total_loss:  21.624293772379556\n",
      "count_amostra: 5040200\n",
      "iteration:  25230 , total_loss:  21.554217847188315\n",
      "count_amostra: 5046200\n",
      "iteration:  25260 , total_loss:  21.710126304626463\n",
      "count_amostra: 5052200\n",
      "iteration:  25290 , total_loss:  21.593221537272136\n",
      "count_amostra: 5058200\n",
      "iteration:  25320 , total_loss:  21.608450508117677\n",
      "count_amostra: 5064200\n",
      "iteration:  25350 , total_loss:  21.744424057006835\n",
      "count_amostra: 5070200\n",
      "iteration:  25380 , total_loss:  21.69998067220052\n",
      "count_amostra: 5076200\n",
      "iteration:  25410 , total_loss:  21.639099566141763\n",
      "count_amostra: 5082200\n",
      "iteration:  25440 , total_loss:  21.580599784851074\n",
      "count_amostra: 5088200\n",
      "iteration:  25470 , total_loss:  21.613949267069497\n",
      "count_amostra: 5094200\n",
      "iteration:  25500 , total_loss:  21.631332397460938\n",
      "count_amostra: 5100200\n",
      "iteration:  25530 , total_loss:  21.69739227294922\n",
      "count_amostra: 5106200\n",
      "iteration:  25560 , total_loss:  21.632281812032065\n",
      "count_amostra: 5112200\n",
      "iteration:  25590 , total_loss:  21.624940999348958\n",
      "count_amostra: 5118200\n",
      "iteration:  25620 , total_loss:  21.671125729878742\n",
      "count_amostra: 5124200\n",
      "iteration:  25650 , total_loss:  21.57880776723226\n",
      "count_amostra: 5130200\n",
      "iteration:  25680 , total_loss:  21.555074055989582\n",
      "count_amostra: 5136200\n",
      "iteration:  25710 , total_loss:  21.67071393330892\n",
      "count_amostra: 5142200\n",
      "iteration:  25740 , total_loss:  21.613671112060548\n",
      "count_amostra: 5148200\n",
      "iteration:  25770 , total_loss:  21.55462589263916\n",
      "count_amostra: 5154200\n",
      "iteration:  25800 , total_loss:  21.616973940531413\n",
      "count_amostra: 5160200\n",
      "iteration:  25830 , total_loss:  21.66828187306722\n",
      "count_amostra: 5166200\n",
      "iteration:  25860 , total_loss:  21.62932071685791\n",
      "count_amostra: 5172200\n",
      "iteration:  25890 , total_loss:  21.571367327372233\n",
      "count_amostra: 5178200\n",
      "iteration:  25920 , total_loss:  21.582283782958985\n",
      "count_amostra: 5184200\n",
      "iteration:  25950 , total_loss:  21.569649124145506\n",
      "count_amostra: 5190200\n",
      "iteration:  25980 , total_loss:  21.594124794006348\n",
      "count_amostra: 5196200\n",
      "iteration:  26010 , total_loss:  21.649847984313965\n",
      "count_amostra: 5202200\n",
      "iteration:  26040 , total_loss:  21.578120867411297\n",
      "count_amostra: 5208200\n",
      "iteration:  26070 , total_loss:  21.684879239400228\n",
      "count_amostra: 5214200\n",
      "iteration:  26100 , total_loss:  21.657818031311034\n",
      "count_amostra: 5220200\n",
      "iteration:  26130 , total_loss:  21.61937313079834\n",
      "count_amostra: 5226200\n",
      "iteration:  26160 , total_loss:  21.646372667948405\n",
      "count_amostra: 5232200\n",
      "iteration:  26190 , total_loss:  21.561071968078615\n",
      "count_amostra: 5238200\n",
      "iteration:  26220 , total_loss:  21.608905855814616\n",
      "count_amostra: 5244200\n",
      "iteration:  26250 , total_loss:  21.648631286621093\n",
      "count_amostra: 5250200\n",
      "iteration:  26280 , total_loss:  21.5639523824056\n",
      "count_amostra: 5256200\n",
      "iteration:  26310 , total_loss:  21.58879222869873\n",
      "count_amostra: 5262200\n",
      "iteration:  26340 , total_loss:  21.559607632954915\n",
      "count_amostra: 5268200\n",
      "iteration:  26370 , total_loss:  21.554216066996258\n",
      "count_amostra: 5274200\n",
      "iteration:  26400 , total_loss:  21.539695421854656\n",
      "count_amostra: 5280200\n",
      "iteration:  26430 , total_loss:  21.59343579610189\n",
      "count_amostra: 5286200\n",
      "iteration:  26460 , total_loss:  21.627846018473306\n",
      "count_amostra: 5292200\n",
      "iteration:  26490 , total_loss:  21.598270479838053\n",
      "count_amostra: 5298200\n",
      "iteration:  26520 , total_loss:  21.587136395772298\n",
      "count_amostra: 5304200\n",
      "iteration:  26550 , total_loss:  21.62656396230062\n",
      "count_amostra: 5310200\n",
      "iteration:  26580 , total_loss:  21.59798418680827\n",
      "count_amostra: 5316200\n",
      "iteration:  26610 , total_loss:  21.581695365905762\n",
      "count_amostra: 5322200\n",
      "iteration:  26640 , total_loss:  21.62896925608317\n",
      "count_amostra: 5328200\n",
      "iteration:  26670 , total_loss:  21.52897440592448\n",
      "count_amostra: 5334200\n",
      "iteration:  26700 , total_loss:  21.50904738108317\n",
      "count_amostra: 5340200\n",
      "iteration:  26730 , total_loss:  21.50844160715739\n",
      "count_amostra: 5346200\n",
      "iteration:  26760 , total_loss:  21.548322677612305\n",
      "count_amostra: 5352200\n",
      "iteration:  26790 , total_loss:  21.47224343617757\n",
      "count_amostra: 5358200\n",
      "iteration:  26820 , total_loss:  21.579662704467772\n",
      "count_amostra: 5364200\n",
      "iteration:  26850 , total_loss:  21.51516653696696\n",
      "count_amostra: 5370200\n",
      "iteration:  26880 , total_loss:  21.564137331644694\n",
      "count_amostra: 5376200\n",
      "iteration:  26910 , total_loss:  21.540187136332193\n",
      "count_amostra: 5382200\n",
      "iteration:  26940 , total_loss:  21.625637372334797\n",
      "count_amostra: 5388200\n",
      "iteration:  26970 , total_loss:  21.628922271728516\n",
      "count_amostra: 5394200\n",
      "iteration:  27000 , total_loss:  21.56437129974365\n",
      "count_amostra: 5400200\n",
      "iteration:  27030 , total_loss:  21.499279149373372\n",
      "count_amostra: 5406200\n",
      "iteration:  27060 , total_loss:  21.53755251566569\n",
      "count_amostra: 5412200\n",
      "iteration:  27090 , total_loss:  21.580160649617515\n",
      "count_amostra: 5418200\n",
      "iteration:  27120 , total_loss:  21.547713152567546\n",
      "count_amostra: 5424200\n",
      "iteration:  27150 , total_loss:  21.509851455688477\n",
      "count_amostra: 5430200\n",
      "iteration:  27180 , total_loss:  21.71480026245117\n",
      "count_amostra: 5436200\n",
      "iteration:  27210 , total_loss:  21.53218110402425\n",
      "count_amostra: 5442200\n",
      "iteration:  27240 , total_loss:  21.540495300292967\n",
      "count_amostra: 5448200\n",
      "iteration:  27270 , total_loss:  21.589333152770998\n",
      "count_amostra: 5454200\n",
      "iteration:  27300 , total_loss:  21.55943400065104\n",
      "count_amostra: 5460200\n",
      "iteration:  27330 , total_loss:  21.4984229405721\n",
      "count_amostra: 5466200\n",
      "iteration:  27360 , total_loss:  21.5608242670695\n",
      "count_amostra: 5472200\n",
      "iteration:  27390 , total_loss:  21.493923123677572\n",
      "count_amostra: 5478200\n",
      "iteration:  27420 , total_loss:  21.559950319925942\n",
      "count_amostra: 5484200\n",
      "iteration:  27450 , total_loss:  21.514332580566407\n",
      "count_amostra: 5490200\n",
      "iteration:  27480 , total_loss:  21.5718111038208\n",
      "count_amostra: 5496200\n",
      "iteration:  27510 , total_loss:  21.532129923502605\n",
      "count_amostra: 5502200\n",
      "iteration:  27540 , total_loss:  21.64105161031087\n",
      "count_amostra: 5508200\n",
      "iteration:  27570 , total_loss:  21.42797393798828\n",
      "count_amostra: 5514200\n",
      "iteration:  27600 , total_loss:  21.533675765991212\n",
      "count_amostra: 5520200\n",
      "iteration:  27630 , total_loss:  21.65322011311849\n",
      "count_amostra: 5526200\n",
      "iteration:  27660 , total_loss:  21.46688741048177\n",
      "count_amostra: 5532200\n",
      "iteration:  27690 , total_loss:  21.533788871765136\n",
      "count_amostra: 5538200\n",
      "iteration:  27720 , total_loss:  21.43900178273519\n",
      "count_amostra: 5544200\n",
      "iteration:  27750 , total_loss:  21.53078276316325\n",
      "count_amostra: 5550200\n",
      "iteration:  27780 , total_loss:  21.500669225056967\n",
      "count_amostra: 5556200\n",
      "iteration:  27810 , total_loss:  21.557002957661947\n",
      "count_amostra: 5562200\n",
      "iteration:  27840 , total_loss:  21.478238423665363\n",
      "count_amostra: 5568200\n",
      "iteration:  27870 , total_loss:  21.47831013997396\n",
      "count_amostra: 5574200\n",
      "iteration:  27900 , total_loss:  21.517187690734865\n",
      "count_amostra: 5580200\n",
      "iteration:  27930 , total_loss:  21.60181605021159\n",
      "count_amostra: 5586200\n",
      "iteration:  27960 , total_loss:  21.45435880025228\n",
      "count_amostra: 5592200\n",
      "iteration:  27990 , total_loss:  21.542187881469726\n",
      "count_amostra: 5598200\n",
      "iteration:  28020 , total_loss:  21.651297950744627\n",
      "count_amostra: 5604200\n",
      "iteration:  28050 , total_loss:  21.579006894429526\n",
      "count_amostra: 5610200\n",
      "iteration:  28080 , total_loss:  21.534386825561523\n",
      "count_amostra: 5616200\n",
      "iteration:  28110 , total_loss:  21.48617820739746\n",
      "count_amostra: 5622200\n",
      "iteration:  28140 , total_loss:  21.585468419392903\n",
      "count_amostra: 5628200\n",
      "iteration:  28170 , total_loss:  21.52353312174479\n",
      "count_amostra: 5634200\n",
      "iteration:  28200 , total_loss:  21.549824142456053\n",
      "count_amostra: 5640200\n",
      "iteration:  28230 , total_loss:  21.479847399393716\n",
      "count_amostra: 5646200\n",
      "iteration:  28260 , total_loss:  21.523913955688478\n",
      "count_amostra: 5652200\n",
      "iteration:  28290 , total_loss:  21.551272773742674\n",
      "count_amostra: 5658200\n",
      "iteration:  28320 , total_loss:  21.55990352630615\n",
      "count_amostra: 5664200\n",
      "iteration:  28350 , total_loss:  21.49604434967041\n",
      "count_amostra: 5670200\n",
      "iteration:  28380 , total_loss:  21.566277313232423\n",
      "count_amostra: 5676200\n",
      "iteration:  28410 , total_loss:  21.52010726928711\n",
      "count_amostra: 5682200\n",
      "iteration:  28440 , total_loss:  21.4777551651001\n",
      "count_amostra: 5688200\n",
      "iteration:  28470 , total_loss:  21.485997327168782\n",
      "count_amostra: 5694200\n",
      "iteration:  28500 , total_loss:  21.450274340311687\n",
      "count_amostra: 5700200\n",
      "iteration:  28530 , total_loss:  21.46337343851725\n",
      "count_amostra: 5706200\n",
      "iteration:  28560 , total_loss:  21.482844734191893\n",
      "count_amostra: 5712200\n",
      "iteration:  28590 , total_loss:  21.475750160217284\n",
      "count_amostra: 5718200\n",
      "iteration:  28620 , total_loss:  21.522766304016113\n",
      "count_amostra: 5724200\n",
      "iteration:  28650 , total_loss:  21.529599634806313\n",
      "count_amostra: 5730200\n",
      "iteration:  28680 , total_loss:  21.46463769276937\n",
      "count_amostra: 5736200\n",
      "iteration:  28710 , total_loss:  21.57774543762207\n",
      "count_amostra: 5742200\n",
      "iteration:  28740 , total_loss:  21.50778147379557\n",
      "count_amostra: 5748200\n",
      "iteration:  28770 , total_loss:  21.54802042643229\n",
      "count_amostra: 5754200\n",
      "iteration:  28800 , total_loss:  21.434970792134603\n",
      "count_amostra: 5760200\n",
      "iteration:  28830 , total_loss:  21.533638381958006\n",
      "count_amostra: 5766200\n",
      "iteration:  28860 , total_loss:  21.525414148966473\n",
      "count_amostra: 5772200\n",
      "iteration:  28890 , total_loss:  21.428321075439452\n",
      "count_amostra: 5778200\n",
      "iteration:  28920 , total_loss:  21.451247278849284\n",
      "count_amostra: 5784200\n",
      "iteration:  28950 , total_loss:  21.40665849049886\n",
      "count_amostra: 5790200\n",
      "iteration:  28980 , total_loss:  21.39606615702311\n",
      "count_amostra: 5796200\n",
      "iteration:  29010 , total_loss:  21.526175944010415\n",
      "count_amostra: 5802200\n",
      "iteration:  29040 , total_loss:  21.524434534708657\n",
      "count_amostra: 5808200\n",
      "iteration:  29070 , total_loss:  21.454626909891765\n",
      "count_amostra: 5814200\n",
      "iteration:  29100 , total_loss:  21.434044710795085\n",
      "count_amostra: 5820200\n",
      "iteration:  29130 , total_loss:  21.428295771280926\n",
      "count_amostra: 5826200\n",
      "iteration:  29160 , total_loss:  21.393540064493816\n",
      "count_amostra: 5832200\n",
      "iteration:  29190 , total_loss:  21.48017972310384\n",
      "count_amostra: 5838200\n",
      "iteration:  29220 , total_loss:  21.447042973836265\n",
      "count_amostra: 5844200\n",
      "iteration:  29250 , total_loss:  21.422245343526203\n",
      "count_amostra: 5850200\n",
      "iteration:  29280 , total_loss:  21.454469617207845\n",
      "count_amostra: 5856200\n",
      "iteration:  29310 , total_loss:  21.544080352783205\n",
      "count_amostra: 5862200\n",
      "iteration:  29340 , total_loss:  21.49673557281494\n",
      "count_amostra: 5868200\n",
      "iteration:  29370 , total_loss:  21.528596687316895\n",
      "count_amostra: 5874200\n",
      "iteration:  29400 , total_loss:  21.491133308410646\n",
      "count_amostra: 5880200\n",
      "iteration:  29430 , total_loss:  21.471683184305828\n",
      "count_amostra: 5886200\n",
      "iteration:  29460 , total_loss:  21.453384145100912\n",
      "count_amostra: 5892200\n",
      "iteration:  29490 , total_loss:  21.53165798187256\n",
      "count_amostra: 5898200\n",
      "iteration:  29520 , total_loss:  21.44659907023112\n",
      "count_amostra: 5904200\n",
      "iteration:  29550 , total_loss:  21.39717877705892\n",
      "count_amostra: 5910200\n",
      "iteration:  29580 , total_loss:  21.424599266052248\n",
      "count_amostra: 5916200\n",
      "iteration:  29610 , total_loss:  21.417943954467773\n",
      "count_amostra: 5922200\n",
      "iteration:  29640 , total_loss:  21.407969983418784\n",
      "count_amostra: 5928200\n",
      "iteration:  29670 , total_loss:  21.45157896677653\n",
      "count_amostra: 5934200\n",
      "iteration:  29700 , total_loss:  21.4853635152181\n",
      "count_amostra: 5940200\n",
      "iteration:  29730 , total_loss:  21.329038238525392\n",
      "count_amostra: 5946200\n",
      "iteration:  29760 , total_loss:  21.451987965901694\n",
      "count_amostra: 5952200\n",
      "iteration:  29790 , total_loss:  21.37845598856608\n",
      "count_amostra: 5958200\n",
      "iteration:  29820 , total_loss:  21.568016560872397\n",
      "count_amostra: 5964200\n",
      "iteration:  29850 , total_loss:  21.4464391708374\n",
      "count_amostra: 5970200\n",
      "iteration:  29880 , total_loss:  21.426220830281576\n",
      "count_amostra: 5976200\n",
      "iteration:  29910 , total_loss:  21.362612215677895\n",
      "count_amostra: 5982200\n",
      "iteration:  29940 , total_loss:  21.438805389404298\n",
      "count_amostra: 5988200\n",
      "iteration:  29970 , total_loss:  21.436942227681477\n",
      "count_amostra: 5994200\n",
      "iteration:  30000 , total_loss:  21.34321715037028\n",
      "count_amostra: 6000200\n",
      "iteration:  30030 , total_loss:  21.434437624613444\n",
      "count_amostra: 6006200\n",
      "iteration:  30060 , total_loss:  21.385323651631673\n",
      "count_amostra: 6012200\n",
      "iteration:  30090 , total_loss:  21.442813301086424\n",
      "count_amostra: 6018200\n",
      "iteration:  30120 , total_loss:  21.3940855662028\n",
      "count_amostra: 6024200\n",
      "iteration:  30150 , total_loss:  21.357960383097332\n",
      "count_amostra: 6030200\n",
      "iteration:  30180 , total_loss:  21.470290819803875\n",
      "count_amostra: 6036200\n",
      "iteration:  30210 , total_loss:  21.50428689320882\n",
      "count_amostra: 6042200\n",
      "iteration:  30240 , total_loss:  21.492166646321614\n",
      "count_amostra: 6048200\n",
      "iteration:  30270 , total_loss:  21.45205243428548\n",
      "count_amostra: 6054200\n",
      "iteration:  30300 , total_loss:  21.420157559712727\n",
      "count_amostra: 6060200\n",
      "iteration:  30330 , total_loss:  21.437215042114257\n",
      "count_amostra: 6066200\n",
      "iteration:  30360 , total_loss:  21.36165428161621\n",
      "count_amostra: 6072200\n",
      "iteration:  30390 , total_loss:  21.4492431640625\n",
      "count_amostra: 6078200\n",
      "iteration:  30420 , total_loss:  21.46591993967692\n",
      "count_amostra: 6084200\n",
      "iteration:  30450 , total_loss:  21.392464447021485\n",
      "count_amostra: 6090200\n",
      "iteration:  30480 , total_loss:  21.46572748819987\n",
      "count_amostra: 6096200\n",
      "iteration:  30510 , total_loss:  21.492904154459634\n",
      "count_amostra: 6102200\n",
      "iteration:  30540 , total_loss:  21.406161244710287\n",
      "count_amostra: 6108200\n",
      "iteration:  30570 , total_loss:  21.345804723103843\n",
      "count_amostra: 6114200\n",
      "iteration:  30600 , total_loss:  21.349094263712566\n",
      "count_amostra: 6120200\n",
      "iteration:  30630 , total_loss:  21.428748194376627\n",
      "count_amostra: 6126200\n",
      "iteration:  30660 , total_loss:  21.340915870666503\n",
      "count_amostra: 6132200\n",
      "iteration:  30690 , total_loss:  21.373192977905273\n",
      "count_amostra: 6138200\n",
      "iteration:  30720 , total_loss:  21.465287017822266\n",
      "count_amostra: 6144200\n",
      "iteration:  30750 , total_loss:  21.48392416636149\n",
      "count_amostra: 6150200\n",
      "iteration:  30780 , total_loss:  21.371032206217446\n",
      "count_amostra: 6156200\n",
      "iteration:  30810 , total_loss:  21.41158256530762\n",
      "count_amostra: 6162200\n",
      "iteration:  30840 , total_loss:  21.368398729960123\n",
      "count_amostra: 6168200\n",
      "iteration:  30870 , total_loss:  21.348831748962404\n",
      "count_amostra: 6174200\n",
      "iteration:  30900 , total_loss:  21.458558654785158\n",
      "count_amostra: 6180200\n",
      "iteration:  30930 , total_loss:  21.368576685587566\n",
      "count_amostra: 6186200\n",
      "iteration:  30960 , total_loss:  21.4453306833903\n",
      "count_amostra: 6192200\n",
      "iteration:  30990 , total_loss:  21.356146876017252\n",
      "count_amostra: 6198200\n",
      "iteration:  31020 , total_loss:  21.369172223409016\n",
      "count_amostra: 6204200\n",
      "iteration:  31050 , total_loss:  21.387331453959145\n",
      "count_amostra: 6210200\n",
      "iteration:  31080 , total_loss:  21.313823191324868\n",
      "count_amostra: 6216200\n",
      "iteration:  31110 , total_loss:  21.369239616394044\n",
      "count_amostra: 6222200\n",
      "iteration:  31140 , total_loss:  21.397050666809083\n",
      "count_amostra: 6228200\n",
      "iteration:  31170 , total_loss:  21.390734481811524\n",
      "count_amostra: 6234200\n",
      "iteration:  31200 , total_loss:  21.37035414377848\n",
      "count_amostra: 6240200\n",
      "iteration:  31230 , total_loss:  21.387890243530272\n",
      "count_amostra: 6246200\n",
      "iteration:  31260 , total_loss:  21.37564951578776\n",
      "count_amostra: 6252200\n",
      "iteration:  31290 , total_loss:  21.369683583577473\n",
      "count_amostra: 6258200\n",
      "iteration:  31320 , total_loss:  21.436489804585776\n",
      "count_amostra: 6264200\n",
      "iteration:  31350 , total_loss:  21.450205485026043\n",
      "count_amostra: 6270200\n",
      "iteration:  31380 , total_loss:  21.313390413920086\n",
      "count_amostra: 6276200\n",
      "iteration:  31410 , total_loss:  21.34529749552409\n",
      "count_amostra: 6282200\n",
      "iteration:  31440 , total_loss:  21.3311554590861\n",
      "count_amostra: 6288200\n",
      "iteration:  31470 , total_loss:  21.394080924987794\n",
      "count_amostra: 6294200\n",
      "iteration:  31500 , total_loss:  21.40833988189697\n",
      "count_amostra: 6300200\n",
      "iteration:  31530 , total_loss:  21.40664094289144\n",
      "count_amostra: 6306200\n",
      "iteration:  31560 , total_loss:  21.33081537882487\n",
      "count_amostra: 6312200\n",
      "iteration:  31590 , total_loss:  21.328043365478514\n",
      "count_amostra: 6318200\n",
      "iteration:  31620 , total_loss:  21.405839029947916\n",
      "count_amostra: 6324200\n",
      "iteration:  31650 , total_loss:  21.296694374084474\n",
      "count_amostra: 6330200\n",
      "iteration:  31680 , total_loss:  21.404550806681314\n",
      "count_amostra: 6336200\n",
      "iteration:  31710 , total_loss:  21.44059289296468\n",
      "count_amostra: 6342200\n",
      "iteration:  31740 , total_loss:  21.328805096944173\n",
      "count_amostra: 6348200\n",
      "iteration:  31770 , total_loss:  21.39875996907552\n",
      "count_amostra: 6354200\n",
      "iteration:  31800 , total_loss:  21.344394048055012\n",
      "count_amostra: 6360200\n",
      "iteration:  31830 , total_loss:  21.40603427886963\n",
      "count_amostra: 6366200\n",
      "iteration:  31860 , total_loss:  21.385294469197593\n",
      "count_amostra: 6372200\n",
      "iteration:  31890 , total_loss:  21.389008649190266\n",
      "count_amostra: 6378200\n",
      "iteration:  31920 , total_loss:  21.353770446777343\n",
      "count_amostra: 6384200\n",
      "iteration:  31950 , total_loss:  21.385452524820963\n",
      "count_amostra: 6390200\n",
      "iteration:  31980 , total_loss:  21.36155300140381\n",
      "count_amostra: 6396200\n",
      "iteration:  32010 , total_loss:  21.304643185933433\n",
      "count_amostra: 6402200\n",
      "iteration:  32040 , total_loss:  21.437734031677245\n",
      "count_amostra: 6408200\n",
      "iteration:  32070 , total_loss:  21.42080637613932\n",
      "count_amostra: 6414200\n",
      "iteration:  32100 , total_loss:  21.293194389343263\n",
      "count_amostra: 6420200\n",
      "iteration:  32130 , total_loss:  21.36327559153239\n",
      "count_amostra: 6426200\n",
      "iteration:  32160 , total_loss:  21.322576840718586\n",
      "count_amostra: 6432200\n",
      "iteration:  32190 , total_loss:  21.34414774576823\n",
      "count_amostra: 6438200\n",
      "iteration:  32220 , total_loss:  21.43390350341797\n",
      "count_amostra: 6444200\n",
      "iteration:  32250 , total_loss:  21.36286118825277\n",
      "count_amostra: 6450200\n",
      "iteration:  32280 , total_loss:  21.355010350545246\n",
      "count_amostra: 6456200\n",
      "iteration:  32310 , total_loss:  21.381987190246583\n",
      "count_amostra: 6462200\n",
      "iteration:  32340 , total_loss:  21.260818608601888\n",
      "count_amostra: 6468200\n",
      "iteration:  32370 , total_loss:  21.34681854248047\n",
      "count_amostra: 6474200\n",
      "iteration:  32400 , total_loss:  21.28062521616618\n",
      "count_amostra: 6480200\n",
      "iteration:  32430 , total_loss:  21.303931617736815\n",
      "count_amostra: 6486200\n",
      "iteration:  32460 , total_loss:  21.359303665161132\n",
      "count_amostra: 6492200\n",
      "iteration:  32490 , total_loss:  21.38287245432536\n",
      "count_amostra: 6498200\n",
      "iteration:  32520 , total_loss:  21.25131066640218\n",
      "count_amostra: 6504200\n",
      "iteration:  32550 , total_loss:  21.311031850179038\n",
      "count_amostra: 6510200\n",
      "iteration:  32580 , total_loss:  21.336181831359863\n",
      "count_amostra: 6516200\n",
      "iteration:  32610 , total_loss:  21.31496461232503\n",
      "count_amostra: 6522200\n",
      "iteration:  32640 , total_loss:  21.315526008605957\n",
      "count_amostra: 6528200\n",
      "iteration:  32670 , total_loss:  21.383688545227052\n",
      "count_amostra: 6534200\n",
      "iteration:  32700 , total_loss:  21.39397144317627\n",
      "count_amostra: 6540200\n",
      "iteration:  32730 , total_loss:  21.3285727818807\n",
      "count_amostra: 6546200\n",
      "iteration:  32760 , total_loss:  21.332790756225585\n",
      "count_amostra: 6552200\n",
      "iteration:  32790 , total_loss:  21.29961191813151\n",
      "count_amostra: 6558200\n",
      "iteration:  32820 , total_loss:  21.251190630594888\n",
      "count_amostra: 6564200\n",
      "iteration:  32850 , total_loss:  21.296179389953615\n",
      "count_amostra: 6570200\n",
      "iteration:  32880 , total_loss:  21.30939547220866\n",
      "count_amostra: 6576200\n",
      "iteration:  32910 , total_loss:  21.341894086201986\n",
      "count_amostra: 6582200\n",
      "iteration:  32940 , total_loss:  21.296898460388185\n",
      "count_amostra: 6588200\n",
      "iteration:  32970 , total_loss:  21.334923426310223\n",
      "count_amostra: 6594200\n",
      "iteration:  33000 , total_loss:  21.261683654785156\n",
      "count_amostra: 6600200\n",
      "iteration:  33030 , total_loss:  21.347740618387856\n",
      "count_amostra: 6606200\n",
      "iteration:  33060 , total_loss:  21.272530301411948\n",
      "count_amostra: 6612200\n",
      "iteration:  33090 , total_loss:  21.279351170857748\n",
      "count_amostra: 6618200\n",
      "iteration:  33120 , total_loss:  21.253010686238607\n",
      "count_amostra: 6624200\n",
      "iteration:  33150 , total_loss:  21.30536060333252\n",
      "count_amostra: 6630200\n",
      "iteration:  33180 , total_loss:  21.410212834676106\n",
      "count_amostra: 6636200\n",
      "iteration:  33210 , total_loss:  21.329647636413576\n",
      "count_amostra: 6642200\n",
      "iteration:  33240 , total_loss:  21.393239974975586\n",
      "count_amostra: 6648200\n",
      "iteration:  33270 , total_loss:  21.316434987386067\n",
      "count_amostra: 6654200\n",
      "iteration:  33300 , total_loss:  21.291776847839355\n",
      "count_amostra: 6660200\n",
      "iteration:  33330 , total_loss:  21.165279833475747\n",
      "count_amostra: 6666200\n",
      "iteration:  33360 , total_loss:  21.332416788736978\n",
      "count_amostra: 6672200\n",
      "iteration:  33390 , total_loss:  21.280676778157552\n",
      "count_amostra: 6678200\n",
      "iteration:  33420 , total_loss:  21.32385794321696\n",
      "count_amostra: 6684200\n",
      "iteration:  33450 , total_loss:  21.42302551269531\n",
      "count_amostra: 6690200\n",
      "iteration:  33480 , total_loss:  21.30426902770996\n",
      "count_amostra: 6696200\n",
      "iteration:  33510 , total_loss:  21.339455223083498\n",
      "count_amostra: 6702200\n",
      "iteration:  33540 , total_loss:  21.18197841644287\n",
      "count_amostra: 6708200\n",
      "iteration:  33570 , total_loss:  21.274332427978514\n",
      "count_amostra: 6714200\n",
      "iteration:  33600 , total_loss:  21.33792355855306\n",
      "count_amostra: 6720200\n",
      "iteration:  33630 , total_loss:  21.245031102498373\n",
      "count_amostra: 6726200\n",
      "iteration:  33660 , total_loss:  21.353501828511558\n",
      "count_amostra: 6732200\n",
      "iteration:  33690 , total_loss:  21.21385383605957\n",
      "count_amostra: 6738200\n",
      "iteration:  33720 , total_loss:  21.201767857869466\n",
      "count_amostra: 6744200\n",
      "iteration:  33750 , total_loss:  21.34221839904785\n",
      "count_amostra: 6750200\n",
      "iteration:  33780 , total_loss:  21.280750783284507\n",
      "count_amostra: 6756200\n",
      "iteration:  33810 , total_loss:  21.285094006856283\n",
      "count_amostra: 6762200\n",
      "iteration:  33840 , total_loss:  21.26327896118164\n",
      "count_amostra: 6768200\n",
      "iteration:  33870 , total_loss:  21.312994130452473\n",
      "count_amostra: 6774200\n",
      "iteration:  33900 , total_loss:  21.350334548950194\n",
      "count_amostra: 6780200\n",
      "iteration:  33930 , total_loss:  21.305671374003094\n",
      "count_amostra: 6786200\n",
      "iteration:  33960 , total_loss:  21.27389456431071\n",
      "count_amostra: 6792200\n",
      "iteration:  33990 , total_loss:  21.21033624013265\n",
      "count_amostra: 6798200\n",
      "iteration:  34020 , total_loss:  21.36818733215332\n",
      "count_amostra: 6804200\n",
      "iteration:  34050 , total_loss:  21.273453330993654\n",
      "count_amostra: 6810200\n",
      "iteration:  34080 , total_loss:  21.26448523203532\n",
      "count_amostra: 6816200\n",
      "iteration:  34110 , total_loss:  21.238445472717284\n",
      "count_amostra: 6822200\n",
      "iteration:  34140 , total_loss:  21.270852851867676\n",
      "count_amostra: 6828200\n",
      "iteration:  34170 , total_loss:  21.262830543518067\n",
      "count_amostra: 6834200\n",
      "iteration:  34200 , total_loss:  21.263458506266275\n",
      "count_amostra: 6840200\n",
      "iteration:  34230 , total_loss:  21.27684097290039\n",
      "count_amostra: 6846200\n",
      "iteration:  34260 , total_loss:  21.320757993062337\n",
      "count_amostra: 6852200\n",
      "iteration:  34290 , total_loss:  21.1991065343221\n",
      "count_amostra: 6858200\n",
      "iteration:  34320 , total_loss:  21.197442690531414\n",
      "count_amostra: 6864200\n",
      "iteration:  34350 , total_loss:  21.27396189371745\n",
      "count_amostra: 6870200\n",
      "iteration:  34380 , total_loss:  21.322950299580892\n",
      "count_amostra: 6876200\n",
      "iteration:  34410 , total_loss:  21.29833958943685\n",
      "count_amostra: 6882200\n",
      "iteration:  34440 , total_loss:  21.190508969624837\n",
      "count_amostra: 6888200\n",
      "iteration:  34470 , total_loss:  21.186130078633628\n",
      "count_amostra: 6894200\n",
      "iteration:  34500 , total_loss:  21.318546358744303\n",
      "count_amostra: 6900200\n",
      "iteration:  34530 , total_loss:  21.335559145609537\n",
      "count_amostra: 6906200\n",
      "iteration:  34560 , total_loss:  21.282141176859536\n",
      "count_amostra: 6912200\n",
      "iteration:  34590 , total_loss:  21.252310371398927\n",
      "count_amostra: 6918200\n",
      "iteration:  34620 , total_loss:  21.271360524495442\n",
      "count_amostra: 6924200\n",
      "iteration:  34650 , total_loss:  21.307441075642902\n",
      "count_amostra: 6930200\n",
      "iteration:  34680 , total_loss:  21.27101256052653\n",
      "count_amostra: 6936200\n",
      "iteration:  34710 , total_loss:  21.30101350148519\n",
      "count_amostra: 6942200\n",
      "iteration:  34740 , total_loss:  21.30902976989746\n",
      "count_amostra: 6948200\n",
      "iteration:  34770 , total_loss:  21.223797289530435\n",
      "count_amostra: 6954200\n",
      "iteration:  34800 , total_loss:  21.276776949564617\n",
      "count_amostra: 6960200\n",
      "iteration:  34830 , total_loss:  21.22513427734375\n",
      "count_amostra: 6966200\n",
      "iteration:  34860 , total_loss:  21.26039021809896\n",
      "count_amostra: 6972200\n",
      "iteration:  34890 , total_loss:  21.197587140401204\n",
      "count_amostra: 6978200\n",
      "iteration:  34920 , total_loss:  21.23786334991455\n",
      "count_amostra: 6984200\n",
      "iteration:  34950 , total_loss:  21.21614513397217\n",
      "count_amostra: 6990200\n",
      "iteration:  34980 , total_loss:  21.18585662841797\n",
      "count_amostra: 6996200\n",
      "iteration:  35010 , total_loss:  21.20705178578695\n",
      "count_amostra: 7002200\n",
      "iteration:  35040 , total_loss:  21.271207491556805\n",
      "count_amostra: 7008200\n",
      "iteration:  35070 , total_loss:  21.276879755655923\n",
      "count_amostra: 7014200\n",
      "iteration:  35100 , total_loss:  21.260571416219076\n",
      "count_amostra: 7020200\n",
      "iteration:  35130 , total_loss:  21.266476440429688\n",
      "count_amostra: 7026200\n",
      "iteration:  35160 , total_loss:  21.169900067647298\n",
      "count_amostra: 7032200\n",
      "iteration:  35190 , total_loss:  21.261112912495932\n",
      "count_amostra: 7038200\n",
      "iteration:  35220 , total_loss:  21.175772349039715\n",
      "count_amostra: 7044200\n",
      "iteration:  35250 , total_loss:  21.254257520039875\n",
      "count_amostra: 7050200\n",
      "iteration:  35280 , total_loss:  21.178703625996906\n",
      "count_amostra: 7056200\n",
      "iteration:  35310 , total_loss:  21.21046371459961\n",
      "count_amostra: 7062200\n",
      "iteration:  35340 , total_loss:  21.20572649637858\n",
      "count_amostra: 7068200\n",
      "iteration:  35370 , total_loss:  21.175605138142902\n",
      "count_amostra: 7074200\n",
      "iteration:  35400 , total_loss:  21.18508815765381\n",
      "count_amostra: 7080200\n",
      "iteration:  35430 , total_loss:  21.250104077657063\n",
      "count_amostra: 7086200\n",
      "iteration:  35460 , total_loss:  21.254332224527996\n",
      "count_amostra: 7092200\n",
      "iteration:  35490 , total_loss:  21.216585286458333\n",
      "count_amostra: 7098200\n",
      "iteration:  35520 , total_loss:  21.177091471354167\n",
      "count_amostra: 7104200\n",
      "iteration:  35550 , total_loss:  21.253793907165527\n",
      "count_amostra: 7110200\n",
      "iteration:  35580 , total_loss:  21.292068862915038\n",
      "count_amostra: 7116200\n",
      "iteration:  35610 , total_loss:  21.1324286142985\n",
      "count_amostra: 7122200\n",
      "iteration:  35640 , total_loss:  21.289423116048177\n",
      "count_amostra: 7128200\n",
      "iteration:  35670 , total_loss:  21.23745714823405\n",
      "count_amostra: 7134200\n",
      "iteration:  35700 , total_loss:  21.180530802408853\n",
      "count_amostra: 7140200\n",
      "iteration:  35730 , total_loss:  21.237576293945313\n",
      "count_amostra: 7146200\n",
      "iteration:  35760 , total_loss:  21.253750864664713\n",
      "count_amostra: 7152200\n",
      "iteration:  35790 , total_loss:  21.176048215230306\n",
      "count_amostra: 7158200\n",
      "iteration:  35820 , total_loss:  21.20847733815511\n",
      "count_amostra: 7164200\n",
      "iteration:  35850 , total_loss:  21.216515858968098\n",
      "count_amostra: 7170200\n",
      "iteration:  35880 , total_loss:  21.24759267171224\n",
      "count_amostra: 7176200\n",
      "iteration:  35910 , total_loss:  21.150726318359375\n",
      "count_amostra: 7182200\n",
      "iteration:  35940 , total_loss:  21.25969149271647\n",
      "count_amostra: 7188200\n",
      "iteration:  35970 , total_loss:  21.116878318786622\n",
      "count_amostra: 7194200\n",
      "iteration:  36000 , total_loss:  21.19369633992513\n",
      "count_amostra: 7200200\n",
      "iteration:  36030 , total_loss:  21.17292175292969\n",
      "count_amostra: 7206200\n",
      "iteration:  36060 , total_loss:  21.209286053975422\n",
      "count_amostra: 7212200\n",
      "iteration:  36090 , total_loss:  21.24655030568441\n",
      "count_amostra: 7218200\n",
      "iteration:  36120 , total_loss:  21.082056109110514\n",
      "count_amostra: 7224200\n",
      "iteration:  36150 , total_loss:  21.204271825154624\n",
      "count_amostra: 7230200\n",
      "iteration:  36180 , total_loss:  21.148822402954103\n",
      "count_amostra: 7236200\n",
      "iteration:  36210 , total_loss:  21.165268834431966\n",
      "count_amostra: 7242200\n",
      "iteration:  36240 , total_loss:  21.204356956481934\n",
      "count_amostra: 7248200\n",
      "iteration:  36270 , total_loss:  21.169423039754232\n",
      "count_amostra: 7254200\n",
      "iteration:  36300 , total_loss:  21.272526423136394\n",
      "count_amostra: 7260200\n",
      "iteration:  36330 , total_loss:  21.17056115468343\n",
      "count_amostra: 7266200\n",
      "iteration:  36360 , total_loss:  21.362350718180338\n",
      "count_amostra: 7272200\n",
      "iteration:  36390 , total_loss:  21.179133033752443\n",
      "count_amostra: 7278200\n",
      "iteration:  36420 , total_loss:  21.202812004089356\n",
      "count_amostra: 7284200\n",
      "iteration:  36450 , total_loss:  21.17441120147705\n",
      "count_amostra: 7290200\n",
      "iteration:  36480 , total_loss:  21.16620814005534\n",
      "count_amostra: 7296200\n",
      "iteration:  36510 , total_loss:  21.169158744812012\n",
      "count_amostra: 7302200\n",
      "iteration:  36540 , total_loss:  21.225624783833823\n",
      "count_amostra: 7308200\n",
      "iteration:  36570 , total_loss:  21.248279317220053\n",
      "count_amostra: 7314200\n",
      "iteration:  36600 , total_loss:  21.18597189585368\n",
      "count_amostra: 7320200\n",
      "iteration:  36630 , total_loss:  21.149182256062826\n",
      "count_amostra: 7326200\n",
      "iteration:  36660 , total_loss:  21.047796376546223\n",
      "count_amostra: 7332200\n",
      "iteration:  36690 , total_loss:  21.17187360127767\n",
      "count_amostra: 7338200\n",
      "iteration:  36720 , total_loss:  21.22025229136149\n",
      "count_amostra: 7344200\n",
      "iteration:  36750 , total_loss:  21.24665444691976\n",
      "count_amostra: 7350200\n",
      "iteration:  36780 , total_loss:  21.121994717915854\n",
      "count_amostra: 7356200\n",
      "iteration:  36810 , total_loss:  21.17108809153239\n",
      "count_amostra: 7362200\n",
      "iteration:  36840 , total_loss:  21.095484733581543\n",
      "count_amostra: 7368200\n",
      "iteration:  36870 , total_loss:  21.223961957295735\n",
      "count_amostra: 7374200\n",
      "iteration:  36900 , total_loss:  21.171464029947916\n",
      "count_amostra: 7380200\n",
      "iteration:  36930 , total_loss:  21.164932187398275\n",
      "count_amostra: 7386200\n",
      "iteration:  36960 , total_loss:  21.22881654103597\n",
      "count_amostra: 7392200\n",
      "iteration:  36990 , total_loss:  21.1308313369751\n",
      "count_amostra: 7398200\n",
      "iteration:  37020 , total_loss:  21.242048835754396\n",
      "count_amostra: 7404200\n",
      "iteration:  37050 , total_loss:  21.15121529897054\n",
      "count_amostra: 7410200\n",
      "iteration:  37080 , total_loss:  21.246693102518716\n",
      "count_amostra: 7416200\n",
      "iteration:  37110 , total_loss:  21.22567145029704\n",
      "count_amostra: 7422200\n",
      "iteration:  37140 , total_loss:  21.18945058186849\n",
      "count_amostra: 7428200\n",
      "iteration:  37170 , total_loss:  21.242577044169106\n",
      "count_amostra: 7434200\n",
      "iteration:  37200 , total_loss:  21.13273194630941\n",
      "count_amostra: 7440200\n",
      "iteration:  37230 , total_loss:  21.190039507548015\n",
      "count_amostra: 7446200\n",
      "iteration:  37260 , total_loss:  21.196668942769367\n",
      "count_amostra: 7452200\n",
      "iteration:  37290 , total_loss:  21.180086135864258\n",
      "count_amostra: 7458200\n",
      "iteration:  37320 , total_loss:  21.164769299825032\n",
      "count_amostra: 7464200\n",
      "iteration:  37350 , total_loss:  21.2051903406779\n",
      "count_amostra: 7470200\n",
      "iteration:  37380 , total_loss:  21.19144477844238\n",
      "count_amostra: 7476200\n",
      "iteration:  37410 , total_loss:  21.153993288675945\n",
      "count_amostra: 7482200\n",
      "iteration:  37440 , total_loss:  21.078756904602052\n",
      "count_amostra: 7488200\n",
      "iteration:  37470 , total_loss:  21.136736424764\n",
      "count_amostra: 7494200\n",
      "iteration:  37500 , total_loss:  21.151128578186036\n",
      "count_amostra: 7500200\n",
      "iteration:  37530 , total_loss:  21.191497866312663\n",
      "count_amostra: 7506200\n",
      "iteration:  37560 , total_loss:  21.201252047220866\n",
      "count_amostra: 7512200\n",
      "iteration:  37590 , total_loss:  21.043584696451823\n",
      "count_amostra: 7518200\n",
      "iteration:  37620 , total_loss:  21.170389493306477\n",
      "count_amostra: 7524200\n",
      "iteration:  37650 , total_loss:  21.13287436167399\n",
      "count_amostra: 7530200\n",
      "iteration:  37680 , total_loss:  21.143491744995117\n",
      "count_amostra: 7536200\n",
      "iteration:  37710 , total_loss:  21.13712304433187\n",
      "count_amostra: 7542200\n",
      "iteration:  37740 , total_loss:  21.2372288386027\n",
      "count_amostra: 7548200\n",
      "iteration:  37770 , total_loss:  21.187637011210125\n",
      "count_amostra: 7554200\n",
      "iteration:  37800 , total_loss:  21.061118761698406\n",
      "count_amostra: 7560200\n",
      "iteration:  37830 , total_loss:  21.175095558166504\n",
      "count_amostra: 7566200\n",
      "iteration:  37860 , total_loss:  21.109262402852377\n",
      "count_amostra: 7572200\n",
      "iteration:  37890 , total_loss:  21.16766948699951\n",
      "count_amostra: 7578200\n",
      "iteration:  37920 , total_loss:  21.1902561823527\n",
      "count_amostra: 7584200\n",
      "iteration:  37950 , total_loss:  21.195798047383626\n",
      "count_amostra: 7590200\n",
      "iteration:  37980 , total_loss:  21.008864402770996\n",
      "count_amostra: 7596200\n",
      "iteration:  38010 , total_loss:  21.103291765848795\n",
      "count_amostra: 7602200\n",
      "iteration:  38040 , total_loss:  21.096081415812176\n",
      "count_amostra: 7608200\n",
      "iteration:  38070 , total_loss:  21.125760714213055\n",
      "count_amostra: 7614200\n",
      "iteration:  38100 , total_loss:  21.114888763427736\n",
      "count_amostra: 7620200\n",
      "iteration:  38130 , total_loss:  21.15384006500244\n",
      "count_amostra: 7626200\n",
      "iteration:  38160 , total_loss:  21.22051099141439\n",
      "count_amostra: 7632200\n",
      "iteration:  38190 , total_loss:  21.19991798400879\n",
      "count_amostra: 7638200\n",
      "iteration:  38220 , total_loss:  21.165062522888185\n",
      "count_amostra: 7644200\n",
      "iteration:  38250 , total_loss:  21.155429077148437\n",
      "count_amostra: 7650200\n",
      "iteration:  38280 , total_loss:  21.075171279907227\n",
      "count_amostra: 7656200\n",
      "iteration:  38310 , total_loss:  21.090749295552573\n",
      "count_amostra: 7662200\n",
      "iteration:  38340 , total_loss:  21.142667770385742\n",
      "count_amostra: 7668200\n",
      "iteration:  38370 , total_loss:  21.14216365814209\n",
      "count_amostra: 7674200\n",
      "iteration:  38400 , total_loss:  21.127930386861166\n",
      "count_amostra: 7680200\n",
      "iteration:  38430 , total_loss:  21.178725051879884\n",
      "count_amostra: 7686200\n",
      "iteration:  38460 , total_loss:  21.087853622436523\n",
      "count_amostra: 7692200\n",
      "iteration:  38490 , total_loss:  21.119249788920083\n",
      "count_amostra: 7698200\n",
      "iteration:  38520 , total_loss:  21.165602429707846\n",
      "count_amostra: 7704200\n",
      "iteration:  38550 , total_loss:  21.243116569519042\n",
      "count_amostra: 7710200\n",
      "iteration:  38580 , total_loss:  21.153685506184896\n",
      "count_amostra: 7716200\n",
      "iteration:  38610 , total_loss:  21.110279719034832\n",
      "count_amostra: 7722200\n",
      "iteration:  38640 , total_loss:  21.147432963053387\n",
      "count_amostra: 7728200\n",
      "iteration:  38670 , total_loss:  21.066637992858887\n",
      "count_amostra: 7734200\n",
      "iteration:  38700 , total_loss:  21.099547894795737\n",
      "count_amostra: 7740200\n",
      "iteration:  38730 , total_loss:  21.146279780069985\n",
      "count_amostra: 7746200\n",
      "iteration:  38760 , total_loss:  21.094095993041993\n",
      "count_amostra: 7752200\n",
      "iteration:  38790 , total_loss:  21.12168509165446\n",
      "count_amostra: 7758200\n",
      "iteration:  38820 , total_loss:  21.149298731486002\n",
      "count_amostra: 7764200\n",
      "iteration:  38850 , total_loss:  21.120802307128905\n",
      "count_amostra: 7770200\n",
      "iteration:  38880 , total_loss:  21.07963981628418\n",
      "count_amostra: 7776200\n",
      "iteration:  38910 , total_loss:  21.132636515299478\n",
      "count_amostra: 7782200\n",
      "iteration:  38940 , total_loss:  21.066739082336426\n",
      "count_amostra: 7788200\n",
      "iteration:  38970 , total_loss:  21.14369926452637\n",
      "count_amostra: 7794200\n",
      "iteration:  39000 , total_loss:  21.177781613667808\n",
      "count_amostra: 7800200\n",
      "iteration:  39030 , total_loss:  21.12229913075765\n",
      "count_amostra: 7806200\n",
      "iteration:  39060 , total_loss:  21.225468508402507\n",
      "count_amostra: 7812200\n",
      "iteration:  39090 , total_loss:  21.062360827128092\n",
      "count_amostra: 7818200\n",
      "iteration:  39120 , total_loss:  21.032794952392578\n",
      "count_amostra: 7824200\n",
      "iteration:  39150 , total_loss:  21.16039422353109\n",
      "count_amostra: 7830200\n",
      "iteration:  39180 , total_loss:  21.167570622762046\n",
      "count_amostra: 7836200\n",
      "iteration:  39210 , total_loss:  21.100131225585937\n",
      "count_amostra: 7842200\n",
      "iteration:  39240 , total_loss:  21.081240717569987\n",
      "count_amostra: 7848200\n",
      "iteration:  39270 , total_loss:  21.12084058125814\n",
      "count_amostra: 7854200\n",
      "iteration:  39300 , total_loss:  21.152084159851075\n",
      "count_amostra: 7860200\n",
      "iteration:  39330 , total_loss:  21.16463432312012\n",
      "count_amostra: 7866200\n",
      "iteration:  39360 , total_loss:  21.162838808695476\n",
      "count_amostra: 7872200\n",
      "iteration:  39390 , total_loss:  21.12545026143392\n",
      "count_amostra: 7878200\n",
      "iteration:  39420 , total_loss:  21.072073872884115\n",
      "count_amostra: 7884200\n",
      "iteration:  39450 , total_loss:  21.08060251871745\n",
      "count_amostra: 7890200\n",
      "iteration:  39480 , total_loss:  21.102886772155763\n",
      "count_amostra: 7896200\n",
      "iteration:  39510 , total_loss:  21.124163754781087\n",
      "count_amostra: 7902200\n",
      "iteration:  39540 , total_loss:  21.073193232218426\n",
      "count_amostra: 7908200\n",
      "iteration:  39570 , total_loss:  21.14279753367106\n",
      "count_amostra: 7914200\n",
      "iteration:  39600 , total_loss:  21.098299280802408\n",
      "count_amostra: 7920200\n",
      "iteration:  39630 , total_loss:  21.132955996195474\n",
      "count_amostra: 7926200\n",
      "iteration:  39660 , total_loss:  21.06011848449707\n",
      "count_amostra: 7932200\n",
      "iteration:  39690 , total_loss:  21.112163925170897\n",
      "count_amostra: 7938200\n",
      "iteration:  39720 , total_loss:  21.059445826212563\n",
      "count_amostra: 7944200\n",
      "iteration:  39750 , total_loss:  21.030340894063315\n",
      "count_amostra: 7950200\n",
      "iteration:  39780 , total_loss:  21.19705359141032\n",
      "count_amostra: 7956200\n",
      "iteration:  39810 , total_loss:  21.0166987101237\n",
      "count_amostra: 7962200\n",
      "iteration:  39840 , total_loss:  21.068518257141115\n",
      "count_amostra: 7968200\n",
      "iteration:  39870 , total_loss:  21.102468554178873\n",
      "count_amostra: 7974200\n",
      "iteration:  39900 , total_loss:  21.049675941467285\n",
      "count_amostra: 7980200\n",
      "iteration:  39930 , total_loss:  21.091231791178384\n",
      "count_amostra: 7986200\n",
      "iteration:  39960 , total_loss:  21.16201540629069\n",
      "count_amostra: 7992200\n",
      "iteration:  39990 , total_loss:  21.087738227844238\n",
      "count_amostra: 7998200\n",
      "iteration:  40020 , total_loss:  21.127820014953613\n",
      "count_amostra: 8004200\n",
      "iteration:  40050 , total_loss:  21.097745196024576\n",
      "count_amostra: 8010200\n",
      "iteration:  40080 , total_loss:  21.01365432739258\n",
      "count_amostra: 8016200\n",
      "iteration:  40110 , total_loss:  21.07464714050293\n",
      "count_amostra: 8022200\n",
      "iteration:  40140 , total_loss:  21.065280278523762\n",
      "count_amostra: 8028200\n",
      "iteration:  40170 , total_loss:  21.011118952433268\n",
      "count_amostra: 8034200\n",
      "iteration:  40200 , total_loss:  21.094943300882974\n",
      "count_amostra: 8040200\n",
      "iteration:  40230 , total_loss:  20.992666308085123\n",
      "count_amostra: 8046200\n",
      "iteration:  40260 , total_loss:  21.06951649983724\n",
      "count_amostra: 8052200\n",
      "iteration:  40290 , total_loss:  21.116250610351564\n",
      "count_amostra: 8058200\n",
      "iteration:  40320 , total_loss:  21.0888245900472\n",
      "count_amostra: 8064200\n",
      "iteration:  40350 , total_loss:  21.042006047566733\n",
      "count_amostra: 8070200\n",
      "iteration:  40380 , total_loss:  20.973029136657715\n",
      "count_amostra: 8076200\n",
      "iteration:  40410 , total_loss:  20.98582820892334\n",
      "count_amostra: 8082200\n",
      "iteration:  40440 , total_loss:  21.047217241923015\n",
      "count_amostra: 8088200\n",
      "iteration:  40470 , total_loss:  21.178131167093913\n",
      "count_amostra: 8094200\n",
      "iteration:  40500 , total_loss:  21.086225954691567\n",
      "count_amostra: 8100200\n",
      "iteration:  40530 , total_loss:  21.113652737935386\n",
      "count_amostra: 8106200\n",
      "iteration:  40560 , total_loss:  21.060661188761394\n",
      "count_amostra: 8112200\n",
      "iteration:  40590 , total_loss:  21.09775447845459\n",
      "count_amostra: 8118200\n",
      "iteration:  40620 , total_loss:  21.089039039611816\n",
      "count_amostra: 8124200\n",
      "iteration:  40650 , total_loss:  21.049184354146323\n",
      "count_amostra: 8130200\n",
      "iteration:  40680 , total_loss:  21.12131144205729\n",
      "count_amostra: 8136200\n",
      "iteration:  40710 , total_loss:  20.976656595865887\n",
      "count_amostra: 8142200\n",
      "iteration:  40740 , total_loss:  21.06384703318278\n",
      "count_amostra: 8148200\n",
      "iteration:  40770 , total_loss:  21.072613271077476\n",
      "count_amostra: 8154200\n",
      "iteration:  40800 , total_loss:  21.084441820780437\n",
      "count_amostra: 8160200\n",
      "iteration:  40830 , total_loss:  21.077651977539062\n",
      "count_amostra: 8166200\n",
      "iteration:  40860 , total_loss:  21.082025146484376\n",
      "count_amostra: 8172200\n",
      "iteration:  40890 , total_loss:  21.021759859720866\n",
      "count_amostra: 8178200\n",
      "iteration:  40920 , total_loss:  21.189526494344076\n",
      "count_amostra: 8184200\n",
      "iteration:  40950 , total_loss:  21.083838844299315\n",
      "count_amostra: 8190200\n",
      "iteration:  40980 , total_loss:  21.089006678263345\n",
      "count_amostra: 8196200\n",
      "iteration:  41010 , total_loss:  21.098473993937173\n",
      "count_amostra: 8202200\n",
      "iteration:  41040 , total_loss:  21.12270189921061\n",
      "count_amostra: 8208200\n",
      "iteration:  41070 , total_loss:  21.11193962097168\n",
      "count_amostra: 8214200\n",
      "iteration:  41100 , total_loss:  21.056441243489584\n",
      "count_amostra: 8220200\n",
      "iteration:  41130 , total_loss:  21.16288693745931\n",
      "count_amostra: 8226200\n",
      "iteration:  41160 , total_loss:  21.081107521057127\n",
      "count_amostra: 8232200\n",
      "iteration:  41190 , total_loss:  20.998837153116863\n",
      "count_amostra: 8238200\n",
      "iteration:  41220 , total_loss:  21.14180145263672\n",
      "count_amostra: 8244200\n",
      "iteration:  41250 , total_loss:  21.04874299367269\n",
      "count_amostra: 8250200\n",
      "iteration:  41280 , total_loss:  21.12093143463135\n",
      "count_amostra: 8256200\n",
      "iteration:  41310 , total_loss:  21.095863405863444\n",
      "count_amostra: 8262200\n",
      "iteration:  41340 , total_loss:  21.0097900390625\n",
      "count_amostra: 8268200\n",
      "iteration:  41370 , total_loss:  20.96275215148926\n",
      "count_amostra: 8274200\n",
      "iteration:  41400 , total_loss:  20.99395662943522\n",
      "count_amostra: 8280200\n",
      "iteration:  41430 , total_loss:  21.021611595153807\n",
      "count_amostra: 8286200\n",
      "iteration:  41460 , total_loss:  21.05933723449707\n",
      "count_amostra: 8292200\n",
      "iteration:  41490 , total_loss:  21.055143038431805\n",
      "count_amostra: 8298200\n",
      "iteration:  41520 , total_loss:  21.13446242014567\n",
      "count_amostra: 8304200\n",
      "iteration:  41550 , total_loss:  20.981794611612955\n",
      "count_amostra: 8310200\n",
      "iteration:  41580 , total_loss:  21.05198942820231\n",
      "count_amostra: 8316200\n",
      "iteration:  41610 , total_loss:  20.997976112365723\n",
      "count_amostra: 8322200\n",
      "iteration:  41640 , total_loss:  21.060040855407713\n",
      "count_amostra: 8328200\n",
      "iteration:  41670 , total_loss:  20.972126515706382\n",
      "count_amostra: 8334200\n",
      "iteration:  41700 , total_loss:  20.994827397664388\n",
      "count_amostra: 8340200\n",
      "iteration:  41730 , total_loss:  21.100623194376627\n",
      "count_amostra: 8346200\n",
      "iteration:  41760 , total_loss:  21.06036885579427\n",
      "count_amostra: 8352200\n",
      "iteration:  41790 , total_loss:  20.996294021606445\n",
      "count_amostra: 8358200\n",
      "iteration:  41820 , total_loss:  20.931130154927573\n",
      "count_amostra: 8364200\n",
      "iteration:  41850 , total_loss:  20.942718060811362\n",
      "count_amostra: 8370200\n",
      "iteration:  41880 , total_loss:  21.13564478556315\n",
      "count_amostra: 8376200\n",
      "iteration:  41910 , total_loss:  21.083698336283366\n",
      "count_amostra: 8382200\n",
      "iteration:  41940 , total_loss:  21.001173528035483\n",
      "count_amostra: 8388200\n",
      "iteration:  41970 , total_loss:  21.01736183166504\n",
      "count_amostra: 8394200\n",
      "iteration:  42000 , total_loss:  21.160353088378905\n",
      "count_amostra: 8400200\n",
      "iteration:  42030 , total_loss:  21.03211390177409\n",
      "count_amostra: 8406200\n",
      "iteration:  42060 , total_loss:  20.90365193684896\n",
      "count_amostra: 8412200\n",
      "iteration:  42090 , total_loss:  20.967168680826823\n",
      "count_amostra: 8418200\n",
      "iteration:  42120 , total_loss:  21.068492317199706\n",
      "count_amostra: 8424200\n",
      "iteration:  42150 , total_loss:  21.009696388244627\n",
      "count_amostra: 8430200\n",
      "iteration:  42180 , total_loss:  21.058413060506187\n",
      "count_amostra: 8436200\n",
      "iteration:  42210 , total_loss:  20.981772232055665\n",
      "count_amostra: 8442200\n",
      "iteration:  42240 , total_loss:  21.070581436157227\n",
      "count_amostra: 8448200\n",
      "iteration:  42270 , total_loss:  21.02255853017171\n",
      "count_amostra: 8454200\n",
      "iteration:  42300 , total_loss:  21.03312307993571\n",
      "count_amostra: 8460200\n",
      "iteration:  42330 , total_loss:  20.841877746582032\n",
      "count_amostra: 8466200\n",
      "iteration:  42360 , total_loss:  21.032376098632813\n",
      "count_amostra: 8472200\n",
      "iteration:  42390 , total_loss:  20.995789273579916\n",
      "count_amostra: 8478200\n",
      "iteration:  42420 , total_loss:  21.008859062194823\n",
      "count_amostra: 8484200\n",
      "iteration:  42450 , total_loss:  21.023394203186037\n",
      "count_amostra: 8490200\n",
      "iteration:  42480 , total_loss:  20.94075870513916\n",
      "count_amostra: 8496200\n",
      "iteration:  42510 , total_loss:  21.063557561238607\n",
      "count_amostra: 8502200\n",
      "iteration:  42540 , total_loss:  21.05642286936442\n",
      "count_amostra: 8508200\n",
      "iteration:  42570 , total_loss:  20.982042121887208\n",
      "count_amostra: 8514200\n",
      "iteration:  42600 , total_loss:  21.101970926920572\n",
      "count_amostra: 8520200\n",
      "iteration:  42630 , total_loss:  20.91858590443929\n",
      "count_amostra: 8526200\n",
      "iteration:  42660 , total_loss:  21.022465070088703\n",
      "count_amostra: 8532200\n",
      "iteration:  42690 , total_loss:  21.030684852600096\n",
      "count_amostra: 8538200\n",
      "iteration:  42720 , total_loss:  21.0072883605957\n",
      "count_amostra: 8544200\n",
      "iteration:  42750 , total_loss:  20.93331502278646\n",
      "count_amostra: 8550200\n",
      "iteration:  42780 , total_loss:  20.961985905965168\n",
      "count_amostra: 8556200\n",
      "iteration:  42810 , total_loss:  21.11189359029134\n",
      "count_amostra: 8562200\n",
      "iteration:  42840 , total_loss:  21.060219510396323\n",
      "count_amostra: 8568200\n",
      "iteration:  42870 , total_loss:  21.084260495503745\n",
      "count_amostra: 8574200\n",
      "iteration:  42900 , total_loss:  21.04758898417155\n",
      "count_amostra: 8580200\n",
      "iteration:  42930 , total_loss:  21.03754889170329\n",
      "count_amostra: 8586200\n",
      "iteration:  42960 , total_loss:  20.947130711873374\n",
      "count_amostra: 8592200\n",
      "iteration:  42990 , total_loss:  21.004774475097655\n",
      "count_amostra: 8598200\n",
      "iteration:  43020 , total_loss:  20.935795720418295\n",
      "count_amostra: 8604200\n",
      "iteration:  43050 , total_loss:  21.08508497873942\n",
      "count_amostra: 8610200\n",
      "iteration:  43080 , total_loss:  20.974343299865723\n",
      "count_amostra: 8616200\n",
      "iteration:  43110 , total_loss:  20.946356519063315\n",
      "count_amostra: 8622200\n",
      "iteration:  43140 , total_loss:  21.0978183110555\n",
      "count_amostra: 8628200\n",
      "iteration:  43170 , total_loss:  21.00394763946533\n",
      "count_amostra: 8634200\n",
      "iteration:  43200 , total_loss:  20.952146784464517\n",
      "count_amostra: 8640200\n",
      "iteration:  43230 , total_loss:  21.001336924235027\n",
      "count_amostra: 8646200\n",
      "iteration:  43260 , total_loss:  20.97714023590088\n",
      "count_amostra: 8652200\n",
      "iteration:  43290 , total_loss:  20.921086438496907\n",
      "count_amostra: 8658200\n",
      "iteration:  43320 , total_loss:  21.075763829549153\n",
      "count_amostra: 8664200\n",
      "iteration:  43350 , total_loss:  20.923589579264323\n",
      "count_amostra: 8670200\n",
      "iteration:  43380 , total_loss:  21.04715035756429\n",
      "count_amostra: 8676200\n",
      "iteration:  43410 , total_loss:  21.0655003229777\n",
      "count_amostra: 8682200\n",
      "iteration:  43440 , total_loss:  20.96650110880534\n",
      "count_amostra: 8688200\n",
      "iteration:  43470 , total_loss:  21.058077494303387\n",
      "count_amostra: 8694200\n",
      "iteration:  43500 , total_loss:  20.9737340927124\n",
      "count_amostra: 8700200\n",
      "iteration:  43530 , total_loss:  21.01580670674642\n",
      "count_amostra: 8706200\n",
      "iteration:  43560 , total_loss:  21.011945088704426\n",
      "count_amostra: 8712200\n",
      "iteration:  43590 , total_loss:  20.923106384277343\n",
      "count_amostra: 8718200\n",
      "iteration:  43620 , total_loss:  20.95487054189046\n",
      "count_amostra: 8724200\n",
      "iteration:  43650 , total_loss:  20.91082852681478\n",
      "count_amostra: 8730200\n",
      "iteration:  43680 , total_loss:  20.97271010080973\n",
      "count_amostra: 8736200\n",
      "iteration:  43710 , total_loss:  20.997528330485025\n",
      "count_amostra: 8742200\n",
      "iteration:  43740 , total_loss:  20.962213007609048\n",
      "count_amostra: 8748200\n",
      "iteration:  43770 , total_loss:  21.02759749094645\n",
      "count_amostra: 8754200\n",
      "iteration:  43800 , total_loss:  20.97702579498291\n",
      "count_amostra: 8760200\n",
      "iteration:  43830 , total_loss:  21.05459499359131\n",
      "count_amostra: 8766200\n",
      "iteration:  43860 , total_loss:  20.98620662689209\n",
      "count_amostra: 8772200\n",
      "iteration:  43890 , total_loss:  20.926242764790853\n",
      "count_amostra: 8778200\n",
      "iteration:  43920 , total_loss:  20.911401748657227\n",
      "count_amostra: 8784200\n",
      "iteration:  43950 , total_loss:  20.985889943440757\n",
      "count_amostra: 8790200\n",
      "iteration:  43980 , total_loss:  20.93886121114095\n",
      "count_amostra: 8796200\n",
      "iteration:  44010 , total_loss:  20.95491142272949\n",
      "count_amostra: 8802200\n",
      "iteration:  44040 , total_loss:  21.003238995869953\n",
      "count_amostra: 8808200\n",
      "iteration:  44070 , total_loss:  21.02533391316732\n",
      "count_amostra: 8814200\n",
      "iteration:  44100 , total_loss:  20.901838302612305\n",
      "count_amostra: 8820200\n",
      "iteration:  44130 , total_loss:  20.968515841166177\n",
      "count_amostra: 8826200\n",
      "iteration:  44160 , total_loss:  20.980059560139974\n",
      "count_amostra: 8832200\n",
      "iteration:  44190 , total_loss:  21.076760864257814\n",
      "count_amostra: 8838200\n",
      "iteration:  44220 , total_loss:  20.965560468037925\n",
      "count_amostra: 8844200\n",
      "iteration:  44250 , total_loss:  20.984375127156575\n",
      "count_amostra: 8850200\n",
      "iteration:  44280 , total_loss:  20.969326527913413\n",
      "count_amostra: 8856200\n",
      "iteration:  44310 , total_loss:  20.898056729634604\n",
      "count_amostra: 8862200\n",
      "iteration:  44340 , total_loss:  20.986603037516275\n",
      "count_amostra: 8868200\n",
      "iteration:  44370 , total_loss:  21.0147523244222\n",
      "count_amostra: 8874200\n",
      "iteration:  44400 , total_loss:  20.978768666585285\n",
      "count_amostra: 8880200\n",
      "iteration:  44430 , total_loss:  20.919335492451985\n",
      "count_amostra: 8886200\n",
      "iteration:  44460 , total_loss:  20.97285639444987\n",
      "count_amostra: 8892200\n",
      "iteration:  44490 , total_loss:  20.86026293436686\n",
      "count_amostra: 8898200\n",
      "iteration:  44520 , total_loss:  20.954040145874025\n",
      "count_amostra: 8904200\n",
      "iteration:  44550 , total_loss:  20.978957176208496\n",
      "count_amostra: 8910200\n",
      "iteration:  44580 , total_loss:  20.97028554280599\n",
      "count_amostra: 8916200\n",
      "iteration:  44610 , total_loss:  21.01250171661377\n",
      "count_amostra: 8922200\n",
      "iteration:  44640 , total_loss:  20.929845492045086\n",
      "count_amostra: 8928200\n",
      "iteration:  44670 , total_loss:  20.925711631774902\n",
      "count_amostra: 8934200\n",
      "iteration:  44700 , total_loss:  21.116848754882813\n",
      "count_amostra: 8940200\n",
      "iteration:  44730 , total_loss:  20.957226943969726\n",
      "count_amostra: 8946200\n",
      "iteration:  44760 , total_loss:  21.008564376831053\n",
      "count_amostra: 8952200\n",
      "iteration:  44790 , total_loss:  20.899020195007324\n",
      "count_amostra: 8958200\n",
      "iteration:  44820 , total_loss:  20.959789530436197\n",
      "count_amostra: 8964200\n",
      "iteration:  44850 , total_loss:  20.917159652709962\n",
      "count_amostra: 8970200\n",
      "iteration:  44880 , total_loss:  20.980377769470216\n",
      "count_amostra: 8976200\n",
      "iteration:  44910 , total_loss:  20.90570526123047\n",
      "count_amostra: 8982200\n",
      "iteration:  44940 , total_loss:  20.925168418884276\n",
      "count_amostra: 8988200\n",
      "iteration:  44970 , total_loss:  20.859978485107423\n",
      "count_amostra: 8994200\n",
      "iteration:  45000 , total_loss:  20.948535792032878\n",
      "count_amostra: 9000200\n",
      "iteration:  45030 , total_loss:  20.994027773539226\n",
      "count_amostra: 9006200\n",
      "iteration:  45060 , total_loss:  21.0005308787028\n",
      "count_amostra: 9012200\n",
      "iteration:  45090 , total_loss:  21.046486790974935\n",
      "count_amostra: 9018200\n",
      "iteration:  45120 , total_loss:  20.96766929626465\n",
      "count_amostra: 9024200\n",
      "iteration:  45150 , total_loss:  20.999839083353677\n",
      "count_amostra: 9030200\n",
      "iteration:  45180 , total_loss:  21.0141575495402\n",
      "count_amostra: 9036200\n",
      "iteration:  45210 , total_loss:  20.98346684773763\n",
      "count_amostra: 9042200\n",
      "iteration:  45240 , total_loss:  21.020907719930012\n",
      "count_amostra: 9048200\n",
      "iteration:  45270 , total_loss:  20.994238471984865\n",
      "count_amostra: 9054200\n",
      "iteration:  45300 , total_loss:  20.920647048950194\n",
      "count_amostra: 9060200\n",
      "iteration:  45330 , total_loss:  20.923458925882976\n",
      "count_amostra: 9066200\n",
      "iteration:  45360 , total_loss:  20.913036155700684\n",
      "count_amostra: 9072200\n",
      "iteration:  45390 , total_loss:  20.931917826334637\n",
      "count_amostra: 9078200\n",
      "iteration:  45420 , total_loss:  20.957578086853026\n",
      "count_amostra: 9084200\n",
      "iteration:  45450 , total_loss:  20.9342498143514\n",
      "count_amostra: 9090200\n",
      "iteration:  45480 , total_loss:  20.915112749735513\n",
      "count_amostra: 9096200\n",
      "iteration:  45510 , total_loss:  20.82396500905355\n",
      "count_amostra: 9102200\n",
      "iteration:  45540 , total_loss:  20.94046548207601\n",
      "count_amostra: 9108200\n",
      "iteration:  45570 , total_loss:  20.989686266581217\n",
      "count_amostra: 9114200\n",
      "iteration:  45600 , total_loss:  20.93790651957194\n",
      "count_amostra: 9120200\n",
      "iteration:  45630 , total_loss:  20.99889087677002\n",
      "count_amostra: 9126200\n",
      "iteration:  45660 , total_loss:  20.959767850240073\n",
      "count_amostra: 9132200\n",
      "iteration:  45690 , total_loss:  21.0132261912028\n",
      "count_amostra: 9138200\n",
      "iteration:  45720 , total_loss:  20.93486932118734\n",
      "count_amostra: 9144200\n",
      "iteration:  45750 , total_loss:  20.87434590657552\n",
      "count_amostra: 9150200\n",
      "iteration:  45780 , total_loss:  20.92828934987386\n",
      "count_amostra: 9156200\n",
      "iteration:  45810 , total_loss:  20.94065221150716\n",
      "count_amostra: 9162200\n",
      "iteration:  45840 , total_loss:  20.931435839335123\n",
      "count_amostra: 9168200\n",
      "iteration:  45870 , total_loss:  20.896799723307293\n",
      "count_amostra: 9174200\n",
      "iteration:  45900 , total_loss:  21.057171440124513\n",
      "count_amostra: 9180200\n",
      "iteration:  45930 , total_loss:  20.86084601084391\n",
      "count_amostra: 9186200\n",
      "iteration:  45960 , total_loss:  20.959470558166505\n",
      "count_amostra: 9192200\n",
      "iteration:  45990 , total_loss:  20.95728391011556\n",
      "count_amostra: 9198200\n",
      "iteration:  46020 , total_loss:  20.90628236134847\n",
      "count_amostra: 9204200\n",
      "iteration:  46050 , total_loss:  20.926275062561036\n",
      "count_amostra: 9210200\n",
      "iteration:  46080 , total_loss:  20.919587580362954\n",
      "count_amostra: 9216200\n",
      "iteration:  46110 , total_loss:  20.914822260538738\n",
      "count_amostra: 9222200\n",
      "iteration:  46140 , total_loss:  21.03005091349284\n",
      "count_amostra: 9228200\n",
      "iteration:  46170 , total_loss:  20.873458989461263\n",
      "count_amostra: 9234200\n",
      "iteration:  46200 , total_loss:  20.970827229817708\n",
      "count_amostra: 9240200\n",
      "iteration:  46230 , total_loss:  21.037335840861\n",
      "count_amostra: 9246200\n",
      "iteration:  46260 , total_loss:  20.904340998331705\n",
      "count_amostra: 9252200\n",
      "iteration:  46290 , total_loss:  20.960690625508626\n",
      "count_amostra: 9258200\n",
      "iteration:  46320 , total_loss:  20.846197509765624\n",
      "count_amostra: 9264200\n",
      "iteration:  46350 , total_loss:  20.942134030659993\n",
      "count_amostra: 9270200\n",
      "iteration:  46380 , total_loss:  20.94362602233887\n",
      "count_amostra: 9276200\n",
      "iteration:  46410 , total_loss:  20.795724868774414\n",
      "count_amostra: 9282200\n",
      "iteration:  46440 , total_loss:  20.919051806132\n",
      "count_amostra: 9288200\n",
      "iteration:  46470 , total_loss:  20.937709871927897\n",
      "count_amostra: 9294200\n",
      "iteration:  46500 , total_loss:  20.88554515838623\n",
      "count_amostra: 9300200\n",
      "iteration:  46530 , total_loss:  21.01338596343994\n",
      "count_amostra: 9306200\n",
      "iteration:  46560 , total_loss:  20.90799929300944\n",
      "count_amostra: 9312200\n",
      "iteration:  46590 , total_loss:  20.937850125630696\n",
      "count_amostra: 9318200\n",
      "iteration:  46620 , total_loss:  20.86101614634196\n",
      "count_amostra: 9324200\n",
      "iteration:  46650 , total_loss:  20.930789883931478\n",
      "count_amostra: 9330200\n",
      "iteration:  46680 , total_loss:  20.8466495513916\n",
      "count_amostra: 9336200\n",
      "iteration:  46710 , total_loss:  20.919009717305503\n",
      "count_amostra: 9342200\n",
      "iteration:  46740 , total_loss:  20.891193008422853\n",
      "count_amostra: 9348200\n",
      "iteration:  46770 , total_loss:  20.96381924947103\n",
      "count_amostra: 9354200\n",
      "iteration:  46800 , total_loss:  20.90981585184733\n",
      "count_amostra: 9360200\n",
      "iteration:  46830 , total_loss:  20.978973642985025\n",
      "count_amostra: 9366200\n",
      "iteration:  46860 , total_loss:  20.977563095092773\n",
      "count_amostra: 9372200\n",
      "iteration:  46890 , total_loss:  20.88015995025635\n",
      "count_amostra: 9378200\n",
      "iteration:  46920 , total_loss:  21.00029799143473\n",
      "count_amostra: 9384200\n",
      "iteration:  46950 , total_loss:  20.921936162312825\n",
      "count_amostra: 9390200\n",
      "iteration:  46980 , total_loss:  20.92066148122152\n",
      "count_amostra: 9396200\n",
      "iteration:  47010 , total_loss:  20.933886528015137\n",
      "count_amostra: 9402200\n",
      "iteration:  47040 , total_loss:  20.9372132619222\n",
      "count_amostra: 9408200\n",
      "iteration:  47070 , total_loss:  20.900535774230956\n",
      "count_amostra: 9414200\n",
      "iteration:  47100 , total_loss:  20.879364585876466\n",
      "count_amostra: 9420200\n",
      "iteration:  47130 , total_loss:  20.856105422973634\n",
      "count_amostra: 9426200\n",
      "iteration:  47160 , total_loss:  20.915286827087403\n",
      "count_amostra: 9432200\n",
      "iteration:  47190 , total_loss:  20.800684293111164\n",
      "count_amostra: 9438200\n",
      "iteration:  47220 , total_loss:  20.95145829518636\n",
      "count_amostra: 9444200\n",
      "iteration:  47250 , total_loss:  20.905438486735026\n",
      "count_amostra: 9450200\n",
      "iteration:  47280 , total_loss:  20.88763147989909\n",
      "count_amostra: 9456200\n",
      "iteration:  47310 , total_loss:  20.89727675120036\n",
      "count_amostra: 9462200\n",
      "iteration:  47340 , total_loss:  20.898821067810058\n",
      "count_amostra: 9468200\n",
      "iteration:  47370 , total_loss:  20.925403594970703\n",
      "count_amostra: 9474200\n",
      "iteration:  47400 , total_loss:  20.849301211039226\n",
      "count_amostra: 9480200\n",
      "iteration:  47430 , total_loss:  20.91468874613444\n",
      "count_amostra: 9486200\n",
      "iteration:  47460 , total_loss:  20.854174423217774\n",
      "count_amostra: 9492200\n",
      "iteration:  47490 , total_loss:  20.983971786499023\n",
      "count_amostra: 9498200\n",
      "iteration:  47520 , total_loss:  20.882561620076498\n",
      "count_amostra: 9504200\n",
      "iteration:  47550 , total_loss:  20.956394004821778\n",
      "count_amostra: 9510200\n",
      "iteration:  47580 , total_loss:  20.929059092203776\n",
      "count_amostra: 9516200\n",
      "iteration:  47610 , total_loss:  20.994513448079427\n",
      "count_amostra: 9522200\n",
      "iteration:  47640 , total_loss:  20.866057713826496\n",
      "count_amostra: 9528200\n",
      "iteration:  47670 , total_loss:  20.931231371561687\n",
      "count_amostra: 9534200\n",
      "iteration:  47700 , total_loss:  20.93781706492106\n",
      "count_amostra: 9540200\n",
      "iteration:  47730 , total_loss:  20.871598879496258\n",
      "count_amostra: 9546200\n",
      "iteration:  47760 , total_loss:  20.85540199279785\n",
      "count_amostra: 9552200\n",
      "iteration:  47790 , total_loss:  20.902017974853514\n",
      "count_amostra: 9558200\n",
      "iteration:  47820 , total_loss:  20.965701866149903\n",
      "count_amostra: 9564200\n",
      "iteration:  47850 , total_loss:  20.91268113454183\n",
      "count_amostra: 9570200\n",
      "iteration:  47880 , total_loss:  20.88360850016276\n",
      "count_amostra: 9576200\n",
      "iteration:  47910 , total_loss:  20.90959924062093\n",
      "count_amostra: 9582200\n",
      "iteration:  47940 , total_loss:  20.850830459594725\n",
      "count_amostra: 9588200\n",
      "iteration:  47970 , total_loss:  20.892304039001466\n",
      "count_amostra: 9594200\n",
      "iteration:  48000 , total_loss:  20.915372212727863\n",
      "count_amostra: 9600200\n",
      "iteration:  48030 , total_loss:  20.92878952026367\n",
      "count_amostra: 9606200\n",
      "iteration:  48060 , total_loss:  20.863642120361327\n",
      "count_amostra: 9612200\n",
      "iteration:  48090 , total_loss:  20.93421827952067\n",
      "count_amostra: 9618200\n",
      "iteration:  48120 , total_loss:  20.907387415568035\n",
      "count_amostra: 9624200\n",
      "iteration:  48150 , total_loss:  20.919664891560874\n",
      "count_amostra: 9630200\n",
      "iteration:  48180 , total_loss:  20.759208552042644\n",
      "count_amostra: 9636200\n",
      "iteration:  48210 , total_loss:  20.850564829508464\n",
      "count_amostra: 9642200\n",
      "iteration:  48240 , total_loss:  20.86378707885742\n",
      "count_amostra: 9648200\n",
      "iteration:  48270 , total_loss:  20.82931480407715\n",
      "count_amostra: 9654200\n",
      "iteration:  48300 , total_loss:  20.812351735432944\n",
      "count_amostra: 9660200\n",
      "iteration:  48330 , total_loss:  20.965467580159505\n",
      "count_amostra: 9666200\n",
      "iteration:  48360 , total_loss:  20.847078450520833\n",
      "count_amostra: 9672200\n",
      "iteration:  48390 , total_loss:  20.90298709869385\n",
      "count_amostra: 9678200\n",
      "iteration:  48420 , total_loss:  20.864779980977378\n",
      "count_amostra: 9684200\n",
      "iteration:  48450 , total_loss:  20.874886131286623\n",
      "count_amostra: 9690200\n",
      "iteration:  48480 , total_loss:  20.855132802327475\n",
      "count_amostra: 9696200\n",
      "iteration:  48510 , total_loss:  20.878838793436685\n",
      "count_amostra: 9702200\n",
      "iteration:  48540 , total_loss:  20.846159426371255\n",
      "count_amostra: 9708200\n",
      "iteration:  48570 , total_loss:  20.896767489115398\n",
      "count_amostra: 9714200\n",
      "iteration:  48600 , total_loss:  20.87546043395996\n",
      "count_amostra: 9720200\n",
      "iteration:  48630 , total_loss:  20.98786678314209\n",
      "count_amostra: 9726200\n",
      "iteration:  48660 , total_loss:  20.824066797892254\n",
      "count_amostra: 9732200\n",
      "iteration:  48690 , total_loss:  20.945171801249185\n",
      "count_amostra: 9738200\n",
      "iteration:  48720 , total_loss:  20.924495506286622\n",
      "count_amostra: 9744200\n",
      "iteration:  48750 , total_loss:  20.829222933451334\n",
      "count_amostra: 9750200\n",
      "iteration:  48780 , total_loss:  20.956643931070964\n",
      "count_amostra: 9756200\n",
      "iteration:  48810 , total_loss:  20.832341448465982\n",
      "count_amostra: 9762200\n",
      "iteration:  48840 , total_loss:  20.821538098653157\n",
      "count_amostra: 9768200\n",
      "iteration:  48870 , total_loss:  20.971990140279136\n",
      "count_amostra: 9774200\n",
      "iteration:  48900 , total_loss:  20.893619283040366\n",
      "count_amostra: 9780200\n",
      "iteration:  48930 , total_loss:  20.897534052530926\n",
      "count_amostra: 9786200\n",
      "iteration:  48960 , total_loss:  20.828285853068035\n",
      "count_amostra: 9792200\n",
      "iteration:  48990 , total_loss:  20.82882086435954\n",
      "count_amostra: 9798200\n",
      "iteration:  49020 , total_loss:  20.864101346333822\n",
      "count_amostra: 9804200\n",
      "iteration:  49050 , total_loss:  20.82016461690267\n",
      "count_amostra: 9810200\n",
      "iteration:  49080 , total_loss:  20.81168327331543\n",
      "count_amostra: 9816200\n",
      "iteration:  49110 , total_loss:  20.87999089558919\n",
      "count_amostra: 9822200\n",
      "iteration:  49140 , total_loss:  20.87638454437256\n",
      "count_amostra: 9828200\n",
      "iteration:  49170 , total_loss:  20.796961530049643\n",
      "count_amostra: 9834200\n",
      "iteration:  49200 , total_loss:  20.91004638671875\n",
      "count_amostra: 9840200\n",
      "iteration:  49230 , total_loss:  20.92387809753418\n",
      "count_amostra: 9846200\n",
      "iteration:  49260 , total_loss:  20.817270469665527\n",
      "count_amostra: 9852200\n",
      "iteration:  49290 , total_loss:  20.933953603108723\n",
      "count_amostra: 9858200\n",
      "iteration:  49320 , total_loss:  20.860045941670737\n",
      "count_amostra: 9864200\n",
      "iteration:  49350 , total_loss:  20.86073830922445\n",
      "count_amostra: 9870200\n",
      "iteration:  49380 , total_loss:  20.90349973042806\n",
      "count_amostra: 9876200\n",
      "iteration:  49410 , total_loss:  20.860054588317873\n",
      "count_amostra: 9882200\n",
      "iteration:  49440 , total_loss:  20.871908060709636\n",
      "count_amostra: 9888200\n",
      "iteration:  49470 , total_loss:  20.883731587727866\n",
      "count_amostra: 9894200\n",
      "iteration:  49500 , total_loss:  20.84592278798421\n",
      "count_amostra: 9900200\n",
      "iteration:  49530 , total_loss:  20.974789428710938\n",
      "count_amostra: 9906200\n",
      "iteration:  49560 , total_loss:  20.967443656921386\n",
      "count_amostra: 9912200\n",
      "iteration:  49590 , total_loss:  20.89282792409261\n",
      "count_amostra: 9918200\n",
      "iteration:  49620 , total_loss:  20.868063545227052\n",
      "count_amostra: 9924200\n",
      "iteration:  49650 , total_loss:  20.81685822804769\n",
      "count_amostra: 9930200\n",
      "iteration:  49680 , total_loss:  20.838868141174316\n",
      "count_amostra: 9936200\n",
      "iteration:  49710 , total_loss:  20.819087791442872\n",
      "count_amostra: 9942200\n",
      "iteration:  49740 , total_loss:  20.86720174153646\n",
      "count_amostra: 9948200\n",
      "iteration:  49770 , total_loss:  20.946624946594238\n",
      "count_amostra: 9954200\n",
      "iteration:  49800 , total_loss:  20.94523893992106\n",
      "count_amostra: 9960200\n",
      "iteration:  49830 , total_loss:  20.890756289164226\n",
      "count_amostra: 9966200\n",
      "iteration:  49860 , total_loss:  20.82553793589274\n",
      "count_amostra: 9972200\n",
      "iteration:  49890 , total_loss:  20.872000885009765\n",
      "count_amostra: 9978200\n",
      "iteration:  49920 , total_loss:  20.925859006245933\n",
      "count_amostra: 9984200\n",
      "iteration:  49950 , total_loss:  20.933065478007\n",
      "count_amostra: 9990200\n",
      "iteration:  49980 , total_loss:  20.8733185450236\n",
      "count_amostra: 9996200\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "from einops import rearrange, pack, unpack\n",
    "\n",
    "# Only show the progress bar once on each machine.\n",
    "# progress_bar = tqdm(range(max_train_steps), disable=not accelerator.is_local_main_process)\n",
    "completed_steps = 0\n",
    "starting_epoch = 0\n",
    "\n",
    "# update the progress_bar if load from checkpoint\n",
    "total_loss=0\n",
    "# progress_bar.update(completed_steps)\n",
    "steps_log=30\n",
    "count_amostra=0\n",
    "num_train_epochs=1\n",
    "for epoch in range(starting_epoch, num_train_epochs):\n",
    "    model.train()\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        # with accelerator.accumulate(model):\n",
    "        results=model(batch['input_ids']) \n",
    "\n",
    "        count_amostra+=int(len(batch['input_ids']))\n",
    "        loss = results.loss\n",
    "        # print(loss)\n",
    "        total_loss += loss.detach().float().cpu().numpy().item()\n",
    "        accelerator.backward(loss)\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()       \n",
    "        \n",
    "        if step%steps_log==0:\n",
    "            print('iteration: ',step,', total_loss: ',total_loss/steps_log)\n",
    "            print('count_amostra:',count_amostra)\n",
    "            total_loss=0\n",
    "    # Checks if the accelerator has performed an optimization step behind the scenes\n",
    "    if accelerator.sync_gradients:\n",
    "        # progress_bar.update(1)\n",
    "        completed_steps += 1\n",
    "\n",
    "    if completed_steps >= max_train_steps:\n",
    "        break\n",
    "\n",
    "    model.eval()\n",
    "    losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.discriminator,'/home/miu/Projects/fast_transformers/models/electra_discriminator_16kvocab_10m.pt')\n",
    "# torch.save(model.discriminator,'/home/miu/Projetos/kiki/fast_transformer/models/bert_discriminator_32kvocab_5m.pt')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Finetuning model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Electra custom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "import torch\n",
    "import os\n",
    "from torch.utils.data import  DataLoader\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "    \n",
    "class ElectraClass(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ElectraClass, self).__init__()\n",
    "        self.l1 = model.discriminator._modules['0']\n",
    "        self.l2 = torch.nn.Dropout(0.3)\n",
    "        self.fl = torch.nn.Flatten()\n",
    "        self.l3 = torch.nn.Linear(768*(MODEL_MAX_SEQ_LEN-2), 3)\n",
    "    \n",
    "    def forward(self, ids):\n",
    "        output= self.l1(ids)\n",
    "        output = self.l2(output)\n",
    "        output = self.fl(output)\n",
    "        output = self.l3(output)\n",
    "        return output\n",
    "\n",
    "electra_model = ElectraClass()\n",
    "electra_model=electra_model.to('cuda')\n",
    "optimizer = torch.optim.Adam(params =  model.parameters(), lr=2e-5,weight_decay=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading builder script: 100%|██████████| 28.8k/28.8k [00:00<00:00, 18.6MB/s]\n",
      "Downloading metadata: 100%|██████████| 28.7k/28.7k [00:00<00:00, 12.3MB/s]\n",
      "Downloading readme: 100%|██████████| 27.9k/27.9k [00:00<00:00, 10.1MB/s]\n",
      "Downloading data: 6.22kB [00:00, 12.8MB/s]/3 [00:00<?, ?it/s]\n",
      "Downloading data: 1.05MB [00:00, 21.6MB/s]/3 [00:01<00:02,  1.08s/it]\n",
      "Downloading data: 441kB [00:00, 10.6MB/s]2/3 [00:01<00:00,  1.32it/s]\n",
      "Downloading data files: 100%|██████████| 3/3 [00:02<00:00,  1.43it/s]\n",
      "Generating train split: 100%|██████████| 3668/3668 [00:00<00:00, 35821.71 examples/s]\n",
      "Generating validation split: 100%|██████████| 408/408 [00:00<00:00, 9943.44 examples/s]\n",
      "Generating test split: 100%|██████████| 1725/1725 [00:00<00:00, 52488.13 examples/s]\n"
     ]
    }
   ],
   "source": [
    "task='mrpc'\n",
    "dataset = load_dataset('glue', task, split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class TokenizedDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "        self.ls=[]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        s=norm.normalize_str(self.dataset[\"sentence1\"][i])+'[SEP]'+norm.normalize_str(self.dataset[\"sentence2\"][i])\n",
    "        tokens = vocab.tokenize(s).tolist()\n",
    "        \n",
    "        self.ls.append(i)\n",
    "        \n",
    "        #trucate\n",
    "        tokens=tokens[:MODEL_MAX_SEQ_LEN - 2]\n",
    "        l=len(tokens)\n",
    "        for j in range(l,MODEL_MAX_SEQ_LEN - 2):\n",
    "            tokens.append(0)\n",
    "        tokens=torch.Tensor(tokens).long()\n",
    "\n",
    "        label=self.dataset[\"label\"][i]\n",
    "        \n",
    "        d={'input_ids':tokens,'label':label}\n",
    "        return d\n",
    "\n",
    "class HFTokenizedDataset(torch.utils.data.Dataset):\n",
    "    \"This wraps the dataset and tokenizes it, ready for the model\"\n",
    "\n",
    "    def __init__(self, dataset, tokenizer):\n",
    "        self.dataset = dataset\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        label=self.dataset[\"label\"][i]\n",
    "        inps=self.tokenizer.encode(\n",
    "            self.dataset[\"sentence1\"][i]+'[SEP]'+self.dataset[\"sentence2\"][i],\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            max_length=MODEL_MAX_SEQ_LEN - 2,\n",
    "            padding=\"max_length\",\n",
    "            return_special_tokens_mask=True,\n",
    "        )[0, ...]\n",
    "        return {'input_ids':inps,\n",
    "            'label':label}\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "if model_training=='bert':\n",
    "    tokenized_dataset = HFTokenizedDataset(dataset, tokenizer)\n",
    "else:\n",
    "    tokenized_dataset = TokenizedDataset(dataset)\n",
    "# train_dataloader=CustomDataloader(tokenized_dataset)\n",
    "train_dataloader = DataLoader(\n",
    "        tokenized_dataset, batch_size=DEVICE_BATCH_SIZE\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "\n",
    "electra_model.train()\n",
    "\n",
    "def loss_fn(outputs, targets):\n",
    "    loss=torch.nn.CrossEntropyLoss()(outputs, targets)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss:  0.025319833755493164\n",
      "Epoch: 1, Loss:  0.2507688021659851\n",
      "Epoch: 2, Loss:  0.2174168998003006\n",
      "Epoch: 3, Loss:  0.19720368683338166\n",
      "Epoch: 4, Loss:  0.17658205151557924\n"
     ]
    }
   ],
   "source": [
    "epochs=5\n",
    "running_loss=0\n",
    "for i in range(epochs):\n",
    "    for _,data in enumerate(train_dataloader):\n",
    "        ids = data['input_ids'].to('cuda', dtype = torch.long)\n",
    "        targets = data['label'].to('cuda', dtype = torch.long)\n",
    "\n",
    "        outputs = electra_model(ids)\n",
    "        \n",
    "        loss = loss_fn(outputs,targets)\n",
    "        # loss=outputs.loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        running_loss+=loss.item()\n",
    "        if _%50==0:\n",
    "            print(f'Epoch: {i}, Loss:  {running_loss/50}')\n",
    "            running_loss=0\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset('glue', task, split='validation')\n",
    "if model_training=='bert':\n",
    "    tokenized_dataset = HFTokenizedDataset(dataset, tokenizer)\n",
    "else:\n",
    "    tokenized_dataset = TokenizedDataset(dataset)\n",
    "\n",
    "eval_dataloader = DataLoader(\n",
    "        tokenized_dataset, batch_size=DEVICE_BATCH_SIZE\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "predictions=[]\n",
    "labels=[]\n",
    "c=0\n",
    "for _,data in enumerate(eval_dataloader):\n",
    "    ids = data['input_ids'].to('cuda', dtype = torch.long)\n",
    "    targets = data['label'].to('cuda', dtype = torch.long)\n",
    "    \n",
    "    outputs = electra_model(ids)\n",
    "    \n",
    "    outputs=torch.argmax(outputs, dim=1).cpu()\n",
    "    targets=targets.cpu()\n",
    "    \n",
    "    outputs=[int(o.item()) for o in outputs]\n",
    "    targets=[int(o.item()) for o in targets]\n",
    "\n",
    "    predictions+=outputs\n",
    "    labels+=targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.53      0.71      0.61       129\n",
      "           1       0.84      0.71      0.77       279\n",
      "\n",
      "    accuracy                           0.71       408\n",
      "   macro avg       0.69      0.71      0.69       408\n",
      "weighted avg       0.74      0.71      0.72       408\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(labels, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## glue mrpc com electra com pretraining de 10m:\n",
    "               precision    recall  f1-score   support\n",
    "\n",
    "           0       0.53      0.71      0.61       129\n",
    "           1       0.84      0.71      0.77       279\n",
    "\n",
    "    accuracy                           0.71       408\n",
    "   macro avg       0.69      0.71      0.69       408\n",
    "weighted avg       0.74      0.71      0.72       408\n",
    "\n",
    "## glue mrpc com electra com pretraining de 1m:\n",
    "            precision    recall  f1-score   support\n",
    "\n",
    "           0       0.47      0.48      0.47       129\n",
    "           1       0.76      0.75      0.75       279\n",
    "\n",
    "    accuracy                           0.66       408\n",
    "   macro avg       0.61      0.61      0.61       408\n",
    "weighted avg       0.66      0.66      0.66       408\n",
    "\n",
    "\n",
    "## glue mrpc com electra sem pretraining:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.50      0.23      0.32       129\n",
    "           1       0.72      0.89      0.79       279\n",
    "\n",
    "    accuracy                           0.68       408\n",
    "   macro avg       0.61      0.56      0.56       408\n",
    "weighted avg       0.65      0.68      0.64       408"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bert custom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import torch\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "class BERTClass(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BERTClass, self).__init__()\n",
    "        self.l1 = transformers.BertModel.from_pretrained('bert-base-cased')\n",
    "        self.l2 = torch.nn.Dropout(0.3)\n",
    "        self.l3 = torch.nn.Linear(768, 3)\n",
    "    \n",
    "    def forward(self, ids, mask, token_type_ids):\n",
    "        outs, output_1= self.l1(ids, attention_mask = mask, token_type_ids = token_type_ids, return_dict=False)\n",
    "        output_2 = self.l2(output_1)\n",
    "        output = self.l3(output_2)\n",
    "        return output\n",
    "    \n",
    "\n",
    "model = BERTClass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "import transformers\n",
    "import torch\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained('bert-base-cased', num_labels=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset glue (/home/kiki/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import  DataLoader\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "task='mrpc'\n",
    "dataset = load_dataset('glue', task, split='train')\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-cased', use_fast=True)\n",
    "# dataset = dataset.map(lambda e: tokenizer(e['sentence1'], truncation=True, padding='max_length'), batched=True)\n",
    "# dataset.set_format(type='torch', columns=['input_ids', 'token_type_ids', 'attention_mask', 'label']) #BERT\n",
    "# dataloader = torch.utils.data.DataLoader(dataset, batch_size=12)\n",
    "\n",
    "model=model.to('cuda')\n",
    "optimizer = torch.optim.Adam(params =  model.parameters(), lr=2e-5,weight_decay=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89502a00dc1f4a599875cfcf74516990",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3668 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def preprocess_function(examples):\n",
    "    if sentence2_key is None:\n",
    "        return tokenizer(examples[sentence1_key], truncation=True, padding='max_length')\n",
    "    return tokenizer(examples[sentence1_key], examples[sentence2_key], truncation=True, padding='max_length')\n",
    "sentence1_key,sentence2_key=(\"sentence1\", \"sentence2\")\n",
    "encoded_dataset = dataset.map(preprocess_function, batched=True)\n",
    "encoded_dataset.set_format(type='torch', columns=['input_ids', 'token_type_ids', 'attention_mask', 'label'])\n",
    "dataloader = torch.utils.data.DataLoader(encoded_dataset, batch_size=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "def loss_fn(outputs, targets):\n",
    "    # if task != \"stsb\":\n",
    "    #     outputs = torch.argmax(outputs, dim=1)\n",
    "    # else:\n",
    "    #     outputs = outputs[:, 0]\n",
    "    # loss=torch.nn.BCEWithLogitsLoss()(outputs,targets)\n",
    "    # loss=torch.nn.CrossEntropyLoss()(outputs,targets)\n",
    "    loss=torch.nn.CrossEntropyLoss()(outputs.view(-1,3), targets.view(-1))\n",
    "    # loss = Variable(loss, requires_grad = True)\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss:  0.007852643132209777\n",
      "Epoch: 0, Loss:  0.20191293969750404\n",
      "Epoch: 0, Loss:  0.16229299038648606\n",
      "Epoch: 0, Loss:  0.14223801746964454\n",
      "Epoch: 0, Loss:  0.1337541800737381\n",
      "Epoch: 0, Loss:  0.11794026836752891\n",
      "Epoch: 0, Loss:  0.10376800172030926\n",
      "Epoch: 1, Loss:  0.0131098260730505\n",
      "Epoch: 1, Loss:  0.10432511921972036\n",
      "Epoch: 1, Loss:  0.10928954795002938\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">23</span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">20 │   │   │   </span>running_loss=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">0</span>                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">21 │   │   </span>                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">22 │   │   </span>optimizer.zero_grad()                                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>23 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>loss.backward()                                                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">24 │   │   </span>optimizer.step()                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">25 </span>                                                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/kiki/miniconda3/envs/llama/lib/python3.9/site-packages/torch/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">_tensor.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">487</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">backward</span>    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 484 │   │   │   │   </span>create_graph=create_graph,                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 485 │   │   │   │   </span>inputs=inputs,                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 486 │   │   │   </span>)                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 487 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>torch.autograd.backward(                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 488 │   │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>, gradient, retain_graph, create_graph, inputs=inputs                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 489 │   │   </span>)                                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 490 </span>                                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/kiki/miniconda3/envs/llama/lib/python3.9/site-packages/torch/autograd/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">__init__.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">200</span> in   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">backward</span>                                                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">197 │   # The reason we repeat same the comment below is that</span>                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">198 │   # some Python versions print out the first line of a multi-line function</span>               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">199 │   # calls in the traceback and some print out the last line</span>                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>200 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>Variable._execution_engine.run_backward(  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># Calls into the C++ engine to run the bac</span>   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">201 │   │   </span>tensors, grad_tensors_, retain_graph, create_graph, inputs,                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">202 │   │   </span>allow_unreachable=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">True</span>, accumulate_grad=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">True</span>)  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># Calls into the C++ engine to ru</span>   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">203 </span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">KeyboardInterrupt</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92m<module>\u001b[0m:\u001b[94m23\u001b[0m                                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m20 \u001b[0m\u001b[2m│   │   │   \u001b[0mrunning_loss=\u001b[94m0\u001b[0m                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m21 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m22 \u001b[0m\u001b[2m│   │   \u001b[0moptimizer.zero_grad()                                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m23 \u001b[2m│   │   \u001b[0mloss.backward()                                                                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m24 \u001b[0m\u001b[2m│   │   \u001b[0moptimizer.step()                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m25 \u001b[0m                                                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/kiki/miniconda3/envs/llama/lib/python3.9/site-packages/torch/\u001b[0m\u001b[1;33m_tensor.py\u001b[0m:\u001b[94m487\u001b[0m in \u001b[92mbackward\u001b[0m    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 484 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mcreate_graph=create_graph,                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 485 \u001b[0m\u001b[2m│   │   │   │   \u001b[0minputs=inputs,                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 486 \u001b[0m\u001b[2m│   │   │   \u001b[0m)                                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 487 \u001b[2m│   │   \u001b[0mtorch.autograd.backward(                                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 488 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[96mself\u001b[0m, gradient, retain_graph, create_graph, inputs=inputs                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 489 \u001b[0m\u001b[2m│   │   \u001b[0m)                                                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 490 \u001b[0m                                                                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/kiki/miniconda3/envs/llama/lib/python3.9/site-packages/torch/autograd/\u001b[0m\u001b[1;33m__init__.py\u001b[0m:\u001b[94m200\u001b[0m in   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[92mbackward\u001b[0m                                                                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m197 \u001b[0m\u001b[2m│   \u001b[0m\u001b[2m# The reason we repeat same the comment below is that\u001b[0m                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m198 \u001b[0m\u001b[2m│   \u001b[0m\u001b[2m# some Python versions print out the first line of a multi-line function\u001b[0m               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m199 \u001b[0m\u001b[2m│   \u001b[0m\u001b[2m# calls in the traceback and some print out the last line\u001b[0m                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m200 \u001b[2m│   \u001b[0mVariable._execution_engine.run_backward(  \u001b[2m# Calls into the C++ engine to run the bac\u001b[0m   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m201 \u001b[0m\u001b[2m│   │   \u001b[0mtensors, grad_tensors_, retain_graph, create_graph, inputs,                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m202 \u001b[0m\u001b[2m│   │   \u001b[0mallow_unreachable=\u001b[94mTrue\u001b[0m, accumulate_grad=\u001b[94mTrue\u001b[0m)  \u001b[2m# Calls into the C++ engine to ru\u001b[0m   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m203 \u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mKeyboardInterrupt\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.train()\n",
    "epochs=5\n",
    "running_loss=0\n",
    "for i in range(epochs):\n",
    "    for _,data in enumerate(dataloader):\n",
    "        ids = data['input_ids'].to('cuda', dtype = torch.long)\n",
    "        mask = data['attention_mask'].to('cuda', dtype = torch.long)\n",
    "        token_type_ids = data['token_type_ids'].to('cuda', dtype = torch.long)\n",
    "        targets = data['label'].to('cuda', dtype = torch.long)\n",
    "\n",
    "        # outputs = model(ids, mask, token_type_ids)\n",
    "        outputs = model(ids, mask, token_type_ids)\n",
    "        loss = loss_fn(outputs.logits,targets)\n",
    "        # loss=outputs.loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        running_loss+=loss.item()\n",
    "        if _%50==0:\n",
    "            print(f'Epoch: {i}, Loss:  {running_loss/200}')\n",
    "            running_loss=0\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset glue (/home/kiki/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b703556b9b84f65b802c9d7269defea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/408 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset('glue', task, split='validation')\n",
    "# dataset = dataset.map(lambda e: tokenizer(e['sentence1'], truncation=True, padding='max_length'), batched=True)\n",
    "# dataset.set_format(type='torch', columns=['input_ids', 'token_type_ids', 'attention_mask', 'label'])\n",
    "# dataloader = torch.utils.data.DataLoader(dataset, batch_size=4)\n",
    "encoded_dataset = dataset.map(preprocess_function, batched=True)\n",
    "encoded_dataset.set_format(type='torch', columns=['input_ids', 'token_type_ids', 'attention_mask', 'label'])\n",
    "dataloader = torch.utils.data.DataLoader(encoded_dataset, batch_size=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "predictions=[]\n",
    "labels=[]\n",
    "c=0\n",
    "for _,data in enumerate(dataloader):\n",
    "    ids = data['input_ids'].to('cuda', dtype = torch.long)\n",
    "    mask = data['attention_mask'].to('cuda', dtype = torch.long)\n",
    "    token_type_ids = data['token_type_ids'].to('cuda', dtype = torch.long)\n",
    "    targets = data['label'].to('cuda', dtype = torch.long)\n",
    "\n",
    "    outputs = model(ids, mask, token_type_ids)\n",
    "    outputs = outputs.logits\n",
    "    \n",
    "    outputs=torch.argmax(outputs, dim=1).cpu()\n",
    "    targets=targets.cpu()\n",
    "    \n",
    "    outputs=[int(o.item()) for o in outputs]\n",
    "    targets=[int(o.item()) for o in targets]\n",
    "\n",
    "    predictions+=outputs\n",
    "    labels+=targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import load_metric\n",
    "# metric = load_metric('glue', 'mrpc')\n",
    "# final_score = metric.compute(predictions=predictions, references =labels )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.84      0.73       129\n",
      "           1       0.92      0.78      0.84       279\n",
      "\n",
      "    accuracy                           0.80       408\n",
      "   macro avg       0.78      0.81      0.79       408\n",
      "weighted avg       0.83      0.80      0.81       408\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(labels, predictions))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.30.0.dev0\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "GLUE_TASKS = [\"cola\", \"mnli\", \"mnli-mm\", \"mrpc\", \"qnli\", \"qqp\", \"rte\", \"sst2\", \"stsb\", \"wnli\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = \"mrpc\"\n",
    "model_checkpoint = \"bert-base-cased\"\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, load_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset glue (/home/kiki/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4aae4f248054178b9cbc635d19a2159",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_244174/1389288479.py:3: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = load_metric('glue', actual_task)\n"
     ]
    }
   ],
   "source": [
    "actual_task = \"mnli\" if task == \"mnli-mm\" else task\n",
    "dataset = load_dataset(\"glue\", actual_task)\n",
    "metric = load_metric('glue', actual_task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "    \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_to_keys = {\n",
    "    \"cola\": (\"sentence\", None),\n",
    "    \"mnli\": (\"premise\", \"hypothesis\"),\n",
    "    \"mnli-mm\": (\"premise\", \"hypothesis\"),\n",
    "    \"mrpc\": (\"sentence1\", \"sentence2\"),\n",
    "    \"qnli\": (\"question\", \"sentence\"),\n",
    "    \"qqp\": (\"question1\", \"question2\"),\n",
    "    \"rte\": (\"sentence1\", \"sentence2\"),\n",
    "    \"sst2\": (\"sentence\", None),\n",
    "    \"stsb\": (\"sentence1\", \"sentence2\"),\n",
    "    \"wnli\": (\"sentence1\", \"sentence2\"),\n",
    "}\n",
    "sentence1_key, sentence2_key = task_to_keys[task]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    if sentence2_key is None:\n",
    "        return tokenizer(examples[sentence1_key], truncation=True)\n",
    "    return tokenizer(examples[sentence1_key], examples[sentence2_key], truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "671e3a45668946a08773dcaea4506d5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3668 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0eb643aa13c4bb784b7d6f68f0e9ee1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/408 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e64158ab1568461a9d131c7d5f675c9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1725 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "encoded_dataset = dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "\n",
    "num_labels = 3 if task.startswith(\"mnli\") else 1 if task==\"stsb\" else 2\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_name = \"pearson\" if task == \"stsb\" else \"matthews_correlation\" if task == \"cola\" else \"accuracy\"\n",
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "\n",
    "args = TrainingArguments(\n",
    "    f\"{model_name}-finetuned-{task}\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    save_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=metric_name,\n",
    "    push_to_hub=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    if task != \"stsb\":\n",
    "        predictions = np.argmax(predictions, axis=1)\n",
    "    else:\n",
    "        predictions = predictions[:, 0]\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning https://huggingface.co/abcp4/bert-base-cased-finetuned-mrpc into local empty directory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "validation_key = \"validation_mismatched\" if task == \"mnli-mm\" else \"validation_matched\" if task == \"mnli\" else \"validation\"\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=encoded_dataset[\"train\"],\n",
    "    eval_dataset=encoded_dataset[validation_key],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57218c19fff94363bfe1478f7d6e2edc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.5459453463554382,\n",
       " 'eval_accuracy': 0.8578431372549019,\n",
       " 'eval_f1': 0.8993055555555555,\n",
       " 'eval_runtime': 0.4866,\n",
       " 'eval_samples_per_second': 838.425,\n",
       " 'eval_steps_per_second': 53.429,\n",
       " 'epoch': 5.0}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3470f0fd3b3a4641bde2110d54183b25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/230 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "preds=trainer.predict(encoded_dataset[\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset glue (/home/kiki/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b1438990ed844a992c43616097c4845",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/408 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset('glue', task, split='validation')\n",
    "dataset = dataset.map(lambda e: tokenizer(e['sentence1'], truncation=True, padding='max_length'), batched=True)\n",
    "dataset.set_format(type='torch', columns=['input_ids', 'token_type_ids', 'attention_mask', 'label'])\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "predictions=[]\n",
    "labels=[]\n",
    "c=0\n",
    "for _,data in enumerate(dataloader):\n",
    "    ids = data['input_ids'].to('cuda', dtype = torch.long)\n",
    "    mask = data['attention_mask'].to('cuda', dtype = torch.long)\n",
    "    token_type_ids = data['token_type_ids'].to('cuda', dtype = torch.long)\n",
    "    targets = data['label'].to('cuda', dtype = torch.long)\n",
    "\n",
    "    outputs = model(ids, mask, token_type_ids)\n",
    "    outputs = outputs.logits\n",
    "    \n",
    "    outputs=torch.argmax(outputs, dim=1).cpu()\n",
    "    targets=targets.cpu()\n",
    "    \n",
    "    outputs=[int(o.item()) for o in outputs]\n",
    "    targets=[int(o.item()) for o in targets]\n",
    "\n",
    "    predictions+=outputs\n",
    "    labels+=targets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 1, 1, 0])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions=np.argmax(preds.predictions, axis=1)\n",
    "labels=encoded_dataset[\"train\"]['label']\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.98      0.98      1194\n",
      "           1       0.99      1.00      0.99      2474\n",
      "\n",
      "    accuracy                           0.99      3668\n",
      "   macro avg       0.99      0.99      0.99      3668\n",
      "weighted avg       0.99      0.99      0.99      3668\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# from datasets import load_metric\n",
    "# metric = load_metric('glue', 'mrpc')\n",
    "# final_score = metric.compute(predictions=predictions, references =labels )\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(labels, predictions))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Torch model to HuggingFaceTransformers(optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Torch model to HuggingFaceTransformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from transformers import PreTrainedModel, PretrainedConfig\n",
    "from transformers import AutoModel, AutoConfig\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from typing import List, Optional, Tuple, Union\n",
    "from transformers.modeling_outputs import (\n",
    "    SequenceClassifierOutput,\n",
    ")\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyConfig(PretrainedConfig):\n",
    "    model_type = 'mymodel'\n",
    "    def __init__(self, important_param=42, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.important_param = important_param\n",
    "\n",
    "class MyModel(PreTrainedModel):\n",
    "    config_class = MyConfig\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.config = config\n",
    "        self.model = nn.Sequential(\n",
    "                          nn.Linear(3, self.config.important_param),\n",
    "                          nn.Sigmoid(),\n",
    "                          nn.Linear(self.config.important_param, 1),\n",
    "                          nn.Sigmoid()\n",
    "                          )\n",
    "    def forward(self, input):\n",
    "        return self.model(input) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = MyConfig(4)\n",
    "model = MyModel(config)\n",
    "model.save_pretrained('./models/electra')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MyModel(\n",
       "  (model): Sequential(\n",
       "    (0): Linear(in_features=3, out_features=4, bias=True)\n",
       "    (1): Sigmoid()\n",
       "    (2): Linear(in_features=4, out_features=1, bias=True)\n",
       "    (3): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "AutoConfig.register(\"mymodel\", MyConfig)\n",
    "AutoModel.register(MyConfig, MyModel)\n",
    "\n",
    "# new_model = MyModel.from_pretrained('./models/electra')\n",
    "new_model = AutoModel.from_pretrained('./models/electra')\n",
    "new_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_model.push_to_hub(\"mymodel-test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "\n",
    "class MySeqConfig(PretrainedConfig):\n",
    "    model_type = 'myseqmodel'\n",
    "    def __init__(self, important_param=42, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.important_param = important_param\n",
    "\n",
    "class MySequenceModel(PreTrainedModel):\n",
    "    config_class = MySeqConfig\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.config = config\n",
    "        self.model = nn.Sequential(\n",
    "                          nn.Linear(3, self.config.important_param),\n",
    "                          nn.Sigmoid(),\n",
    "                          nn.Linear(self.config.important_param, 1),\n",
    "                          nn.Sigmoid()\n",
    "                          )\n",
    "    def forward(self, input_ids: Optional[torch.Tensor] = None,\n",
    "                attention_mask: Optional[torch.Tensor] = None,\n",
    "                token_type_ids: Optional[torch.Tensor] = None,\n",
    "                position_ids: Optional[torch.Tensor] = None,\n",
    "                head_mask: Optional[torch.Tensor] = None,\n",
    "                inputs_embeds: Optional[torch.Tensor] = None,\n",
    "                labels: Optional[torch.Tensor] = None,\n",
    "                output_attentions: Optional[bool] = None,\n",
    "                output_hidden_states: Optional[bool] = None,\n",
    "                return_dict: Optional[bool] = None,)-> Union[Tuple[torch.Tensor], SequenceClassifierOutput]:\n",
    "        \n",
    "        print(input_ids.shape)\n",
    "        loss = Variable(torch.zeros(1).to('cuda'), requires_grad=True)\n",
    "\n",
    "        \n",
    "        return SequenceClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=torch.zeros((3)).to('cuda'),\n",
    "            hidden_states=torch.zeros(128).to('cuda'),\n",
    "            attentions=torch.zeros(128).to('cuda'),\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = MySeqConfig(4)\n",
    "model = MySequenceModel(config)\n",
    "model.save_pretrained('./models/electra')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AutoModelForSequenceClassification.register(MySeqConfig, MySequenceModel)\n",
    "AutoConfig.register(\"myseqmodel\", MySeqConfig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=AutoModelForSequenceClassification.from_pretrained('./models/electra')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "06/24/2023 21:48:38 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: False\n",
      "06/24/2023 21:48:38 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_backend=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=True,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=no,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=2e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=output_evals/runs/Jun24_21-48-38_kiki-System-Product-Name,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=500,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_type=linear,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=5.0,\n",
      "optim=adamw_hf,\n",
      "optim_args=None,\n",
      "output_dir=output_evals,\n",
      "overwrite_output_dir=False,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=8,\n",
      "per_device_train_batch_size=32,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=['tensorboard', 'wandb'],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=output_evals,\n",
      "save_on_each_node=False,\n",
      "save_safetensors=False,\n",
      "save_steps=500,\n",
      "save_strategy=no,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      "xpu_backend=None,\n",
      ")\n",
      "06/24/2023 21:48:39 - INFO - datasets.info - Loading Dataset Infos from /home/kiki/.cache/huggingface/modules/datasets_modules/datasets/glue/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad\n",
      "06/24/2023 21:48:39 - INFO - datasets.builder - Overwrite dataset info from restored data version.\n",
      "06/24/2023 21:48:39 - INFO - datasets.info - Loading Dataset info from /home/kiki/.cache/huggingface/datasets/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad\n",
      "06/24/2023 21:48:39 - WARNING - datasets.builder - Found cached dataset glue (/home/kiki/.cache/huggingface/datasets/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n",
      "06/24/2023 21:48:39 - INFO - datasets.info - Loading Dataset info from /home/kiki/.cache/huggingface/datasets/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad\n",
      "100%|███████████████████████████████████████████| 3/3 [00:00<00:00, 2040.36it/s]\n",
      "[INFO|configuration_utils.py:669] 2023-06-24 21:48:40,194 >> loading configuration file config.json from cache at /home/kiki/.cache/huggingface/hub/models--bert-base-cased/snapshots/5532cc56f74641d4bb33641f5c76a55d11f846e0/config.json\n",
      "[INFO|configuration_utils.py:725] 2023-06-24 21:48:40,200 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"finetuning_task\": \"cola\",\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.30.0.dev0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "[INFO|configuration_utils.py:669] 2023-06-24 21:48:40,399 >> loading configuration file config.json from cache at /home/kiki/.cache/huggingface/hub/models--bert-base-cased/snapshots/5532cc56f74641d4bb33641f5c76a55d11f846e0/config.json\n",
      "[INFO|configuration_utils.py:725] 2023-06-24 21:48:40,400 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.30.0.dev0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1810] 2023-06-24 21:48:40,401 >> loading file vocab.txt from cache at /home/kiki/.cache/huggingface/hub/models--bert-base-cased/snapshots/5532cc56f74641d4bb33641f5c76a55d11f846e0/vocab.txt\n",
      "[INFO|tokenization_utils_base.py:1810] 2023-06-24 21:48:40,401 >> loading file tokenizer.json from cache at /home/kiki/.cache/huggingface/hub/models--bert-base-cased/snapshots/5532cc56f74641d4bb33641f5c76a55d11f846e0/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:1810] 2023-06-24 21:48:40,401 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1810] 2023-06-24 21:48:40,401 >> loading file special_tokens_map.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1810] 2023-06-24 21:48:40,401 >> loading file tokenizer_config.json from cache at /home/kiki/.cache/huggingface/hub/models--bert-base-cased/snapshots/5532cc56f74641d4bb33641f5c76a55d11f846e0/tokenizer_config.json\n",
      "[INFO|configuration_utils.py:669] 2023-06-24 21:48:40,401 >> loading configuration file config.json from cache at /home/kiki/.cache/huggingface/hub/models--bert-base-cased/snapshots/5532cc56f74641d4bb33641f5c76a55d11f846e0/config.json\n",
      "[INFO|configuration_utils.py:725] 2023-06-24 21:48:40,402 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.30.0.dev0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:2535] 2023-06-24 21:48:40,435 >> loading weights file model.safetensors from cache at /home/kiki/.cache/huggingface/hub/models--bert-base-cased/snapshots/5532cc56f74641d4bb33641f5c76a55d11f846e0/model.safetensors\n",
      "[WARNING|modeling_utils.py:3229] 2023-06-24 21:48:40,813 >> Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "[WARNING|modeling_utils.py:3241] 2023-06-24 21:48:40,813 >> Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "06/24/2023 21:48:40 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/kiki/.cache/huggingface/datasets/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-76c8170a2a261ff8.arrow\n",
      "Running tokenizer on dataset:   0%|             | 0/1043 [00:00<?, ? examples/s]06/24/2023 21:48:40 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/kiki/.cache/huggingface/datasets/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-a2bf5301681fb373.arrow\n",
      "06/24/2023 21:48:40 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/kiki/.cache/huggingface/datasets/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-43a8c8283e9dbbef.arrow\n",
      "06/24/2023 21:48:40 - INFO - __main__ - Sample 1824 of the training set: {'sentence': 'I acknowledged that my father, he was tight as an owl.', 'label': 0, 'idx': 1824, 'input_ids': [101, 146, 8646, 1115, 1139, 1401, 117, 1119, 1108, 3600, 1112, 1126, 19976, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
      "06/24/2023 21:48:40 - INFO - __main__ - Sample 409 of the training set: {'sentence': 'For him to do that would be a mistake.', 'label': 1, 'idx': 409, 'input_ids': [101, 1370, 1140, 1106, 1202, 1115, 1156, 1129, 170, 6223, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
      "06/24/2023 21:48:40 - INFO - __main__ - Sample 4506 of the training set: {'sentence': 'Mary sang a song, but Lee never did.', 'label': 1, 'idx': 4506, 'input_ids': [101, 2090, 6407, 170, 1461, 117, 1133, 2499, 1309, 1225, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
      "[INFO|trainer.py:763] 2023-06-24 21:48:42,393 >> The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/home/kiki/miniconda3/envs/llama/lib/python3.9/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "[INFO|trainer.py:1812] 2023-06-24 21:48:42,397 >> ***** Running training *****\n",
      "[INFO|trainer.py:1813] 2023-06-24 21:48:42,397 >>   Num examples = 8,551\n",
      "[INFO|trainer.py:1814] 2023-06-24 21:48:42,397 >>   Num Epochs = 5\n",
      "[INFO|trainer.py:1815] 2023-06-24 21:48:42,397 >>   Instantaneous batch size per device = 32\n",
      "[INFO|trainer.py:1816] 2023-06-24 21:48:42,397 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "[INFO|trainer.py:1817] 2023-06-24 21:48:42,397 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1818] 2023-06-24 21:48:42,397 >>   Total optimization steps = 1,340\n",
      "[INFO|trainer.py:1819] 2023-06-24 21:48:42,398 >>   Number of trainable parameters = 108,311,810\n",
      "[INFO|integrations.py:720] 2023-06-24 21:48:42,402 >> Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mabcp4\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.4 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/mnt/e026e6cb-2abe-4ed4-bf51-0381c5a02c4b/Servidor/LM_Pretraining/Fast_Transformers/wandb/run-20230624_214843-jlxgfmk4\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mefficient-butterfly-34\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/abcp4/huggingface\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/abcp4/huggingface/runs/jlxgfmk4\u001b[0m\n",
      "{'loss': 0.4082, 'learning_rate': 1.2537313432835823e-05, 'epoch': 1.87}        \n",
      "{'loss': 0.1739, 'learning_rate': 5.074626865671642e-06, 'epoch': 3.73}         \n",
      "100%|██████████████████████████████████████▉| 1339/1340 [03:23<00:00,  6.52it/s][INFO|trainer.py:2085] 2023-06-24 21:52:09,773 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 207.3794, 'train_samples_per_second': 206.168, 'train_steps_per_second': 6.462, 'train_loss': 0.24341670933054455, 'epoch': 5.0}\n",
      "100%|███████████████████████████████████████| 1340/1340 [03:23<00:00,  6.59it/s]\n",
      "***** train metrics *****\n",
      "  epoch                    =        5.0\n",
      "  train_loss               =     0.2434\n",
      "  train_runtime            = 0:03:27.37\n",
      "  train_samples            =       8551\n",
      "  train_samples_per_second =    206.168\n",
      "  train_steps_per_second   =      6.462\n",
      "06/24/2023 21:52:09 - INFO - __main__ - *** Evaluate ***\n",
      "[INFO|trainer.py:763] 2023-06-24 21:52:09,779 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:3217] 2023-06-24 21:52:09,780 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:3219] 2023-06-24 21:52:09,780 >>   Num examples = 1043\n",
      "[INFO|trainer.py:3222] 2023-06-24 21:52:09,780 >>   Batch size = 8\n",
      "100%|█████████████████████████████████████████| 131/131 [00:01<00:00, 66.00it/s]\n",
      "***** eval metrics *****\n",
      "  epoch                     =        5.0\n",
      "  eval_loss                 =     0.6722\n",
      "  eval_matthews_correlation =     0.6082\n",
      "  eval_runtime              = 0:00:01.99\n",
      "  eval_samples              =       1043\n",
      "  eval_samples_per_second   =     522.76\n",
      "  eval_steps_per_second     =     65.658\n",
      "06/24/2023 21:52:11 - INFO - __main__ - *** Predict ***\n",
      "[INFO|trainer.py:763] 2023-06-24 21:52:11,781 >> The following columns in the test set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:3217] 2023-06-24 21:52:11,782 >> ***** Running Prediction *****\n",
      "[INFO|trainer.py:3219] 2023-06-24 21:52:11,782 >>   Num examples = 1063\n",
      "[INFO|trainer.py:3222] 2023-06-24 21:52:11,782 >>   Batch size = 8\n",
      "100%|█████████████████████████████████████████| 133/133 [00:02<00:00, 66.26it/s]\n",
      "06/24/2023 21:52:13 - INFO - __main__ - ***** Predict results cola *****\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                      eval/loss ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      eval/matthews_correlation ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   eval/runtime ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        eval/samples_per_second ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/steps_per_second ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    train/epoch ▁▅██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              train/global_step ▁▅██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/learning_rate █▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                     train/loss █▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               train/total_flos ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               train/train_loss ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/train_runtime ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train/train_samples_per_second ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   train/train_steps_per_second ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                      eval/loss 0.6722\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      eval/matthews_correlation 0.60817\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   eval/runtime 1.9952\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        eval/samples_per_second 522.76\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/steps_per_second 65.658\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    train/epoch 5.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              train/global_step 1340\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/learning_rate 1e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                     train/loss 0.1739\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               train/total_flos 2724443033823600.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               train/train_loss 0.24342\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/train_runtime 207.3794\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train/train_samples_per_second 206.168\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   train/train_steps_per_second 6.462\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mefficient-butterfly-34\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/abcp4/huggingface/runs/jlxgfmk4\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230624_214843-jlxgfmk4/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Glue\n",
    "!rm -r output_evals/runs\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "!python glue_eval.py --task_name cola --model_name_or_path bert-base-cased --tokenizer_name bert-base-cased --output_dir output_evals --do_train  --do_eval  --do_predict --max_seq_length 124 --per_device_train_batch_size 32 --learning_rate 2e-5 --lr_scheduler_type linear --num_train_epochs 5  --save_strategy no --seed 42 "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
