{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "#use gpu 1 only\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Iterator\n",
    "import time\n",
    "import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "from tokenizers import BertWordPieceTokenizer, Regex, normalizers\n",
    "from tqdm import tqdm\n",
    "from transformers import (\n",
    "    BertConfig,\n",
    "    BertForMaskedLM,\n",
    "    BertTokenizerFast,  \n",
    "    DataCollatorForLanguageModeling,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")      \n",
    "    \n",
    "import math\n",
    "from functools import reduce\n",
    "from collections import namedtuple\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "# from reformer_pytorch import ReformerLM\n",
    "# from x_transformers import TransformerWrapper, Decoder,Encoder\n",
    "from torch.utils.data import DataLoader\n",
    "import math \n",
    "from accelerate import Accelerator\n",
    "\n",
    "import torch.nn.functional as F\n",
    "# from einops import rearrange, pack, unpack\n",
    "import sys\n",
    "\n",
    "import os\n",
    "#get cur dir\n",
    "cur_dir = os.getcwd()\n",
    "if 'miu' in cur_dir:\n",
    "    sys.path.append('/home/miu/Projects/NLPLab/')\n",
    "elif 'kiki' in cur_dir:\n",
    "    sys.path.append('/home/kiki/dados_servidor/Servidor/NLPLab/')\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size:  2040\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Resolving data files: 100%|██████████| 53/53 [00:00<00:00, 80.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded dataset!!\n"
     ]
    }
   ],
   "source": [
    "model_training='hf'#'hf' , 'tokemonster\n",
    "# LIMIT_DATASET = 2016 * 4  # keep small for development, set to None for full dataset\n",
    "#(3090) Para 128 tokens(Bert) e 32000 vocab: 1000 = 6 seg, 10000 = 1min, 100000 = 10min, 1m = 100min, 10m = 16h, 100m = 160h ou 6.6 dias\n",
    "#(3090) Para 84 tokens(tokenmonster salva 35%) e 24000 vocab(Bert): 1000 = 4.3s, 10000 = 43s, 100000 = 7.1m, 1m = 71min, 10m = 11.8h, 100m = 118h ou 5 dias\n",
    "#(4090) Para 84 tokens(tokenmonster salva 35%) e 24000 vocab(Electra): 1000 = 3.5s, 10000 = 35s, 100000 = 5.9m, 1m = 59min, 10m = 9.8h, 100m = 98h ou 4 dias\n",
    "#(4090) Para 84 tokens(tokenmonster salva 35%) e 16000 vocab (Electra): 1000 = 2.1s, 10000 = 21s, 100000 = 3.53m, 1m = 35.3min, 10m = 5.8h, 100m = 58h ou 2.4 dias\n",
    "\n",
    "#com retnet. Confirmado que é O(n). Se lembrando que não existe mta vantagem além do custo crescer sequencialmente, já que o conteudo \n",
    "#(4090) Para 84 tokens(tokenmonster salva 35%) batch size 200 e 16000 vocab (Electra): 1000 = 2.1s, 10000 = 21s, 100000 = 3.53m, 1m = 35.3min, 10m = 5.8h, 100m = 58h ou 2.4 dias\n",
    "#(4090) Para 168 tokens(tokenmonster salva 35%) batch size 100 e 16000 vocab (Electra): 1000 = 3.7s, 10000 = 37.5s,\n",
    "#(4090) Para 1000 tokens(tokenmonster salva 35%) batch size 12 e 16000 vocab (Electra):            , 10000 = 4min32s,\n",
    "\n",
    "\n",
    "#HIPOTÉTICO(Se conseguisse representar um texto com menos tokens):\n",
    "#(4090) Para 24 tokens(tokenmonster salva 35%)(Electra): 1000 = , 10000 = , 100000 = 3.19m, 1m = 32min, 10m = 6.5h, 100m = 65h ou 2.7 dias\n",
    "#HIPOTÉTICO(Se diminuisse o vocabulario para 1024 tokens diferentes)\n",
    "#(4090) Para 84 tokens(tokenmonster salva 35%) e 1024 vocab: 1000 =1.6s , 10000 = 16.2s,\n",
    "\n",
    "#losses:\n",
    "#1m dataset, electra, tokenmonster: iteration:  7800 , total_loss:  26.277032788594564\n",
    "#10m dataset, 84 tokens(tokenmonster salva 35%) e 16000 vocab (Electra) iteration:  49980 , total_loss:  20.8733185450236. 339min total\n",
    "\n",
    "\n",
    "LIMIT_DATASET = 100_000\n",
    "RANDOM_SEED = 42\n",
    "NUM_TOKENIZER_TRAINING_ITEMS = 1_000_000  # I made this up, but it seems reasonable\n",
    "\n",
    "# VOCAB_SIZE = 32_768  # from Cramming\n",
    "# DEVICE_BATCH_SIZE = 100 # aprox 128, adjust to get near 100% gpu memory use\n",
    "# MODEL_MAX_SEQ_LEN = 128  # from Cramming\n",
    "\n",
    "# VOCAB_SIZE = 16_000  # tokenmonster\n",
    "VOCAB_SIZE = 32_768  # bert\n",
    "DEVICE_BATCH_SIZE = 12 # Token monster aguenta um batch size de (200-248)!! Geralmente melhora a qualidade do treino\n",
    "# MODEL_MAX_SEQ_LEN = 84*2 # token_monster\n",
    "MODEL_MAX_SEQ_LEN = 512 # bert\n",
    "\n",
    "\n",
    "MASK_ID=4\n",
    "PAD_ID=0\n",
    "gradient_accumulation_steps = 2048 // DEVICE_BATCH_SIZE  # roughly based on Cramming\n",
    "batch_size = DEVICE_BATCH_SIZE * gradient_accumulation_steps\n",
    "print('batch_size: ',batch_size)\n",
    "RUN_DIR = Path(\"data\") / f\"run_{time.strftime('%Y%m%d-%H%M%S')}\"\n",
    "CHECKPOINT_DIR = RUN_DIR / \"training_checkpoints\"\n",
    "MODEL_DIR = RUN_DIR / \"model\"\n",
    "TOKENIZER_PATH = RUN_DIR / \"tokenizer.json\"\n",
    "TRAINER_HISTORY_PATH = RUN_DIR / \"trainer_history.json\"\n",
    "\n",
    "RUN_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "dataset = datasets.load_dataset(\n",
    "    \"sradc/chunked-shuffled-wikipedia20220301en-bookcorpusopen\",\n",
    "    split=f\"train[:{LIMIT_DATASET}]\" if LIMIT_DATASET else \"train\",\n",
    "    revision=\"0e6fada2dd43136e4a3f637da41de2e596aee674\",\n",
    ")\n",
    "print('loaded dataset!!')\n",
    "\n",
    "from utils.process_tokenizer import get_tokenizer\n",
    "tokenized_dataset,norm,vocab,tokenizer=get_tokenizer(dataset,NUM_TOKENIZER_TRAINING_ITEMS,VOCAB_SIZE,TOKENIZER_PATH,'smlm',MODEL_MAX_SEQ_LEN+2,model_training)\n",
    "# MASK_ID=vocab.tokenize(\"[MASK]\")[0]\n",
    "N_DOMAINS=3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PreTrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from x_transformers import TransformerWrapper, Decoder,Encoder\n",
    "\n",
    "\n",
    "#encoder decoder\n",
    "colt5_dict={\n",
    "            'encoder':{'is_colt5_encoder':True,'light_ff_mult':0.5,'heavy_ff_mult':4,'num_heavy_tokens':25,\n",
    "                   'light_dim_head':64,'light_heads':8,'light_window_size':128,\n",
    "                   'heavy_dim_head':64,'heavy_heads':8,'num_heavy_tokens_q':25,\n",
    "                   'num_heavy_tokens_kv':25},\n",
    "            }\n",
    "\n",
    "# model = TransformerWrapper(\n",
    "base_model = TransformerWrapper(\n",
    "    num_tokens = VOCAB_SIZE+8,\n",
    "    max_seq_len = MODEL_MAX_SEQ_LEN,\n",
    "    attn_layers = Encoder(\n",
    "        dim = int(768),\n",
    "        depth = 12,\n",
    "        heads = 12,\n",
    "        attn_flash = True,\n",
    "        colt5_dict=colt5_dict['encoder'],\n",
    "        # rel_pos_bias = True\n",
    "    ),\n",
    "    emb_dim=128,\n",
    ")\n",
    "\n",
    "class SpanBert(nn.Module):\n",
    "    def __init__(self, model, max_targets=20, position_embedding_size=200,num_samples=5):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.position_embeddings = nn.Embedding(max_targets, position_embedding_size)\n",
    "        model_dim = self.model.emb_dim\n",
    "        self.mlp_layer_norm = nn.LayerNorm(768 * 2 + position_embedding_size)\n",
    "        self.decoder = nn.Linear(768 * 2 + position_embedding_size, VOCAB_SIZE+8,bias=False)\n",
    "        self.bias = nn.Parameter(torch.zeros(VOCAB_SIZE+8))\n",
    "        self.max_targets = max_targets\n",
    "        self.num_samples=num_samples\n",
    "\n",
    "    def forward(self, x,span_starts,span_ends):\n",
    "        out,hidden_states=self.model(x,return_logits_and_embeddings=True)\n",
    "            \n",
    "        # print('out.shape: ',out.shape)\n",
    "        # print('span_starts: ',span_starts)\n",
    "        # print('span_ends: ',span_ends)  \n",
    "        # print('hidden_states.shape: ',hidden_states.shape)\n",
    "\n",
    "        #get external boundary tokens indices(before MASK_ID occurs and after it)\n",
    "        indexes=[]\n",
    "        pairs=[]\n",
    "        for i in range(len(span_starts)):\n",
    "            for j in range(len(span_starts[i])):\n",
    "                s=span_starts[i][j]-1\n",
    "                e=span_ends[i][j] + 1\n",
    "                if e>=len(x[i]):\n",
    "                    e=len(x[i])-1\n",
    "                if s<0:\n",
    "                    s=0\n",
    "                start_emb = hidden_states[i,s]\n",
    "                end_emb = hidden_states[i,e]\n",
    "                for k in range(s,e):\n",
    "                    pairs.append(torch.cat((start_emb,end_emb,self.position_embeddings(torch.tensor(k-s).to(x.device))),-1))\n",
    "                \n",
    "        # print('pairs',len(pairs))\n",
    "        #get 5 only\n",
    "        # pairs=torch.stack(pairs[:self.num_samples])\n",
    "        pairs=torch.stack(pairs[:])\n",
    "        # print('pairs',pairs.shape)\n",
    "        p=self.mlp_layer_norm(pairs)\n",
    "        # print('p: ',p.shape)\n",
    "        d=self.decoder(p) + self.bias\n",
    "        #get argmax\n",
    "        #d=torch.argmax(d,-1)\n",
    "        #reshape d from (a,b) to (1,a,b)\n",
    "        d=torch.unsqueeze(d,0)\n",
    "        #print('d: ',d.shape)\n",
    "        return out,d\n",
    "\n",
    "model = SpanBert(base_model, max_targets=20, position_embedding_size=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_ids are the indices corresponding to each token in the sentence.\n",
    "# attention_mask indicates whether a token should be attended to or not.\n",
    "# token_type_ids identifies which sequence a token belongs to when there is more than one sequence\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "        tokenized_dataset, shuffle=True, batch_size=DEVICE_BATCH_SIZE\n",
    "    )\n",
    "from utils.utils import get_optimizer_scheduler\n",
    "optimizer,lr_scheduler,max_train_steps = get_optimizer_scheduler(model,train_dataloader,gradient_accumulation_steps,learning_rate=5e-5,weight_decay=0, num_warmup_steps=0, max_train_steps=None,lr_scheduler_type='linear',num_train_epochs=1)\n",
    "\n",
    "accelerator = Accelerator()\n",
    "# Prepare everything with our `accelerator`.\n",
    "model, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(\n",
    "    model, optimizer, train_dataloader, lr_scheduler\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration:  0 , total_loss:  0.7049860636393229\n",
      "count_amostra: 12\n",
      "iteration:  30 , total_loss:  18.67620455423991\n",
      "count_amostra: 372\n",
      "iteration:  60 , total_loss:  16.353540166219076\n",
      "count_amostra: 732\n",
      "iteration:  90 , total_loss:  15.38111327489217\n",
      "count_amostra: 1092\n",
      "iteration:  120 , total_loss:  15.070745372772217\n",
      "count_amostra: 1452\n",
      "iteration:  150 , total_loss:  15.057285817464193\n",
      "count_amostra: 1812\n",
      "iteration:  180 , total_loss:  15.046465714772543\n",
      "count_amostra: 2172\n",
      "iteration:  210 , total_loss:  14.798621431986492\n",
      "count_amostra: 2532\n",
      "iteration:  240 , total_loss:  14.885129102071126\n",
      "count_amostra: 2892\n",
      "iteration:  270 , total_loss:  14.8073605855306\n",
      "count_amostra: 3252\n",
      "iteration:  300 , total_loss:  14.67681531906128\n",
      "count_amostra: 3612\n",
      "iteration:  330 , total_loss:  14.427381706237792\n",
      "count_amostra: 3972\n",
      "iteration:  360 , total_loss:  14.165073235829672\n",
      "count_amostra: 4332\n",
      "iteration:  390 , total_loss:  14.128851381937663\n",
      "count_amostra: 4692\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (623 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration:  420 , total_loss:  14.079149278004964\n",
      "count_amostra: 5052\n",
      "iteration:  450 , total_loss:  13.812104606628418\n",
      "count_amostra: 5412\n",
      "iteration:  480 , total_loss:  13.73639497756958\n",
      "count_amostra: 5772\n",
      "iteration:  510 , total_loss:  13.514529132843018\n",
      "count_amostra: 6132\n",
      "iteration:  540 , total_loss:  13.4769118309021\n",
      "count_amostra: 6492\n",
      "iteration:  570 , total_loss:  13.322250016530354\n",
      "count_amostra: 6852\n",
      "iteration:  600 , total_loss:  13.247062587738037\n",
      "count_amostra: 7212\n",
      "iteration:  630 , total_loss:  13.322978591918945\n",
      "count_amostra: 7572\n",
      "iteration:  660 , total_loss:  13.087895234425863\n",
      "count_amostra: 7932\n",
      "iteration:  690 , total_loss:  13.028565216064454\n",
      "count_amostra: 8292\n",
      "iteration:  720 , total_loss:  12.818256028493245\n",
      "count_amostra: 8652\n",
      "iteration:  750 , total_loss:  12.920156224568684\n",
      "count_amostra: 9012\n",
      "iteration:  780 , total_loss:  12.688611761728923\n",
      "count_amostra: 9372\n",
      "iteration:  810 , total_loss:  12.703775723775228\n",
      "count_amostra: 9732\n",
      "iteration:  840 , total_loss:  12.725774415334065\n",
      "count_amostra: 10092\n",
      "iteration:  870 , total_loss:  12.57789068222046\n",
      "count_amostra: 10452\n",
      "iteration:  900 , total_loss:  12.560325654347738\n",
      "count_amostra: 10812\n",
      "iteration:  930 , total_loss:  12.611541970570881\n",
      "count_amostra: 11172\n",
      "iteration:  960 , total_loss:  12.461597601572672\n",
      "count_amostra: 11532\n",
      "iteration:  990 , total_loss:  12.43762731552124\n",
      "count_amostra: 11892\n",
      "iteration:  1020 , total_loss:  12.35657320022583\n",
      "count_amostra: 12252\n",
      "iteration:  1050 , total_loss:  12.142449378967285\n",
      "count_amostra: 12612\n",
      "iteration:  1080 , total_loss:  12.182458368937175\n",
      "count_amostra: 12972\n",
      "iteration:  1110 , total_loss:  12.048035176595052\n",
      "count_amostra: 13332\n",
      "iteration:  1140 , total_loss:  12.144027773539225\n",
      "count_amostra: 13692\n",
      "iteration:  1170 , total_loss:  12.04485855102539\n",
      "count_amostra: 14052\n",
      "iteration:  1200 , total_loss:  12.000950241088868\n",
      "count_amostra: 14412\n",
      "iteration:  1230 , total_loss:  11.890976428985596\n",
      "count_amostra: 14772\n",
      "iteration:  1260 , total_loss:  11.91581106185913\n",
      "count_amostra: 15132\n",
      "iteration:  1290 , total_loss:  11.667917601267497\n",
      "count_amostra: 15492\n",
      "iteration:  1320 , total_loss:  11.747722085316976\n",
      "count_amostra: 15852\n",
      "iteration:  1350 , total_loss:  11.787232971191406\n",
      "count_amostra: 16212\n",
      "iteration:  1380 , total_loss:  11.68785670598348\n",
      "count_amostra: 16572\n",
      "iteration:  1410 , total_loss:  11.580803298950196\n",
      "count_amostra: 16932\n",
      "iteration:  1440 , total_loss:  11.59270076751709\n",
      "count_amostra: 17292\n",
      "iteration:  1470 , total_loss:  11.527508449554443\n",
      "count_amostra: 17652\n",
      "iteration:  1500 , total_loss:  11.67512248357137\n",
      "count_amostra: 18012\n",
      "iteration:  1530 , total_loss:  11.378379631042481\n",
      "count_amostra: 18372\n",
      "iteration:  1560 , total_loss:  11.495932706197102\n",
      "count_amostra: 18732\n",
      "iteration:  1590 , total_loss:  11.3600323677063\n",
      "count_amostra: 19092\n",
      "iteration:  1620 , total_loss:  11.30128558476766\n",
      "count_amostra: 19452\n",
      "iteration:  1650 , total_loss:  11.296347490946452\n",
      "count_amostra: 19812\n",
      "iteration:  1680 , total_loss:  11.348502063751221\n",
      "count_amostra: 20172\n",
      "iteration:  1710 , total_loss:  11.226310634613037\n",
      "count_amostra: 20532\n",
      "iteration:  1740 , total_loss:  11.223321215311687\n",
      "count_amostra: 20892\n",
      "iteration:  1770 , total_loss:  11.049447536468506\n",
      "count_amostra: 21252\n",
      "iteration:  1800 , total_loss:  11.039846801757813\n",
      "count_amostra: 21612\n",
      "iteration:  1830 , total_loss:  10.976892566680908\n",
      "count_amostra: 21972\n",
      "iteration:  1860 , total_loss:  11.001812839508057\n",
      "count_amostra: 22332\n",
      "iteration:  1890 , total_loss:  10.94043951034546\n",
      "count_amostra: 22692\n",
      "iteration:  1920 , total_loss:  11.046718374888103\n",
      "count_amostra: 23052\n",
      "iteration:  1950 , total_loss:  10.969495709737142\n",
      "count_amostra: 23412\n",
      "iteration:  1980 , total_loss:  10.850212796529133\n",
      "count_amostra: 23772\n",
      "iteration:  2010 , total_loss:  10.810469182332357\n",
      "count_amostra: 24132\n",
      "iteration:  2040 , total_loss:  10.702812989552816\n",
      "count_amostra: 24492\n",
      "iteration:  2070 , total_loss:  10.847726504007975\n",
      "count_amostra: 24852\n",
      "iteration:  2100 , total_loss:  10.799099667867024\n",
      "count_amostra: 25212\n",
      "iteration:  2130 , total_loss:  10.743778864542643\n",
      "count_amostra: 25572\n",
      "iteration:  2160 , total_loss:  10.64419002532959\n",
      "count_amostra: 25932\n",
      "iteration:  2190 , total_loss:  10.693586254119873\n",
      "count_amostra: 26292\n",
      "iteration:  2220 , total_loss:  10.751340897878011\n",
      "count_amostra: 26652\n",
      "iteration:  2250 , total_loss:  10.57289260228475\n",
      "count_amostra: 27012\n",
      "iteration:  2280 , total_loss:  10.510682996114095\n",
      "count_amostra: 27372\n",
      "iteration:  2310 , total_loss:  10.647683334350585\n",
      "count_amostra: 27732\n",
      "iteration:  2340 , total_loss:  10.455953566233317\n",
      "count_amostra: 28092\n",
      "iteration:  2370 , total_loss:  10.458444595336914\n",
      "count_amostra: 28452\n",
      "iteration:  2400 , total_loss:  10.517382780710856\n",
      "count_amostra: 28812\n",
      "iteration:  2430 , total_loss:  10.448924700419107\n",
      "count_amostra: 29172\n",
      "iteration:  2460 , total_loss:  10.254729874928792\n",
      "count_amostra: 29532\n",
      "iteration:  2490 , total_loss:  10.509887822469075\n",
      "count_amostra: 29892\n",
      "iteration:  2520 , total_loss:  10.344465510050457\n",
      "count_amostra: 30252\n",
      "iteration:  2550 , total_loss:  10.336631361643473\n",
      "count_amostra: 30612\n",
      "iteration:  2580 , total_loss:  10.239831161499023\n",
      "count_amostra: 30972\n",
      "iteration:  2610 , total_loss:  10.246552499135335\n",
      "count_amostra: 31332\n",
      "iteration:  2640 , total_loss:  10.175216833750406\n",
      "count_amostra: 31692\n",
      "iteration:  2670 , total_loss:  10.223073164621988\n",
      "count_amostra: 32052\n",
      "iteration:  2700 , total_loss:  10.23645731608073\n",
      "count_amostra: 32412\n",
      "iteration:  2730 , total_loss:  10.163564745585123\n",
      "count_amostra: 32772\n",
      "iteration:  2760 , total_loss:  10.103153800964355\n",
      "count_amostra: 33132\n",
      "iteration:  2790 , total_loss:  10.251035181681315\n",
      "count_amostra: 33492\n",
      "iteration:  2820 , total_loss:  9.987998390197754\n",
      "count_amostra: 33852\n",
      "iteration:  2850 , total_loss:  10.26990524927775\n",
      "count_amostra: 34212\n",
      "iteration:  2880 , total_loss:  10.031509590148925\n",
      "count_amostra: 34572\n",
      "iteration:  2910 , total_loss:  10.074519570668539\n",
      "count_amostra: 34932\n",
      "iteration:  2940 , total_loss:  10.060526498158772\n",
      "count_amostra: 35292\n",
      "iteration:  2970 , total_loss:  10.072389888763428\n",
      "count_amostra: 35652\n",
      "iteration:  3000 , total_loss:  10.0708664894104\n",
      "count_amostra: 36012\n",
      "iteration:  3030 , total_loss:  10.010358619689942\n",
      "count_amostra: 36372\n",
      "iteration:  3060 , total_loss:  9.95312032699585\n",
      "count_amostra: 36732\n",
      "iteration:  3090 , total_loss:  9.93760347366333\n",
      "count_amostra: 37092\n",
      "iteration:  3120 , total_loss:  9.838759676615398\n",
      "count_amostra: 37452\n",
      "iteration:  3150 , total_loss:  9.821488157908123\n",
      "count_amostra: 37812\n",
      "iteration:  3180 , total_loss:  9.895059776306152\n",
      "count_amostra: 38172\n",
      "iteration:  3210 , total_loss:  9.859364986419678\n",
      "count_amostra: 38532\n",
      "iteration:  3240 , total_loss:  9.871478112538655\n",
      "count_amostra: 38892\n",
      "iteration:  3270 , total_loss:  9.82954381306966\n",
      "count_amostra: 39252\n",
      "iteration:  3300 , total_loss:  9.886597442626954\n",
      "count_amostra: 39612\n",
      "iteration:  3330 , total_loss:  9.926554203033447\n",
      "count_amostra: 39972\n",
      "iteration:  3360 , total_loss:  9.85204906463623\n",
      "count_amostra: 40332\n",
      "iteration:  3390 , total_loss:  9.794269243876139\n",
      "count_amostra: 40692\n",
      "iteration:  3420 , total_loss:  9.760730997721355\n",
      "count_amostra: 41052\n",
      "iteration:  3450 , total_loss:  9.781947676340739\n",
      "count_amostra: 41412\n",
      "iteration:  3480 , total_loss:  9.786160691579182\n",
      "count_amostra: 41772\n",
      "iteration:  3510 , total_loss:  9.782880147298178\n",
      "count_amostra: 42132\n",
      "iteration:  3540 , total_loss:  9.742759291330973\n",
      "count_amostra: 42492\n",
      "iteration:  3570 , total_loss:  9.805883344014486\n",
      "count_amostra: 42852\n",
      "iteration:  3600 , total_loss:  9.70750347773234\n",
      "count_amostra: 43212\n",
      "iteration:  3630 , total_loss:  9.600188732147217\n",
      "count_amostra: 43572\n",
      "iteration:  3660 , total_loss:  9.590251286824545\n",
      "count_amostra: 43932\n",
      "iteration:  3690 , total_loss:  9.66123275756836\n",
      "count_amostra: 44292\n",
      "iteration:  3720 , total_loss:  9.54373041788737\n",
      "count_amostra: 44652\n",
      "iteration:  3750 , total_loss:  9.749334907531738\n",
      "count_amostra: 45012\n",
      "iteration:  3780 , total_loss:  9.656540711720785\n",
      "count_amostra: 45372\n",
      "iteration:  3810 , total_loss:  9.631832440694174\n",
      "count_amostra: 45732\n",
      "iteration:  3840 , total_loss:  9.666851584116618\n",
      "count_amostra: 46092\n",
      "iteration:  3870 , total_loss:  9.49847542444865\n",
      "count_amostra: 46452\n",
      "iteration:  3900 , total_loss:  9.49162311553955\n",
      "count_amostra: 46812\n",
      "iteration:  3930 , total_loss:  9.567451699574788\n",
      "count_amostra: 47172\n",
      "iteration:  3960 , total_loss:  9.450160312652589\n",
      "count_amostra: 47532\n",
      "iteration:  3990 , total_loss:  9.476590633392334\n",
      "count_amostra: 47892\n",
      "iteration:  4020 , total_loss:  9.440134874979655\n",
      "count_amostra: 48252\n",
      "iteration:  4050 , total_loss:  9.542932224273681\n",
      "count_amostra: 48612\n",
      "iteration:  4080 , total_loss:  9.5001921971639\n",
      "count_amostra: 48972\n",
      "iteration:  4110 , total_loss:  9.534702014923095\n",
      "count_amostra: 49332\n",
      "iteration:  4140 , total_loss:  9.535984261830647\n",
      "count_amostra: 49692\n",
      "iteration:  4170 , total_loss:  9.355471769968668\n",
      "count_amostra: 50052\n",
      "iteration:  4200 , total_loss:  9.354973951975504\n",
      "count_amostra: 50412\n",
      "iteration:  4230 , total_loss:  9.34382438659668\n",
      "count_amostra: 50772\n",
      "iteration:  4260 , total_loss:  9.439482975006104\n",
      "count_amostra: 51132\n",
      "iteration:  4290 , total_loss:  9.136499214172364\n",
      "count_amostra: 51492\n",
      "iteration:  4320 , total_loss:  9.401445452372233\n",
      "count_amostra: 51852\n",
      "iteration:  4350 , total_loss:  9.577242374420166\n",
      "count_amostra: 52212\n",
      "iteration:  4380 , total_loss:  9.311571661631266\n",
      "count_amostra: 52572\n",
      "iteration:  4410 , total_loss:  9.369800917307536\n",
      "count_amostra: 52932\n",
      "iteration:  4440 , total_loss:  9.320643107096354\n",
      "count_amostra: 53292\n",
      "iteration:  4470 , total_loss:  9.26847546895345\n",
      "count_amostra: 53652\n",
      "iteration:  4500 , total_loss:  9.27850596110026\n",
      "count_amostra: 54012\n",
      "iteration:  4530 , total_loss:  9.320673433939616\n",
      "count_amostra: 54372\n",
      "iteration:  4560 , total_loss:  9.27118476231893\n",
      "count_amostra: 54732\n",
      "iteration:  4590 , total_loss:  9.318985112508138\n",
      "count_amostra: 55092\n",
      "iteration:  4620 , total_loss:  9.237802219390868\n",
      "count_amostra: 55452\n",
      "iteration:  4650 , total_loss:  9.313746929168701\n",
      "count_amostra: 55812\n",
      "iteration:  4680 , total_loss:  9.199673843383788\n",
      "count_amostra: 56172\n",
      "iteration:  4710 , total_loss:  9.252018260955811\n",
      "count_amostra: 56532\n",
      "iteration:  4740 , total_loss:  9.266367371877035\n",
      "count_amostra: 56892\n",
      "iteration:  4770 , total_loss:  9.361621602376301\n",
      "count_amostra: 57252\n",
      "iteration:  4800 , total_loss:  9.17336270014445\n",
      "count_amostra: 57612\n",
      "iteration:  4830 , total_loss:  9.166790103912353\n",
      "count_amostra: 57972\n",
      "iteration:  4860 , total_loss:  9.241107972462972\n",
      "count_amostra: 58332\n",
      "iteration:  4890 , total_loss:  9.2013245900472\n",
      "count_amostra: 58692\n",
      "iteration:  4920 , total_loss:  9.158264541625977\n",
      "count_amostra: 59052\n",
      "iteration:  4950 , total_loss:  9.184695116678874\n",
      "count_amostra: 59412\n",
      "iteration:  4980 , total_loss:  9.297219371795654\n",
      "count_amostra: 59772\n",
      "iteration:  5010 , total_loss:  9.159489154815674\n",
      "count_amostra: 60132\n",
      "iteration:  5040 , total_loss:  9.196828428904215\n",
      "count_amostra: 60492\n",
      "iteration:  5070 , total_loss:  9.128049182891846\n",
      "count_amostra: 60852\n",
      "iteration:  5100 , total_loss:  9.242807133992512\n",
      "count_amostra: 61212\n",
      "iteration:  5130 , total_loss:  9.146061992645263\n",
      "count_amostra: 61572\n",
      "iteration:  5160 , total_loss:  9.056128342946371\n",
      "count_amostra: 61932\n",
      "iteration:  5190 , total_loss:  9.25681864420573\n",
      "count_amostra: 62292\n",
      "iteration:  5220 , total_loss:  9.07904084523519\n",
      "count_amostra: 62652\n",
      "iteration:  5250 , total_loss:  9.121182378133138\n",
      "count_amostra: 63012\n",
      "iteration:  5280 , total_loss:  9.154304027557373\n",
      "count_amostra: 63372\n",
      "iteration:  5310 , total_loss:  9.03343407313029\n",
      "count_amostra: 63732\n",
      "iteration:  5340 , total_loss:  9.00570805867513\n",
      "count_amostra: 64092\n",
      "iteration:  5370 , total_loss:  9.182576497395834\n",
      "count_amostra: 64452\n",
      "iteration:  5400 , total_loss:  9.074410756429037\n",
      "count_amostra: 64812\n",
      "iteration:  5430 , total_loss:  9.058004315694173\n",
      "count_amostra: 65172\n",
      "iteration:  5460 , total_loss:  9.081885147094727\n",
      "count_amostra: 65532\n",
      "iteration:  5490 , total_loss:  9.075079409281413\n",
      "count_amostra: 65892\n",
      "iteration:  5520 , total_loss:  8.951426951090495\n",
      "count_amostra: 66252\n",
      "iteration:  5550 , total_loss:  9.179314390818279\n",
      "count_amostra: 66612\n",
      "iteration:  5580 , total_loss:  9.047587935129801\n",
      "count_amostra: 66972\n",
      "iteration:  5610 , total_loss:  9.026738611857096\n",
      "count_amostra: 67332\n",
      "iteration:  5640 , total_loss:  9.078764724731446\n",
      "count_amostra: 67692\n",
      "iteration:  5670 , total_loss:  8.980144484837849\n",
      "count_amostra: 68052\n",
      "iteration:  5700 , total_loss:  9.111363474527995\n",
      "count_amostra: 68412\n",
      "iteration:  5730 , total_loss:  9.053384526570637\n",
      "count_amostra: 68772\n",
      "iteration:  5760 , total_loss:  8.932198905944825\n",
      "count_amostra: 69132\n",
      "iteration:  5790 , total_loss:  8.929460144042968\n",
      "count_amostra: 69492\n",
      "iteration:  5820 , total_loss:  8.952046839396159\n",
      "count_amostra: 69852\n",
      "iteration:  5850 , total_loss:  9.139630095163982\n",
      "count_amostra: 70212\n",
      "iteration:  5880 , total_loss:  9.064921792348226\n",
      "count_amostra: 70572\n",
      "iteration:  5910 , total_loss:  9.053732617696125\n",
      "count_amostra: 70932\n",
      "iteration:  5940 , total_loss:  9.044225470225017\n",
      "count_amostra: 71292\n",
      "iteration:  5970 , total_loss:  8.948907152811687\n",
      "count_amostra: 71652\n",
      "iteration:  6000 , total_loss:  8.930273755391438\n",
      "count_amostra: 72012\n",
      "iteration:  6030 , total_loss:  8.927860609690349\n",
      "count_amostra: 72372\n",
      "iteration:  6060 , total_loss:  8.93039649327596\n",
      "count_amostra: 72732\n",
      "iteration:  6090 , total_loss:  8.913015492757161\n",
      "count_amostra: 73092\n",
      "iteration:  6120 , total_loss:  8.935482470194499\n",
      "count_amostra: 73452\n",
      "iteration:  6150 , total_loss:  8.97250034014384\n",
      "count_amostra: 73812\n",
      "iteration:  6180 , total_loss:  8.998899841308594\n",
      "count_amostra: 74172\n",
      "iteration:  6210 , total_loss:  8.931896654764811\n",
      "count_amostra: 74532\n",
      "iteration:  6240 , total_loss:  8.908497428894043\n",
      "count_amostra: 74892\n",
      "iteration:  6270 , total_loss:  8.892009925842284\n",
      "count_amostra: 75252\n",
      "iteration:  6300 , total_loss:  8.838301293055217\n",
      "count_amostra: 75612\n",
      "iteration:  6330 , total_loss:  8.894842179616292\n",
      "count_amostra: 75972\n",
      "iteration:  6360 , total_loss:  8.832141335805257\n",
      "count_amostra: 76332\n",
      "iteration:  6390 , total_loss:  8.87867685953776\n",
      "count_amostra: 76692\n",
      "iteration:  6420 , total_loss:  8.96895621617635\n",
      "count_amostra: 77052\n",
      "iteration:  6450 , total_loss:  8.830638376871745\n",
      "count_amostra: 77412\n",
      "iteration:  6480 , total_loss:  8.852003542582194\n",
      "count_amostra: 77772\n",
      "iteration:  6510 , total_loss:  8.850577322642009\n",
      "count_amostra: 78132\n",
      "iteration:  6540 , total_loss:  8.921515432993571\n",
      "count_amostra: 78492\n",
      "iteration:  6570 , total_loss:  8.902864344914754\n",
      "count_amostra: 78852\n",
      "iteration:  6600 , total_loss:  8.812407620747884\n",
      "count_amostra: 79212\n",
      "iteration:  6630 , total_loss:  8.814788786570231\n",
      "count_amostra: 79572\n",
      "iteration:  6660 , total_loss:  8.90868263244629\n",
      "count_amostra: 79932\n",
      "iteration:  6690 , total_loss:  8.85083179473877\n",
      "count_amostra: 80292\n",
      "iteration:  6720 , total_loss:  8.941264406840007\n",
      "count_amostra: 80652\n",
      "iteration:  6750 , total_loss:  8.877767817179363\n",
      "count_amostra: 81012\n",
      "iteration:  6780 , total_loss:  8.984654808044434\n",
      "count_amostra: 81372\n",
      "iteration:  6810 , total_loss:  8.776974391937255\n",
      "count_amostra: 81732\n",
      "iteration:  6840 , total_loss:  8.765737215677897\n",
      "count_amostra: 82092\n",
      "iteration:  6870 , total_loss:  8.755489826202393\n",
      "count_amostra: 82452\n",
      "iteration:  6900 , total_loss:  8.767959785461425\n",
      "count_amostra: 82812\n",
      "iteration:  6930 , total_loss:  8.891878986358643\n",
      "count_amostra: 83172\n",
      "iteration:  6960 , total_loss:  8.791906356811523\n",
      "count_amostra: 83532\n",
      "iteration:  6990 , total_loss:  8.757543166478476\n",
      "count_amostra: 83892\n",
      "iteration:  7020 , total_loss:  8.852563730875652\n",
      "count_amostra: 84252\n",
      "iteration:  7050 , total_loss:  8.839980665842692\n",
      "count_amostra: 84612\n",
      "iteration:  7080 , total_loss:  8.95832478205363\n",
      "count_amostra: 84972\n",
      "iteration:  7110 , total_loss:  8.827916622161865\n",
      "count_amostra: 85332\n",
      "iteration:  7140 , total_loss:  8.765122000376383\n",
      "count_amostra: 85692\n",
      "iteration:  7170 , total_loss:  8.743572807312011\n",
      "count_amostra: 86052\n",
      "iteration:  7200 , total_loss:  8.727966086069744\n",
      "count_amostra: 86412\n",
      "iteration:  7230 , total_loss:  8.923900747299195\n",
      "count_amostra: 86772\n",
      "iteration:  7260 , total_loss:  8.689547920227051\n",
      "count_amostra: 87132\n",
      "iteration:  7290 , total_loss:  8.751711050669352\n",
      "count_amostra: 87492\n",
      "iteration:  7320 , total_loss:  8.806451797485352\n",
      "count_amostra: 87852\n",
      "iteration:  7350 , total_loss:  8.777725569407146\n",
      "count_amostra: 88212\n",
      "iteration:  7380 , total_loss:  8.76361312866211\n",
      "count_amostra: 88572\n",
      "iteration:  7410 , total_loss:  8.789835166931152\n",
      "count_amostra: 88932\n",
      "iteration:  7440 , total_loss:  8.74197727839152\n",
      "count_amostra: 89292\n",
      "iteration:  7470 , total_loss:  8.837605158487955\n",
      "count_amostra: 89652\n",
      "iteration:  7500 , total_loss:  8.614913336435954\n",
      "count_amostra: 90012\n",
      "iteration:  7530 , total_loss:  8.761507129669189\n",
      "count_amostra: 90372\n",
      "iteration:  7560 , total_loss:  8.884134769439697\n",
      "count_amostra: 90732\n",
      "iteration:  7590 , total_loss:  8.834904003143311\n",
      "count_amostra: 91092\n",
      "iteration:  7620 , total_loss:  8.74346990585327\n",
      "count_amostra: 91452\n",
      "iteration:  7650 , total_loss:  8.830341974894205\n",
      "count_amostra: 91812\n",
      "iteration:  7680 , total_loss:  8.710076109568279\n",
      "count_amostra: 92172\n",
      "iteration:  7710 , total_loss:  8.837939008076985\n",
      "count_amostra: 92532\n",
      "iteration:  7740 , total_loss:  8.742137241363526\n",
      "count_amostra: 92892\n",
      "iteration:  7770 , total_loss:  8.720291026433308\n",
      "count_amostra: 93252\n",
      "iteration:  7800 , total_loss:  8.752700424194336\n",
      "count_amostra: 93612\n",
      "iteration:  7830 , total_loss:  8.658217684427898\n",
      "count_amostra: 93972\n",
      "iteration:  7860 , total_loss:  8.686037397384643\n",
      "count_amostra: 94332\n",
      "iteration:  7890 , total_loss:  8.733518314361572\n",
      "count_amostra: 94692\n",
      "iteration:  7920 , total_loss:  8.787508742014568\n",
      "count_amostra: 95052\n",
      "iteration:  7950 , total_loss:  8.689917929967244\n",
      "count_amostra: 95412\n",
      "iteration:  7980 , total_loss:  8.72905346552531\n",
      "count_amostra: 95772\n",
      "iteration:  8010 , total_loss:  8.742743905385336\n",
      "count_amostra: 96132\n",
      "iteration:  8040 , total_loss:  8.818954785664877\n",
      "count_amostra: 96492\n",
      "iteration:  8070 , total_loss:  8.837466462453206\n",
      "count_amostra: 96852\n",
      "iteration:  8100 , total_loss:  8.782943852742513\n",
      "count_amostra: 97212\n",
      "iteration:  8130 , total_loss:  8.713467947642009\n",
      "count_amostra: 97572\n",
      "iteration:  8160 , total_loss:  8.81715367635091\n",
      "count_amostra: 97932\n",
      "iteration:  8190 , total_loss:  8.814369360605875\n",
      "count_amostra: 98292\n",
      "iteration:  8220 , total_loss:  8.801922130584718\n",
      "count_amostra: 98652\n",
      "iteration:  8250 , total_loss:  8.676349544525147\n",
      "count_amostra: 99012\n",
      "iteration:  8280 , total_loss:  8.662222003936767\n",
      "count_amostra: 99372\n",
      "iteration:  8310 , total_loss:  8.726813983917236\n",
      "count_amostra: 99732\n"
     ]
    }
   ],
   "source": [
    "# Only show the progress bar once on each machine.\n",
    "# progress_bar = tqdm(range(max_train_steps), disable=not accelerator.is_local_main_process)\n",
    "completed_steps = 0\n",
    "starting_epoch = 0\n",
    "\n",
    "# update the progress_bar if load from checkpoint\n",
    "total_loss=0\n",
    "# progress_bar.update(completed_steps)\n",
    "steps_log=30\n",
    "count_amostra=0\n",
    "num_train_epochs=1\n",
    "for epoch in range(starting_epoch, num_train_epochs):\n",
    "    model.train()\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "        # with accelerator.accumulate(model):\n",
    "        spans_start=batch['spans_start']\n",
    "        spans_end=batch['spans_end']\n",
    "        spans_start=[[int(j) for j in i.split()] for i in spans_start]\n",
    "        spans_end=[[int(j) for j in i.split()] for i in spans_end]    \n",
    "\n",
    "        logits_mlm,logits_span=model(batch['input_ids'],spans_start,spans_end) \n",
    "        \n",
    "        span_tokens=[]\n",
    "        for i in range(len(spans_start)):\n",
    "            for j in range(len(spans_start[i])):\n",
    "                s=spans_start[i][j] - 1\n",
    "                e=spans_end[i][j] + 1\n",
    "                if e>=len(batch['input_ids'][i]):\n",
    "                    e=len(batch['input_ids'][i])-1\n",
    "                if s<0:\n",
    "                    s=0\n",
    "                for k in range(s,e):\n",
    "                    span_tokens.append(batch['unmasked_tokens'][i][k])\n",
    "        span_tokens=torch.tensor(span_tokens).to(batch['input_ids'].device)\n",
    "        span_tokens=torch.unsqueeze(span_tokens,0)\n",
    "        #print('span_tokens: ',span_tokens.shape)\n",
    "        #print(batch['unmasked_tokens'].shape)\n",
    "        #print(logits_mlm.transpose(1, 2).shape)\n",
    "        #print(logits_span.transpose(1, 2).shape)\n",
    "        loss_mlm = F.cross_entropy(logits_mlm.transpose(1, 2),batch['unmasked_tokens'],ignore_index = PAD_ID)\n",
    "        loss_span = F.cross_entropy(logits_span.transpose(1, 2),span_tokens,ignore_index = PAD_ID)\n",
    "        \n",
    "        loss=loss_mlm+loss_span\n",
    "        \n",
    "        #save \n",
    "        if count_amostra%10000==0:\n",
    "            torch.save(model.state_dict(), str('models/'+f\"latest_model.pt\"))\n",
    "            \n",
    "        count_amostra+=int(len(batch['input_ids']))\n",
    "        \n",
    "        #print(loss)\n",
    "        \n",
    "        total_loss += loss.detach().float().cpu().numpy().item()\n",
    "        accelerator.backward(loss)\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()       \n",
    "        \n",
    "        if step%steps_log==0:\n",
    "            print('iteration: ',step,', total_loss: ',total_loss/steps_log)\n",
    "            print('count_amostra:',count_amostra)\n",
    "            total_loss=0\n",
    "    # Checks if the accelerator has performed an optimization step behind the scenes\n",
    "    if accelerator.sync_gradients:\n",
    "        # progress_bar.update(1)\n",
    "        completed_steps += 1\n",
    "    if completed_steps >= max_train_steps:\n",
    "        break\n",
    "    model.eval()\n",
    "    losses = [] \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 100min para 100k amostras (32k vocab, 512 tokens, batch size 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_model=model.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#convert to huggingface Transformers\n",
    "\n",
    "from transformers import BertConfig, BertForMaskedLM, BertTokenizerFast\n",
    "\n",
    "config = BertConfig(\n",
    "    vocab_size=VOCAB_SIZE+8,\n",
    "    max_position_embeddings=MODEL_MAX_SEQ_LEN,\n",
    "    num_attention_heads=12,\n",
    "    num_hidden_layers=12,\n",
    "    type_vocab_size=2,\n",
    ")\n",
    "\n",
    "bert_model=BertForMaskedLM(config=config)\n",
    "bert_model.bert=torch_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model.save_pretrained('models/bert_spanbert')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1m39s\n",
    "# 3m 27s sequence of 256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trainer hf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "        num_rows: 4000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"squad\", split=\"train[:5000]\")\n",
    "dataset = dataset.train_test_split(test_size=0.2)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = MODEL_MAX_SEQ_LEN#384\n",
    "stride = 128\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "def preprocess_validation_examples(examples):\n",
    "    questions = [q.strip() for q in examples[\"question\"]]\n",
    "    inputs = tokenizer(\n",
    "        questions,\n",
    "        examples[\"context\"],\n",
    "        max_length=max_length,\n",
    "        truncation=\"only_second\",\n",
    "        stride=stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    sample_map = inputs.pop(\"overflow_to_sample_mapping\")\n",
    "    example_ids = []\n",
    "\n",
    "    for i in range(len(inputs[\"input_ids\"])):\n",
    "        sample_idx = sample_map[i]\n",
    "        example_ids.append(examples[\"id\"][sample_idx])\n",
    "\n",
    "        sequence_ids = inputs.sequence_ids(i)\n",
    "        offset = inputs[\"offset_mapping\"][i]\n",
    "        inputs[\"offset_mapping\"][i] = [\n",
    "            o if sequence_ids[k] == 1 else None for k, o in enumerate(offset)\n",
    "        ]\n",
    "\n",
    "    inputs[\"example_id\"] = example_ids\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1000/1000 [00:00<00:00, 4986.67 examples/s]\n"
     ]
    }
   ],
   "source": [
    "validation_dataset = dataset[\"test\"].map(\n",
    "    preprocess_validation_examples,\n",
    "    batched=True,\n",
    "    remove_columns=dataset[\"test\"].column_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    questions = [q.strip() for q in examples[\"question\"]]\n",
    "    inputs = tokenizer(\n",
    "        questions,\n",
    "        examples[\"context\"],\n",
    "        max_length=tokenizer.model_max_length,\n",
    "        truncation=\"only_second\",\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    offset_mapping = inputs.pop(\"offset_mapping\")\n",
    "    answers = examples[\"answers\"]\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "\n",
    "    for i, offset in enumerate(offset_mapping):\n",
    "        answer = answers[i]\n",
    "        start_char = answer[\"answer_start\"][0]\n",
    "        end_char = answer[\"answer_start\"][0] + len(answer[\"text\"][0])\n",
    "        sequence_ids = inputs.sequence_ids(i)\n",
    "        # print('sequence_ids: ',sequence_ids)\n",
    "        # print('sequence_ids: ',len(sequence_ids))\n",
    "\n",
    "        # Find the start and end of the context\n",
    "        idx = 0\n",
    "        while sequence_ids[idx] != 1:\n",
    "            idx += 1\n",
    "        context_start = idx\n",
    "        while sequence_ids[idx] == 1 and idx < len(sequence_ids) - 1:\n",
    "            idx += 1\n",
    "        context_end = idx - 1\n",
    "\n",
    "        # If the answer is not fully inside the context, label it (0, 0)\n",
    "        if offset[context_start][0] > end_char or offset[context_end][1] < start_char:\n",
    "            start_positions.append(0)\n",
    "            end_positions.append(0)\n",
    "        else:\n",
    "            # Otherwise it's the start and end token positions\n",
    "            idx = context_start\n",
    "            while idx <= context_end and offset[idx][0] <= start_char:\n",
    "                idx += 1\n",
    "            start_positions.append(idx - 1)\n",
    "\n",
    "            idx = context_end\n",
    "            while idx >= context_start and offset[idx][1] >= end_char:\n",
    "                idx -= 1\n",
    "            end_positions.append(idx + 1)\n",
    "\n",
    "    inputs[\"start_positions\"] = start_positions\n",
    "    inputs[\"end_positions\"] = end_positions\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 4000/4000 [00:01<00:00, 2606.58 examples/s]\n",
      "Map: 100%|██████████| 1000/1000 [00:00<00:00, 2577.99 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import DefaultDataCollator\n",
    "\n",
    "tokenized_dataset = dataset.map(preprocess_function, batched=True, remove_columns=dataset['train'].column_names,batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at /home/miu/Projects/NLPLab/berts/models/bert_spanbert and are newly initialized: ['bert.encoder.layer.5.attention.output.dense.weight', 'bert.encoder.layer.3.output.LayerNorm.weight', 'bert.encoder.layer.0.output.dense.weight', 'bert.encoder.layer.6.attention.self.key.weight', 'bert.encoder.layer.11.intermediate.dense.weight', 'bert.encoder.layer.2.attention.self.query.weight', 'bert.encoder.layer.4.output.dense.weight', 'bert.encoder.layer.4.output.LayerNorm.bias', 'bert.encoder.layer.2.attention.self.value.bias', 'bert.encoder.layer.4.intermediate.dense.bias', 'bert.encoder.layer.8.attention.output.dense.weight', 'bert.encoder.layer.5.output.LayerNorm.weight', 'bert.encoder.layer.10.intermediate.dense.bias', 'bert.encoder.layer.10.output.LayerNorm.bias', 'bert.encoder.layer.10.attention.self.key.weight', 'bert.encoder.layer.11.attention.self.query.weight', 'bert.encoder.layer.7.intermediate.dense.weight', 'bert.encoder.layer.6.attention.self.value.bias', 'bert.encoder.layer.2.output.LayerNorm.bias', 'bert.encoder.layer.6.attention.output.dense.weight', 'bert.encoder.layer.10.attention.self.key.bias', 'bert.encoder.layer.1.output.dense.weight', 'bert.encoder.layer.11.attention.self.key.bias', 'bert.embeddings.position_embeddings.weight', 'bert.encoder.layer.9.attention.self.key.weight', 'bert.encoder.layer.5.attention.self.key.bias', 'bert.encoder.layer.3.intermediate.dense.weight', 'bert.encoder.layer.0.attention.self.value.bias', 'bert.encoder.layer.4.attention.self.query.weight', 'bert.encoder.layer.6.attention.self.query.weight', 'bert.encoder.layer.8.output.dense.bias', 'bert.encoder.layer.8.output.dense.weight', 'bert.encoder.layer.10.attention.output.LayerNorm.weight', 'bert.encoder.layer.0.attention.output.LayerNorm.bias', 'bert.encoder.layer.3.attention.self.key.bias', 'bert.encoder.layer.7.attention.output.dense.weight', 'bert.encoder.layer.10.attention.self.query.bias', 'bert.encoder.layer.7.attention.output.dense.bias', 'bert.encoder.layer.2.intermediate.dense.bias', 'bert.encoder.layer.2.attention.self.key.bias', 'bert.encoder.layer.4.attention.output.dense.bias', 'bert.encoder.layer.7.intermediate.dense.bias', 'bert.encoder.layer.11.output.dense.weight', 'bert.encoder.layer.11.output.dense.bias', 'bert.encoder.layer.1.attention.output.dense.weight', 'bert.encoder.layer.3.output.dense.bias', 'bert.encoder.layer.6.attention.self.query.bias', 'bert.encoder.layer.6.output.dense.bias', 'bert.encoder.layer.11.attention.self.query.bias', 'bert.embeddings.LayerNorm.bias', 'bert.encoder.layer.3.attention.output.LayerNorm.weight', 'bert.encoder.layer.4.attention.output.LayerNorm.weight', 'bert.encoder.layer.6.attention.self.key.bias', 'bert.encoder.layer.1.attention.output.LayerNorm.bias', 'bert.encoder.layer.3.attention.self.value.bias', 'bert.encoder.layer.6.attention.output.LayerNorm.weight', 'bert.encoder.layer.1.output.LayerNorm.weight', 'bert.encoder.layer.0.output.LayerNorm.weight', 'bert.encoder.layer.3.attention.output.dense.weight', 'bert.encoder.layer.7.output.LayerNorm.bias', 'bert.encoder.layer.3.output.LayerNorm.bias', 'bert.encoder.layer.4.attention.self.value.bias', 'bert.encoder.layer.9.attention.output.LayerNorm.bias', 'bert.encoder.layer.5.attention.self.query.weight', 'bert.encoder.layer.0.attention.self.key.weight', 'bert.encoder.layer.2.attention.output.dense.weight', 'bert.encoder.layer.4.attention.output.dense.weight', 'bert.encoder.layer.7.attention.self.query.weight', 'bert.encoder.layer.0.intermediate.dense.weight', 'bert.encoder.layer.1.intermediate.dense.bias', 'bert.encoder.layer.5.attention.self.value.weight', 'bert.encoder.layer.7.output.dense.bias', 'bert.encoder.layer.5.attention.output.dense.bias', 'bert.encoder.layer.4.attention.self.key.weight', 'bert.encoder.layer.6.intermediate.dense.weight', 'bert.encoder.layer.7.attention.self.query.bias', 'bert.encoder.layer.8.attention.self.value.bias', 'bert.encoder.layer.9.intermediate.dense.weight', 'bert.encoder.layer.11.attention.output.LayerNorm.bias', 'bert.encoder.layer.2.intermediate.dense.weight', 'bert.encoder.layer.8.output.LayerNorm.weight', 'bert.encoder.layer.6.output.LayerNorm.weight', 'bert.encoder.layer.0.attention.output.dense.weight', 'bert.encoder.layer.3.attention.self.query.bias', 'bert.encoder.layer.1.attention.self.key.weight', 'bert.encoder.layer.7.attention.output.LayerNorm.bias', 'bert.encoder.layer.4.intermediate.dense.weight', 'bert.encoder.layer.5.intermediate.dense.bias', 'bert.encoder.layer.2.attention.self.value.weight', 'bert.encoder.layer.3.intermediate.dense.bias', 'bert.encoder.layer.0.attention.self.key.bias', 'bert.encoder.layer.4.attention.self.query.bias', 'bert.encoder.layer.2.attention.self.query.bias', 'bert.encoder.layer.6.intermediate.dense.bias', 'bert.encoder.layer.4.output.dense.bias', 'bert.encoder.layer.1.output.dense.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.weight', 'bert.encoder.layer.7.attention.self.value.bias', 'bert.encoder.layer.9.output.LayerNorm.weight', 'bert.encoder.layer.7.attention.self.key.weight', 'bert.encoder.layer.2.attention.self.key.weight', 'bert.encoder.layer.7.attention.self.key.bias', 'bert.encoder.layer.11.attention.output.LayerNorm.weight', 'bert.encoder.layer.5.attention.output.LayerNorm.weight', 'bert.encoder.layer.9.attention.output.LayerNorm.weight', 'bert.encoder.layer.0.intermediate.dense.bias', 'bert.encoder.layer.1.attention.self.value.bias', 'bert.encoder.layer.9.attention.self.query.bias', 'bert.encoder.layer.5.intermediate.dense.weight', 'bert.encoder.layer.5.attention.self.value.bias', 'bert.encoder.layer.8.attention.self.query.bias', 'bert.encoder.layer.7.output.LayerNorm.weight', 'bert.encoder.layer.4.attention.self.value.weight', 'bert.encoder.layer.10.attention.self.value.bias', 'bert.encoder.layer.2.output.dense.weight', 'bert.encoder.layer.3.attention.self.query.weight', 'bert.encoder.layer.1.attention.self.query.bias', 'bert.encoder.layer.9.attention.self.value.bias', 'bert.encoder.layer.5.output.LayerNorm.bias', 'bert.encoder.layer.10.output.dense.bias', 'bert.encoder.layer.11.output.LayerNorm.bias', 'bert.encoder.layer.10.output.LayerNorm.weight', 'bert.encoder.layer.10.attention.output.LayerNorm.bias', 'bert.encoder.layer.8.attention.self.value.weight', 'bert.encoder.layer.9.attention.output.dense.bias', 'bert.encoder.layer.10.attention.output.dense.bias', 'qa_outputs.weight', 'bert.embeddings.word_embeddings.weight', 'bert.encoder.layer.2.attention.output.LayerNorm.weight', 'bert.encoder.layer.6.attention.self.value.weight', 'bert.encoder.layer.11.output.LayerNorm.weight', 'bert.encoder.layer.8.attention.self.key.weight', 'bert.encoder.layer.4.attention.output.LayerNorm.bias', 'bert.encoder.layer.5.attention.self.key.weight', 'bert.encoder.layer.0.attention.self.query.weight', 'bert.encoder.layer.1.attention.output.LayerNorm.weight', 'bert.encoder.layer.11.attention.self.value.weight', 'bert.encoder.layer.11.attention.self.value.bias', 'bert.encoder.layer.5.output.dense.weight', 'bert.encoder.layer.9.intermediate.dense.bias', 'bert.encoder.layer.7.attention.self.value.weight', 'bert.encoder.layer.0.attention.self.value.weight', 'bert.encoder.layer.0.attention.output.dense.bias', 'bert.encoder.layer.9.output.dense.bias', 'bert.encoder.layer.2.attention.output.LayerNorm.bias', 'bert.encoder.layer.11.intermediate.dense.bias', 'bert.encoder.layer.0.output.LayerNorm.bias', 'bert.encoder.layer.10.attention.self.value.weight', 'bert.encoder.layer.10.attention.output.dense.weight', 'bert.encoder.layer.2.attention.output.dense.bias', 'bert.encoder.layer.6.attention.output.dense.bias', 'bert.encoder.layer.1.attention.self.key.bias', 'bert.encoder.layer.6.output.LayerNorm.bias', 'bert.encoder.layer.3.attention.self.value.weight', 'bert.encoder.layer.9.output.LayerNorm.bias', 'bert.encoder.layer.4.attention.self.key.bias', 'bert.encoder.layer.8.output.LayerNorm.bias', 'bert.encoder.layer.11.attention.self.key.weight', 'qa_outputs.bias', 'bert.encoder.layer.10.output.dense.weight', 'bert.encoder.layer.10.attention.self.query.weight', 'bert.encoder.layer.0.attention.self.query.bias', 'bert.encoder.layer.4.output.LayerNorm.weight', 'bert.encoder.layer.8.attention.output.dense.bias', 'bert.encoder.layer.11.attention.output.dense.weight', 'bert.encoder.layer.9.attention.self.key.bias', 'bert.encoder.layer.1.attention.self.query.weight', 'bert.encoder.layer.8.attention.self.key.bias', 'bert.embeddings.token_type_embeddings.weight', 'bert.encoder.layer.1.attention.output.dense.bias', 'bert.encoder.layer.1.attention.self.value.weight', 'bert.encoder.layer.5.output.dense.bias', 'bert.encoder.layer.3.attention.self.key.weight', 'bert.encoder.layer.8.attention.output.LayerNorm.bias', 'bert.encoder.layer.1.intermediate.dense.weight', 'bert.encoder.layer.10.intermediate.dense.weight', 'bert.encoder.layer.8.intermediate.dense.bias', 'bert.encoder.layer.2.output.LayerNorm.weight', 'bert.encoder.layer.9.attention.self.query.weight', 'bert.encoder.layer.5.attention.output.LayerNorm.bias', 'bert.encoder.layer.9.attention.self.value.weight', 'bert.encoder.layer.1.output.LayerNorm.bias', 'bert.encoder.layer.0.output.dense.bias', 'bert.encoder.layer.9.attention.output.dense.weight', 'bert.encoder.layer.6.attention.output.LayerNorm.bias', 'bert.embeddings.LayerNorm.weight', 'bert.encoder.layer.3.attention.output.LayerNorm.bias', 'bert.encoder.layer.2.output.dense.bias', 'bert.encoder.layer.7.attention.output.LayerNorm.weight', 'bert.encoder.layer.8.attention.output.LayerNorm.weight', 'bert.encoder.layer.6.output.dense.weight', 'bert.encoder.layer.5.attention.self.query.bias', 'bert.encoder.layer.3.output.dense.weight', 'bert.encoder.layer.7.output.dense.weight', 'bert.encoder.layer.11.attention.output.dense.bias', 'bert.encoder.layer.9.output.dense.weight', 'bert.encoder.layer.8.attention.self.query.weight', 'bert.encoder.layer.3.attention.output.dense.bias', 'bert.encoder.layer.8.intermediate.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "from transformers import TrainingArguments\n",
    "from transformers import AutoModelForQuestionAnswering, AutoTokenizer\n",
    "\n",
    "#BertForMaskedLM\n",
    "from transformers import BertForMaskedLM\n",
    "\n",
    "args = TrainingArguments(\n",
    "    \"bert-finetuned-squad\",\n",
    "    evaluation_strategy=\"no\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    fp16=True,\n",
    "    push_to_hub=False,\n",
    ")\n",
    "# bert_model = BertForMaskedLM.from_pretrained(pretrained_model_name_or_path='bert-base-uncased')\n",
    "# bert_model = AutoModelForQuestionAnswering.from_pretrained(\"bert-base-uncased\")\n",
    "# bert_model = AutoModelForQuestionAnswering.from_pretrained(\"/home/kiki/dados_servidor/Servidor/NLPLab/berts/models/bert_spanbert\")\n",
    "if 'miu' in cur_dir:\n",
    "    bert_model = AutoModelForQuestionAnswering.from_pretrained(\"/home/miu/Projects/NLPLab/berts/models/bert_spanbert\")\n",
    "else:\n",
    "    bert_model = AutoModelForQuestionAnswering.from_pretrained(\"/home/kiki/dados_servidor/Servidor/NLPLab/berts/models/bert_spanbert\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 500/2500 [00:39<02:36, 12.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.7696, 'learning_rate': 1.6000000000000003e-05, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 1000/2500 [01:20<01:59, 12.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.1061, 'learning_rate': 1.2e-05, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 1500/2500 [02:00<01:18, 12.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.7644, 'learning_rate': 8.000000000000001e-06, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 2000/2500 [02:40<00:39, 12.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.5092, 'learning_rate': 4.000000000000001e-06, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2500/2500 [03:20<00:00, 12.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.3401, 'learning_rate': 8e-09, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2500/2500 [03:21<00:00, 12.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 201.6545, 'train_samples_per_second': 99.18, 'train_steps_per_second': 12.397, 'train_loss': 3.897864306640625, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2500, training_loss=3.897864306640625, metrics={'train_runtime': 201.6545, 'train_samples_per_second': 99.18, 'train_steps_per_second': 12.397, 'train_loss': 3.897864306640625, 'epoch': 5.0})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=bert_model,\n",
    "    args=args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import collections\n",
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"squad\")\n",
    "n_best = 20\n",
    "max_answer_length = 30\n",
    "predicted_answers = []\n",
    "\n",
    "def compute_metrics(start_logits, end_logits, features, examples):\n",
    "    example_to_features = collections.defaultdict(list)\n",
    "    for idx, feature in enumerate(features):\n",
    "        example_to_features[feature[\"example_id\"]].append(idx)\n",
    "\n",
    "    predicted_answers = []\n",
    "    for example in tqdm(examples):\n",
    "        example_id = example[\"id\"]\n",
    "        context = example[\"context\"]\n",
    "        answers = []\n",
    "\n",
    "        # Loop through all features associated with that example\n",
    "        for feature_index in example_to_features[example_id]:\n",
    "            start_logit = start_logits[feature_index]\n",
    "            end_logit = end_logits[feature_index]\n",
    "            offsets = features[feature_index][\"offset_mapping\"]\n",
    "\n",
    "            start_indexes = np.argsort(start_logit)[-1 : -n_best - 1 : -1].tolist()\n",
    "            end_indexes = np.argsort(end_logit)[-1 : -n_best - 1 : -1].tolist()\n",
    "            for start_index in start_indexes:\n",
    "                for end_index in end_indexes:\n",
    "                    # Skip answers that are not fully in the context\n",
    "                    if offsets[start_index] is None or offsets[end_index] is None:\n",
    "                        continue\n",
    "                    # Skip answers with a length that is either < 0 or > max_answer_length\n",
    "                    if (\n",
    "                        end_index < start_index\n",
    "                        or end_index - start_index + 1 > max_answer_length\n",
    "                    ):\n",
    "                        continue\n",
    "\n",
    "                    answer = {\n",
    "                        \"text\": context[offsets[start_index][0] : offsets[end_index][1]],\n",
    "                        \"logit_score\": start_logit[start_index] + end_logit[end_index],\n",
    "                    }\n",
    "                    answers.append(answer)\n",
    "\n",
    "        # Select the answer with the best score\n",
    "        if len(answers) > 0:\n",
    "            best_answer = max(answers, key=lambda x: x[\"logit_score\"])\n",
    "            predicted_answers.append(\n",
    "                {\"id\": example_id, \"prediction_text\": best_answer[\"text\"]}\n",
    "            )\n",
    "        else:\n",
    "            predicted_answers.append({\"id\": example_id, \"prediction_text\": \"\"})\n",
    "\n",
    "    theoretical_answers = [{\"id\": ex[\"id\"], \"answers\": ex[\"answers\"]} for ex in examples]\n",
    "    return metric.compute(predictions=predicted_answers, references=theoretical_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 126/126 [00:02<00:00, 44.57it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 1592.93it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'exact_match': 4.1, 'f1': 9.138038735196309}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import evaluate\n",
    "\n",
    "predictions, _, _ = trainer.predict(validation_dataset)\n",
    "start_logits, end_logits = predictions\n",
    "compute_metrics(start_logits, end_logits, validation_dataset, dataset[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "615ecaf565ec41b191455553ed20c811",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'exact_match': 1.8, 'f1': 7.352985910764617}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import evaluate\n",
    "\n",
    "predictions, _, _ = trainer.predict(validation_dataset)\n",
    "start_logits, end_logits = predictions\n",
    "compute_metrics(start_logits, end_logits, validation_dataset, dataset[\"test\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    questions = [q.strip() for q in examples[\"question\"]]\n",
    "    inputs = tokenizer(\n",
    "        questions,\n",
    "        examples[\"context\"],\n",
    "        max_length=MODEL_MAX_SEQ_LEN,\n",
    "        truncation=\"only_second\",\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    offset_mapping = inputs.pop(\"offset_mapping\")\n",
    "    answers = examples[\"answers\"]\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "\n",
    "    for i, offset in enumerate(offset_mapping):\n",
    "        answer = answers[i]\n",
    "        start_char = answer[\"answer_start\"][0]\n",
    "        end_char = answer[\"answer_start\"][0] + len(answer[\"text\"][0])\n",
    "        sequence_ids = inputs.sequence_ids(i)\n",
    "        # print('sequence_ids: ',sequence_ids)\n",
    "        # print('sequence_ids: ',len(sequence_ids))\n",
    "\n",
    "        # Find the start and end of the context\n",
    "        idx = 0\n",
    "        while sequence_ids[idx] != 1:\n",
    "            idx += 1\n",
    "        context_start = idx\n",
    "        while sequence_ids[idx] == 1 and idx < len(sequence_ids) - 1:\n",
    "            idx += 1\n",
    "        context_end = idx - 1\n",
    "\n",
    "        # If the answer is not fully inside the context, label it (0, 0)\n",
    "        if offset[context_start][0] > end_char or offset[context_end][1] < start_char:\n",
    "            start_positions.append(0)\n",
    "            end_positions.append(0)\n",
    "        else:\n",
    "            # Otherwise it's the start and end token positions\n",
    "            idx = context_start\n",
    "            while idx <= context_end and offset[idx][0] <= start_char:\n",
    "                idx += 1\n",
    "            start_positions.append(idx - 1)\n",
    "\n",
    "            idx = context_end\n",
    "            while idx >= context_start and offset[idx][1] >= end_char:\n",
    "                idx -= 1\n",
    "            end_positions.append(idx + 1)\n",
    "\n",
    "    inputs[\"start_positions\"] = start_positions\n",
    "    inputs[\"end_positions\"] = end_positions\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0cd68e925fa419797249af0b0b3f7e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58a140775ef843948e972b299929ead9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import DefaultDataCollator\n",
    "\n",
    "tokenized_dataset = dataset.map(preprocess_function, batched=True, remove_columns=dataset['train'].column_names,batch_size=4)\n",
    "\n",
    "data_collator = DefaultDataCollator()\n",
    "\n",
    "#data loader\n",
    "train_dataloader = DataLoader(\n",
    "        tokenized_dataset['train'], shuffle=True, batch_size=DEVICE_BATCH_SIZE,collate_fn=data_collator\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'token_type_ids', 'attention_mask', 'start_positions', 'end_positions'],\n",
       "    num_rows: 1000\n",
       "})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hidden layer extractor class, for magically adding adapter to language model to be pretrained\n",
    "class HiddenLayerExtractor(nn.Module):\n",
    "    def __init__(self, net, layer = -2):\n",
    "        super().__init__()\n",
    "        self.net = net\n",
    "        self.layer = layer\n",
    "\n",
    "        self.hidden = None\n",
    "        self.hook_registered = False\n",
    "\n",
    "    def _find_layer(self):\n",
    "        if type(self.layer) == str:\n",
    "            modules = dict([*self.net.named_modules()])\n",
    "            return modules.get(self.layer, None)\n",
    "        elif type(self.layer) == int:\n",
    "            children = [*self.net.children()]\n",
    "            return children[self.layer]\n",
    "        return None\n",
    "\n",
    "    def _hook(self, _, __, output):\n",
    "        self.hidden = output\n",
    "\n",
    "    def _register_hook(self):\n",
    "        layer = self._find_layer()\n",
    "        assert layer is not None, f'hidden layer ({self.layer}) not found'\n",
    "        handle = layer.register_forward_hook(self._hook)\n",
    "        self.hook_registered = True\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.layer == -1:\n",
    "            return self.net(x)\n",
    "\n",
    "        if not self.hook_registered:\n",
    "            self._register_hook()\n",
    "\n",
    "        _ = self.net(x)\n",
    "        hidden = self.hidden\n",
    "        self.hidden = None\n",
    "        assert hidden is not None, f'hidden layer {self.layer} never emitted an output'\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_extractor = HiddenLayerExtractor(model.model, -2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "import torch\n",
    "import os\n",
    "from torch.utils.data import  DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "class Classifier(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Classifier, self).__init__()\n",
    "        # self.l1 = model._modules['model']\n",
    "        self.l1 = model_extractor\n",
    "        self.l2 = torch.nn.Dropout(0.3)\n",
    "        self.fl = torch.nn.Flatten()\n",
    "        self.s = torch.nn.Linear(768*(MODEL_MAX_SEQ_LEN), 768)\n",
    "        self.e = torch.nn.Linear(768*(MODEL_MAX_SEQ_LEN), 768)\n",
    "        \n",
    "    \n",
    "    def forward(self, ids):\n",
    "        words_embs= self.l1(ids)\n",
    "        # print('l1:',words_embs.shape)\n",
    "        output = self.l2(words_embs)\n",
    "        # print('l2:',output.shape)\n",
    "        output = self.fl(output)\n",
    "        # print('fl:',output.shape)\n",
    "        s_output = self.s(output)\n",
    "        e_output = self.e(output)\n",
    "        return s_output,e_output,words_embs\n",
    "\n",
    "classifier_model = Classifier()\n",
    "classifier_model=classifier_model.to('cuda')\n",
    "optimizer = torch.optim.Adam(params =  classifier_model.parameters(), lr=5e-5,weight_decay=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "\n",
    "classifier_model.train()\n",
    "\n",
    "def loss_fn(outputs, targets):\n",
    "    loss=torch.nn.CrossEntropyLoss()(outputs.to('cpu'), targets.to('cpu'))\n",
    "    return loss\n",
    "\n",
    "def get_index(s_embs,e_embs,words_embs,logits_only=False):\n",
    "    s_index=torch.bmm(words_embs,s_embs.unsqueeze(2)).squeeze(2)\n",
    "    e_index=torch.bmm(words_embs,e_embs.unsqueeze(2)).squeeze(2)\n",
    "    if logits_only:\n",
    "        return s_index,e_index\n",
    "\n",
    "    s_index=torch.softmax(s_index,dim=1)\n",
    "    e_index=torch.softmax(e_index,dim=1)\n",
    "    return s_index,e_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_167542/1414205731.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input_ids=torch.tensor(data['input_ids']).to('cuda', dtype = torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss:  0.11083119392395019\n",
      "Epoch: 1, Loss:  0.11102809906005859\n",
      "Epoch: 2, Loss:  0.11102415084838867\n",
      "Epoch: 3, Loss:  0.11083263397216797\n",
      "Epoch: 4, Loss:  0.1101995849609375\n"
     ]
    }
   ],
   "source": [
    "#FINETUNING\n",
    "epochs=5\n",
    "running_loss=0\n",
    "for i in range(epochs):\n",
    "    for j,data in enumerate(train_dataloader):\n",
    "        input_ids=torch.tensor(data['input_ids']).to('cuda', dtype = torch.long)\n",
    "        spans_start = data['start_positions']\n",
    "        spans_end = data['end_positions']\n",
    "\n",
    "        s_embs,e_embs,words_embs = classifier_model(input_ids)\n",
    "        s_index,e_index=get_index(s_embs,e_embs,words_embs)\n",
    "        start_loss = loss_fn(s_index,spans_start)\n",
    "        end_loss = loss_fn(e_index,spans_end)        \n",
    "        \n",
    "        loss=(start_loss+end_loss)/2\n",
    "        running_loss+=loss.item()\n",
    "        if j%50==0:\n",
    "            # print(s_index.argmax(dim=1))\n",
    "            # print(e_index.argmax(dim=1))\n",
    "            # print(targets[:,0])\n",
    "            # print(targets[:,1])\n",
    "            print(f'Epoch: {i}, Loss:  {running_loss/50}')\n",
    "            running_loss=0\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #evaluate\n",
    "# from tqdm.auto import tqdm\n",
    "# import evaluate\n",
    "# import collections\n",
    "# import numpy as np\n",
    "\n",
    "# metric = evaluate.load(\"squad\")\n",
    "# n_best = 20\n",
    "# max_answer_length = 30\n",
    "# predicted_answers = []\n",
    "\n",
    "# def compute_metrics(start_logits, end_logits, features, examples):\n",
    "#     example_to_features = collections.defaultdict(list)\n",
    "#     for idx, feature in enumerate(features):\n",
    "#         example_to_features[feature[\"example_id\"]].append(idx)\n",
    "\n",
    "#     predicted_answers = []\n",
    "#     for example in tqdm(examples):\n",
    "#         example_id = example[\"id\"]\n",
    "#         context = example[\"context\"]\n",
    "#         answers = []\n",
    "\n",
    "#         # Loop through all features associated with that example\n",
    "#         for feature_index in example_to_features[example_id]:\n",
    "#             start_logit = start_logits[feature_index]\n",
    "#             end_logit = end_logits[feature_index]\n",
    "#             offsets = features[feature_index][\"offset_mapping\"]\n",
    "\n",
    "#             start_indexes = np.argsort(start_logit)[-1 : -n_best - 1 : -1].tolist()\n",
    "#             end_indexes = np.argsort(end_logit)[-1 : -n_best - 1 : -1].tolist()\n",
    "#             for start_index in start_indexes:\n",
    "#                 for end_index in end_indexes:\n",
    "#                     # Skip answers that are not fully in the context\n",
    "#                     if offsets[start_index] is None or offsets[end_index] is None:\n",
    "#                         continue\n",
    "#                     # Skip answers with a length that is either < 0 or > max_answer_length\n",
    "#                     if (\n",
    "#                         end_index < start_index\n",
    "#                         or end_index - start_index + 1 > max_answer_length\n",
    "#                     ):\n",
    "#                         continue\n",
    "\n",
    "#                     answer = {\n",
    "#                         \"text\": context[offsets[start_index][0] : offsets[end_index][1]],\n",
    "#                         \"logit_score\": start_logit[start_index] + end_logit[end_index],\n",
    "#                     }\n",
    "#                     answers.append(answer)\n",
    "\n",
    "#         # Select the answer with the best score\n",
    "#         if len(answers) > 0:\n",
    "#             best_answer = max(answers, key=lambda x: x[\"logit_score\"])\n",
    "#             predicted_answers.append(\n",
    "#                 {\"id\": example_id, \"prediction_text\": best_answer[\"text\"]}\n",
    "#             )\n",
    "#         else:\n",
    "#             predicted_answers.append({\"id\": example_id, \"prediction_text\": \"\"})\n",
    "\n",
    "#     theoretical_answers = [{\"id\": ex[\"id\"], \"answers\": ex[\"answers\"]} for ex in examples]\n",
    "#     return metric.compute(predictions=predicted_answers, references=theoretical_answers)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'token_type_ids', 'attention_mask', 'start_positions', 'end_positions'],\n",
       "    num_rows: 1000\n",
       "})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:43<00:00, 23.09it/s]\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "model.eval()\n",
    "eval_loss = 0\n",
    "eval_steps = 0\n",
    "start_logits=[]\n",
    "end_logits=[]\n",
    "predicted_answers=[]\n",
    "theoretical_answers=[]\n",
    "\n",
    "# for step, sample in enumerate(tokenized_dataset['test']):\n",
    "#tqdm\n",
    "for step, sample in enumerate(tqdm.tqdm(tokenized_dataset['test'])):\n",
    "    with torch.no_grad():\n",
    "        input_ids=torch.tensor([sample['input_ids']]).to('cuda', dtype = torch.long)\n",
    "        s_embs,e_embs,words_embs = classifier_model(input_ids)\n",
    "        s_logits,e_logits =get_index(s_embs,e_embs,words_embs,logits_only=True)\n",
    "        s_logits =s_logits.cpu().numpy()\n",
    "        e_logits = e_logits.cpu().numpy()\n",
    "        start_logits.extend(s_logits)\n",
    "        end_logits.extend(e_logits)\n",
    "\n",
    "        text = tokenizer.decode(sample['input_ids'], skip_special_tokens=True)\n",
    "        start_position = sample['start_positions']\n",
    "        end_position = sample['end_positions']\n",
    "        theoretical_answers.append({\"id\": step, \"answers\": [{\"text\": text[start_position:end_position]}]})\n",
    "\n",
    "        start_position = np.argmax(s_logits)\n",
    "        end_position = np.argmax(e_logits)\n",
    "        predicted_answers.append({\"id\": step, \"prediction_text\": text[start_position:end_position]})\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">6</span>                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3 # import numpy as np</span>                                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">4 </span>                                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">5 </span>metric = evaluate.load(<span style=\"color: #808000; text-decoration-color: #808000\">\"squad\"</span>)                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>6 metric.compute(predictions=predicted_answers, references=theoretical_answers)                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">7 </span>                                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/kiki/miniconda3/envs/llama/lib/python3.9/site-packages/evaluate/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">module.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">432</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">compute</span>   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">429 │   │   </span>compute_kwargs = {k: kwargs[k] <span style=\"color: #0000ff; text-decoration-color: #0000ff\">for</span> k <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> kwargs <span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> k <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._feature_names()   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">430 │   │   </span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">431 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">any</span>(v <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">is</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">for</span> v <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> inputs.values()):                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>432 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.add_batch(**inputs)                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">433 │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._finalize()                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">434 │   │   </span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">435 │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.cache_file_name = <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>                                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/kiki/miniconda3/envs/llama/lib/python3.9/site-packages/evaluate/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">module.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">512</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">add_batch</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">509 │   │   │   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">f\"Input predictions: {</span>summarize_if_long_list(predictions)<span style=\"color: #808000; text-decoration-color: #808000\">},\\n\"</span>         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">510 │   │   │   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">f\"Input references: {</span>summarize_if_long_list(references)<span style=\"color: #808000; text-decoration-color: #808000\">}\"</span>              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">511 │   │   │   │   </span>)                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>512 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">raise</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">ValueError</span>(error_msg) <span style=\"color: #0000ff; text-decoration-color: #0000ff\">from</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">None</span>                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">513 │   </span>                                                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">514 │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">add</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>, *, prediction=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>, reference=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>, **kwargs):                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">515 </span><span style=\"color: #bfbfbf; text-decoration-color: #bfbfbf\">│   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">\"\"\"Add one prediction and reference for the evaluation module's stack.</span>             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">ValueError: </span>Predictions and/or references don't match the expected format.\n",
       "Expected format: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'predictions'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'id'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Value</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">dtype</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'string'</span>, <span style=\"color: #808000; text-decoration-color: #808000\">id</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span><span style=\"font-weight: bold\">)</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'prediction_text'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Value</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">dtype</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'string'</span>, \n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">id</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span><span style=\"font-weight: bold\">)}</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'references'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'id'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Value</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">dtype</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'string'</span>, <span style=\"color: #808000; text-decoration-color: #808000\">id</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span><span style=\"font-weight: bold\">)</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'answers'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Sequence</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">feature</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'text'</span>: \n",
       "<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Value</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">dtype</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'string'</span>, <span style=\"color: #808000; text-decoration-color: #808000\">id</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span><span style=\"font-weight: bold\">)</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'answer_start'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Value</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">dtype</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'int32'</span>, <span style=\"color: #808000; text-decoration-color: #808000\">id</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span><span style=\"font-weight: bold\">)}</span>, <span style=\"color: #808000; text-decoration-color: #808000\">length</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-1</span>, <span style=\"color: #808000; text-decoration-color: #808000\">id</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span><span style=\"font-weight: bold\">)}}</span>,\n",
       "Input predictions: <span style=\"font-weight: bold\">[{</span><span style=\"color: #008000; text-decoration-color: #008000\">'id'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'prediction_text'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">' name? chr'</span><span style=\"font-weight: bold\">}</span>, <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'id'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'prediction_text'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">''</span><span style=\"font-weight: bold\">}</span>, <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'id'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'prediction_text'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'yor'</span><span style=\"font-weight: bold\">}</span>, <span style=\"color: #808000; text-decoration-color: #808000\">...</span>, <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'id'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">997</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'prediction_text'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'in'</span><span style=\"font-weight: bold\">}</span>, <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'id'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">998</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'prediction_text'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'g '</span><span style=\"font-weight: bold\">}</span>, <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'id'</span>: \n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">999</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'prediction_text'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'nist? '</span><span style=\"font-weight: bold\">}]</span>,\n",
       "Input references: <span style=\"font-weight: bold\">[{</span><span style=\"color: #008000; text-decoration-color: #008000\">'id'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'answers'</span>: <span style=\"font-weight: bold\">[{</span><span style=\"color: #008000; text-decoration-color: #008000\">'text'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'tz was '</span><span style=\"font-weight: bold\">}]}</span>, <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'id'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'answers'</span>: <span style=\"font-weight: bold\">[{</span><span style=\"color: #008000; text-decoration-color: #008000\">'text'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">''</span><span style=\"font-weight: bold\">}]}</span>, <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'id'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'answers'</span>: <span style=\"font-weight: bold\">[{</span><span style=\"color: #008000; text-decoration-color: #008000\">'text'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'he ne'</span><span style=\"font-weight: bold\">}]}</span>, <span style=\"color: #808000; text-decoration-color: #808000\">...</span>, <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'id'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">997</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'answers'</span>: <span style=\"font-weight: bold\">[{</span><span style=\"color: #008000; text-decoration-color: #008000\">'text'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'ort'</span><span style=\"font-weight: bold\">}]}</span>, <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'id'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">998</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'answers'</span>: <span style=\"font-weight: bold\">[{</span><span style=\"color: #008000; text-decoration-color: #008000\">'text'</span>: \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'pe of '</span><span style=\"font-weight: bold\">}]}</span>, <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'id'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">999</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'answers'</span>: <span style=\"font-weight: bold\">[{</span><span style=\"color: #008000; text-decoration-color: #008000\">'text'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'escr'</span><span style=\"font-weight: bold\">}]}]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92m<module>\u001b[0m:\u001b[94m6\u001b[0m                                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m3 \u001b[0m\u001b[2m# import numpy as np\u001b[0m                                                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m4 \u001b[0m                                                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m5 \u001b[0mmetric = evaluate.load(\u001b[33m\"\u001b[0m\u001b[33msquad\u001b[0m\u001b[33m\"\u001b[0m)                                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m6 metric.compute(predictions=predicted_answers, references=theoretical_answers)                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m7 \u001b[0m                                                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/kiki/miniconda3/envs/llama/lib/python3.9/site-packages/evaluate/\u001b[0m\u001b[1;33mmodule.py\u001b[0m:\u001b[94m432\u001b[0m in \u001b[92mcompute\u001b[0m   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m429 \u001b[0m\u001b[2m│   │   \u001b[0mcompute_kwargs = {k: kwargs[k] \u001b[94mfor\u001b[0m k \u001b[95min\u001b[0m kwargs \u001b[94mif\u001b[0m k \u001b[95mnot\u001b[0m \u001b[95min\u001b[0m \u001b[96mself\u001b[0m._feature_names()   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m430 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m431 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[96many\u001b[0m(v \u001b[95mis\u001b[0m \u001b[95mnot\u001b[0m \u001b[94mNone\u001b[0m \u001b[94mfor\u001b[0m v \u001b[95min\u001b[0m inputs.values()):                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m432 \u001b[2m│   │   │   \u001b[0m\u001b[96mself\u001b[0m.add_batch(**inputs)                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m433 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[96mself\u001b[0m._finalize()                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m434 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m435 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[96mself\u001b[0m.cache_file_name = \u001b[94mNone\u001b[0m                                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/kiki/miniconda3/envs/llama/lib/python3.9/site-packages/evaluate/\u001b[0m\u001b[1;33mmodule.py\u001b[0m:\u001b[94m512\u001b[0m in \u001b[92madd_batch\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m509 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0m\u001b[33mf\u001b[0m\u001b[33m\"\u001b[0m\u001b[33mInput predictions: \u001b[0m\u001b[33m{\u001b[0msummarize_if_long_list(predictions)\u001b[33m}\u001b[0m\u001b[33m,\u001b[0m\u001b[33m\\n\u001b[0m\u001b[33m\"\u001b[0m         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m510 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0m\u001b[33mf\u001b[0m\u001b[33m\"\u001b[0m\u001b[33mInput references: \u001b[0m\u001b[33m{\u001b[0msummarize_if_long_list(references)\u001b[33m}\u001b[0m\u001b[33m\"\u001b[0m              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m511 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m)                                                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m512 \u001b[2m│   │   │   \u001b[0m\u001b[94mraise\u001b[0m \u001b[96mValueError\u001b[0m(error_msg) \u001b[94mfrom\u001b[0m \u001b[96mNone\u001b[0m                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m513 \u001b[0m\u001b[2m│   \u001b[0m                                                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m514 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92madd\u001b[0m(\u001b[96mself\u001b[0m, *, prediction=\u001b[94mNone\u001b[0m, reference=\u001b[94mNone\u001b[0m, **kwargs):                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m515 \u001b[0m\u001b[2;90m│   │   \u001b[0m\u001b[33m\"\"\"Add one prediction and reference for the evaluation module's stack.\u001b[0m             \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mValueError: \u001b[0mPredictions and/or references don't match the expected format.\n",
       "Expected format: \u001b[1m{\u001b[0m\u001b[32m'predictions'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'id'\u001b[0m: \u001b[1;35mValue\u001b[0m\u001b[1m(\u001b[0m\u001b[33mdtype\u001b[0m=\u001b[32m'string'\u001b[0m, \u001b[33mid\u001b[0m=\u001b[3;35mNone\u001b[0m\u001b[1m)\u001b[0m, \u001b[32m'prediction_text'\u001b[0m: \u001b[1;35mValue\u001b[0m\u001b[1m(\u001b[0m\u001b[33mdtype\u001b[0m=\u001b[32m'string'\u001b[0m, \n",
       "\u001b[33mid\u001b[0m=\u001b[3;35mNone\u001b[0m\u001b[1m)\u001b[0m\u001b[1m}\u001b[0m, \u001b[32m'references'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'id'\u001b[0m: \u001b[1;35mValue\u001b[0m\u001b[1m(\u001b[0m\u001b[33mdtype\u001b[0m=\u001b[32m'string'\u001b[0m, \u001b[33mid\u001b[0m=\u001b[3;35mNone\u001b[0m\u001b[1m)\u001b[0m, \u001b[32m'answers'\u001b[0m: \u001b[1;35mSequence\u001b[0m\u001b[1m(\u001b[0m\u001b[33mfeature\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'text'\u001b[0m: \n",
       "\u001b[1;35mValue\u001b[0m\u001b[1m(\u001b[0m\u001b[33mdtype\u001b[0m=\u001b[32m'string'\u001b[0m, \u001b[33mid\u001b[0m=\u001b[3;35mNone\u001b[0m\u001b[1m)\u001b[0m, \u001b[32m'answer_start'\u001b[0m: \u001b[1;35mValue\u001b[0m\u001b[1m(\u001b[0m\u001b[33mdtype\u001b[0m=\u001b[32m'int32'\u001b[0m, \u001b[33mid\u001b[0m=\u001b[3;35mNone\u001b[0m\u001b[1m)\u001b[0m\u001b[1m}\u001b[0m, \u001b[33mlength\u001b[0m=\u001b[1;36m-1\u001b[0m, \u001b[33mid\u001b[0m=\u001b[3;35mNone\u001b[0m\u001b[1m)\u001b[0m\u001b[1m}\u001b[0m\u001b[1m}\u001b[0m,\n",
       "Input predictions: \u001b[1m[\u001b[0m\u001b[1m{\u001b[0m\u001b[32m'id'\u001b[0m: \u001b[1;36m0\u001b[0m, \u001b[32m'prediction_text'\u001b[0m: \u001b[32m' name? chr'\u001b[0m\u001b[1m}\u001b[0m, \u001b[1m{\u001b[0m\u001b[32m'id'\u001b[0m: \u001b[1;36m1\u001b[0m, \u001b[32m'prediction_text'\u001b[0m: \u001b[32m''\u001b[0m\u001b[1m}\u001b[0m, \u001b[1m{\u001b[0m\u001b[32m'id'\u001b[0m: \u001b[1;36m2\u001b[0m, \n",
       "\u001b[32m'prediction_text'\u001b[0m: \u001b[32m'yor'\u001b[0m\u001b[1m}\u001b[0m, \u001b[33m...\u001b[0m, \u001b[1m{\u001b[0m\u001b[32m'id'\u001b[0m: \u001b[1;36m997\u001b[0m, \u001b[32m'prediction_text'\u001b[0m: \u001b[32m'in'\u001b[0m\u001b[1m}\u001b[0m, \u001b[1m{\u001b[0m\u001b[32m'id'\u001b[0m: \u001b[1;36m998\u001b[0m, \u001b[32m'prediction_text'\u001b[0m: \u001b[32m'g '\u001b[0m\u001b[1m}\u001b[0m, \u001b[1m{\u001b[0m\u001b[32m'id'\u001b[0m: \n",
       "\u001b[1;36m999\u001b[0m, \u001b[32m'prediction_text'\u001b[0m: \u001b[32m'nist? '\u001b[0m\u001b[1m}\u001b[0m\u001b[1m]\u001b[0m,\n",
       "Input references: \u001b[1m[\u001b[0m\u001b[1m{\u001b[0m\u001b[32m'id'\u001b[0m: \u001b[1;36m0\u001b[0m, \u001b[32m'answers'\u001b[0m: \u001b[1m[\u001b[0m\u001b[1m{\u001b[0m\u001b[32m'text'\u001b[0m: \u001b[32m'tz was '\u001b[0m\u001b[1m}\u001b[0m\u001b[1m]\u001b[0m\u001b[1m}\u001b[0m, \u001b[1m{\u001b[0m\u001b[32m'id'\u001b[0m: \u001b[1;36m1\u001b[0m, \u001b[32m'answers'\u001b[0m: \u001b[1m[\u001b[0m\u001b[1m{\u001b[0m\u001b[32m'text'\u001b[0m: \u001b[32m''\u001b[0m\u001b[1m}\u001b[0m\u001b[1m]\u001b[0m\u001b[1m}\u001b[0m, \u001b[1m{\u001b[0m\u001b[32m'id'\u001b[0m: \u001b[1;36m2\u001b[0m, \n",
       "\u001b[32m'answers'\u001b[0m: \u001b[1m[\u001b[0m\u001b[1m{\u001b[0m\u001b[32m'text'\u001b[0m: \u001b[32m'he ne'\u001b[0m\u001b[1m}\u001b[0m\u001b[1m]\u001b[0m\u001b[1m}\u001b[0m, \u001b[33m...\u001b[0m, \u001b[1m{\u001b[0m\u001b[32m'id'\u001b[0m: \u001b[1;36m997\u001b[0m, \u001b[32m'answers'\u001b[0m: \u001b[1m[\u001b[0m\u001b[1m{\u001b[0m\u001b[32m'text'\u001b[0m: \u001b[32m'ort'\u001b[0m\u001b[1m}\u001b[0m\u001b[1m]\u001b[0m\u001b[1m}\u001b[0m, \u001b[1m{\u001b[0m\u001b[32m'id'\u001b[0m: \u001b[1;36m998\u001b[0m, \u001b[32m'answers'\u001b[0m: \u001b[1m[\u001b[0m\u001b[1m{\u001b[0m\u001b[32m'text'\u001b[0m: \n",
       "\u001b[32m'pe of '\u001b[0m\u001b[1m}\u001b[0m\u001b[1m]\u001b[0m\u001b[1m}\u001b[0m, \u001b[1m{\u001b[0m\u001b[32m'id'\u001b[0m: \u001b[1;36m999\u001b[0m, \u001b[32m'answers'\u001b[0m: \u001b[1m[\u001b[0m\u001b[1m{\u001b[0m\u001b[32m'text'\u001b[0m: \u001b[32m'escr'\u001b[0m\u001b[1m}\u001b[0m\u001b[1m]\u001b[0m\u001b[1m}\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import evaluate\n",
    "# import collections\n",
    "# import numpy as np\n",
    "\n",
    "metric = evaluate.load(\"squad\")\n",
    "metric.compute(predictions=predicted_answers, references=theoretical_answers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tokenmonster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset squad (/home/kiki/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "87599"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset('squad', split='train')\n",
    "# dataset = dataset.train_test_split(test_size=0.2)\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '5733be284776f41900661182',\n",
       " 'title': 'University_of_Notre_Dame',\n",
       " 'context': 'Architecturally, the school has a Catholic character. Atop the Main Building\\'s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.',\n",
       " 'question': 'To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?',\n",
       " 'answers': {'text': ['Saint Bernadette Soubirous'], 'answer_start': [515]}}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 1 <span style=\"color: #0000ff; text-decoration-color: #0000ff\">from</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff; text-decoration: underline\">utils</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">import</span> get_word2subword                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 2 </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">get_answer_span</span>(word2subword_map,answer):                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 3 │   </span>best_chain=[]                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 4 │   </span>current_chain=[]                                                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">ImportError: </span>cannot import name <span style=\"color: #008000; text-decoration-color: #008000\">'get_word2subword'</span> from <span style=\"color: #008000; text-decoration-color: #008000\">'utils'</span> \n",
       "<span style=\"font-weight: bold\">(</span><span style=\"color: #800080; text-decoration-color: #800080\">/home/kiki/dados_servidor/Servidor/LM_Pretraining/EfficientPretrain/utils/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">__init__.py</span><span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92m<module>\u001b[0m:\u001b[94m1\u001b[0m                                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 1 \u001b[94mfrom\u001b[0m \u001b[4;96mutils\u001b[0m \u001b[94mimport\u001b[0m get_word2subword                                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 2 \u001b[0m\u001b[94mdef\u001b[0m \u001b[92mget_answer_span\u001b[0m(word2subword_map,answer):                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 3 \u001b[0m\u001b[2m│   \u001b[0mbest_chain=[]                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 4 \u001b[0m\u001b[2m│   \u001b[0mcurrent_chain=[]                                                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mImportError: \u001b[0mcannot import name \u001b[32m'get_word2subword'\u001b[0m from \u001b[32m'utils'\u001b[0m \n",
       "\u001b[1m(\u001b[0m\u001b[35m/home/kiki/dados_servidor/Servidor/LM_Pretraining/EfficientPretrain/utils/\u001b[0m\u001b[95m__init__.py\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from utils import get_word2subword\n",
    "def get_answer_span(word2subword_map,answer):\n",
    "    best_chain=[]\n",
    "    current_chain=[]\n",
    "    current_chain_count=0\n",
    "    for item in word2subword_map:\n",
    "        word=list(item.keys())[0]\n",
    "        value=list(item.values())\n",
    "        if current_chain_count>0:\n",
    "            if current_chain_count>len(best_chain):\n",
    "                    best_chain=current_chain\n",
    "            if word in answer:\n",
    "                current_chain.append((word,value))\n",
    "                current_chain_count+=1\n",
    "            else:\n",
    "                current_chain=[]\n",
    "                current_chain_count=0\n",
    "            \n",
    "        else:\n",
    "            if word in answer:\n",
    "                current_chain.append((word,value))\n",
    "                current_chain_count+=1\n",
    "    if len(best_chain)==0:\n",
    "        return [0,0]\n",
    "    answer_id_start=best_chain[0][1][0][0]\n",
    "    answer_id_end=best_chain[-1][1][0][-1]\n",
    "    if answer_id_start>MODEL_MAX_SEQ_LEN-10:\n",
    "        answer_id_start=0\n",
    "        answer_id_end=0\n",
    "    if answer_id_end>MODEL_MAX_SEQ_LEN -10:\n",
    "        answer_id_start=0\n",
    "        answer_id_end=0\n",
    "\n",
    "    return [answer_id_start,answer_id_end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n"
     ]
    }
   ],
   "source": [
    "#operacao dataset[\"answers\"][0] gasta 0.6 segundos. melhor coletar todas as respostas antes\n",
    "answers=[]\n",
    "for i in range(len(dataset)):\n",
    "    answers.append(dataset[i]['answers']['text'][0])\n",
    "    if i%10000==0:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# context=dataset[\"context\"][0]\n",
    "# tokens  = vocab.tokenize(context)\n",
    "# answer=answers[0]\n",
    "# t,word2subword_map=get_word2subword(context)\n",
    "# print(len(tokens),answer)\n",
    "\n",
    "# best_chain=get_answer_span(word2subword_map,answer)\n",
    "# best_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class TokenizedDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        question=self.dataset[\"question\"][i].lower()+'[SEP]'\n",
    "        context=self.dataset[\"context\"][i].lower()\n",
    "        input=norm.normalize_str(question+context)\n",
    "        answer=norm.normalize_str(answers[i])\n",
    "        tokens,word2subword_map=get_word2subword(vocab,input)\n",
    "        label=get_answer_span(word2subword_map,answer)\n",
    "        label=torch.Tensor(label).long()\n",
    "                \n",
    "        #trucate\n",
    "        tokens=tokens[:MODEL_MAX_SEQ_LEN - 2]\n",
    "        l=len(tokens)\n",
    "        for j in range(l,MODEL_MAX_SEQ_LEN - 2):\n",
    "            tokens.append(0)\n",
    "        tokens=torch.Tensor(tokens).long()\n",
    "        \n",
    "        d={'input_ids':tokens,'label':label}\n",
    "        return d\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "tokenized_dataset = TokenizedDataset(dataset)\n",
    "# train_dataloader=CustomDataloader(tokenized_dataset)\n",
    "train_dataloader = DataLoader(\n",
    "        tokenized_dataset, batch_size=10\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hidden layer extractor class, for magically adding adapter to language model to be pretrained\n",
    "class HiddenLayerExtractor(nn.Module):\n",
    "    def __init__(self, net, layer = -2):\n",
    "        super().__init__()\n",
    "        self.net = net\n",
    "        self.layer = layer\n",
    "\n",
    "        self.hidden = None\n",
    "        self.hook_registered = False\n",
    "\n",
    "    def _find_layer(self):\n",
    "        if type(self.layer) == str:\n",
    "            modules = dict([*self.net.named_modules()])\n",
    "            return modules.get(self.layer, None)\n",
    "        elif type(self.layer) == int:\n",
    "            children = [*self.net.children()]\n",
    "            return children[self.layer]\n",
    "        return None\n",
    "\n",
    "    def _hook(self, _, __, output):\n",
    "        self.hidden = output\n",
    "\n",
    "    def _register_hook(self):\n",
    "        layer = self._find_layer()\n",
    "        assert layer is not None, f'hidden layer ({self.layer}) not found'\n",
    "        handle = layer.register_forward_hook(self._hook)\n",
    "        self.hook_registered = True\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.layer == -1:\n",
    "            return self.net(x)\n",
    "\n",
    "        if not self.hook_registered:\n",
    "            self._register_hook()\n",
    "\n",
    "        _ = self.net(x)\n",
    "        hidden = self.hidden\n",
    "        self.hidden = None\n",
    "        assert hidden is not None, f'hidden layer {self.layer} never emitted an output'\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_extractor = HiddenLayerExtractor(model.model,-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "import torch\n",
    "import os\n",
    "from torch.utils.data import  DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "class Classifier(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Classifier, self).__init__()\n",
    "        # self.l1 = model._modules['model']\n",
    "        self.l1 = model_extractor\n",
    "        self.l2 = torch.nn.Dropout(0.3)\n",
    "        self.fl = torch.nn.Flatten()\n",
    "        self.s = torch.nn.Linear(768*(MODEL_MAX_SEQ_LEN-2), 768)\n",
    "        self.e = torch.nn.Linear(768*(MODEL_MAX_SEQ_LEN-2), 768)\n",
    "        \n",
    "    \n",
    "    def forward(self, ids):\n",
    "        words_embs= self.l1(ids)\n",
    "        # print('l1:',output.shape)\n",
    "        output = self.l2(words_embs)\n",
    "        # print('l2:',output.shape)\n",
    "        output = self.fl(output)\n",
    "        # print('fl:',output.shape)\n",
    "        s_output = self.s(output)\n",
    "        e_output = self.e(output)\n",
    "        return s_output,e_output,words_embs\n",
    "\n",
    "classifier_model = Classifier()\n",
    "classifier_model=classifier_model.to('cuda')\n",
    "optimizer = torch.optim.Adam(params =  classifier_model.parameters(), lr=5e-5,weight_decay=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "\n",
    "classifier_model.train()\n",
    "\n",
    "def loss_fn(outputs, targets):\n",
    "    loss=torch.nn.CrossEntropyLoss()(outputs, targets)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">19</span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">16 </span>epochs=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">5</span>                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">17 </span>running_loss=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">0</span>                                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">18 </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">for</span> i <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">range</span>(epochs):                                                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>19 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">for</span> j,data <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">enumerate</span>(train_dataloader):                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">20 │   │   # print(j)</span>                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">21 │   │   </span>ids = data[<span style=\"color: #808000; text-decoration-color: #808000\">'input_ids'</span>].to(<span style=\"color: #808000; text-decoration-color: #808000\">'cuda'</span>, dtype = torch.long)                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">22 │   │   </span>targets = data[<span style=\"color: #808000; text-decoration-color: #808000\">'label'</span>].to(<span style=\"color: #808000; text-decoration-color: #808000\">'cuda'</span>, dtype = torch.long)                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/kiki/miniconda3/envs/llama/lib/python3.9/site-packages/torch/utils/data/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">dataloader.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">633</span>  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">__next__</span>                                                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 630 │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._sampler_iter <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">is</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>:                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 631 │   │   │   │   # TODO(https://github.com/pytorch/pytorch/issues/76750)</span>                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 632 │   │   │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._reset()  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># type: ignore[call-arg]</span>                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 633 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>data = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._next_data()                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 634 │   │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._num_yielded += <span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>                                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 635 │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._dataset_kind == _DatasetKind.Iterable <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">and</span> \\                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 636 │   │   │   │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._IterableDataset_len_called <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">is</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">and</span> \\                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/kiki/miniconda3/envs/llama/lib/python3.9/site-packages/torch/utils/data/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">dataloader.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">677</span>  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_next_data</span>                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 674 │   </span>                                                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 675 │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_next_data</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>):                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 676 │   │   </span>index = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._next_index()  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># may raise StopIteration</span>                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 677 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>data = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._dataset_fetcher.fetch(index)  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># may raise StopIteration</span>              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 678 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._pin_memory:                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 679 │   │   │   </span>data = _utils.pin_memory.pin_memory(data, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._pin_memory_device)            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 680 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> data                                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/kiki/miniconda3/envs/llama/lib/python3.9/site-packages/torch/utils/data/_utils/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">fetch.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">51</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">fetch</span>                                                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">48 │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">hasattr</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.dataset, <span style=\"color: #808000; text-decoration-color: #808000\">\"__getitems__\"</span>) <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">and</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.dataset.__getitems__:         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">49 │   │   │   │   </span>data = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.dataset.__getitems__(possibly_batched_index)                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">50 │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span>:                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>51 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>data = [<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.dataset[idx] <span style=\"color: #0000ff; text-decoration-color: #0000ff\">for</span> idx <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> possibly_batched_index]                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">52 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span>:                                                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">53 │   │   │   </span>data = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.dataset[possibly_batched_index]                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">54 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.collate_fn(data)                                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/kiki/miniconda3/envs/llama/lib/python3.9/site-packages/torch/utils/data/_utils/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">fetch.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">51</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;listcomp&gt;</span>                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">48 │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">hasattr</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.dataset, <span style=\"color: #808000; text-decoration-color: #808000\">\"__getitems__\"</span>) <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">and</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.dataset.__getitems__:         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">49 │   │   │   │   </span>data = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.dataset.__getitems__(possibly_batched_index)                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">50 │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span>:                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>51 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>data = [<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.dataset[idx] <span style=\"color: #0000ff; text-decoration-color: #0000ff\">for</span> idx <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> possibly_batched_index]                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">52 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span>:                                                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">53 │   │   │   </span>data = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.dataset[possibly_batched_index]                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">54 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.collate_fn(data)                                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">__getitem__</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">14</span>                                                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">11 │   │   </span>context=<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.dataset[<span style=\"color: #808000; text-decoration-color: #808000\">\"context\"</span>][i].lower()                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">12 │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">input</span>=norm.normalize_str(question+context)                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">13 │   │   </span>answer=norm.normalize_str(answers[i])                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>14 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>tokens,word2subword_map=get_word2subword(vocab,<span style=\"color: #00ffff; text-decoration-color: #00ffff\">input</span>)                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">15 │   │   </span>label=get_answer_span(word2subword_map,answer)                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">16 │   │   </span>label=torch.Tensor(label).long()                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">17 </span>                                                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">NameError: </span>name <span style=\"color: #008000; text-decoration-color: #008000\">'get_word2subword'</span> is not defined\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92m<module>\u001b[0m:\u001b[94m19\u001b[0m                                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m16 \u001b[0mepochs=\u001b[94m5\u001b[0m                                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m17 \u001b[0mrunning_loss=\u001b[94m0\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m18 \u001b[0m\u001b[94mfor\u001b[0m i \u001b[95min\u001b[0m \u001b[96mrange\u001b[0m(epochs):                                                                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m19 \u001b[2m│   \u001b[0m\u001b[94mfor\u001b[0m j,data \u001b[95min\u001b[0m \u001b[96menumerate\u001b[0m(train_dataloader):                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m20 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# print(j)\u001b[0m                                                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m21 \u001b[0m\u001b[2m│   │   \u001b[0mids = data[\u001b[33m'\u001b[0m\u001b[33minput_ids\u001b[0m\u001b[33m'\u001b[0m].to(\u001b[33m'\u001b[0m\u001b[33mcuda\u001b[0m\u001b[33m'\u001b[0m, dtype = torch.long)                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m22 \u001b[0m\u001b[2m│   │   \u001b[0mtargets = data[\u001b[33m'\u001b[0m\u001b[33mlabel\u001b[0m\u001b[33m'\u001b[0m].to(\u001b[33m'\u001b[0m\u001b[33mcuda\u001b[0m\u001b[33m'\u001b[0m, dtype = torch.long)                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/kiki/miniconda3/envs/llama/lib/python3.9/site-packages/torch/utils/data/\u001b[0m\u001b[1;33mdataloader.py\u001b[0m:\u001b[94m633\u001b[0m  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92m__next__\u001b[0m                                                                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 630 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[96mself\u001b[0m._sampler_iter \u001b[95mis\u001b[0m \u001b[94mNone\u001b[0m:                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 631 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[2m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 632 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[96mself\u001b[0m._reset()  \u001b[2m# type: ignore[call-arg]\u001b[0m                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 633 \u001b[2m│   │   │   \u001b[0mdata = \u001b[96mself\u001b[0m._next_data()                                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 634 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[96mself\u001b[0m._num_yielded += \u001b[94m1\u001b[0m                                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 635 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[96mself\u001b[0m._dataset_kind == _DatasetKind.Iterable \u001b[95mand\u001b[0m \\                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 636 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0m\u001b[96mself\u001b[0m._IterableDataset_len_called \u001b[95mis\u001b[0m \u001b[95mnot\u001b[0m \u001b[94mNone\u001b[0m \u001b[95mand\u001b[0m \\                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/kiki/miniconda3/envs/llama/lib/python3.9/site-packages/torch/utils/data/\u001b[0m\u001b[1;33mdataloader.py\u001b[0m:\u001b[94m677\u001b[0m  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92m_next_data\u001b[0m                                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 674 \u001b[0m\u001b[2m│   \u001b[0m                                                                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 675 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92m_next_data\u001b[0m(\u001b[96mself\u001b[0m):                                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 676 \u001b[0m\u001b[2m│   │   \u001b[0mindex = \u001b[96mself\u001b[0m._next_index()  \u001b[2m# may raise StopIteration\u001b[0m                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 677 \u001b[2m│   │   \u001b[0mdata = \u001b[96mself\u001b[0m._dataset_fetcher.fetch(index)  \u001b[2m# may raise StopIteration\u001b[0m              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 678 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[96mself\u001b[0m._pin_memory:                                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 679 \u001b[0m\u001b[2m│   │   │   \u001b[0mdata = _utils.pin_memory.pin_memory(data, \u001b[96mself\u001b[0m._pin_memory_device)            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 680 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m data                                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/kiki/miniconda3/envs/llama/lib/python3.9/site-packages/torch/utils/data/_utils/\u001b[0m\u001b[1;33mfetch.py\u001b[0m:\u001b[94m51\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92mfetch\u001b[0m                                                                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m48 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[96mhasattr\u001b[0m(\u001b[96mself\u001b[0m.dataset, \u001b[33m\"\u001b[0m\u001b[33m__getitems__\u001b[0m\u001b[33m\"\u001b[0m) \u001b[95mand\u001b[0m \u001b[96mself\u001b[0m.dataset.__getitems__:         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m49 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mdata = \u001b[96mself\u001b[0m.dataset.__getitems__(possibly_batched_index)                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m50 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94melse\u001b[0m:                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m51 \u001b[2m│   │   │   │   \u001b[0mdata = [\u001b[96mself\u001b[0m.dataset[idx] \u001b[94mfor\u001b[0m idx \u001b[95min\u001b[0m possibly_batched_index]                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m52 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94melse\u001b[0m:                                                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m53 \u001b[0m\u001b[2m│   │   │   \u001b[0mdata = \u001b[96mself\u001b[0m.dataset[possibly_batched_index]                                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m54 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m \u001b[96mself\u001b[0m.collate_fn(data)                                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/kiki/miniconda3/envs/llama/lib/python3.9/site-packages/torch/utils/data/_utils/\u001b[0m\u001b[1;33mfetch.py\u001b[0m:\u001b[94m51\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92m<listcomp>\u001b[0m                                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m48 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[96mhasattr\u001b[0m(\u001b[96mself\u001b[0m.dataset, \u001b[33m\"\u001b[0m\u001b[33m__getitems__\u001b[0m\u001b[33m\"\u001b[0m) \u001b[95mand\u001b[0m \u001b[96mself\u001b[0m.dataset.__getitems__:         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m49 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mdata = \u001b[96mself\u001b[0m.dataset.__getitems__(possibly_batched_index)                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m50 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94melse\u001b[0m:                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m51 \u001b[2m│   │   │   │   \u001b[0mdata = [\u001b[96mself\u001b[0m.dataset[idx] \u001b[94mfor\u001b[0m idx \u001b[95min\u001b[0m possibly_batched_index]                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m52 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94melse\u001b[0m:                                                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m53 \u001b[0m\u001b[2m│   │   │   \u001b[0mdata = \u001b[96mself\u001b[0m.dataset[possibly_batched_index]                                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m54 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m \u001b[96mself\u001b[0m.collate_fn(data)                                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92m__getitem__\u001b[0m:\u001b[94m14\u001b[0m                                                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m11 \u001b[0m\u001b[2m│   │   \u001b[0mcontext=\u001b[96mself\u001b[0m.dataset[\u001b[33m\"\u001b[0m\u001b[33mcontext\u001b[0m\u001b[33m\"\u001b[0m][i].lower()                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m12 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[96minput\u001b[0m=norm.normalize_str(question+context)                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m13 \u001b[0m\u001b[2m│   │   \u001b[0manswer=norm.normalize_str(answers[i])                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m14 \u001b[2m│   │   \u001b[0mtokens,word2subword_map=get_word2subword(vocab,\u001b[96minput\u001b[0m)                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m15 \u001b[0m\u001b[2m│   │   \u001b[0mlabel=get_answer_span(word2subword_map,answer)                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m16 \u001b[0m\u001b[2m│   │   \u001b[0mlabel=torch.Tensor(label).long()                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m17 \u001b[0m                                                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mNameError: \u001b[0mname \u001b[32m'get_word2subword'\u001b[0m is not defined\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#FINETUNING\n",
    "\n",
    "#dot product s_embs(b,768) and e_embs(b,768) with words_embs(b,seq,768) to get index of start and end. Use softmax\n",
    "def get_index(s_embs,e_embs,words_embs):\n",
    "    s_index=torch.bmm(words_embs,s_embs.unsqueeze(2)).squeeze(2)\n",
    "    e_index=torch.bmm(words_embs,e_embs.unsqueeze(2)).squeeze(2)\n",
    "\n",
    "    s_index=torch.softmax(s_index,dim=1)\n",
    "    e_index=torch.softmax(e_index,dim=1)\n",
    "\n",
    "    # s_index=torch.argmax(s_index,dim=1)\n",
    "    # e_index=torch.argmax(e_index,dim=1)\n",
    "\n",
    "    return s_index,e_index\n",
    "    \n",
    "epochs=5\n",
    "running_loss=0\n",
    "for i in range(epochs):\n",
    "    for j,data in enumerate(train_dataloader):\n",
    "        # print(j)\n",
    "        ids = data['input_ids'].to('cuda', dtype = torch.long)\n",
    "        targets = data['label'].to('cuda', dtype = torch.long)\n",
    "        # print('targets: ',targets)\n",
    "\n",
    "        s_embs,e_embs,words_embs = classifier_model(ids)\n",
    "\n",
    "        s_index,e_index=get_index(s_embs,e_embs,words_embs)\n",
    "        # print('s_index: ',s_index)\n",
    "        # print('e_index: ',e_index)\n",
    "        start_loss = loss_fn(s_index,targets[:,0])\n",
    "        end_loss = loss_fn(e_index,targets[:,1])        \n",
    "        \n",
    "        loss=(start_loss+end_loss)/2\n",
    "        running_loss+=loss.item()\n",
    "        if j%50==0:\n",
    "            # print(s_index.argmax(dim=1))\n",
    "            # print(e_index.argmax(dim=1))\n",
    "            # print(targets[:,0])\n",
    "            # print(targets[:,1])\n",
    "            print(f'Epoch: {i}, Loss:  {running_loss/50}')\n",
    "            running_loss=0\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1 s_index.argmax(dim=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>),e_index.argmax(dim=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>),targets[:,<span style=\"color: #0000ff; text-decoration-color: #0000ff\">0</span>],targets[:,<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>]                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2 </span>                                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">NameError: </span>name <span style=\"color: #008000; text-decoration-color: #008000\">'targets'</span> is not defined\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92m<module>\u001b[0m:\u001b[94m1\u001b[0m                                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1 s_index.argmax(dim=\u001b[94m1\u001b[0m),e_index.argmax(dim=\u001b[94m1\u001b[0m),targets[:,\u001b[94m0\u001b[0m],targets[:,\u001b[94m1\u001b[0m]                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2 \u001b[0m                                                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mNameError: \u001b[0mname \u001b[32m'targets'\u001b[0m is not defined\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "s_index.argmax(dim=1),e_index.argmax(dim=1),targets[:,0],targets[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 1.9533e-37, 1.4153e-43, 0.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 9.5765e-42, 0.0000e+00,\n",
       "        0.0000e+00, 1.3214e-42, 0.0000e+00, 4.0468e-23, 0.0000e+00, 1.4044e-23,\n",
       "        3.1101e-09, 2.5207e-15, 1.4376e-20, 7.3730e-08, 1.2263e-09, 5.9128e-13,\n",
       "        4.1037e-08, 6.9646e-12, 3.4369e-20, 1.0490e-15, 9.4521e-17, 1.0543e-20,\n",
       "        1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.2082e-12, 2.8668e-23,\n",
       "        2.9138e-22, 0.0000e+00, 3.6187e-28, 0.0000e+00, 1.7171e-22, 4.2841e-24,\n",
       "        0.0000e+00, 0.0000e+00, 5.9639e-24, 1.9267e-26, 6.0702e-25, 1.3960e-14,\n",
       "        8.5253e-28, 4.1053e-30, 0.0000e+00, 9.2813e-28, 1.0258e-21, 3.5931e-33,\n",
       "        3.2127e-22, 1.3028e-35, 7.4481e-35, 3.7411e-39, 1.5598e-33, 9.2239e-27,\n",
       "        6.2570e-33, 3.3466e-38, 1.2530e-28, 5.8503e-40, 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00], device='cuda:0',\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_index[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1 targets.shape                                                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2 </span>                                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">NameError: </span>name <span style=\"color: #008000; text-decoration-color: #008000\">'targets'</span> is not defined\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92m<module>\u001b[0m:\u001b[94m1\u001b[0m                                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1 targets.shape                                                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2 \u001b[0m                                                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mNameError: \u001b[0mname \u001b[32m'targets'\u001b[0m is not defined\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "targets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1 <span style=\"color: #0000ff; text-decoration-color: #0000ff\">2</span>/<span style=\"color: #0000ff; text-decoration-color: #0000ff\">0</span>                                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2 </span>                                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">ZeroDivisionError: </span>division by zero\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92m<module>\u001b[0m:\u001b[94m1\u001b[0m                                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1 \u001b[94m2\u001b[0m/\u001b[94m0\u001b[0m                                                                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2 \u001b[0m                                                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mZeroDivisionError: \u001b[0mdivision by zero\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "2/0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">2</span>                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1 #EVALUATE</span>                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>2 dataset = load_dataset(<span style=\"color: #808000; text-decoration-color: #808000\">'glue'</span>, task, split=<span style=\"color: #808000; text-decoration-color: #808000\">'validation'</span>)                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3 </span>tokenized_dataset = TokenizedDataset(dataset)                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">4 </span>                                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">5 </span>eval_dataloader = DataLoader(                                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">NameError: </span>name <span style=\"color: #008000; text-decoration-color: #008000\">'task'</span> is not defined\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92m<module>\u001b[0m:\u001b[94m2\u001b[0m                                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1 \u001b[0m\u001b[2m#EVALUATE\u001b[0m                                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m2 dataset = load_dataset(\u001b[33m'\u001b[0m\u001b[33mglue\u001b[0m\u001b[33m'\u001b[0m, task, split=\u001b[33m'\u001b[0m\u001b[33mvalidation\u001b[0m\u001b[33m'\u001b[0m)                                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m3 \u001b[0mtokenized_dataset = TokenizedDataset(dataset)                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m4 \u001b[0m                                                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m5 \u001b[0meval_dataloader = DataLoader(                                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mNameError: \u001b[0mname \u001b[32m'task'\u001b[0m is not defined\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#EVALUATE\n",
    "dataset = load_dataset('glue', task, split='validation')\n",
    "tokenized_dataset = TokenizedDataset(dataset)\n",
    "\n",
    "eval_dataloader = DataLoader(\n",
    "        tokenized_dataset, batch_size=DEVICE_BATCH_SIZE\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">7</span>                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 4 </span>c=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">0</span>                                                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 5 </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">for</span> _,data <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">enumerate</span>(eval_dataloader):                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 6 │   </span>ids = data[<span style=\"color: #808000; text-decoration-color: #808000\">'input_ids'</span>].to(<span style=\"color: #808000; text-decoration-color: #808000\">'cuda'</span>, dtype = torch.long)                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 7 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>targets = data[<span style=\"color: #808000; text-decoration-color: #808000\">'label'</span>].to(<span style=\"color: #808000; text-decoration-color: #808000\">'cuda'</span>, dtype = torch.long)                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 8 │   </span>                                                                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 9 │   </span>outputs = classifier_model(ids)                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">10 </span>                                                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">KeyError: </span><span style=\"color: #008000; text-decoration-color: #008000\">'label'</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92m<module>\u001b[0m:\u001b[94m7\u001b[0m                                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 4 \u001b[0mc=\u001b[94m0\u001b[0m                                                                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 5 \u001b[0m\u001b[94mfor\u001b[0m _,data \u001b[95min\u001b[0m \u001b[96menumerate\u001b[0m(eval_dataloader):                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 6 \u001b[0m\u001b[2m│   \u001b[0mids = data[\u001b[33m'\u001b[0m\u001b[33minput_ids\u001b[0m\u001b[33m'\u001b[0m].to(\u001b[33m'\u001b[0m\u001b[33mcuda\u001b[0m\u001b[33m'\u001b[0m, dtype = torch.long)                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 7 \u001b[2m│   \u001b[0mtargets = data[\u001b[33m'\u001b[0m\u001b[33mlabel\u001b[0m\u001b[33m'\u001b[0m].to(\u001b[33m'\u001b[0m\u001b[33mcuda\u001b[0m\u001b[33m'\u001b[0m, dtype = torch.long)                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 8 \u001b[0m\u001b[2m│   \u001b[0m                                                                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 9 \u001b[0m\u001b[2m│   \u001b[0moutputs = classifier_model(ids)                                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m10 \u001b[0m                                                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mKeyError: \u001b[0m\u001b[32m'label'\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.eval()\n",
    "predictions=[]\n",
    "labels=[]\n",
    "c=0\n",
    "for _,data in enumerate(eval_dataloader):\n",
    "    ids = data['input_ids'].to('cuda', dtype = torch.long)\n",
    "    targets = data['label'].to('cuda', dtype = torch.long)\n",
    "    \n",
    "    outputs = classifier_model(ids)\n",
    "    \n",
    "    outputs=torch.argmax(outputs, dim=1).cpu()\n",
    "    targets=targets.cpu()\n",
    "    \n",
    "    outputs=[int(o.item()) for o in outputs]\n",
    "    targets=[int(o.item()) for o in targets]\n",
    "\n",
    "    predictions+=outputs\n",
    "    labels+=targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">2</span>                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1 </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">from</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff; text-decoration: underline\">sklearn.metrics</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">import</span> classification_report                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>2 <span style=\"color: #00ffff; text-decoration-color: #00ffff\">print</span>(classification_report(labels, predictions))                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3 </span>                                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/kiki/miniconda3/envs/llama/lib/python3.9/site-packages/sklearn/metrics/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">_classification.py</span>: <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">2363</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">classification_report</span>                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2360 │   │   │   </span>report_dict[label] = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">dict</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">zip</span>(headers, [i.item() <span style=\"color: #0000ff; text-decoration-color: #0000ff\">for</span> i <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> scores]))           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2361 │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span>:                                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2362 │   │   </span>longest_last_line_heading = <span style=\"color: #808000; text-decoration-color: #808000\">\"weighted avg\"</span>                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>2363 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>name_width = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">max</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">len</span>(cn) <span style=\"color: #0000ff; text-decoration-color: #0000ff\">for</span> cn <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> target_names)                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2364 │   │   </span>width = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">max</span>(name_width, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">len</span>(longest_last_line_heading), digits)                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2365 │   │   </span>head_fmt = <span style=\"color: #808000; text-decoration-color: #808000\">\"{:&gt;{width}s} \"</span> + <span style=\"color: #808000; text-decoration-color: #808000\">\" {:&gt;9}\"</span> * <span style=\"color: #00ffff; text-decoration-color: #00ffff\">len</span>(headers)                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2366 │   │   </span>report = head_fmt.format(<span style=\"color: #808000; text-decoration-color: #808000\">\"\"</span>, *headers, width=width)                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">ValueError: </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">max</span><span style=\"font-weight: bold\">()</span> arg is an empty sequence\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92m<module>\u001b[0m:\u001b[94m2\u001b[0m                                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1 \u001b[0m\u001b[94mfrom\u001b[0m \u001b[4;96msklearn\u001b[0m\u001b[4;96m.\u001b[0m\u001b[4;96mmetrics\u001b[0m \u001b[94mimport\u001b[0m classification_report                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m2 \u001b[96mprint\u001b[0m(classification_report(labels, predictions))                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m3 \u001b[0m                                                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/kiki/miniconda3/envs/llama/lib/python3.9/site-packages/sklearn/metrics/\u001b[0m\u001b[1;33m_classification.py\u001b[0m: \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[94m2363\u001b[0m in \u001b[92mclassification_report\u001b[0m                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2360 \u001b[0m\u001b[2m│   │   │   \u001b[0mreport_dict[label] = \u001b[96mdict\u001b[0m(\u001b[96mzip\u001b[0m(headers, [i.item() \u001b[94mfor\u001b[0m i \u001b[95min\u001b[0m scores]))           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2361 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94melse\u001b[0m:                                                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2362 \u001b[0m\u001b[2m│   │   \u001b[0mlongest_last_line_heading = \u001b[33m\"\u001b[0m\u001b[33mweighted avg\u001b[0m\u001b[33m\"\u001b[0m                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m2363 \u001b[2m│   │   \u001b[0mname_width = \u001b[96mmax\u001b[0m(\u001b[96mlen\u001b[0m(cn) \u001b[94mfor\u001b[0m cn \u001b[95min\u001b[0m target_names)                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2364 \u001b[0m\u001b[2m│   │   \u001b[0mwidth = \u001b[96mmax\u001b[0m(name_width, \u001b[96mlen\u001b[0m(longest_last_line_heading), digits)                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2365 \u001b[0m\u001b[2m│   │   \u001b[0mhead_fmt = \u001b[33m\"\u001b[0m\u001b[33m{\u001b[0m\u001b[33m:>\u001b[0m\u001b[33m{width}\u001b[0m\u001b[33ms} \u001b[0m\u001b[33m\"\u001b[0m + \u001b[33m\"\u001b[0m\u001b[33m \u001b[0m\u001b[33m{:>9}\u001b[0m\u001b[33m\"\u001b[0m * \u001b[96mlen\u001b[0m(headers)                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2366 \u001b[0m\u001b[2m│   │   \u001b[0mreport = head_fmt.format(\u001b[33m\"\u001b[0m\u001b[33m\"\u001b[0m, *headers, width=width)                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mValueError: \u001b[0m\u001b[1;35mmax\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m arg is an empty sequence\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(labels, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hidden layer extractor class, for magically adding adapter to language model to be pretrained\n",
    "class HiddenLayerExtractor(nn.Module):\n",
    "    def __init__(self, net, layer = -2):\n",
    "        super().__init__()\n",
    "        self.net = net\n",
    "        self.layer = layer\n",
    "\n",
    "        self.hidden = None\n",
    "        self.hook_registered = False\n",
    "\n",
    "    def _find_layer(self):\n",
    "        if type(self.layer) == str:\n",
    "            modules = dict([*self.net.named_modules()])\n",
    "            return modules.get(self.layer, None)\n",
    "        elif type(self.layer) == int:\n",
    "            children = [*self.net.children()]\n",
    "            return children[self.layer]\n",
    "        return None\n",
    "\n",
    "    def _hook(self, _, __, output):\n",
    "        self.hidden = output\n",
    "\n",
    "    def _register_hook(self):\n",
    "        layer = self._find_layer()\n",
    "        assert layer is not None, f'hidden layer ({self.layer}) not found'\n",
    "        handle = layer.register_forward_hook(self._hook)\n",
    "        self.hook_registered = True\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.layer == -1:\n",
    "            return self.net(x)\n",
    "\n",
    "        if not self.hook_registered:\n",
    "            self._register_hook()\n",
    "\n",
    "        _ = self.net(x)\n",
    "        hidden = self.hidden\n",
    "        self.hidden = None\n",
    "        assert hidden is not None, f'hidden layer {self.layer} never emitted an output'\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_extractor = HiddenLayerExtractor(model.model,-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "import torch\n",
    "import os\n",
    "from torch.utils.data import  DataLoader\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "class Classifier(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Classifier, self).__init__()\n",
    "        # self.l1 = model._modules['model']\n",
    "        self.l1 = model_extractor\n",
    "        self.l2 = torch.nn.Dropout(0.3)\n",
    "        self.fl = torch.nn.Flatten()\n",
    "        self.l3 = torch.nn.Linear(768*(MODEL_MAX_SEQ_LEN-2), 3)\n",
    "    \n",
    "    def forward(self, ids):\n",
    "        output= self.l1(ids)\n",
    "        # print('l1:',output.shape)\n",
    "        output = self.l2(output)\n",
    "        # print('l2:',output.shape)\n",
    "        output = self.fl(output)\n",
    "        # print('fl:',output.shape)\n",
    "        output = self.l3(output)\n",
    "        # print('l3:',output.shape)\n",
    "        return output\n",
    "\n",
    "classifier_model = Classifier()\n",
    "classifier_model=classifier_model.to('cuda')\n",
    "optimizer = torch.optim.Adam(params =  classifier_model.parameters(), lr=2e-5,weight_decay=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset glue (/home/kiki/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    }
   ],
   "source": [
    "task='mrpc'\n",
    "dataset = load_dataset('glue', task, split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class TokenizedDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "        self.ls=[]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        s=norm.normalize_str(self.dataset[\"sentence1\"][i])+'[SEP]'+norm.normalize_str(self.dataset[\"sentence2\"][i])\n",
    "        tokens = vocab.tokenize(s).tolist()\n",
    "        \n",
    "        self.ls.append(i)\n",
    "        \n",
    "        #trucate\n",
    "        tokens=tokens[:MODEL_MAX_SEQ_LEN - 2]\n",
    "        l=len(tokens)\n",
    "        for j in range(l,MODEL_MAX_SEQ_LEN - 2):\n",
    "            tokens.append(0)\n",
    "        tokens=torch.Tensor(tokens).long()\n",
    "\n",
    "        label=self.dataset[\"label\"][i]\n",
    "        \n",
    "        d={'input_ids':tokens,'label':label}\n",
    "        return d\n",
    "\n",
    "\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "tokenized_dataset = TokenizedDataset(dataset)\n",
    "# train_dataloader=CustomDataloader(tokenized_dataset)\n",
    "train_dataloader = DataLoader(\n",
    "        tokenized_dataset, batch_size=DEVICE_BATCH_SIZE\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "\n",
    "classifier_model.train()\n",
    "\n",
    "def loss_fn(outputs, targets):\n",
    "    loss=torch.nn.CrossEntropyLoss()(outputs, targets)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss:  0.016936131715774537\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">16</span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">13 │   │   # loss=outputs.loss</span>                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">14 │   │   </span>                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">15 │   │   </span>optimizer.zero_grad()                                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>16 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>running_loss+=loss.item()                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">17 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> _%<span style=\"color: #0000ff; text-decoration-color: #0000ff\">50</span>==<span style=\"color: #0000ff; text-decoration-color: #0000ff\">0</span>:                                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">18 │   │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">print</span>(<span style=\"color: #808000; text-decoration-color: #808000\">f'Epoch: {</span>i<span style=\"color: #808000; text-decoration-color: #808000\">}, Loss:  {</span>running_loss/<span style=\"color: #0000ff; text-decoration-color: #0000ff\">50</span><span style=\"color: #808000; text-decoration-color: #808000\">}'</span>)                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">19 │   │   │   </span>running_loss=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">0</span>                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">KeyboardInterrupt</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92m<module>\u001b[0m:\u001b[94m16\u001b[0m                                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m13 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# loss=outputs.loss\u001b[0m                                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m14 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m15 \u001b[0m\u001b[2m│   │   \u001b[0moptimizer.zero_grad()                                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m16 \u001b[2m│   │   \u001b[0mrunning_loss+=loss.item()                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m17 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m _%\u001b[94m50\u001b[0m==\u001b[94m0\u001b[0m:                                                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m18 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[96mprint\u001b[0m(\u001b[33mf\u001b[0m\u001b[33m'\u001b[0m\u001b[33mEpoch: \u001b[0m\u001b[33m{\u001b[0mi\u001b[33m}\u001b[0m\u001b[33m, Loss:  \u001b[0m\u001b[33m{\u001b[0mrunning_loss/\u001b[94m50\u001b[0m\u001b[33m}\u001b[0m\u001b[33m'\u001b[0m)                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m19 \u001b[0m\u001b[2m│   │   │   \u001b[0mrunning_loss=\u001b[94m0\u001b[0m                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mKeyboardInterrupt\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#FINETUNING\n",
    "\n",
    "epochs=5\n",
    "running_loss=0\n",
    "for i in range(epochs):\n",
    "    for _,data in enumerate(train_dataloader):\n",
    "        ids = data['input_ids'].to('cuda', dtype = torch.long)\n",
    "        targets = data['label'].to('cuda', dtype = torch.long)\n",
    "\n",
    "        outputs = classifier_model(ids)\n",
    "        \n",
    "        loss = loss_fn(outputs,targets)\n",
    "        # loss=outputs.loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        running_loss+=loss.item()\n",
    "        if _%50==0:\n",
    "            print(f'Epoch: {i}, Loss:  {running_loss/50}')\n",
    "            running_loss=0\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EVALUATE\n",
    "dataset = load_dataset('glue', task, split='validation')\n",
    "tokenized_dataset = TokenizedDataset(dataset)\n",
    "\n",
    "eval_dataloader = DataLoader(\n",
    "        tokenized_dataset, batch_size=DEVICE_BATCH_SIZE\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "predictions=[]\n",
    "labels=[]\n",
    "c=0\n",
    "for _,data in enumerate(eval_dataloader):\n",
    "    ids = data['input_ids'].to('cuda', dtype = torch.long)\n",
    "    targets = data['label'].to('cuda', dtype = torch.long)\n",
    "    \n",
    "    outputs = classifier_model(ids)\n",
    "    \n",
    "    outputs=torch.argmax(outputs, dim=1).cpu()\n",
    "    targets=targets.cpu()\n",
    "    \n",
    "    outputs=[int(o.item()) for o in outputs]\n",
    "    targets=[int(o.item()) for o in targets]\n",
    "\n",
    "    predictions+=outputs\n",
    "    labels+=targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.49      0.14      0.22       129\n",
      "           1       0.70      0.93      0.80       279\n",
      "\n",
      "    accuracy                           0.68       408\n",
      "   macro avg       0.59      0.54      0.51       408\n",
      "weighted avg       0.63      0.68      0.62       408\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(labels, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spanbert 3 layer 3 heads 10000 dataset\n",
    "#               precision    recall  f1-score   support\n",
    "\n",
    "#            0       0.38      0.22      0.28       129\n",
    "#            1       0.70      0.84      0.76       279\n",
    "\n",
    "#     accuracy                           0.64       408\n",
    "#    macro avg       0.54      0.53      0.52       408\n",
    "# weighted avg       0.60      0.64      0.61       408"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
