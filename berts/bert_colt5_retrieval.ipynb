{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.1+cu117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/miu/miniconda3/envs/torch/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "print(torch.__version__)\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "torch.cuda.is_available()\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Iterator\n",
    "import time\n",
    "\n",
    "import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "from tokenizers import BertWordPieceTokenizer, Regex, normalizers\n",
    "from tqdm import tqdm\n",
    "from transformers import (\n",
    "    BertConfig,\n",
    "    BertForMaskedLM,\n",
    "    BertTokenizerFast,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "import math\n",
    "from functools import reduce\n",
    "from collections import namedtuple\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "# from reformer_pytorch import ReformerLM\n",
    "# from x_transformers import TransformerWrapper, Decoder,Encoder\n",
    "from torch.utils.data import DataLoader\n",
    "import math \n",
    "from accelerate import Accelerator\n",
    "import torch.nn.functional as F\n",
    "# from einops import rearrange, pack, unpack\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size:  2040\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Resolving data files: 100%|██████████| 53/53 [00:00<00:00, 73.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded dataset!!\n"
     ]
    }
   ],
   "source": [
    "model_training='tokemonster'#'bert' , 'tokemonster\n",
    "# LIMIT_DATASET = 2016 * 4  # keep small for development, set to None for full dataset\n",
    "#(3090) Para 128 tokens(Bert) e 32000 vocab: 1000 = 6 seg, 10000 = 1min, 100000 = 10min, 1m = 100min, 10m = 16h, 100m = 160h ou 6.6 dias\n",
    "#(3090) Para 84 tokens(tokenmonster salva 35%) e 24000 vocab(Bert): 1000 = 4.3s, 10000 = 43s, 100000 = 7.1m, 1m = 71min, 10m = 11.8h, 100m = 118h ou 5 dias\n",
    "#(4090) Para 84 tokens(tokenmonster salva 35%) e 24000 vocab(Electra): 1000 = 3.5s, 10000 = 35s, 100000 = 5.9m, 1m = 59min, 10m = 9.8h, 100m = 98h ou 4 dias\n",
    "#(4090) Para 84 tokens(tokenmonster salva 35%) e 16000 vocab (Electra): 1000 = 2.1s, 10000 = 21s, 100000 = 3.53m, 1m = 35.3min, 10m = 5.8h, 100m = 58h ou 2.4 dias\n",
    "\n",
    "#com retnet. Confirmado que é O(n). Se lembrando que não existe mta vantagem além do custo crescer sequencialmente, já que o conteudo \n",
    "#(4090) Para 84 tokens(tokenmonster salva 35%) batch size 200 e 16000 vocab (Electra): 1000 = 2.1s, 10000 = 21s, 100000 = 3.53m, 1m = 35.3min, 10m = 5.8h, 100m = 58h ou 2.4 dias\n",
    "#(4090) Para 168 tokens(tokenmonster salva 35%) batch size 100 e 16000 vocab (Electra): 1000 = 3.7s, 10000 = 37.5s,\n",
    "#(4090) Para 1000 tokens(tokenmonster salva 35%) batch size 12 e 16000 vocab (Electra):            , 10000 = 4min32s,\n",
    "\n",
    "\n",
    "#HIPOTÉTICO(Se conseguisse representar um texto com menos tokens):\n",
    "#(4090) Para 24 tokens(tokenmonster salva 35%)(Electra): 1000 = , 10000 = , 100000 = 3.19m, 1m = 32min, 10m = 6.5h, 100m = 65h ou 2.7 dias\n",
    "#HIPOTÉTICO(Se diminuisse o vocabulario para 1024 tokens diferentes)\n",
    "#(4090) Para 84 tokens(tokenmonster salva 35%) e 1024 vocab: 1000 =1.6s , 10000 = 16.2s,\n",
    "\n",
    "#losses:\n",
    "#1m dataset, electra, tokenmonster: iteration:  7800 , total_loss:  26.277032788594564\n",
    "#10m dataset, 84 tokens(tokenmonster salva 35%) e 16000 vocab (Electra) iteration:  49980 , total_loss:  20.8733185450236. 339min total\n",
    "\n",
    "\n",
    "LIMIT_DATASET = 10_000\n",
    "RANDOM_SEED = 42\n",
    "NUM_TOKENIZER_TRAINING_ITEMS = 1_000_000  # I made this up, but it seems reasonable\n",
    "# VOCAB_SIZE = 32_768  # from Cramming\n",
    "# DEVICE_BATCH_SIZE = 100 # aprox 128, adjust to get near 100% gpu memory use\n",
    "# MODEL_MAX_SEQ_LEN = 128  # from Cramming\n",
    "\n",
    "VOCAB_SIZE = 16_000  # tokenmonster\n",
    "# VOCAB_SIZE = 1_024  # tokenmonster\n",
    "DEVICE_BATCH_SIZE = 20 # Token monster aguenta um batch size de (200-248)!! Geralmente melhora a qualidade do treino\n",
    "MODEL_MAX_SEQ_LEN = 84 # token_monster\n",
    "\n",
    "\n",
    "MASK_ID=4\n",
    "PAD_ID=0\n",
    "gradient_accumulation_steps = 2048 // DEVICE_BATCH_SIZE  # roughly based on Cramming\n",
    "batch_size = DEVICE_BATCH_SIZE * gradient_accumulation_steps\n",
    "print('batch_size: ',batch_size)\n",
    "RUN_DIR = Path(\"data\") / f\"run_{time.strftime('%Y%m%d-%H%M%S')}\"\n",
    "CHECKPOINT_DIR = RUN_DIR / \"training_checkpoints\"\n",
    "MODEL_DIR = RUN_DIR / \"model\"\n",
    "TOKENIZER_PATH = RUN_DIR / \"tokenizer.json\"\n",
    "TRAINER_HISTORY_PATH = RUN_DIR / \"trainer_history.json\"\n",
    "\n",
    "RUN_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "dataset = datasets.load_dataset(\n",
    "    \"sradc/chunked-shuffled-wikipedia20220301en-bookcorpusopen\",\n",
    "    split=f\"train[:{LIMIT_DATASET}]\" if LIMIT_DATASET else \"train\",\n",
    "    revision=\"0e6fada2dd43136e4a3f637da41de2e596aee674\",\n",
    ")\n",
    "print('loaded dataset!!')\n",
    "\n",
    "from process_tokenizer import get_tokenizer\n",
    "# tokenizer,tokenized_dataset,norm,vocab=get_tokenizer(dataset,NUM_TOKENIZER_TRAINING_ITEMS,VOCAB_SIZE,TOKENIZER_PATH,'clm',MODEL_MAX_SEQ_LEN+2)\n",
    "tokenized_dataset,norm,vocab=get_tokenizer(dataset,NUM_TOKENIZER_TRAINING_ITEMS,VOCAB_SIZE,TOKENIZER_PATH,'mlm',MODEL_MAX_SEQ_LEN+2)\n",
    "# tokenized_dataset,norm,vocab=get_tokenizer(dataset,NUM_TOKENIZER_TRAINING_ITEMS,VOCAB_SIZE,TOKENIZER_PATH,'smlm',MODEL_MAX_SEQ_LEN+2)\n",
    "MASK_ID=vocab.tokenize(\"[MASK]\")[0]\n",
    "N_DOMAINS=3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from fastbm25 import fastbm25\n",
    "corpus = [text for text in dataset['text']]\n",
    "corpus = [doc.translate(str.maketrans('', '', string.punctuation)).replace('\\n',\"\").lower().strip().split() for doc in corpus]\n",
    "bm25 = fastbm25(corpus)\n",
    "is_domain=False\n",
    "SEQ_LEN=MODEL_MAX_SEQ_LEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#given a query string, and a list of subwords, and embeddings for each subwords, return a list of embeddings for each word\n",
    "def get_word2subword(query):\n",
    "    tokens  = vocab.tokenize(query)\n",
    "    # tokens_str = [vocab.decode([int(i)]) for i in tokens]\n",
    "    #for i in range(len(tokens_str)):\n",
    "    #    print('token',i,':',tokens_str[i])\n",
    "    sentence_token_id = 0\n",
    "    decoder = vocab.decoder()\n",
    "    word2subword_map = [{p:[]} for p in query.split()]\n",
    "\n",
    "    temp_word = \"\"\n",
    "    temp_subwords = []\n",
    "\n",
    "    for id, token_id in enumerate(tokens):\n",
    "      token = decoder.decode(tokens[id]).strip()\n",
    "      subwords = token.split()\n",
    "\n",
    "      for sub_id, sub in enumerate(subwords):\n",
    "        word_map = word2subword_map[sentence_token_id]\n",
    "\n",
    "        if sub in word_map:\n",
    "          word_map[sub] = [id]\n",
    "          sentence_token_id += 1\n",
    "        else:\n",
    "          temp_word += sub\n",
    "          temp_subwords.append(id)\n",
    "\n",
    "        if temp_word in word_map:\n",
    "            word_map[temp_word] = temp_subwords\n",
    "            temp_word = \"\"\n",
    "            temp_subwords = []\n",
    "            sentence_token_id += 1\n",
    "\n",
    "    return word2subword_map \n",
    "\n",
    "#join subword embeddings\n",
    "def get_words_embs(x,query):\n",
    "    #query = vocab.decode([int(i) for i in query_ids[0]])\n",
    "\n",
    "    # query=' '.join(query.translate(str.maketrans('', '', string.punctuation)).replace('\\n',\"\").lower().split())\n",
    "    query = ' '.join(query.lower().split())\n",
    "\n",
    "    query=query.strip()\n",
    "    words_embs=[]\n",
    "    # print('query: ',query)\n",
    "    word2subword_map = get_word2subword(query)\n",
    "    #print('x.shape: ',x.shape)\n",
    "    #print('word2subword_map: ',word2subword_map)\n",
    "    for i in range(len(word2subword_map)):\n",
    "        word_map = word2subword_map[i]\n",
    "        word = list(word_map.keys())[0]\n",
    "        subword_ids = []\n",
    "        for id in word_map[word]:\n",
    "            # if id < MODEL_MAX_SEQ_LEN:\n",
    "            if id < SEQ_LEN:\n",
    "                subword_ids.append(id)\n",
    "        subword_embs = x[:,subword_ids,:]\n",
    "        word_emb = subword_embs.mean(dim=1)\n",
    "        words_embs.append(word_emb)\n",
    "    words_embs = torch.stack(words_embs,dim=1)\n",
    "    #print('words_embs.shape: ',words_embs.shape)\n",
    "    return words_embs\n",
    "\n",
    "def search_similars(query,weights,n=10,domain=0):\n",
    "    #print('query: ',query)\n",
    "    tokenized_query =query.translate(str.maketrans('', '', string.punctuation)).replace('\\n',\"\").lower().split()\n",
    "    #print('tokenized_query: ',tokenized_query)\n",
    "    #print(len(tokenized_query))\n",
    "    if is_domain:\n",
    "        result = domain_bm25[domain].top_k_sentence_weighted(tokenized_query,weights,k=n)\n",
    "    else:\n",
    "        result = bm25.top_k_sentence_weighted(tokenized_query,weights,k=n)\n",
    "        # result = bm25.top_k_sentence(tokenized_query,k=n)\n",
    "\n",
    "    sorted_scores=[r[1] for r in result]\n",
    "    top_n = [dataset['text'][i] for i in sorted_scores[1:n+1]]\n",
    "    return top_n\n",
    "    \n",
    "def trunc_pad(similar_docs,n=4):\n",
    "    trunc_pad_similar_docs=[]\n",
    "    c=0\n",
    "    for doc in similar_docs:\n",
    "        if c==0:\n",
    "            doc='[RET]'+doc\n",
    "        doc=vocab.tokenize(doc)[:SEQ_LEN]\n",
    "        doc=list(doc)\n",
    "        for j in range(SEQ_LEN-len(doc)):\n",
    "            doc.append(PAD_ID)\n",
    "        trunc_pad_similar_docs.append(doc)\n",
    "        c+=1\n",
    "    #caso não tenha docs suficientes\n",
    "    for i in range(n-len(similar_docs)):\n",
    "        trunc_pad_similar_docs.append([PAD_ID]*SEQ_LEN)\n",
    "\n",
    "    tok = vocab.tokenize('[RET]')\n",
    "    trunc_pad_similar_docs[-1][-len(tok):]=tok\n",
    "    similar_docs=trunc_pad_similar_docs\n",
    "    return similar_docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hidden layer extractor class, for magically adding adapter to language model to be pretrained\n",
    "class HiddenLayerExtractor(nn.Module):\n",
    "    def __init__(self, net, layer = -2):\n",
    "        super().__init__()\n",
    "        self.net = net\n",
    "        self.layer = layer\n",
    "\n",
    "        self.hidden = None\n",
    "        self.hook_registered = False\n",
    "\n",
    "    def _find_layer(self):\n",
    "        if type(self.layer) == str:\n",
    "            modules = dict([*self.net.named_modules()])\n",
    "            return modules.get(self.layer, None)\n",
    "        elif type(self.layer) == int:\n",
    "            children = [*self.net.children()]\n",
    "            return children[self.layer]\n",
    "        return None\n",
    "\n",
    "    def _hook(self, _, __, output):\n",
    "        self.hidden = output\n",
    "\n",
    "    def _register_hook(self):\n",
    "        layer = self._find_layer()\n",
    "        assert layer is not None, f'hidden layer ({self.layer}) not found'\n",
    "        handle = layer.register_forward_hook(self._hook)\n",
    "        self.hook_registered = True\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.layer == -1:\n",
    "            return self.net(x)\n",
    "\n",
    "        if not self.hook_registered:\n",
    "            self._register_hook()\n",
    "\n",
    "        _ = self.net(x)\n",
    "        hidden = self.hidden\n",
    "        self.hidden = None\n",
    "        assert hidden is not None, f'hidden layer {self.layer} never emitted an output'\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from x_transformers import TransformerWrapper, Decoder,Encoder\n",
    "\n",
    "RETRIEVAL_NUM=5\n",
    "\n",
    "#encoder decoder\n",
    "colt5_dict={\n",
    "            'encoder':{'is_colt5_encoder':True,'light_ff_mult':0.5,'heavy_ff_mult':4,'num_heavy_tokens':300,\n",
    "                   'light_dim_head':64,'light_heads':8,'light_window_size':128,\n",
    "                   'heavy_dim_head':64,'heavy_heads':8,'num_heavy_tokens_q':300,\n",
    "                   'num_heavy_tokens_kv':300},\n",
    "            }\n",
    "\n",
    "# model = TransformerWrapper(\n",
    "model = TransformerWrapper(\n",
    "    num_tokens = VOCAB_SIZE+8,\n",
    "    max_seq_len = MODEL_MAX_SEQ_LEN*(RETRIEVAL_NUM+1),\n",
    "    attn_layers = Encoder(\n",
    "        dim = int(768),\n",
    "        depth = 12,\n",
    "        heads =12,\n",
    "        attn_flash = True,\n",
    "        colt5_dict=colt5_dict['encoder'],\n",
    "        # rel_pos_bias = True\n",
    "    ),\n",
    "    emb_dim=128,\n",
    ")\n",
    "feature_extractor = HiddenLayerExtractor(model, layer=-2)\n",
    "query_weights_layer = torch.nn.Linear(768, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_ids are the indices corresponding to each token in the sentence.\n",
    "# attention_mask indicates whether a token should be attended to or not.\n",
    "# token_type_ids identifies which sequence a token belongs to when there is more than one sequence\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "        tokenized_dataset, shuffle=True, batch_size=DEVICE_BATCH_SIZE\n",
    "    )\n",
    "from utils import get_optimizer_scheduler\n",
    "optimizer,lr_scheduler,max_train_steps = get_optimizer_scheduler(model,train_dataloader,gradient_accumulation_steps,learning_rate=5e-5,weight_decay=0, num_warmup_steps=0, max_train_steps=None,lr_scheduler_type='linear',num_train_epochs=1)\n",
    "\n",
    "accelerator = Accelerator()\n",
    "# Prepare everything with our `accelerator`.\n",
    "model, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(\n",
    "    model, optimizer, train_dataloader, lr_scheduler\n",
    ")\n",
    "feature_extractor=feature_extractor.to(accelerator.device)\n",
    "query_weights_layer=query_weights_layer.to(accelerator.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def add_retrival_docs(output_words,words_embs,query_weights,query_ids):\n",
    "    similar_docs= search_similars(output_words,query_weights,RETRIEVAL_NUM)\n",
    "    #trunc and pad\n",
    "    similar_docs=trunc_pad(similar_docs,RETRIEVAL_NUM)\n",
    "    similar_docs = np.asarray(similar_docs,dtype=np.int32)\n",
    "    similar_docs = torch.from_numpy(similar_docs).to(words_embs.device)\n",
    "    similar_docs = similar_docs.reshape((1,similar_docs.shape[0]*similar_docs.shape[1]))\n",
    "    input_ids = torch.cat((similar_docs[0],query_ids[0]),dim=0)\n",
    "    input_ids = input_ids.flatten().unsqueeze(0)\n",
    "    return input_ids\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration:  0 , total_loss:  0.32674589157104494\n",
      "count_amostra: 20\n",
      "iteration:  30 , total_loss:  8.696722904841105\n",
      "count_amostra: 620\n",
      "iteration:  60 , total_loss:  7.973774194717407\n",
      "count_amostra: 1220\n"
     ]
    }
   ],
   "source": [
    "# Only show the progress bar once on each machine.\n",
    "# progress_bar = tqdm(range(max_train_steps), disable=not accelerator.is_local_main_process)\n",
    "completed_steps = 0\n",
    "starting_epoch = 0\n",
    "\n",
    "# update the progress_bar if load from checkpoint\n",
    "total_loss=0\n",
    "# progress_bar.update(completed_steps)\n",
    "steps_log=30\n",
    "count_amostra=0\n",
    "num_train_epochs=1\n",
    "for epoch in range(starting_epoch, num_train_epochs):\n",
    "    \n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        # model.eval()\n",
    "        # feature_extractor.eval()\n",
    "        ### STAGE 1: Retrieval ###\n",
    "        query_ids=batch['input_ids']\n",
    "        # print('query_ids.shape: ',query_ids.shape)\n",
    "\n",
    "        #get subwords embeddings and convert to word embeddings for term weighting later\n",
    "        subwords_embs = feature_extractor(query_ids)\n",
    "        # print('subwords_embs.shape: ',subwords_embs.shape)\n",
    "        #call to_logits layer to replace masked tokens with predictions\n",
    "        output_logits = model.to_logits(subwords_embs)\n",
    "        output_ids = torch.argmax(output_logits,dim=-1)\n",
    "        output_words = vocab.decode([int(i) for i in output_ids[0]])\n",
    "        # print('output_words: ',output_words)\n",
    "        #get words embeddings to calculate weights\n",
    "        words_embs = get_words_embs(subwords_embs,output_words)\n",
    "        # print('words_embs.shape: ',words_embs.shape)\n",
    "\n",
    "        #search similar docs with weights computed from word embs, and add to input_ids\n",
    "        query_weights = query_weights_layer(words_embs)[0]\n",
    "        query_weights = query_weights.flatten()\n",
    "        query_weights = torch.sigmoid(query_weights)\n",
    "        input_ids=add_retrival_docs(output_words,words_embs,query_weights,query_ids)\n",
    "        # print('inputs.shape: ',input_ids.shape)\n",
    "\n",
    "        model.train()\n",
    "        ### STAGE 2: Training ###\n",
    "        logits=model(input_ids) \n",
    "        logits=logits[:,RETRIEVAL_NUM*MODEL_MAX_SEQ_LEN:,:]\n",
    "        input_ids=input_ids[:,RETRIEVAL_NUM*MODEL_MAX_SEQ_LEN:]\n",
    "        # print('logits.shape: ',logits.shape)\n",
    "        # print('input_ids.shape: ',input_ids.shape)\n",
    "\n",
    "        loss = F.cross_entropy(logits.transpose(1, 2),input_ids,ignore_index = PAD_ID)\n",
    "        count_amostra+=int(len(batch['input_ids']))\n",
    "        # print(loss)\n",
    "        total_loss += loss.detach().float().cpu().numpy().item()\n",
    "        accelerator.backward(loss)\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()   \n",
    "        \n",
    "        if step%steps_log==0:\n",
    "            print('iteration: ',step,', total_loss: ',total_loss/steps_log)\n",
    "            print('count_amostra:',count_amostra)\n",
    "            total_loss=0\n",
    "    # Checks if the accelerator has performed an optimization step behind the scenes\n",
    "    if accelerator.sync_gradients:\n",
    "        # progress_bar.update(1)\n",
    "        completed_steps += 1\n",
    "\n",
    "    if completed_steps >= max_train_steps:\n",
    "        break\n",
    "\n",
    "    model.eval()\n",
    "    losses = []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_extractor = HiddenLayerExtractor(model,-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "import torch\n",
    "import os\n",
    "from torch.utils.data import  DataLoader\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "class Classifier(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Classifier, self).__init__()\n",
    "        # self.l1 = model._modules['model']\n",
    "        self.l1 = model_extractor\n",
    "        self.l2 = torch.nn.Dropout(0.3)\n",
    "        self.fl = torch.nn.Flatten()\n",
    "        self.l3 = torch.nn.Linear(768*(MODEL_MAX_SEQ_LEN-2), 3)\n",
    "    \n",
    "    def forward(self, ids):\n",
    "        output= self.l1(ids)\n",
    "        # print('l1:',output.shape)\n",
    "        output = self.l2(output)\n",
    "        # print('l2:',output.shape)\n",
    "        output = self.fl(output)\n",
    "        # print('fl:',output.shape)\n",
    "        output = self.l3(output)\n",
    "        # print('l3:',output.shape)\n",
    "        return output\n",
    "\n",
    "classifier_model = Classifier()\n",
    "classifier_model=classifier_model.to('cuda')\n",
    "optimizer = torch.optim.Adam(params =  classifier_model.parameters(), lr=2e-5,weight_decay=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "task='mrpc'\n",
    "dataset = load_dataset('glue', task, split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class TokenizedDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "        self.ls=[]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        s=norm.normalize_str(self.dataset[\"sentence1\"][i])+'[SEP]'+norm.normalize_str(self.dataset[\"sentence2\"][i])\n",
    "        tokens = vocab.tokenize(s).tolist()\n",
    "        \n",
    "        self.ls.append(i)\n",
    "        \n",
    "        #trucate\n",
    "        tokens=tokens[:MODEL_MAX_SEQ_LEN - 2]\n",
    "        l=len(tokens)\n",
    "        for j in range(l,MODEL_MAX_SEQ_LEN - 2):\n",
    "            tokens.append(0)\n",
    "        tokens=torch.Tensor(tokens).long()\n",
    "\n",
    "        label=self.dataset[\"label\"][i]\n",
    "        \n",
    "        d={'input_ids':tokens,'label':label}\n",
    "        return d\n",
    "\n",
    "\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "tokenized_dataset = TokenizedDataset(dataset)\n",
    "# train_dataloader=CustomDataloader(tokenized_dataset)\n",
    "train_dataloader = DataLoader(\n",
    "        tokenized_dataset, batch_size=DEVICE_BATCH_SIZE\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "\n",
    "classifier_model.train()\n",
    "\n",
    "def loss_fn(outputs, targets):\n",
    "    loss=torch.nn.CrossEntropyLoss()(outputs, targets)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss:  0.02589289903640747\n",
      "Epoch: 0, Loss:  0.7359633594751358\n",
      "Epoch: 0, Loss:  0.6966659402847291\n",
      "Epoch: 0, Loss:  0.6798816281557083\n",
      "Epoch: 1, Loss:  0.4578602337837219\n",
      "Epoch: 1, Loss:  0.7293077337741852\n",
      "Epoch: 1, Loss:  0.6903081750869751\n",
      "Epoch: 1, Loss:  0.6618105518817902\n",
      "Epoch: 2, Loss:  0.4600534611940384\n",
      "Epoch: 2, Loss:  0.677624973654747\n",
      "Epoch: 2, Loss:  0.6836879336833954\n",
      "Epoch: 2, Loss:  0.6674785310029984\n",
      "Epoch: 3, Loss:  0.44524687469005586\n",
      "Epoch: 3, Loss:  0.6862362623214722\n",
      "Epoch: 3, Loss:  0.6841507893800736\n",
      "Epoch: 3, Loss:  0.6469327342510224\n",
      "Epoch: 4, Loss:  0.45256554782390596\n",
      "Epoch: 4, Loss:  0.7167031174898147\n",
      "Epoch: 4, Loss:  0.6726771318912506\n",
      "Epoch: 4, Loss:  0.6612649184465408\n"
     ]
    }
   ],
   "source": [
    "#FINETUNING\n",
    "\n",
    "epochs=5\n",
    "running_loss=0\n",
    "for i in range(epochs):\n",
    "    for _,data in enumerate(train_dataloader):\n",
    "        ids = data['input_ids'].to('cuda', dtype = torch.long)\n",
    "        targets = data['label'].to('cuda', dtype = torch.long)\n",
    "\n",
    "        outputs = classifier_model(ids)\n",
    "        \n",
    "        loss = loss_fn(outputs,targets)\n",
    "        # loss=outputs.loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        running_loss+=loss.item()\n",
    "        if _%50==0:\n",
    "            print(f'Epoch: {i}, Loss:  {running_loss/50}')\n",
    "            running_loss=0\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EVALUATE\n",
    "dataset = load_dataset('glue', task, split='validation')\n",
    "tokenized_dataset = TokenizedDataset(dataset)\n",
    "\n",
    "eval_dataloader = DataLoader(\n",
    "        tokenized_dataset, batch_size=DEVICE_BATCH_SIZE\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "predictions=[]\n",
    "labels=[]\n",
    "c=0\n",
    "for _,data in enumerate(eval_dataloader):\n",
    "    ids = data['input_ids'].to('cuda', dtype = torch.long)\n",
    "    targets = data['label'].to('cuda', dtype = torch.long)\n",
    "    \n",
    "    outputs = classifier_model(ids)\n",
    "    \n",
    "    outputs=torch.argmax(outputs, dim=1).cpu()\n",
    "    targets=targets.cpu()\n",
    "    \n",
    "    outputs=[int(o.item()) for o in outputs]\n",
    "    targets=[int(o.item()) for o in targets]\n",
    "\n",
    "    predictions+=outputs\n",
    "    labels+=targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.25      0.01      0.02       129\n",
      "           1       0.68      0.99      0.81       279\n",
      "\n",
      "    accuracy                           0.68       408\n",
      "   macro avg       0.47      0.50      0.41       408\n",
      "weighted avg       0.55      0.68      0.56       408\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(labels, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bert 3 layers  10000 dataset\n",
    "#               precision    recall  f1-score   support\n",
    "\n",
    "#            0       0.61      0.09      0.15       129\n",
    "#            1       0.70      0.97      0.81       279\n",
    "\n",
    "#     accuracy                           0.69       408\n",
    "#    macro avg       0.65      0.53      0.48       408\n",
    "# weighted avg       0.67      0.69      0.60       408"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
