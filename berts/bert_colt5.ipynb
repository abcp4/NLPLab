{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.2\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "print(torch.__version__)\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "torch.cuda.is_available()\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Iterator\n",
    "import time\n",
    "\n",
    "import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "from tokenizers import BertWordPieceTokenizer, Regex, normalizers\n",
    "from tqdm import tqdm\n",
    "from transformers import (\n",
    "    BertConfig,\n",
    "    BertForMaskedLM,\n",
    "    BertTokenizerFast,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "import math\n",
    "from functools import reduce\n",
    "from collections import namedtuple\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "# from reformer_pytorch import ReformerLM\n",
    "# from x_transformers import TransformerWrapper, Decoder,Encoder\n",
    "from torch.utils.data import DataLoader\n",
    "import math \n",
    "from accelerate import Accelerator\n",
    "import torch.nn.functional as F\n",
    "# from einops import rearrange, pack, unpack\n",
    "import sys\n",
    "import os\n",
    "\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "#get cur dir\n",
    "cur_dir = os.getcwd()\n",
    "if 'miu' in cur_dir:\n",
    "    sys.path.append('/home/miu/Projects/NLPLab/')\n",
    "elif 'kiki' in cur_dir:\n",
    "    sys.path.append('/home/kiki/dados_servidor/Servidor/NLPLab/')\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PreTrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size:  2000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8631900d0715441881afedb9cdc6674d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/53 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded dataset!!\n"
     ]
    }
   ],
   "source": [
    "model_training='hf'#'bert' , 'tokemonster\n",
    "# LIMIT_DATASET = 2016 * 4  # keep small for development, set to None for full dataset\n",
    "#(3090) Para 128 tokens(Bert) e 32000 vocab: 1000 = 6 seg, 10000 = 1min, 100000 = 10min, 1m = 100min, 10m = 16h, 100m = 160h ou 6.6 dias\n",
    "#(3090) Para 84 tokens(tokenmonster salva 35%) e 24000 vocab(Bert): 1000 = 4.3s, 10000 = 43s, 100000 = 7.1m, 1m = 71min, 10m = 11.8h, 100m = 118h ou 5 dias\n",
    "#(4090) Para 84 tokens(tokenmonster salva 35%) e 24000 vocab(Electra): 1000 = 3.5s, 10000 = 35s, 100000 = 5.9m, 1m = 59min, 10m = 9.8h, 100m = 98h ou 4 dias\n",
    "#(4090) Para 84 tokens(tokenmonster salva 35%) e 16000 vocab (Electra): 1000 = 2.1s, 10000 = 21s, 100000 = 3.53m, 1m = 35.3min, 10m = 5.8h, 100m = 58h ou 2.4 dias\n",
    "\n",
    "#com retnet. Confirmado que é O(n). Se lembrando que não existe mta vantagem além do custo crescer sequencialmente, já que o conteudo \n",
    "#(4090) Para 84 tokens(tokenmonster salva 35%) batch size 200 e 16000 vocab (Electra): 1000 = 2.1s, 10000 = 21s, 100000 = 3.53m, 1m = 35.3min, 10m = 5.8h, 100m = 58h ou 2.4 dias\n",
    "#(4090) Para 168 tokens(tokenmonster salva 35%) batch size 100 e 16000 vocab (Electra): 1000 = 3.7s, 10000 = 37.5s,\n",
    "#(4090) Para 1000 tokens(tokenmonster salva 35%) batch size 12 e 16000 vocab (Electra):            , 10000 = 4min32s,\n",
    "\n",
    "\n",
    "#HIPOTÉTICO(Se conseguisse representar um texto com menos tokens):\n",
    "#(4090) Para 24 tokens(tokenmonster salva 35%)(Electra): 1000 = , 10000 = , 100000 = 3.19m, 1m = 32min, 10m = 6.5h, 100m = 65h ou 2.7 dias\n",
    "#HIPOTÉTICO(Se diminuisse o vocabulario para 1024 tokens diferentes)\n",
    "#(4090) Para 84 tokens(tokenmonster salva 35%) e 1024 vocab: 1000 =1.6s , 10000 = 16.2s,\n",
    "\n",
    "#losses:\n",
    "#1m dataset, electra, tokenmonster: iteration:  7800 , total_loss:  26.277032788594564\n",
    "#10m dataset, 84 tokens(tokenmonster salva 35%) e 16000 vocab (Electra) iteration:  49980 , total_loss:  20.8733185450236. 339min total\n",
    "\n",
    "\n",
    "LIMIT_DATASET = 10000\n",
    "RANDOM_SEED = 42\n",
    "NUM_TOKENIZER_TRAINING_ITEMS = 1_000_000  # I made this up, but it seems reasonable\n",
    "# VOCAB_SIZE = 32_768  # from Cramming\n",
    "# DEVICE_BATCH_SIZE = 100 # aprox 128, adjust to get near 100% gpu memory use\n",
    "# MODEL_MAX_SEQ_LEN = 128  # from Cramming\n",
    "\n",
    "VOCAB_SIZE = 32_768  # tokenmonster\n",
    "# VOCAB_SIZE = 1_024  # tokenmonster\n",
    "DEVICE_BATCH_SIZE = 100 # Token monster aguenta um batch size de (200-248)!! Geralmente melhora a qualidade do treino\n",
    "MODEL_MAX_SEQ_LEN = 128 # token_monster\n",
    "\n",
    "\n",
    "MASK_ID=4\n",
    "PAD_ID=0\n",
    "gradient_accumulation_steps = 2048 // DEVICE_BATCH_SIZE  # roughly based on Cramming\n",
    "batch_size = DEVICE_BATCH_SIZE * gradient_accumulation_steps\n",
    "print('batch_size: ',batch_size)\n",
    "RUN_DIR = Path(\"data\") / f\"run_{time.strftime('%Y%m%d-%H%M%S')}\"\n",
    "CHECKPOINT_DIR = RUN_DIR / \"training_checkpoints\"\n",
    "MODEL_DIR = RUN_DIR / \"model\"\n",
    "TOKENIZER_PATH = RUN_DIR / \"tokenizer.json\"\n",
    "TRAINER_HISTORY_PATH = RUN_DIR / \"trainer_history.json\"\n",
    "\n",
    "RUN_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "dataset = datasets.load_dataset(\n",
    "    \"sradc/chunked-shuffled-wikipedia20220301en-bookcorpusopen\",\n",
    "    split=f\"train[:{LIMIT_DATASET}]\" if LIMIT_DATASET else \"train\",\n",
    "    revision=\"0e6fada2dd43136e4a3f637da41de2e596aee674\",\n",
    ")\n",
    "print('loaded dataset!!')\n",
    "\n",
    "from utils.process_tokenizer import get_tokenizer\n",
    "# tokenizer,tokenized_dataset,norm,vocab=get_tokenizer(dataset,NUM_TOKENIZER_TRAINING_ITEMS,VOCAB_SIZE,TOKENIZER_PATH,'clm',MODEL_MAX_SEQ_LEN+2)\n",
    "tokenized_dataset,norm,vocab,tokenizer=get_tokenizer(dataset,NUM_TOKENIZER_TRAINING_ITEMS,VOCAB_SIZE\n",
    "                                                     ,TOKENIZER_PATH,'mlm',MODEL_MAX_SEQ_LEN+2,tok_type='hf'\n",
    "                                                     ,tokenizer_name='bert-base-uncased')\n",
    "# tokenized_dataset,norm,vocab=get_tokenizer(dataset,NUM_TOKENIZER_TRAINING_ITEMS,VOCAB_SIZE,TOKENIZER_PATH,'smlm',MODEL_MAX_SEQ_LEN+2)\n",
    "MASK_ID=vocab.tokenize(\"[MASK]\")[0]\n",
    "N_DOMAINS=3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from x_transformers import TransformerWrapper, Decoder,Encoder\n",
    "\n",
    "#encoder decoder\n",
    "colt5_dict={\n",
    "            'encoder':{'is_colt5_encoder':True,'light_ff_mult':0.5,'heavy_ff_mult':4,'num_heavy_tokens':30,\n",
    "                   'light_dim_head':64,'light_heads':8,'light_window_size':128,\n",
    "                   'heavy_dim_head':64,'heavy_heads':8,'num_heavy_tokens_q':30,\n",
    "                   'num_heavy_tokens_kv':30},\n",
    "            }\n",
    "\n",
    "# model = TransformerWrapper(\n",
    "model = TransformerWrapper(\n",
    "    num_tokens = VOCAB_SIZE+8,\n",
    "    max_seq_len = MODEL_MAX_SEQ_LEN,\n",
    "    attn_layers = Encoder(\n",
    "        dim = int(768),\n",
    "        depth = 12,\n",
    "        heads =12,\n",
    "        attn_flash = True,\n",
    "        colt5_dict=colt5_dict['encoder'],\n",
    "        # rel_pos_bias = True\n",
    "    ),\n",
    "    emb_dim=128,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_ids are the indices corresponding to each token in the sentence.\n",
    "# attention_mask indicates whether a token should be attended to or not.\n",
    "# token_type_ids identifies which sequence a token belongs to when there is more than one sequence\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "        tokenized_dataset, shuffle=True, batch_size=DEVICE_BATCH_SIZE\n",
    "    )\n",
    "from utils.utils import get_optimizer_scheduler\n",
    "optimizer,lr_scheduler,max_train_steps = get_optimizer_scheduler(model,train_dataloader,gradient_accumulation_steps,learning_rate=5e-5,weight_decay=0, num_warmup_steps=0, max_train_steps=None,lr_scheduler_type='linear',num_train_epochs=1)\n",
    "\n",
    "accelerator = Accelerator()\n",
    "# Prepare everything with our `accelerator`.\n",
    "model, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(\n",
    "    model, optimizer, train_dataloader, lr_scheduler\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 20/100000 [00:10<13:12:35,  2.10it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (611 > 512). Running this sequence through the model will result in indexing errors\n",
      "100%|██████████| 100000/100000 [13:01:40<00:00,  2.13it/s] \n"
     ]
    }
   ],
   "source": [
    "# Only show the progress bar once on each machine.\n",
    "import tqdm\n",
    "completed_steps = 0\n",
    "starting_epoch = 0\n",
    "\n",
    "# update the progress_bar if load from checkpoint\n",
    "total_loss=0\n",
    "# progress_bar.update(completed_steps)\n",
    "steps_log=30\n",
    "count_amostra=0\n",
    "num_train_epochs=1\n",
    "for epoch in range(starting_epoch, num_train_epochs):\n",
    "    model.train()\n",
    "    # for step, batch in enumerate(train_dataloader):\n",
    "    for step, batch in enumerate(tqdm.tqdm(train_dataloader)):\n",
    "\n",
    "        # with accelerator.accumulate(model):\n",
    "       \n",
    "        logits=model(batch['input_ids']) \n",
    "        loss = F.cross_entropy(logits.transpose(1, 2),batch['input_ids'],ignore_index = PAD_ID)\n",
    "        count_amostra+=int(len(batch['input_ids']))\n",
    "        # print(loss)\n",
    "        total_loss += loss.detach().float().cpu().numpy().item()\n",
    "        accelerator.backward(loss)\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()       \n",
    "        \n",
    "        # if step%steps_log==0:\n",
    "        #     print('iteration: ',step,', total_loss: ',total_loss/steps_log)\n",
    "        #     print('count_amostra:',count_amostra)\n",
    "        #     total_loss=0\n",
    "    # Checks if the accelerator has performed an optimization step behind the scenes\n",
    "    if accelerator.sync_gradients:\n",
    "        # progress_bar.update(1)\n",
    "        completed_steps += 1\n",
    "\n",
    "    if completed_steps >= max_train_steps:\n",
    "        break\n",
    "\n",
    "    model.eval()\n",
    "    losses = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#convert to huggingface Transformers\n",
    "\n",
    "from transformers import BertConfig, BertForMaskedLM, BertTokenizerFast\n",
    "\n",
    "config = BertConfig(\n",
    "    vocab_size=VOCAB_SIZE+8,\n",
    "    max_position_embeddings=MODEL_MAX_SEQ_LEN,\n",
    "    num_attention_heads=12,\n",
    "    num_hidden_layers=12,\n",
    "    type_vocab_size=2,\n",
    ")\n",
    "\n",
    "bert_model=BertForMaskedLM(config=config)\n",
    "bert_model.bert=model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model.save_pretrained('models/bert')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QA Trainer hf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "        num_rows: 4000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"squad\", split=\"train[:5000]\")\n",
    "dataset = dataset.train_test_split(test_size=0.2)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = MODEL_MAX_SEQ_LEN#384\n",
    "stride = 100\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "def preprocess_validation_examples(examples):\n",
    "    questions = [q.strip() for q in examples[\"question\"]]\n",
    "    inputs = tokenizer(\n",
    "        questions,\n",
    "        examples[\"context\"],\n",
    "        max_length=max_length,\n",
    "        truncation=\"only_second\",\n",
    "        stride=stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    sample_map = inputs.pop(\"overflow_to_sample_mapping\")\n",
    "    example_ids = []\n",
    "\n",
    "    for i in range(len(inputs[\"input_ids\"])):\n",
    "        sample_idx = sample_map[i]\n",
    "        example_ids.append(examples[\"id\"][sample_idx])\n",
    "\n",
    "        sequence_ids = inputs.sequence_ids(i)\n",
    "        offset = inputs[\"offset_mapping\"][i]\n",
    "        inputs[\"offset_mapping\"][i] = [\n",
    "            o if sequence_ids[k] == 1 else None for k, o in enumerate(offset)\n",
    "        ]\n",
    "\n",
    "    inputs[\"example_id\"] = example_ids\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e46e67cc61614618b4328896d2fe0944",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "thread '<unnamed>' panicked at /home/runner/work/tokenizers/tokenizers/tokenizers/src/tokenizer/encoding.rs:319:9:\n",
      "`stride` must be strictly less than `max_len=100` (note that `max_len` may be shorter than the max length of the original model, as it subtracts the number of special characters\n",
      "note: run with `RUST_BACKTRACE=1` environment variable to display a backtrace\n",
      "thread '<unnamed>' panicked at /home/runner/work/tokenizers/tokenizers/tokenizers/src/tokenizer/encoding.rs:319:9:\n",
      "`stride` must be strictly less than `max_len=95` (note that `max_len` may be shorter than the max length of the original model, as it subtracts the number of special characters\n",
      "thread '<unnamed>' panicked at /home/runner/work/tokenizers/tokenizers/tokenizers/src/tokenizer/encoding.rs:319:9:\n",
      "`stride` must be strictly less than `max_len=98` (note that `max_len` may be shorter than the max length of the original model, as it subtracts the number of special characters\n",
      "thread '<unnamed>' panicked at /home/runner/work/tokenizers/tokenizers/tokenizers/src/tokenizer/encoding.rs:319:9:\n",
      "`stride` must be strictly less than `max_len=97` (note that `max_len` may be shorter than the max length of the original model, as it subtracts the number of special characters\n",
      "thread '<unnamed>' panicked at /home/runner/work/tokenizers/tokenizers/tokenizers/src/tokenizer/encoding.rs:319:9:\n",
      "`stride` must be strictly less than `max_len=97` (note that `max_len` may be shorter than the max length of the original model, as it subtracts the number of special characters\n",
      "thread '<unnamed>' panicked at /home/runner/work/tokenizers/tokenizers/tokenizers/src/tokenizer/encoding.rs:319:9:\n",
      "`stride` must be strictly less than `max_len=89` (note that `max_len` may be shorter than the max length of the original model, as it subtracts the number of special characters\n",
      "thread '<unnamed>' panicked at /home/runner/work/tokenizers/tokenizers/tokenizers/src/tokenizer/encoding.rs:319:9:\n",
      "`stride` must be strictly less than `max_len=97` (note that `max_len` may be shorter than the max length of the original model, as it subtracts the number of special characters\n",
      "thread '<unnamed>' panicked at /home/runner/work/tokenizers/tokenizers/tokenizers/src/tokenizer/encoding.rs:319:9:\n",
      "`stride` must be strictly less than `max_len=98` (note that `max_len` may be shorter than the max length of the original model, as it subtracts the number of special characters\n"
     ]
    },
    {
     "ename": "PanicException",
     "evalue": "`stride` must be strictly less than `max_len=98` (note that `max_len` may be shorter than the max length of the original model, as it subtracts the number of special characters",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPanicException\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/miu/Projects/NLPLab/berts/bert_colt5.ipynb Cell 13\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/miu/Projects/NLPLab/berts/bert_colt5.ipynb#X15sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m validation_dataset \u001b[39m=\u001b[39m dataset[\u001b[39m\"\u001b[39;49m\u001b[39mtest\u001b[39;49m\u001b[39m\"\u001b[39;49m]\u001b[39m.\u001b[39;49mmap(\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/miu/Projects/NLPLab/berts/bert_colt5.ipynb#X15sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     preprocess_validation_examples,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/miu/Projects/NLPLab/berts/bert_colt5.ipynb#X15sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     batched\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/miu/Projects/NLPLab/berts/bert_colt5.ipynb#X15sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     remove_columns\u001b[39m=\u001b[39;49mdataset[\u001b[39m\"\u001b[39;49m\u001b[39mtest\u001b[39;49m\u001b[39m\"\u001b[39;49m]\u001b[39m.\u001b[39;49mcolumn_names,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/miu/Projects/NLPLab/berts/bert_colt5.ipynb#X15sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.10/site-packages/datasets/arrow_dataset.py:592\u001b[0m, in \u001b[0;36mtransmit_tasks.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    590\u001b[0m     \u001b[39mself\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mself\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    591\u001b[0m \u001b[39m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 592\u001b[0m out: Union[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mDatasetDict\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    593\u001b[0m datasets: List[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(out\u001b[39m.\u001b[39mvalues()) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(out, \u001b[39mdict\u001b[39m) \u001b[39melse\u001b[39;00m [out]\n\u001b[1;32m    594\u001b[0m \u001b[39mfor\u001b[39;00m dataset \u001b[39min\u001b[39;00m datasets:\n\u001b[1;32m    595\u001b[0m     \u001b[39m# Remove task templates if a column mapping of the template is no longer valid\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.10/site-packages/datasets/arrow_dataset.py:557\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    550\u001b[0m self_format \u001b[39m=\u001b[39m {\n\u001b[1;32m    551\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtype\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_type,\n\u001b[1;32m    552\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mformat_kwargs\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_kwargs,\n\u001b[1;32m    553\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mcolumns\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_columns,\n\u001b[1;32m    554\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39moutput_all_columns\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output_all_columns,\n\u001b[1;32m    555\u001b[0m }\n\u001b[1;32m    556\u001b[0m \u001b[39m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 557\u001b[0m out: Union[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mDatasetDict\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    558\u001b[0m datasets: List[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(out\u001b[39m.\u001b[39mvalues()) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(out, \u001b[39mdict\u001b[39m) \u001b[39melse\u001b[39;00m [out]\n\u001b[1;32m    559\u001b[0m \u001b[39m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.10/site-packages/datasets/arrow_dataset.py:3093\u001b[0m, in \u001b[0;36mDataset.map\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[1;32m   3087\u001b[0m \u001b[39mif\u001b[39;00m transformed_dataset \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   3088\u001b[0m     \u001b[39mwith\u001b[39;00m hf_tqdm(\n\u001b[1;32m   3089\u001b[0m         unit\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m examples\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   3090\u001b[0m         total\u001b[39m=\u001b[39mpbar_total,\n\u001b[1;32m   3091\u001b[0m         desc\u001b[39m=\u001b[39mdesc \u001b[39mor\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mMap\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   3092\u001b[0m     ) \u001b[39mas\u001b[39;00m pbar:\n\u001b[0;32m-> 3093\u001b[0m         \u001b[39mfor\u001b[39;00m rank, done, content \u001b[39min\u001b[39;00m Dataset\u001b[39m.\u001b[39m_map_single(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mdataset_kwargs):\n\u001b[1;32m   3094\u001b[0m             \u001b[39mif\u001b[39;00m done:\n\u001b[1;32m   3095\u001b[0m                 shards_done \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.10/site-packages/datasets/arrow_dataset.py:3470\u001b[0m, in \u001b[0;36mDataset._map_single\u001b[0;34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset)\u001b[0m\n\u001b[1;32m   3466\u001b[0m indices \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\n\u001b[1;32m   3467\u001b[0m     \u001b[39mrange\u001b[39m(\u001b[39m*\u001b[39m(\u001b[39mslice\u001b[39m(i, i \u001b[39m+\u001b[39m batch_size)\u001b[39m.\u001b[39mindices(shard\u001b[39m.\u001b[39mnum_rows)))\n\u001b[1;32m   3468\u001b[0m )  \u001b[39m# Something simpler?\u001b[39;00m\n\u001b[1;32m   3469\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 3470\u001b[0m     batch \u001b[39m=\u001b[39m apply_function_on_filtered_inputs(\n\u001b[1;32m   3471\u001b[0m         batch,\n\u001b[1;32m   3472\u001b[0m         indices,\n\u001b[1;32m   3473\u001b[0m         check_same_num_examples\u001b[39m=\u001b[39;49m\u001b[39mlen\u001b[39;49m(shard\u001b[39m.\u001b[39;49mlist_indexes()) \u001b[39m>\u001b[39;49m \u001b[39m0\u001b[39;49m,\n\u001b[1;32m   3474\u001b[0m         offset\u001b[39m=\u001b[39;49moffset,\n\u001b[1;32m   3475\u001b[0m     )\n\u001b[1;32m   3476\u001b[0m \u001b[39mexcept\u001b[39;00m NumExamplesMismatchError:\n\u001b[1;32m   3477\u001b[0m     \u001b[39mraise\u001b[39;00m DatasetTransformationNotAllowedError(\n\u001b[1;32m   3478\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mUsing `.map` in batched mode on a dataset with attached indexes is allowed only if it doesn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt create or remove existing examples. You can first run `.drop_index() to remove your index and then re-add it.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   3479\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.10/site-packages/datasets/arrow_dataset.py:3349\u001b[0m, in \u001b[0;36mDataset._map_single.<locals>.apply_function_on_filtered_inputs\u001b[0;34m(pa_inputs, indices, check_same_num_examples, offset)\u001b[0m\n\u001b[1;32m   3347\u001b[0m \u001b[39mif\u001b[39;00m with_rank:\n\u001b[1;32m   3348\u001b[0m     additional_args \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (rank,)\n\u001b[0;32m-> 3349\u001b[0m processed_inputs \u001b[39m=\u001b[39m function(\u001b[39m*\u001b[39;49mfn_args, \u001b[39m*\u001b[39;49madditional_args, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfn_kwargs)\n\u001b[1;32m   3350\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(processed_inputs, LazyDict):\n\u001b[1;32m   3351\u001b[0m     processed_inputs \u001b[39m=\u001b[39m {\n\u001b[1;32m   3352\u001b[0m         k: v \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m processed_inputs\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mitems() \u001b[39mif\u001b[39;00m k \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m processed_inputs\u001b[39m.\u001b[39mkeys_to_format\n\u001b[1;32m   3353\u001b[0m     }\n",
      "\u001b[1;32m/home/miu/Projects/NLPLab/berts/bert_colt5.ipynb Cell 13\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/miu/Projects/NLPLab/berts/bert_colt5.ipynb#X15sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpreprocess_validation_examples\u001b[39m(examples):\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/miu/Projects/NLPLab/berts/bert_colt5.ipynb#X15sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     questions \u001b[39m=\u001b[39m [q\u001b[39m.\u001b[39mstrip() \u001b[39mfor\u001b[39;00m q \u001b[39min\u001b[39;00m examples[\u001b[39m\"\u001b[39m\u001b[39mquestion\u001b[39m\u001b[39m\"\u001b[39m]]\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/miu/Projects/NLPLab/berts/bert_colt5.ipynb#X15sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     inputs \u001b[39m=\u001b[39m tokenizer(\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/miu/Projects/NLPLab/berts/bert_colt5.ipynb#X15sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m         questions,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/miu/Projects/NLPLab/berts/bert_colt5.ipynb#X15sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m         examples[\u001b[39m\"\u001b[39;49m\u001b[39mcontext\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/miu/Projects/NLPLab/berts/bert_colt5.ipynb#X15sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m         max_length\u001b[39m=\u001b[39;49mmax_length,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/miu/Projects/NLPLab/berts/bert_colt5.ipynb#X15sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m         truncation\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39monly_second\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/miu/Projects/NLPLab/berts/bert_colt5.ipynb#X15sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m         stride\u001b[39m=\u001b[39;49mstride,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/miu/Projects/NLPLab/berts/bert_colt5.ipynb#X15sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m         return_overflowing_tokens\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/miu/Projects/NLPLab/berts/bert_colt5.ipynb#X15sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m         return_offsets_mapping\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/miu/Projects/NLPLab/berts/bert_colt5.ipynb#X15sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m         padding\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mmax_length\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/miu/Projects/NLPLab/berts/bert_colt5.ipynb#X15sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     )\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/miu/Projects/NLPLab/berts/bert_colt5.ipynb#X15sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m     sample_map \u001b[39m=\u001b[39m inputs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39moverflow_to_sample_mapping\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/miu/Projects/NLPLab/berts/bert_colt5.ipynb#X15sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m     example_ids \u001b[39m=\u001b[39m []\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2802\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.__call__\u001b[0;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2800\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_in_target_context_manager:\n\u001b[1;32m   2801\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_switch_to_input_mode()\n\u001b[0;32m-> 2802\u001b[0m     encodings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_one(text\u001b[39m=\u001b[39;49mtext, text_pair\u001b[39m=\u001b[39;49mtext_pair, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mall_kwargs)\n\u001b[1;32m   2803\u001b[0m \u001b[39mif\u001b[39;00m text_target \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   2804\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_switch_to_target_mode()\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2888\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._call_one\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2883\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   2884\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbatch length of `text`: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(text)\u001b[39m}\u001b[39;00m\u001b[39m does not match batch length of `text_pair`:\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2885\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(text_pair)\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2886\u001b[0m         )\n\u001b[1;32m   2887\u001b[0m     batch_text_or_text_pairs \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mzip\u001b[39m(text, text_pair)) \u001b[39mif\u001b[39;00m text_pair \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m text\n\u001b[0;32m-> 2888\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbatch_encode_plus(\n\u001b[1;32m   2889\u001b[0m         batch_text_or_text_pairs\u001b[39m=\u001b[39;49mbatch_text_or_text_pairs,\n\u001b[1;32m   2890\u001b[0m         add_special_tokens\u001b[39m=\u001b[39;49madd_special_tokens,\n\u001b[1;32m   2891\u001b[0m         padding\u001b[39m=\u001b[39;49mpadding,\n\u001b[1;32m   2892\u001b[0m         truncation\u001b[39m=\u001b[39;49mtruncation,\n\u001b[1;32m   2893\u001b[0m         max_length\u001b[39m=\u001b[39;49mmax_length,\n\u001b[1;32m   2894\u001b[0m         stride\u001b[39m=\u001b[39;49mstride,\n\u001b[1;32m   2895\u001b[0m         is_split_into_words\u001b[39m=\u001b[39;49mis_split_into_words,\n\u001b[1;32m   2896\u001b[0m         pad_to_multiple_of\u001b[39m=\u001b[39;49mpad_to_multiple_of,\n\u001b[1;32m   2897\u001b[0m         return_tensors\u001b[39m=\u001b[39;49mreturn_tensors,\n\u001b[1;32m   2898\u001b[0m         return_token_type_ids\u001b[39m=\u001b[39;49mreturn_token_type_ids,\n\u001b[1;32m   2899\u001b[0m         return_attention_mask\u001b[39m=\u001b[39;49mreturn_attention_mask,\n\u001b[1;32m   2900\u001b[0m         return_overflowing_tokens\u001b[39m=\u001b[39;49mreturn_overflowing_tokens,\n\u001b[1;32m   2901\u001b[0m         return_special_tokens_mask\u001b[39m=\u001b[39;49mreturn_special_tokens_mask,\n\u001b[1;32m   2902\u001b[0m         return_offsets_mapping\u001b[39m=\u001b[39;49mreturn_offsets_mapping,\n\u001b[1;32m   2903\u001b[0m         return_length\u001b[39m=\u001b[39;49mreturn_length,\n\u001b[1;32m   2904\u001b[0m         verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m   2905\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m   2906\u001b[0m     )\n\u001b[1;32m   2907\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   2908\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencode_plus(\n\u001b[1;32m   2909\u001b[0m         text\u001b[39m=\u001b[39mtext,\n\u001b[1;32m   2910\u001b[0m         text_pair\u001b[39m=\u001b[39mtext_pair,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2926\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m   2927\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3079\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   3069\u001b[0m \u001b[39m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[1;32m   3070\u001b[0m padding_strategy, truncation_strategy, max_length, kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_padding_truncation_strategies(\n\u001b[1;32m   3071\u001b[0m     padding\u001b[39m=\u001b[39mpadding,\n\u001b[1;32m   3072\u001b[0m     truncation\u001b[39m=\u001b[39mtruncation,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3076\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m   3077\u001b[0m )\n\u001b[0;32m-> 3079\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_batch_encode_plus(\n\u001b[1;32m   3080\u001b[0m     batch_text_or_text_pairs\u001b[39m=\u001b[39;49mbatch_text_or_text_pairs,\n\u001b[1;32m   3081\u001b[0m     add_special_tokens\u001b[39m=\u001b[39;49madd_special_tokens,\n\u001b[1;32m   3082\u001b[0m     padding_strategy\u001b[39m=\u001b[39;49mpadding_strategy,\n\u001b[1;32m   3083\u001b[0m     truncation_strategy\u001b[39m=\u001b[39;49mtruncation_strategy,\n\u001b[1;32m   3084\u001b[0m     max_length\u001b[39m=\u001b[39;49mmax_length,\n\u001b[1;32m   3085\u001b[0m     stride\u001b[39m=\u001b[39;49mstride,\n\u001b[1;32m   3086\u001b[0m     is_split_into_words\u001b[39m=\u001b[39;49mis_split_into_words,\n\u001b[1;32m   3087\u001b[0m     pad_to_multiple_of\u001b[39m=\u001b[39;49mpad_to_multiple_of,\n\u001b[1;32m   3088\u001b[0m     return_tensors\u001b[39m=\u001b[39;49mreturn_tensors,\n\u001b[1;32m   3089\u001b[0m     return_token_type_ids\u001b[39m=\u001b[39;49mreturn_token_type_ids,\n\u001b[1;32m   3090\u001b[0m     return_attention_mask\u001b[39m=\u001b[39;49mreturn_attention_mask,\n\u001b[1;32m   3091\u001b[0m     return_overflowing_tokens\u001b[39m=\u001b[39;49mreturn_overflowing_tokens,\n\u001b[1;32m   3092\u001b[0m     return_special_tokens_mask\u001b[39m=\u001b[39;49mreturn_special_tokens_mask,\n\u001b[1;32m   3093\u001b[0m     return_offsets_mapping\u001b[39m=\u001b[39;49mreturn_offsets_mapping,\n\u001b[1;32m   3094\u001b[0m     return_length\u001b[39m=\u001b[39;49mreturn_length,\n\u001b[1;32m   3095\u001b[0m     verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m   3096\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m   3097\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py:504\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast._batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose)\u001b[0m\n\u001b[1;32m    495\u001b[0m \u001b[39m# Set the truncation and padding strategy and restore the initial configuration\u001b[39;00m\n\u001b[1;32m    496\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mset_truncation_and_padding(\n\u001b[1;32m    497\u001b[0m     padding_strategy\u001b[39m=\u001b[39mpadding_strategy,\n\u001b[1;32m    498\u001b[0m     truncation_strategy\u001b[39m=\u001b[39mtruncation_strategy,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    501\u001b[0m     pad_to_multiple_of\u001b[39m=\u001b[39mpad_to_multiple_of,\n\u001b[1;32m    502\u001b[0m )\n\u001b[0;32m--> 504\u001b[0m encodings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_tokenizer\u001b[39m.\u001b[39;49mencode_batch(\n\u001b[1;32m    505\u001b[0m     batch_text_or_text_pairs,\n\u001b[1;32m    506\u001b[0m     add_special_tokens\u001b[39m=\u001b[39;49madd_special_tokens,\n\u001b[1;32m    507\u001b[0m     is_pretokenized\u001b[39m=\u001b[39;49mis_split_into_words,\n\u001b[1;32m    508\u001b[0m )\n\u001b[1;32m    510\u001b[0m \u001b[39m# Convert encoding to dict\u001b[39;00m\n\u001b[1;32m    511\u001b[0m \u001b[39m# `Tokens` has type: Tuple[\u001b[39;00m\n\u001b[1;32m    512\u001b[0m \u001b[39m#                       List[Dict[str, List[List[int]]]] or List[Dict[str, 2D-Tensor]],\u001b[39;00m\n\u001b[1;32m    513\u001b[0m \u001b[39m#                       List[EncodingFast]\u001b[39;00m\n\u001b[1;32m    514\u001b[0m \u001b[39m#                    ]\u001b[39;00m\n\u001b[1;32m    515\u001b[0m \u001b[39m# with nested dimensions corresponding to batch, overflows, sequence length\u001b[39;00m\n\u001b[1;32m    516\u001b[0m tokens_and_encodings \u001b[39m=\u001b[39m [\n\u001b[1;32m    517\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_convert_encoding(\n\u001b[1;32m    518\u001b[0m         encoding\u001b[39m=\u001b[39mencoding,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    527\u001b[0m     \u001b[39mfor\u001b[39;00m encoding \u001b[39min\u001b[39;00m encodings\n\u001b[1;32m    528\u001b[0m ]\n",
      "\u001b[0;31mPanicException\u001b[0m: `stride` must be strictly less than `max_len=98` (note that `max_len` may be shorter than the max length of the original model, as it subtracts the number of special characters"
     ]
    }
   ],
   "source": [
    "validation_dataset = dataset[\"test\"].map(\n",
    "    preprocess_validation_examples,\n",
    "    batched=True,\n",
    "    remove_columns=dataset[\"test\"].column_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    questions = [q.strip() for q in examples[\"question\"]]\n",
    "    inputs = tokenizer(\n",
    "        questions,\n",
    "        examples[\"context\"],\n",
    "        max_length=tokenizer.model_max_length,\n",
    "        truncation=\"only_second\",\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    offset_mapping = inputs.pop(\"offset_mapping\")\n",
    "    answers = examples[\"answers\"]\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "\n",
    "    for i, offset in enumerate(offset_mapping):\n",
    "        answer = answers[i]\n",
    "        start_char = answer[\"answer_start\"][0]\n",
    "        end_char = answer[\"answer_start\"][0] + len(answer[\"text\"][0])\n",
    "        sequence_ids = inputs.sequence_ids(i)\n",
    "        # print('sequence_ids: ',sequence_ids)\n",
    "        # print('sequence_ids: ',len(sequence_ids))\n",
    "\n",
    "        # Find the start and end of the context\n",
    "        idx = 0\n",
    "        while sequence_ids[idx] != 1:\n",
    "            idx += 1\n",
    "        context_start = idx\n",
    "        while sequence_ids[idx] == 1 and idx < len(sequence_ids) - 1:\n",
    "            idx += 1\n",
    "        context_end = idx - 1\n",
    "\n",
    "        # If the answer is not fully inside the context, label it (0, 0)\n",
    "        if offset[context_start][0] > end_char or offset[context_end][1] < start_char:\n",
    "            start_positions.append(0)\n",
    "            end_positions.append(0)\n",
    "        else:\n",
    "            # Otherwise it's the start and end token positions\n",
    "            idx = context_start\n",
    "            while idx <= context_end and offset[idx][0] <= start_char:\n",
    "                idx += 1\n",
    "            start_positions.append(idx - 1)\n",
    "\n",
    "            idx = context_end\n",
    "            while idx >= context_start and offset[idx][1] >= end_char:\n",
    "                idx -= 1\n",
    "            end_positions.append(idx + 1)\n",
    "\n",
    "    inputs[\"start_positions\"] = start_positions\n",
    "    inputs[\"end_positions\"] = end_positions\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/4000 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 4000/4000 [00:02<00:00, 1393.92 examples/s]\n",
      "Map: 100%|██████████| 1000/1000 [00:00<00:00, 1404.71 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import DefaultDataCollator\n",
    "\n",
    "tokenized_dataset = dataset.map(preprocess_function, batched=True, remove_columns=dataset['train'].column_names,batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at /home/miu/Projects/NLPLab/berts/models/bert and are newly initialized: ['bert.encoder.layer.3.intermediate.dense.weight', 'bert.encoder.layer.9.attention.output.LayerNorm.bias', 'bert.encoder.layer.6.attention.output.dense.bias', 'bert.encoder.layer.1.attention.output.dense.weight', 'bert.encoder.layer.2.attention.self.value.bias', 'bert.encoder.layer.8.attention.output.LayerNorm.bias', 'bert.encoder.layer.10.attention.output.dense.weight', 'bert.encoder.layer.8.output.LayerNorm.weight', 'bert.encoder.layer.6.attention.output.dense.weight', 'bert.encoder.layer.2.attention.self.query.weight', 'bert.encoder.layer.4.intermediate.dense.weight', 'bert.encoder.layer.4.attention.output.LayerNorm.bias', 'bert.encoder.layer.1.output.LayerNorm.weight', 'bert.encoder.layer.9.attention.self.value.bias', 'bert.encoder.layer.5.attention.self.query.weight', 'qa_outputs.bias', 'bert.encoder.layer.8.attention.self.value.bias', 'bert.encoder.layer.0.output.LayerNorm.bias', 'bert.encoder.layer.1.output.dense.weight', 'bert.encoder.layer.8.attention.self.key.weight', 'bert.encoder.layer.11.output.dense.weight', 'bert.encoder.layer.2.attention.self.key.weight', 'bert.encoder.layer.9.attention.output.dense.bias', 'bert.embeddings.word_embeddings.weight', 'bert.encoder.layer.8.output.dense.weight', 'bert.encoder.layer.11.intermediate.dense.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.bias', 'bert.encoder.layer.5.attention.self.value.bias', 'bert.encoder.layer.5.output.LayerNorm.bias', 'bert.encoder.layer.10.output.LayerNorm.weight', 'bert.encoder.layer.3.output.LayerNorm.weight', 'bert.embeddings.token_type_embeddings.weight', 'bert.encoder.layer.11.attention.output.LayerNorm.weight', 'bert.encoder.layer.7.attention.self.key.weight', 'bert.encoder.layer.0.attention.self.key.weight', 'bert.encoder.layer.0.attention.self.query.bias', 'bert.encoder.layer.9.intermediate.dense.weight', 'bert.encoder.layer.2.attention.self.key.bias', 'bert.encoder.layer.10.output.dense.bias', 'bert.encoder.layer.3.attention.self.key.bias', 'bert.encoder.layer.6.attention.self.value.weight', 'bert.encoder.layer.3.attention.output.dense.bias', 'bert.encoder.layer.10.attention.output.LayerNorm.weight', 'bert.encoder.layer.7.output.LayerNorm.bias', 'bert.encoder.layer.5.attention.self.key.bias', 'bert.encoder.layer.5.output.dense.bias', 'bert.encoder.layer.1.output.dense.bias', 'bert.encoder.layer.9.attention.output.dense.weight', 'bert.encoder.layer.11.attention.self.query.bias', 'bert.encoder.layer.9.attention.self.key.weight', 'bert.encoder.layer.8.attention.self.key.bias', 'bert.encoder.layer.9.output.LayerNorm.bias', 'bert.encoder.layer.4.attention.self.value.weight', 'bert.encoder.layer.1.attention.output.dense.bias', 'bert.encoder.layer.3.output.dense.weight', 'bert.encoder.layer.8.attention.output.dense.weight', 'bert.encoder.layer.7.intermediate.dense.bias', 'bert.encoder.layer.0.attention.self.value.weight', 'bert.encoder.layer.8.attention.output.LayerNorm.weight', 'bert.encoder.layer.7.attention.self.value.weight', 'bert.encoder.layer.3.attention.self.query.weight', 'bert.encoder.layer.6.attention.self.key.weight', 'bert.encoder.layer.6.intermediate.dense.bias', 'bert.encoder.layer.10.attention.self.query.weight', 'bert.encoder.layer.3.attention.output.LayerNorm.bias', 'bert.encoder.layer.4.output.dense.bias', 'bert.encoder.layer.5.attention.output.LayerNorm.weight', 'bert.encoder.layer.8.attention.self.value.weight', 'bert.encoder.layer.9.output.dense.weight', 'bert.encoder.layer.8.output.dense.bias', 'bert.encoder.layer.0.attention.output.dense.bias', 'bert.encoder.layer.7.attention.self.query.weight', 'bert.encoder.layer.1.attention.self.key.weight', 'bert.encoder.layer.6.output.dense.bias', 'bert.encoder.layer.11.attention.self.value.weight', 'bert.encoder.layer.7.attention.output.dense.weight', 'bert.encoder.layer.0.output.dense.bias', 'bert.encoder.layer.10.attention.output.dense.bias', 'bert.encoder.layer.2.attention.self.query.bias', 'bert.encoder.layer.5.output.LayerNorm.weight', 'bert.encoder.layer.4.intermediate.dense.bias', 'bert.encoder.layer.11.attention.self.key.bias', 'bert.encoder.layer.1.attention.self.value.bias', 'bert.encoder.layer.2.attention.output.LayerNorm.weight', 'bert.encoder.layer.1.attention.self.key.bias', 'bert.encoder.layer.6.attention.self.key.bias', 'bert.encoder.layer.8.attention.self.query.bias', 'bert.encoder.layer.6.attention.self.query.weight', 'bert.encoder.layer.3.attention.self.value.bias', 'bert.encoder.layer.10.output.LayerNorm.bias', 'bert.encoder.layer.7.output.dense.weight', 'bert.encoder.layer.5.attention.output.dense.bias', 'bert.encoder.layer.3.attention.self.value.weight', 'bert.encoder.layer.9.output.dense.bias', 'bert.encoder.layer.10.attention.self.value.weight', 'bert.encoder.layer.6.intermediate.dense.weight', 'bert.encoder.layer.7.output.LayerNorm.weight', 'bert.encoder.layer.11.attention.self.query.weight', 'bert.encoder.layer.6.attention.self.query.bias', 'bert.encoder.layer.11.attention.output.dense.bias', 'bert.encoder.layer.4.attention.self.key.weight', 'bert.encoder.layer.7.attention.self.query.bias', 'bert.encoder.layer.8.intermediate.dense.bias', 'bert.encoder.layer.10.attention.self.key.weight', 'bert.encoder.layer.2.attention.output.dense.weight', 'bert.encoder.layer.2.output.dense.bias', 'bert.encoder.layer.5.output.dense.weight', 'bert.encoder.layer.4.attention.output.dense.bias', 'bert.encoder.layer.11.attention.self.key.weight', 'bert.encoder.layer.10.output.dense.weight', 'bert.encoder.layer.5.attention.output.dense.weight', 'bert.encoder.layer.10.attention.output.LayerNorm.bias', 'bert.encoder.layer.5.attention.self.query.bias', 'bert.encoder.layer.1.output.LayerNorm.bias', 'bert.encoder.layer.2.intermediate.dense.weight', 'bert.encoder.layer.0.attention.self.key.bias', 'bert.encoder.layer.2.intermediate.dense.bias', 'bert.encoder.layer.4.attention.self.query.bias', 'bert.encoder.layer.7.intermediate.dense.weight', 'bert.encoder.layer.10.intermediate.dense.weight', 'bert.encoder.layer.1.attention.self.value.weight', 'bert.encoder.layer.7.attention.output.LayerNorm.weight', 'bert.encoder.layer.3.intermediate.dense.bias', 'bert.encoder.layer.11.attention.self.value.bias', 'bert.encoder.layer.4.attention.self.query.weight', 'bert.encoder.layer.6.output.LayerNorm.weight', 'bert.encoder.layer.3.attention.self.key.weight', 'bert.encoder.layer.9.attention.self.query.weight', 'bert.encoder.layer.0.attention.self.value.bias', 'bert.encoder.layer.3.attention.output.dense.weight', 'bert.encoder.layer.0.output.LayerNorm.weight', 'bert.encoder.layer.7.attention.self.value.bias', 'bert.encoder.layer.3.attention.self.query.bias', 'bert.encoder.layer.1.intermediate.dense.bias', 'bert.encoder.layer.9.attention.self.key.bias', 'bert.encoder.layer.5.attention.self.key.weight', 'bert.encoder.layer.4.attention.output.LayerNorm.weight', 'bert.encoder.layer.1.intermediate.dense.weight', 'bert.encoder.layer.2.attention.output.LayerNorm.bias', 'bert.encoder.layer.8.output.LayerNorm.bias', 'bert.encoder.layer.11.output.LayerNorm.weight', 'qa_outputs.weight', 'bert.encoder.layer.11.output.dense.bias', 'bert.encoder.layer.4.output.LayerNorm.weight', 'bert.encoder.layer.7.attention.self.key.bias', 'bert.encoder.layer.6.output.dense.weight', 'bert.encoder.layer.2.output.LayerNorm.bias', 'bert.encoder.layer.7.attention.output.dense.bias', 'bert.encoder.layer.0.intermediate.dense.bias', 'bert.encoder.layer.11.attention.output.dense.weight', 'bert.encoder.layer.11.intermediate.dense.weight', 'bert.encoder.layer.0.attention.output.dense.weight', 'bert.embeddings.LayerNorm.weight', 'bert.encoder.layer.10.attention.self.key.bias', 'bert.encoder.layer.5.attention.output.LayerNorm.bias', 'bert.encoder.layer.5.intermediate.dense.weight', 'bert.embeddings.position_embeddings.weight', 'bert.encoder.layer.0.intermediate.dense.weight', 'bert.encoder.layer.4.output.LayerNorm.bias', 'bert.encoder.layer.8.intermediate.dense.weight', 'bert.encoder.layer.9.attention.self.query.bias', 'bert.encoder.layer.2.attention.output.dense.bias', 'bert.encoder.layer.3.attention.output.LayerNorm.weight', 'bert.encoder.layer.10.attention.self.value.bias', 'bert.encoder.layer.1.attention.self.query.weight', 'bert.encoder.layer.1.attention.self.query.bias', 'bert.encoder.layer.5.intermediate.dense.bias', 'bert.encoder.layer.4.output.dense.weight', 'bert.encoder.layer.6.attention.self.value.bias', 'bert.encoder.layer.3.output.dense.bias', 'bert.encoder.layer.9.attention.output.LayerNorm.weight', 'bert.encoder.layer.7.attention.output.LayerNorm.bias', 'bert.encoder.layer.1.attention.output.LayerNorm.bias', 'bert.encoder.layer.4.attention.self.value.bias', 'bert.encoder.layer.6.attention.output.LayerNorm.weight', 'bert.encoder.layer.9.intermediate.dense.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.weight', 'bert.encoder.layer.10.intermediate.dense.bias', 'bert.embeddings.LayerNorm.bias', 'bert.encoder.layer.1.attention.output.LayerNorm.weight', 'bert.encoder.layer.6.attention.output.LayerNorm.bias', 'bert.encoder.layer.6.output.LayerNorm.bias', 'bert.encoder.layer.2.output.dense.weight', 'bert.encoder.layer.8.attention.self.query.weight', 'bert.encoder.layer.11.output.LayerNorm.bias', 'bert.encoder.layer.2.attention.self.value.weight', 'bert.encoder.layer.7.output.dense.bias', 'bert.encoder.layer.11.attention.output.LayerNorm.bias', 'bert.encoder.layer.4.attention.self.key.bias', 'bert.encoder.layer.10.attention.self.query.bias', 'bert.encoder.layer.0.attention.self.query.weight', 'bert.encoder.layer.9.attention.self.value.weight', 'bert.encoder.layer.3.output.LayerNorm.bias', 'bert.encoder.layer.4.attention.output.dense.weight', 'bert.encoder.layer.8.attention.output.dense.bias', 'bert.encoder.layer.9.output.LayerNorm.weight', 'bert.encoder.layer.5.attention.self.value.weight', 'bert.encoder.layer.2.output.LayerNorm.weight', 'bert.encoder.layer.0.output.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "from transformers import TrainingArguments\n",
    "from transformers import AutoModelForQuestionAnswering, AutoTokenizer\n",
    "\n",
    "#BertForMaskedLM\n",
    "from transformers import BertForMaskedLM\n",
    "\n",
    "args = TrainingArguments(\n",
    "    \"bert-finetuned-squad\",\n",
    "    evaluation_strategy=\"no\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    fp16=True,\n",
    "    push_to_hub=False,\n",
    ")\n",
    "# bert_model = BertForMaskedLM.from_pretrained(pretrained_model_name_or_path='bert-base-uncased')\n",
    "# bert_model = AutoModelForQuestionAnswering.from_pretrained(\"bert-base-uncased\")\n",
    "# bert_model = AutoModelForQuestionAnswering.from_pretrained(\"/home/kiki/dados_servidor/Servidor/NLPLab/berts/models/bert_spanbert\")\n",
    "if 'miu' in cur_dir:\n",
    "    bert_model = AutoModelForQuestionAnswering.from_pretrained(\"/home/miu/Projects/NLPLab/berts/models/bert\")\n",
    "else:\n",
    "    bert_model = AutoModelForQuestionAnswering.from_pretrained(\"/home/kiki/dados_servidor/Servidor/NLPLab/berts/models/bert\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mabcp4\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/miu/Projects/NLPLab/berts/wandb/run-20231224_114640-25o8xhra</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/abcp4/huggingface/runs/25o8xhra' target=\"_blank\">youthful-firebrand-172</a></strong> to <a href='https://wandb.ai/abcp4/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/abcp4/huggingface' target=\"_blank\">https://wandb.ai/abcp4/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/abcp4/huggingface/runs/25o8xhra' target=\"_blank\">https://wandb.ai/abcp4/huggingface/runs/25o8xhra</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2500 [00:00<?, ?it/s]You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      " 20%|██        | 500/2500 [00:39<02:38, 12.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.7581, 'learning_rate': 1.6000000000000003e-05, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 1000/2500 [01:20<01:57, 12.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.0755, 'learning_rate': 1.2e-05, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 1500/2500 [02:00<01:19, 12.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.7534, 'learning_rate': 8.000000000000001e-06, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 2000/2500 [02:40<00:39, 12.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.5015, 'learning_rate': 4.000000000000001e-06, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2500/2500 [03:21<00:00, 12.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.3212, 'learning_rate': 8e-09, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2500/2500 [03:22<00:00, 12.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 206.7869, 'train_samples_per_second': 96.718, 'train_steps_per_second': 12.09, 'train_loss': 3.8819138671875, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2500, training_loss=3.8819138671875, metrics={'train_runtime': 206.7869, 'train_samples_per_second': 96.718, 'train_steps_per_second': 12.09, 'train_loss': 3.8819138671875, 'epoch': 5.0})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=bert_model,\n",
    "    args=args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import collections\n",
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"squad\")\n",
    "n_best = 20\n",
    "max_answer_length = 30\n",
    "predicted_answers = []\n",
    "\n",
    "def compute_metrics(start_logits, end_logits, features, examples):\n",
    "    example_to_features = collections.defaultdict(list)\n",
    "    for idx, feature in enumerate(features):\n",
    "        example_to_features[feature[\"example_id\"]].append(idx)\n",
    "\n",
    "    predicted_answers = []\n",
    "    for example in tqdm(examples):\n",
    "        example_id = example[\"id\"]\n",
    "        context = example[\"context\"]\n",
    "        answers = []\n",
    "\n",
    "        # Loop through all features associated with that example\n",
    "        for feature_index in example_to_features[example_id]:\n",
    "            start_logit = start_logits[feature_index]\n",
    "            end_logit = end_logits[feature_index]\n",
    "            offsets = features[feature_index][\"offset_mapping\"]\n",
    "\n",
    "            start_indexes = np.argsort(start_logit)[-1 : -n_best - 1 : -1].tolist()\n",
    "            end_indexes = np.argsort(end_logit)[-1 : -n_best - 1 : -1].tolist()\n",
    "            for start_index in start_indexes:\n",
    "                for end_index in end_indexes:\n",
    "                    # Skip answers that are not fully in the context\n",
    "                    if offsets[start_index] is None or offsets[end_index] is None:\n",
    "                        continue\n",
    "                    # Skip answers with a length that is either < 0 or > max_answer_length\n",
    "                    if (\n",
    "                        end_index < start_index\n",
    "                        or end_index - start_index + 1 > max_answer_length\n",
    "                    ):\n",
    "                        continue\n",
    "\n",
    "                    answer = {\n",
    "                        \"text\": context[offsets[start_index][0] : offsets[end_index][1]],\n",
    "                        \"logit_score\": start_logit[start_index] + end_logit[end_index],\n",
    "                    }\n",
    "                    answers.append(answer)\n",
    "\n",
    "        # Select the answer with the best score\n",
    "        if len(answers) > 0:\n",
    "            best_answer = max(answers, key=lambda x: x[\"logit_score\"])\n",
    "            predicted_answers.append(\n",
    "                {\"id\": example_id, \"prediction_text\": best_answer[\"text\"]}\n",
    "            )\n",
    "        else:\n",
    "            predicted_answers.append({\"id\": example_id, \"prediction_text\": \"\"})\n",
    "\n",
    "    theoretical_answers = [{\"id\": ex[\"id\"], \"answers\": ex[\"answers\"]} for ex in examples]\n",
    "    return metric.compute(predictions=predicted_answers, references=theoretical_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 126/126 [00:02<00:00, 44.42it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 1607.05it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'exact_match': 3.9, 'f1': 8.451318538500729}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import evaluate\n",
    "\n",
    "predictions, _, _ = trainer.predict(validation_dataset)\n",
    "start_logits, end_logits = predictions\n",
    "compute_metrics(start_logits, end_logits, validation_dataset, dataset[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 126/126 [00:02<00:00, 44.45it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 1573.49it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'exact_match': 4.2, 'f1': 8.794485888565596}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#100k 512 tokens\n",
    "{'exact_match': 4.2, 'f1': 8.794485888565596}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SEQ trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel,AutoConfig,AutoModelForSequenceClassification\n",
    "\n",
    "# model_path =\"bert-base-uncased\"\n",
    "# model_path =\"/home/miu/Projects/crammed-bert\"\n",
    "model_path =\"models/bert\"\n",
    "\n",
    "tokenizer_path=\"bert-base-uncased\"\n",
    "# tokenizer_path=\"/home/miu/Projects/crammed-bert\"\n",
    "\n",
    "# model=AutoModel.from_pretrained(model_path, return_dict=False)\n",
    "config = AutoConfig.from_pretrained(model_path)\n",
    "# classifier_model=AutoModelForSequenceClassification.from_pretrained(model_path, config=config)\n",
    "# classifier_model.vocab_size = 32_768\n",
    "# classifier_model.vocab_size = 30_000\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n",
    "tokenizer.model_max_length=128\n",
    "MODEL_MAX_SEQ_LEN=tokenizer.model_max_length\n",
    "HF_MODEL=True\n",
    "MODEL_MAX_SEQ_LEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "GLUE_TASKS = [\"cola\", \"mnli\", \"mnli-mm\", \"mrpc\", \"qnli\", \"qqp\", \"rte\", \"sst2\", \"stsb\", \"wnli\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = \"mrpc\"\n",
    "model_checkpoint = model_path\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, load_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_324388/1389288479.py:3: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = load_metric('glue', actual_task)\n",
      "/home/miu/miniconda3/envs/torch/lib/python3.10/site-packages/datasets/load.py:752: FutureWarning: The repository for glue contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.16.0/metrics/glue/glue.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "actual_task = \"mnli\" if task == \"mnli-mm\" else task\n",
    "dataset = load_dataset(\"glue\", actual_task)\n",
    "metric = load_metric('glue', actual_task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_to_keys = {\n",
    "    \"cola\": (\"sentence\", None),\n",
    "    \"mnli\": (\"premise\", \"hypothesis\"),\n",
    "    \"mnli-mm\": (\"premise\", \"hypothesis\"),\n",
    "    \"mrpc\": (\"sentence1\", \"sentence2\"),\n",
    "    \"qnli\": (\"question\", \"sentence\"),\n",
    "    \"qqp\": (\"question1\", \"question2\"),\n",
    "    \"rte\": (\"sentence1\", \"sentence2\"),\n",
    "    \"sst2\": (\"sentence\", None),\n",
    "    \"stsb\": (\"sentence1\", \"sentence2\"),\n",
    "    \"wnli\": (\"sentence1\", \"sentence2\"),\n",
    "}\n",
    "sentence1_key, sentence2_key = task_to_keys[task]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    if sentence2_key is None:\n",
    "        return tokenizer(examples[sentence1_key], truncation=True)\n",
    "    return tokenizer(examples[sentence1_key], examples[sentence2_key], truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_dataset = dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at models/bert and are newly initialized: ['bert.encoder.layer.9.attention.self.value.bias', 'classifier.bias', 'bert.encoder.layer.5.output.dense.bias', 'bert.encoder.layer.7.output.dense.weight', 'bert.encoder.layer.8.attention.output.dense.weight', 'bert.encoder.layer.0.attention.self.query.bias', 'bert.encoder.layer.5.attention.self.query.bias', 'bert.encoder.layer.10.output.dense.bias', 'bert.encoder.layer.3.attention.output.dense.weight', 'bert.encoder.layer.1.attention.output.dense.bias', 'bert.encoder.layer.2.attention.self.query.bias', 'bert.encoder.layer.10.attention.self.key.weight', 'bert.encoder.layer.3.attention.output.dense.bias', 'bert.encoder.layer.3.attention.self.key.bias', 'classifier.weight', 'bert.encoder.layer.3.output.dense.weight', 'bert.encoder.layer.5.attention.self.key.weight', 'bert.encoder.layer.2.attention.self.key.weight', 'bert.embeddings.LayerNorm.bias', 'bert.encoder.layer.2.attention.output.LayerNorm.bias', 'bert.encoder.layer.2.attention.output.dense.weight', 'bert.encoder.layer.6.attention.output.LayerNorm.weight', 'bert.encoder.layer.11.attention.output.dense.weight', 'bert.encoder.layer.3.attention.self.query.weight', 'bert.encoder.layer.5.intermediate.dense.bias', 'bert.encoder.layer.4.attention.self.query.weight', 'bert.encoder.layer.1.output.LayerNorm.bias', 'bert.encoder.layer.1.attention.self.key.weight', 'bert.encoder.layer.9.output.dense.weight', 'bert.encoder.layer.2.attention.output.dense.bias', 'bert.encoder.layer.10.intermediate.dense.bias', 'bert.encoder.layer.3.attention.self.key.weight', 'bert.encoder.layer.9.attention.self.key.bias', 'bert.encoder.layer.3.attention.self.query.bias', 'bert.encoder.layer.0.output.dense.weight', 'bert.encoder.layer.6.output.LayerNorm.weight', 'bert.embeddings.position_embeddings.weight', 'bert.encoder.layer.7.intermediate.dense.bias', 'bert.embeddings.LayerNorm.weight', 'bert.encoder.layer.10.intermediate.dense.weight', 'bert.encoder.layer.4.attention.output.dense.bias', 'bert.encoder.layer.6.output.dense.bias', 'bert.encoder.layer.6.output.dense.weight', 'bert.encoder.layer.11.output.LayerNorm.weight', 'bert.encoder.layer.1.attention.output.dense.weight', 'bert.encoder.layer.8.attention.output.LayerNorm.bias', 'bert.encoder.layer.10.output.LayerNorm.bias', 'bert.encoder.layer.3.output.LayerNorm.bias', 'bert.encoder.layer.3.output.LayerNorm.weight', 'bert.encoder.layer.7.attention.self.key.weight', 'bert.encoder.layer.10.attention.self.value.weight', 'bert.encoder.layer.10.attention.self.key.bias', 'bert.encoder.layer.2.attention.self.value.weight', 'bert.encoder.layer.1.intermediate.dense.bias', 'bert.encoder.layer.9.attention.output.LayerNorm.weight', 'bert.encoder.layer.0.intermediate.dense.weight', 'bert.encoder.layer.10.attention.output.LayerNorm.bias', 'bert.encoder.layer.1.intermediate.dense.weight', 'bert.encoder.layer.7.attention.self.query.bias', 'bert.encoder.layer.2.output.dense.weight', 'bert.encoder.layer.5.attention.self.value.bias', 'bert.encoder.layer.5.attention.output.dense.weight', 'bert.encoder.layer.6.intermediate.dense.weight', 'bert.encoder.layer.4.output.LayerNorm.bias', 'bert.encoder.layer.11.attention.self.query.weight', 'bert.encoder.layer.11.output.dense.bias', 'bert.encoder.layer.11.attention.self.query.bias', 'bert.encoder.layer.4.attention.self.key.weight', 'bert.encoder.layer.5.output.dense.weight', 'bert.encoder.layer.0.attention.self.key.weight', 'bert.encoder.layer.7.attention.self.value.weight', 'bert.encoder.layer.0.intermediate.dense.bias', 'bert.encoder.layer.6.attention.self.value.bias', 'bert.encoder.layer.2.intermediate.dense.bias', 'bert.encoder.layer.4.output.dense.bias', 'bert.encoder.layer.6.intermediate.dense.bias', 'bert.encoder.layer.4.output.LayerNorm.weight', 'bert.encoder.layer.9.intermediate.dense.weight', 'bert.encoder.layer.1.output.dense.bias', 'bert.encoder.layer.4.output.dense.weight', 'bert.encoder.layer.4.attention.self.query.bias', 'bert.encoder.layer.3.attention.output.LayerNorm.bias', 'bert.encoder.layer.0.output.LayerNorm.bias', 'bert.encoder.layer.11.attention.self.value.weight', 'bert.encoder.layer.8.attention.self.key.bias', 'bert.encoder.layer.7.attention.output.dense.bias', 'bert.encoder.layer.0.attention.output.dense.weight', 'bert.encoder.layer.7.attention.output.dense.weight', 'bert.encoder.layer.0.output.dense.bias', 'bert.encoder.layer.10.attention.output.dense.bias', 'bert.encoder.layer.0.attention.self.key.bias', 'bert.embeddings.word_embeddings.weight', 'bert.encoder.layer.5.attention.self.key.bias', 'bert.encoder.layer.8.attention.self.query.bias', 'bert.encoder.layer.8.attention.self.query.weight', 'bert.encoder.layer.10.attention.output.LayerNorm.weight', 'bert.encoder.layer.7.attention.self.key.bias', 'bert.encoder.layer.11.attention.output.dense.bias', 'bert.encoder.layer.1.attention.output.LayerNorm.bias', 'bert.encoder.layer.4.intermediate.dense.weight', 'bert.encoder.layer.9.output.LayerNorm.weight', 'bert.encoder.layer.10.attention.self.query.weight', 'bert.encoder.layer.11.intermediate.dense.weight', 'bert.encoder.layer.9.attention.self.query.weight', 'bert.encoder.layer.10.attention.self.value.bias', 'bert.encoder.layer.9.attention.output.dense.bias', 'bert.encoder.layer.1.output.LayerNorm.weight', 'bert.encoder.layer.2.attention.output.LayerNorm.weight', 'bert.encoder.layer.4.attention.self.key.bias', 'bert.encoder.layer.7.attention.output.LayerNorm.bias', 'bert.encoder.layer.0.attention.self.value.weight', 'bert.encoder.layer.3.output.dense.bias', 'bert.encoder.layer.4.intermediate.dense.bias', 'bert.encoder.layer.1.attention.output.LayerNorm.weight', 'bert.encoder.layer.5.intermediate.dense.weight', 'bert.encoder.layer.0.output.LayerNorm.weight', 'bert.encoder.layer.1.attention.self.query.weight', 'bert.encoder.layer.6.attention.self.key.weight', 'bert.pooler.dense.bias', 'bert.encoder.layer.5.output.LayerNorm.bias', 'bert.encoder.layer.2.output.LayerNorm.weight', 'bert.encoder.layer.11.output.dense.weight', 'bert.encoder.layer.2.output.dense.bias', 'bert.encoder.layer.5.output.LayerNorm.weight', 'bert.encoder.layer.7.attention.self.value.bias', 'bert.encoder.layer.5.attention.output.LayerNorm.weight', 'bert.encoder.layer.9.output.LayerNorm.bias', 'bert.encoder.layer.1.attention.self.value.bias', 'bert.embeddings.token_type_embeddings.weight', 'bert.encoder.layer.8.output.LayerNorm.bias', 'bert.encoder.layer.10.output.LayerNorm.weight', 'bert.encoder.layer.3.attention.output.LayerNorm.weight', 'bert.encoder.layer.4.attention.output.dense.weight', 'bert.encoder.layer.9.attention.output.LayerNorm.bias', 'bert.encoder.layer.7.attention.output.LayerNorm.weight', 'bert.pooler.dense.weight', 'bert.encoder.layer.7.output.LayerNorm.bias', 'bert.encoder.layer.10.attention.output.dense.weight', 'bert.encoder.layer.10.attention.self.query.bias', 'bert.encoder.layer.2.attention.self.key.bias', 'bert.encoder.layer.2.attention.self.query.weight', 'bert.encoder.layer.3.intermediate.dense.bias', 'bert.encoder.layer.5.attention.output.LayerNorm.bias', 'bert.encoder.layer.5.attention.output.dense.bias', 'bert.encoder.layer.1.attention.self.query.bias', 'bert.encoder.layer.8.attention.self.key.weight', 'bert.encoder.layer.6.attention.output.dense.bias', 'bert.encoder.layer.8.attention.self.value.bias', 'bert.encoder.layer.2.intermediate.dense.weight', 'bert.encoder.layer.6.attention.self.query.weight', 'bert.encoder.layer.11.attention.self.key.weight', 'bert.encoder.layer.8.output.dense.weight', 'bert.encoder.layer.11.attention.self.key.bias', 'bert.encoder.layer.0.attention.output.dense.bias', 'bert.encoder.layer.7.intermediate.dense.weight', 'bert.encoder.layer.1.output.dense.weight', 'bert.encoder.layer.7.attention.self.query.weight', 'bert.encoder.layer.9.intermediate.dense.bias', 'bert.encoder.layer.11.attention.output.LayerNorm.weight', 'bert.encoder.layer.6.attention.self.key.bias', 'bert.encoder.layer.5.attention.self.value.weight', 'bert.encoder.layer.10.output.dense.weight', 'bert.encoder.layer.11.output.LayerNorm.bias', 'bert.encoder.layer.7.output.LayerNorm.weight', 'bert.encoder.layer.7.output.dense.bias', 'bert.encoder.layer.0.attention.self.query.weight', 'bert.encoder.layer.8.attention.output.LayerNorm.weight', 'bert.encoder.layer.6.output.LayerNorm.bias', 'bert.encoder.layer.9.attention.self.value.weight', 'bert.encoder.layer.11.attention.self.value.bias', 'bert.encoder.layer.2.attention.self.value.bias', 'bert.encoder.layer.6.attention.self.query.bias', 'bert.encoder.layer.6.attention.output.LayerNorm.bias', 'bert.encoder.layer.4.attention.self.value.bias', 'bert.encoder.layer.8.output.dense.bias', 'bert.encoder.layer.4.attention.output.LayerNorm.weight', 'bert.encoder.layer.8.intermediate.dense.weight', 'bert.encoder.layer.8.output.LayerNorm.weight', 'bert.encoder.layer.1.attention.self.value.weight', 'bert.encoder.layer.3.attention.self.value.bias', 'bert.encoder.layer.6.attention.output.dense.weight', 'bert.encoder.layer.9.output.dense.bias', 'bert.encoder.layer.8.attention.self.value.weight', 'bert.encoder.layer.3.attention.self.value.weight', 'bert.encoder.layer.4.attention.self.value.weight', 'bert.encoder.layer.9.attention.self.key.weight', 'bert.encoder.layer.3.intermediate.dense.weight', 'bert.encoder.layer.11.intermediate.dense.bias', 'bert.encoder.layer.1.attention.self.key.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.bias', 'bert.encoder.layer.9.attention.self.query.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.weight', 'bert.encoder.layer.8.attention.output.dense.bias', 'bert.encoder.layer.8.intermediate.dense.bias', 'bert.encoder.layer.4.attention.output.LayerNorm.bias', 'bert.encoder.layer.0.attention.self.value.bias', 'bert.encoder.layer.9.attention.output.dense.weight', 'bert.encoder.layer.2.output.LayerNorm.bias', 'bert.encoder.layer.11.attention.output.LayerNorm.bias', 'bert.encoder.layer.5.attention.self.query.weight', 'bert.encoder.layer.6.attention.self.value.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "\n",
    "num_labels = 3 if task.startswith(\"mnli\") else 1 if task==\"stsb\" else 2\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=num_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Custom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['labels', 'input_ids', 'token_type_ids', 'attention_mask']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_dataset = encoded_dataset.remove_columns([\"sentence1\", \"sentence2\", \"idx\"])\n",
    "encoded_dataset = encoded_dataset.rename_column(\"label\", \"labels\")\n",
    "encoded_dataset.set_format(\"torch\")\n",
    "encoded_dataset[\"train\"].column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from transformers import DataCollatorWithPadding\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    encoded_dataset[\"train\"], shuffle=True, batch_size=8, collate_fn=data_collator\n",
    ")\n",
    "eval_dataloader = DataLoader(\n",
    "    encoded_dataset[\"validation\"], batch_size=8, collate_fn=data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import Accelerator\n",
    "from transformers import AdamW, AutoModelForSequenceClassification, get_scheduler\n",
    "\n",
    "accelerator = Accelerator()\n",
    "optimizer = AdamW(model.parameters(), lr=3e-5)\n",
    "\n",
    "num_epochs = 1\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 34/459 [13:08<2:44:21, 23.20s/it]\n",
      "  0%|          | 0/459 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 459/459 [05:00<00:00,  1.53it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    # for batch in train_dataloader:\n",
    "    for batch in tqdm(train_dataloader):\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        accelerator.backward(loss)\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51/51 [00:08<00:00,  6.13it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.6838235294117647, 'f1': 0.8122270742358079}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"glue\", \"mrpc\")\n",
    "model.eval()\n",
    "\n",
    "for batch in tqdm(eval_dataloader):\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**batch)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "\n",
    "metric.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_name = \"pearson\" if task == \"stsb\" else \"matthews_correlation\" if task == \"cola\" else \"accuracy\"\n",
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "\n",
    "args = TrainingArguments(\n",
    "    f\"{model_name}-finetuned-{task}\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    save_strategy = \"no\",\n",
    "    # learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=5,\n",
    "    learning_rate=4e-5,\n",
    "    warmup_steps=0.1,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    weight_decay=0.01,\n",
    "    # load_best_model_at_end=True,\n",
    "    metric_for_best_model=metric_name,\n",
    "    push_to_hub=False,\n",
    "    report_to=\"none\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    if task != \"stsb\":\n",
    "        predictions = np.argmax(predictions, axis=1)\n",
    "    else:\n",
    "        predictions = predictions[:, 0]\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_key = \"validation_mismatched\" if task == \"mnli-mm\" else \"validation_matched\" if task == \"mnli\" else \"validation\"\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=encoded_dataset[\"train\"],\n",
    "    eval_dataset=encoded_dataset[validation_key],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1150' max='1150' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1150/1150 00:45, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.623602</td>\n",
       "      <td>0.683824</td>\n",
       "      <td>0.812227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.626583</td>\n",
       "      <td>0.683824</td>\n",
       "      <td>0.812227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.646300</td>\n",
       "      <td>0.617421</td>\n",
       "      <td>0.678922</td>\n",
       "      <td>0.786297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.646300</td>\n",
       "      <td>0.633018</td>\n",
       "      <td>0.676471</td>\n",
       "      <td>0.791798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.609000</td>\n",
       "      <td>0.629557</td>\n",
       "      <td>0.681373</td>\n",
       "      <td>0.785479</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1150, training_loss=0.6179507247261379, metrics={'train_runtime': 45.9213, 'train_samples_per_second': 399.379, 'train_steps_per_second': 25.043, 'total_flos': 716101959374880.0, 'train_loss': 0.6179507247261379, 'epoch': 5.0})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1150' max='1150' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1150/1150 00:45, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.423086</td>\n",
       "      <td>0.828431</td>\n",
       "      <td>0.879725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.405031</td>\n",
       "      <td>0.838235</td>\n",
       "      <td>0.886207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.437900</td>\n",
       "      <td>0.495247</td>\n",
       "      <td>0.848039</td>\n",
       "      <td>0.894198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.437900</td>\n",
       "      <td>0.608123</td>\n",
       "      <td>0.860294</td>\n",
       "      <td>0.901893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.144300</td>\n",
       "      <td>0.650582</td>\n",
       "      <td>0.865196</td>\n",
       "      <td>0.904679</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1150, training_loss=0.2635675811767578, metrics={'train_runtime': 45.7017, 'train_samples_per_second': 401.298, 'train_steps_per_second': 25.163, 'total_flos': 716101959374880.0, 'train_loss': 0.2635675811767578, 'epoch': 5.0})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1150' max='1150' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1150/1150 00:45, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.613799</td>\n",
       "      <td>0.683824</td>\n",
       "      <td>0.812227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.593775</td>\n",
       "      <td>0.686275</td>\n",
       "      <td>0.799373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.627900</td>\n",
       "      <td>0.622357</td>\n",
       "      <td>0.713235</td>\n",
       "      <td>0.807249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.627900</td>\n",
       "      <td>0.688114</td>\n",
       "      <td>0.644608</td>\n",
       "      <td>0.725898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.496900</td>\n",
       "      <td>0.775074</td>\n",
       "      <td>0.678922</td>\n",
       "      <td>0.771379</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1150, training_loss=0.5382972617771314, metrics={'train_runtime': 45.7071, 'train_samples_per_second': 401.251, 'train_steps_per_second': 25.16, 'total_flos': 716101959374880.0, 'train_loss': 0.5382972617771314, 'epoch': 5.0})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6' max='26' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 6/26 00:00 < 00:00, 116.39 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.6295565366744995,\n",
       " 'eval_accuracy': 0.6813725490196079,\n",
       " 'eval_f1': 0.7854785478547854,\n",
       " 'eval_runtime': 0.2419,\n",
       " 'eval_samples_per_second': 1686.844,\n",
       " 'eval_steps_per_second': 107.495,\n",
       " 'epoch': 5.0}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='26' max='26' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [26/26 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.7750740647315979,\n",
       " 'eval_accuracy': 0.678921568627451,\n",
       " 'eval_f1': 0.7713787085514833,\n",
       " 'eval_runtime': 0.2396,\n",
       " 'eval_samples_per_second': 1703.04,\n",
       " 'eval_steps_per_second': 108.527,\n",
       " 'epoch': 5.0}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57218c19fff94363bfe1478f7d6e2edc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.5459453463554382,\n",
       " 'eval_accuracy': 0.8578431372549019,\n",
       " 'eval_f1': 0.8993055555555555,\n",
       " 'eval_runtime': 0.4866,\n",
       " 'eval_samples_per_second': 838.425,\n",
       " 'eval_steps_per_second': 53.429,\n",
       " 'epoch': 5.0}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SEQ custom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at models/bert and are newly initialized: ['bert.encoder.layer.1.attention.self.key.weight', 'bert.encoder.layer.9.output.dense.bias', 'bert.encoder.layer.5.attention.self.value.bias', 'bert.encoder.layer.5.output.dense.weight', 'bert.encoder.layer.11.output.LayerNorm.bias', 'bert.encoder.layer.11.output.LayerNorm.weight', 'bert.encoder.layer.1.output.dense.weight', 'bert.encoder.layer.2.output.LayerNorm.weight', 'bert.encoder.layer.5.attention.output.LayerNorm.bias', 'bert.encoder.layer.6.intermediate.dense.bias', 'bert.encoder.layer.10.attention.self.query.bias', 'bert.encoder.layer.11.attention.output.LayerNorm.weight', 'bert.encoder.layer.4.attention.output.dense.weight', 'bert.encoder.layer.9.attention.output.LayerNorm.bias', 'bert.encoder.layer.7.intermediate.dense.bias', 'bert.encoder.layer.2.attention.self.value.bias', 'bert.encoder.layer.3.attention.self.value.bias', 'bert.encoder.layer.8.attention.output.LayerNorm.bias', 'bert.encoder.layer.10.output.LayerNorm.bias', 'bert.encoder.layer.8.attention.self.query.bias', 'bert.encoder.layer.11.attention.self.query.bias', 'bert.encoder.layer.5.output.dense.bias', 'bert.encoder.layer.2.intermediate.dense.weight', 'bert.encoder.layer.6.attention.output.dense.bias', 'bert.encoder.layer.1.attention.self.value.weight', 'bert.embeddings.LayerNorm.bias', 'bert.encoder.layer.3.output.LayerNorm.weight', 'bert.encoder.layer.10.output.dense.bias', 'bert.encoder.layer.9.attention.self.value.bias', 'bert.encoder.layer.10.attention.output.dense.weight', 'bert.encoder.layer.7.intermediate.dense.weight', 'bert.encoder.layer.11.intermediate.dense.weight', 'bert.encoder.layer.11.attention.output.dense.weight', 'bert.encoder.layer.9.output.dense.weight', 'bert.encoder.layer.7.attention.self.key.weight', 'bert.encoder.layer.7.attention.self.value.bias', 'bert.encoder.layer.7.attention.output.dense.weight', 'bert.encoder.layer.8.attention.self.key.weight', 'bert.encoder.layer.8.intermediate.dense.weight', 'bert.encoder.layer.1.attention.self.value.bias', 'bert.encoder.layer.3.attention.output.dense.weight', 'bert.encoder.layer.10.output.dense.weight', 'bert.encoder.layer.5.attention.output.dense.bias', 'bert.encoder.layer.7.attention.self.key.bias', 'bert.encoder.layer.1.output.dense.bias', 'bert.encoder.layer.7.output.LayerNorm.weight', 'bert.encoder.layer.2.attention.self.value.weight', 'bert.encoder.layer.4.output.LayerNorm.bias', 'bert.encoder.layer.10.intermediate.dense.weight', 'bert.encoder.layer.9.attention.self.key.bias', 'bert.encoder.layer.1.attention.self.query.bias', 'bert.encoder.layer.4.output.dense.bias', 'bert.encoder.layer.8.intermediate.dense.bias', 'bert.encoder.layer.9.intermediate.dense.weight', 'bert.embeddings.LayerNorm.weight', 'bert.encoder.layer.1.attention.self.query.weight', 'bert.encoder.layer.4.attention.self.key.bias', 'bert.encoder.layer.4.attention.output.LayerNorm.weight', 'bert.encoder.layer.11.intermediate.dense.bias', 'bert.encoder.layer.6.output.LayerNorm.weight', 'bert.encoder.layer.3.attention.self.key.weight', 'bert.encoder.layer.7.output.LayerNorm.bias', 'bert.embeddings.token_type_embeddings.weight', 'bert.encoder.layer.2.attention.self.query.bias', 'bert.encoder.layer.4.intermediate.dense.bias', 'bert.encoder.layer.0.output.LayerNorm.weight', 'bert.pooler.dense.weight', 'bert.encoder.layer.1.attention.output.LayerNorm.weight', 'bert.encoder.layer.5.attention.self.query.weight', 'bert.encoder.layer.3.attention.output.LayerNorm.weight', 'bert.encoder.layer.7.attention.output.dense.bias', 'bert.encoder.layer.0.intermediate.dense.bias', 'bert.encoder.layer.2.intermediate.dense.bias', 'bert.encoder.layer.0.attention.output.dense.bias', 'bert.encoder.layer.7.attention.self.query.weight', 'bert.encoder.layer.4.intermediate.dense.weight', 'bert.encoder.layer.6.attention.self.key.bias', 'bert.encoder.layer.9.attention.self.key.weight', 'bert.encoder.layer.6.attention.output.dense.weight', 'bert.encoder.layer.3.intermediate.dense.weight', 'bert.encoder.layer.7.attention.output.LayerNorm.bias', 'bert.encoder.layer.8.attention.self.key.bias', 'bert.encoder.layer.10.attention.output.dense.bias', 'bert.encoder.layer.10.attention.self.value.weight', 'bert.encoder.layer.8.attention.output.dense.weight', 'bert.encoder.layer.1.attention.output.LayerNorm.bias', 'bert.encoder.layer.6.attention.output.LayerNorm.weight', 'bert.encoder.layer.1.attention.output.dense.bias', 'bert.encoder.layer.11.attention.self.value.bias', 'bert.encoder.layer.0.intermediate.dense.weight', 'bert.encoder.layer.8.attention.output.LayerNorm.weight', 'bert.encoder.layer.3.attention.self.query.bias', 'bert.encoder.layer.3.attention.output.dense.bias', 'bert.encoder.layer.5.attention.self.value.weight', 'bert.encoder.layer.9.attention.output.dense.weight', 'bert.encoder.layer.7.attention.self.query.bias', 'bert.encoder.layer.0.attention.self.key.bias', 'bert.encoder.layer.7.output.dense.weight', 'bert.encoder.layer.4.attention.self.value.weight', 'bert.encoder.layer.10.output.LayerNorm.weight', 'bert.encoder.layer.11.attention.self.query.weight', 'bert.encoder.layer.9.attention.self.value.weight', 'bert.encoder.layer.1.output.LayerNorm.weight', 'bert.encoder.layer.0.attention.output.dense.weight', 'bert.encoder.layer.7.attention.self.value.weight', 'bert.encoder.layer.0.attention.output.LayerNorm.bias', 'bert.encoder.layer.10.attention.self.query.weight', 'bert.encoder.layer.5.attention.self.key.weight', 'bert.encoder.layer.10.attention.self.key.bias', 'bert.encoder.layer.2.attention.output.LayerNorm.bias', 'bert.encoder.layer.6.output.dense.bias', 'bert.encoder.layer.6.attention.self.key.weight', 'bert.encoder.layer.4.attention.output.LayerNorm.bias', 'bert.encoder.layer.3.output.dense.bias', 'bert.encoder.layer.9.output.LayerNorm.weight', 'bert.encoder.layer.10.attention.self.value.bias', 'bert.encoder.layer.0.attention.self.query.weight', 'bert.encoder.layer.3.attention.self.key.bias', 'bert.encoder.layer.3.intermediate.dense.bias', 'bert.embeddings.position_embeddings.weight', 'bert.encoder.layer.0.output.LayerNorm.bias', 'bert.encoder.layer.0.attention.self.query.bias', 'bert.encoder.layer.2.output.dense.weight', 'bert.encoder.layer.4.attention.self.query.weight', 'bert.encoder.layer.9.attention.self.query.weight', 'bert.encoder.layer.11.output.dense.bias', 'bert.encoder.layer.11.attention.output.dense.bias', 'bert.encoder.layer.11.output.dense.weight', 'bert.encoder.layer.10.intermediate.dense.bias', 'bert.pooler.dense.bias', 'bert.encoder.layer.1.intermediate.dense.weight', 'bert.encoder.layer.2.attention.self.key.bias', 'bert.encoder.layer.4.attention.output.dense.bias', 'bert.encoder.layer.11.attention.self.value.weight', 'bert.encoder.layer.4.attention.self.query.bias', 'bert.encoder.layer.5.attention.self.key.bias', 'bert.encoder.layer.11.attention.self.key.weight', 'bert.encoder.layer.3.output.LayerNorm.bias', 'bert.encoder.layer.8.attention.self.value.weight', 'bert.encoder.layer.8.attention.self.query.weight', 'bert.encoder.layer.5.output.LayerNorm.weight', 'bert.encoder.layer.7.output.dense.bias', 'bert.encoder.layer.0.attention.self.value.weight', 'bert.encoder.layer.1.attention.output.dense.weight', 'bert.encoder.layer.10.attention.output.LayerNorm.bias', 'bert.encoder.layer.0.output.dense.bias', 'bert.encoder.layer.11.attention.output.LayerNorm.bias', 'bert.encoder.layer.5.attention.self.query.bias', 'bert.encoder.layer.5.output.LayerNorm.bias', 'bert.encoder.layer.6.attention.self.value.bias', 'bert.encoder.layer.10.attention.output.LayerNorm.weight', 'bert.encoder.layer.11.attention.self.key.bias', 'bert.encoder.layer.2.output.LayerNorm.bias', 'bert.encoder.layer.3.attention.output.LayerNorm.bias', 'bert.encoder.layer.0.attention.self.value.bias', 'bert.encoder.layer.8.attention.output.dense.bias', 'bert.encoder.layer.2.attention.output.dense.bias', 'bert.encoder.layer.4.attention.self.value.bias', 'bert.encoder.layer.5.intermediate.dense.weight', 'bert.encoder.layer.8.output.dense.bias', 'bert.encoder.layer.0.output.dense.weight', 'bert.encoder.layer.5.attention.output.dense.weight', 'bert.encoder.layer.4.output.LayerNorm.weight', 'bert.encoder.layer.6.attention.output.LayerNorm.bias', 'bert.encoder.layer.6.output.dense.weight', 'bert.encoder.layer.8.output.LayerNorm.weight', 'bert.encoder.layer.6.attention.self.value.weight', 'bert.encoder.layer.2.attention.self.key.weight', 'bert.encoder.layer.7.attention.output.LayerNorm.weight', 'bert.encoder.layer.6.attention.self.query.weight', 'bert.encoder.layer.4.attention.self.key.weight', 'bert.encoder.layer.4.output.dense.weight', 'bert.encoder.layer.2.attention.self.query.weight', 'bert.encoder.layer.9.intermediate.dense.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.weight', 'bert.encoder.layer.8.output.LayerNorm.bias', 'bert.encoder.layer.8.attention.self.value.bias', 'bert.encoder.layer.2.attention.output.dense.weight', 'bert.encoder.layer.1.intermediate.dense.bias', 'bert.encoder.layer.1.attention.self.key.bias', 'bert.encoder.layer.6.attention.self.query.bias', 'bert.encoder.layer.5.attention.output.LayerNorm.weight', 'bert.encoder.layer.0.attention.self.key.weight', 'bert.encoder.layer.9.attention.output.LayerNorm.weight', 'bert.encoder.layer.10.attention.self.key.weight', 'bert.encoder.layer.3.output.dense.weight', 'bert.encoder.layer.8.output.dense.weight', 'bert.encoder.layer.9.output.LayerNorm.bias', 'bert.encoder.layer.2.output.dense.bias', 'bert.encoder.layer.1.output.LayerNorm.bias', 'bert.encoder.layer.6.intermediate.dense.weight', 'bert.encoder.layer.2.attention.output.LayerNorm.weight', 'bert.embeddings.word_embeddings.weight', 'bert.encoder.layer.3.attention.self.value.weight', 'bert.encoder.layer.9.attention.self.query.bias', 'bert.encoder.layer.9.attention.output.dense.bias', 'bert.encoder.layer.3.attention.self.query.weight', 'bert.encoder.layer.6.output.LayerNorm.bias', 'bert.encoder.layer.5.intermediate.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel,AutoConfig,AutoModelForSequenceClassification\n",
    "\n",
    "# model_path =\"bert-base-uncased\"\n",
    "# model_path =\"/home/miu/Projects/crammed-bert\"\n",
    "model_path =\"models/bert\"\n",
    "\n",
    "tokenizer_path=\"bert-base-uncased\"\n",
    "# tokenizer_path=\"/home/miu/Projects/crammed-bert\"\n",
    "\n",
    "model=AutoModel.from_pretrained(model_path, return_dict=False)\n",
    "config = AutoConfig.from_pretrained(model_path)\n",
    "# classifier_model=AutoModelForSequenceClassification.from_pretrained(model_path, config=config)\n",
    "# classifier_model.vocab_size = 32_768\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n",
    "tokenizer.model_max_length=128\n",
    "MODEL_MAX_SEQ_LEN=tokenizer.model_max_length\n",
    "HF_MODEL=True\n",
    "MODEL_MAX_SEQ_LEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hidden layer extractor class, for magically adding adapter to language model to be pretrained\n",
    "class HiddenLayerExtractor(nn.Module):\n",
    "    def __init__(self, net, layer = -2):\n",
    "        super().__init__()\n",
    "        self.net = net\n",
    "        self.layer = layer\n",
    "\n",
    "        self.hidden = None\n",
    "        self.hook_registered = False\n",
    "\n",
    "    def _find_layer(self):\n",
    "        if type(self.layer) == str:\n",
    "            modules = dict([*self.net.named_modules()])\n",
    "            return modules.get(self.layer, None)\n",
    "        elif type(self.layer) == int:\n",
    "            children = [*self.net.children()]\n",
    "            return children[self.layer]\n",
    "        return None\n",
    "\n",
    "    def _hook(self, _, __, output):\n",
    "        self.hidden = output\n",
    "\n",
    "    def _register_hook(self):\n",
    "        layer = self._find_layer()\n",
    "        assert layer is not None, f'hidden layer ({self.layer}) not found'\n",
    "        handle = layer.register_forward_hook(self._hook)\n",
    "        self.hook_registered = True\n",
    "\n",
    "    def forward(self, x,att_mask=None):\n",
    "        if self.layer == -1:\n",
    "            return self.net(x,att_mask)\n",
    "\n",
    "        if not self.hook_registered:\n",
    "            self._register_hook()\n",
    "\n",
    "        _ = self.net(x,att_mask)\n",
    "        hidden = self.hidden\n",
    "        self.hidden = None\n",
    "        assert hidden is not None, f'hidden layer {self.layer} never emitted an output'\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_extractor = HiddenLayerExtractor(model,-2)\n",
    "model_extractor = HiddenLayerExtractor(model,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "import torch\n",
    "import os\n",
    "from torch.utils.data import  DataLoader\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "class Classifier(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.encoder = model_extractor\n",
    "        \n",
    "        # self.d = torch.nn.Dropout(0.3)\n",
    "        self.fl = torch.nn.Flatten()\n",
    "        self.l1 = torch.nn.Linear(768*(MODEL_MAX_SEQ_LEN), 768)\n",
    "        self.l2 = torch.nn.Linear(768, 128)\n",
    "        self.l3 = torch.nn.Linear(128, 2)\n",
    "        self.t=torch.nn.Tanh()\n",
    "    \n",
    "    def forward(self, ids,att_mask=None):\n",
    "        \n",
    "        output= self.encoder(ids,att_mask=att_mask)[0]\n",
    "        #get cls token\n",
    "        # output=output[:,0]\n",
    "        output = self.fl(output)\n",
    "        output = self.t(output)\n",
    "        output = self.l1(output)\n",
    "        output = self.l2(output)\n",
    "        output = self.l3(output)\n",
    "            \n",
    "        return output\n",
    "\n",
    "classifier_model = Classifier()\n",
    "classifier_model=classifier_model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class TokenizedDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "        self.ls=[]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        \n",
    "        #with hf tokenizer\n",
    "        s=self.dataset[\"sentence1\"][i]+'[SEP]'+self.dataset[\"sentence2\"][i]\n",
    "        t = tokenizer(s, padding='max_length', truncation=True, max_length=MODEL_MAX_SEQ_LEN, return_tensors=\"pt\")\n",
    "        tokens=t['input_ids']\n",
    "        att_mask=t['attention_mask']\n",
    "        \n",
    "        #Tokenizer do crammed bert ta extraindo ids fora no vocabulario total!!!\n",
    "        for k in range(len(tokens)):\n",
    "            for l in range(len(tokens[k])):\n",
    "                if tokens[k][l]>=tokenizer.vocab_size-2500:\n",
    "                    tokens[k][l]=0\n",
    "        \n",
    "        self.ls.append(i)\n",
    "\n",
    "        label=self.dataset[\"label\"][i]\n",
    "        \n",
    "        d={'input_ids':tokens,'label':label,'attention_mask':att_mask}\n",
    "        return d\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "epochs=4\n",
    "batch_size=16\n",
    "task='mrpc'\n",
    "dataset = load_dataset('glue', task, split='train')\n",
    "\n",
    "tokenized_dataset = TokenizedDataset(dataset)\n",
    "# train_dataloader=CustomDataloader(tokenized_dataset)\n",
    "train_dataloader = DataLoader(\n",
    "        tokenized_dataset, batch_size=batch_size\n",
    "    )\n",
    "\n",
    "def group_parameters(model, limited_decay_keys,weight_decay):\n",
    "    model_parameters = list(model.named_parameters())\n",
    "    if len(limited_decay_keys) > 0:\n",
    "        grouped_parameters = optimizer_grouped_parameters = [\n",
    "            {\n",
    "                \"params\": [p for n, p in model_parameters if not any(nd in n for nd in limited_decay_keys)],\n",
    "                \"weight_decay\": weight_decay,\n",
    "            },\n",
    "            {\n",
    "                \"params\": [p for n, p in model_parameters if any(nd in n for nd in limited_decay_keys)],\n",
    "                \"weight_decay\": 0.0,\n",
    "            },\n",
    "        ]\n",
    "    else:\n",
    "        grouped_parameters = [p for n, p in model_parameters]\n",
    "    return grouped_parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hf bert\n",
    "# optimizer = torch.optim.Adam(params =  classifier_model.parameters(), lr=2e-5,weight_decay=0.01)\n",
    "\n",
    "# #cramming bert\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "#'AdamW', 'lr': 4e-05, 'betas': [0.9, 0.98], 'eps': 1e-06, 'weight_decay': 0.0, 'amsgrad': False\n",
    "# optimizer = torch.optim.Adam(params =  classifier_model.parameters(), lr=4e-5)\n",
    "grouped_parameters = group_parameters(classifier_model, ['bias', 'LayerNorm.bias','LayerNorm.weight', 'norm'],0.01)\n",
    "# optimizer = torch.optim.AdamW(params =  classifier_model.parameters(), lr=4e-5,weight_decay=0.0,amsgrad=False,betas=(0.9, 0.98),eps=1e-06)\n",
    "optimizer = torch.optim.AdamW(params =  grouped_parameters, lr=4e-05,weight_decay=0.0,amsgrad=False,betas=(0.9, 0.98),eps=1e-06)\n",
    "num_cycles=0.5\n",
    "num_training_steps=len(train_dataloader)*batch_size*epochs\n",
    "scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=0.1, num_cycles=num_cycles, num_training_steps=num_training_steps)\n",
    "\n",
    "loss_list = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/230 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 230/230 [00:38<00:00,  5.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0  loss:  0.7191867249167484\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.31      0.09      0.14      1194\n",
      "           1       0.67      0.90      0.77      2474\n",
      "\n",
      "    accuracy                           0.64      3668\n",
      "   macro avg       0.49      0.50      0.46      3668\n",
      "weighted avg       0.56      0.64      0.57      3668\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 230/230 [00:39<00:00,  5.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  1  loss:  1.3523259504981664\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.53      0.03      0.06      1194\n",
      "           1       0.68      0.99      0.80      2474\n",
      "\n",
      "    accuracy                           0.68      3668\n",
      "   macro avg       0.60      0.51      0.43      3668\n",
      "weighted avg       0.63      0.68      0.56      3668\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 230/230 [00:39<00:00,  5.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  2  loss:  1.9420064615814583\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.20      0.30      1194\n",
      "           1       0.71      0.94      0.81      2474\n",
      "\n",
      "    accuracy                           0.70      3668\n",
      "   macro avg       0.66      0.57      0.56      3668\n",
      "weighted avg       0.67      0.70      0.64      3668\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 230/230 [00:39<00:00,  5.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  3  loss:  2.415924946283517\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.49      0.58      1194\n",
      "           1       0.78      0.90      0.84      2474\n",
      "\n",
      "    accuracy                           0.77      3668\n",
      "   macro avg       0.74      0.69      0.71      3668\n",
      "weighted avg       0.76      0.77      0.75      3668\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#FINETUNING\n",
    "import tqdm\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "\n",
    "classifier_model.train()\n",
    "loss_fct = torch.nn.CrossEntropyLoss()\n",
    "running_loss=0\n",
    "\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=True)\n",
    "for i in range(epochs):\n",
    "    predictions=[]\n",
    "    labels=[]\n",
    "    for _,data in enumerate(tqdm.tqdm(train_dataloader)):\n",
    "        \n",
    "        ids = data['input_ids'].to('cuda', dtype = torch.long)\n",
    "        att_mask = data['attention_mask'].to('cuda', dtype = torch.long)\n",
    "        if len(ids.shape)==3:\n",
    "            ids=ids.squeeze(1)\n",
    "        targets = data['label'].to('cuda', dtype = torch.long)\n",
    "\n",
    "        outputs = classifier_model(ids,att_mask)\n",
    "        # outputs=outputs[0]\n",
    "        \n",
    "        loss = loss_fct(outputs.view(-1, 2), targets.view(-1))\n",
    "        # loss=outputs.loss\n",
    "\n",
    "        running_loss+=loss.item()\n",
    "        \n",
    "        scaler.scale(loss).backward()\n",
    "        # if _%2==0:\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        optimizer.zero_grad()\n",
    "        scheduler.step()\n",
    "\n",
    "        outputs=torch.argmax(outputs, dim=1).cpu()\n",
    "        targets=targets.cpu()\n",
    "        \n",
    "        outputs=[int(o.item()) for o in outputs]\n",
    "        targets=[int(o.item()) for o in targets]\n",
    "\n",
    "        predictions+=outputs\n",
    "        labels+=targets\n",
    "    print('epoch: ',i,' loss: ',running_loss/len(train_dataloader))\n",
    "    from sklearn.metrics import classification_report\n",
    "    print(classification_report(labels, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EVALUATE\n",
    "# dataset = load_dataset('glue', task, split='validation')\n",
    "dataset = load_dataset('glue', task, split='test')\n",
    "tokenized_dataset = TokenizedDataset(dataset)\n",
    "\n",
    "eval_dataloader = DataLoader(\n",
    "        tokenized_dataset, batch_size=8\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_model.eval()\n",
    "predictions=[]\n",
    "labels=[]\n",
    "c=0\n",
    "for _,data in enumerate(eval_dataloader):\n",
    "    ids = data['input_ids'].to('cuda', dtype = torch.long)\n",
    "    if len(ids.shape)==3:\n",
    "        ids=ids.squeeze(1)\n",
    "    targets = data['label'].to('cuda', dtype = torch.long)\n",
    "    att_mask = data['attention_mask'].to('cuda', dtype = torch.long)\n",
    "    \n",
    "    outputs = classifier_model(ids,att_mask)\n",
    "    # outputs=outputs[0]\n",
    "    \n",
    "    outputs=torch.argmax(outputs, dim=1).cpu()\n",
    "    targets=targets.cpu()\n",
    "    \n",
    "    outputs=[int(o.item()) for o in outputs]\n",
    "    targets=[int(o.item()) for o in targets]\n",
    "\n",
    "    predictions+=outputs\n",
    "    labels+=targets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.46      0.35      0.40       578\n",
      "           1       0.71      0.79      0.75      1147\n",
      "\n",
      "    accuracy                           0.65      1725\n",
      "   macro avg       0.58      0.57      0.57      1725\n",
      "weighted avg       0.63      0.65      0.63      1725\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(labels, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.40      0.78      0.53       578\n",
      "           1       0.78      0.40      0.53      1147\n",
      "\n",
      "    accuracy                           0.53      1725\n",
      "   macro avg       0.59      0.59      0.53      1725\n",
      "weighted avg       0.65      0.53      0.53      1725\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bert 3 layers  10000 dataset\n",
    "#               precision    recall  f1-score   support\n",
    "\n",
    "#            0       0.61      0.09      0.15       129\n",
    "#            1       0.70      0.97      0.81       279\n",
    "\n",
    "#     accuracy                           0.69       408\n",
    "#    macro avg       0.65      0.53      0.48       408\n",
    "# weighted avg       0.67      0.69      0.60       408\n",
    "\n",
    "#bert normal\n",
    "#               precision    recall  f1-score   support\n",
    "\n",
    "#            0       0.67      0.90      0.77       129\n",
    "#            1       0.94      0.80      0.86       279\n",
    "\n",
    "#     accuracy                           0.83       408\n",
    "#    macro avg       0.81      0.85      0.82       408\n",
    "# weighted avg       0.86      0.83      0.83       408\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DensePhrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "        num_rows: 4000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"squad\", split=\"train[:5000]\")\n",
    "dataset = dataset.train_test_split(test_size=0.2)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load model and tokenizer\n",
    "from transformers import AutoTokenizer, AutoModel,AutoModelForQuestionAnswering,AutoConfig\n",
    "from transformers import TrainingArguments\n",
    "from transformers.utils import cached_file\n",
    "\n",
    "# tokenizer_path=\"bert-base-uncased\"\n",
    "# model_path =\"/home/miu/Projects/NLPLab/berts/models/bert\"\n",
    "# model_path=\"bert-base-uncased\"\n",
    "\n",
    "# tokenizer_path=\"/home/miu/Projects/crammed-bert\"\n",
    "# model_path =\"/home/miu/Projects/crammed-bert\"\n",
    "\n",
    "model_path='/home/miu/Projects/NLP/spanbert'\n",
    "model_path='princeton-nlp/densephrases-multi'\n",
    "tokenizer_path=\"bert-base-cased\"\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n",
    "tokenizer.model_max_length=128\n",
    "\n",
    "\n",
    "#model = AutoModelForQuestionAnswering.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "config = AutoConfig.from_pretrained(\"bert-base-uncased\")\n",
    "# config = AutoConfig.from_pretrained(\"/home/miu/Projects/crammed-bert\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedModel\n",
    "import copy\n",
    "from torch.nn import CrossEntropyLoss\n",
    "# Encoders: three LMs\n",
    "\n",
    "class Encoder(PreTrainedModel):\n",
    "# class Encoder():\n",
    "    def __init__(self,return_phrase=False,\n",
    "                 return_query=False):\n",
    "        super().__init__(config)\n",
    "        # super().__init__()\n",
    "        self.phrase_encoder = AutoModel.from_pretrained(model_path)\n",
    "        self.query_start_encoder = copy.deepcopy(self.phrase_encoder)\n",
    "        self.query_end_encoder = copy.deepcopy(self.phrase_encoder)\n",
    "        # Additional parameters\n",
    "        self.filter_linear = torch.nn.Linear(768, 2)\n",
    "        self.return_phrase = return_phrase\n",
    "        self.return_query = return_query\n",
    "\n",
    "    def embed_phrase(self, input_ids, attention_mask, token_type_ids):\n",
    "        \n",
    "        \"\"\" Get phrase embeddings (token-wise) \"\"\"\n",
    "        outputs = self.phrase_encoder(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "        )\n",
    "        return outputs[0], outputs[0]\n",
    "\n",
    "    def embed_query(self, input_ids_, attention_mask_, token_type_ids_):\n",
    "        \"\"\" Get query start/end embeddings \"\"\"\n",
    "        # print('input_ids_.shape: ',input_ids_.shape)\n",
    "        # Two LM based query reps\n",
    "        outputs_s_ = self.query_start_encoder(\n",
    "            input_ids_,\n",
    "            attention_mask=attention_mask_,\n",
    "            token_type_ids=token_type_ids_,\n",
    "        )\n",
    "        outputs_e_ = self.query_end_encoder(\n",
    "            input_ids_,\n",
    "            attention_mask=attention_mask_,\n",
    "            token_type_ids=token_type_ids_,\n",
    "        )\n",
    "        sequence_output_s_ = outputs_s_[0]\n",
    "        sequence_output_e_ = outputs_e_[0]\n",
    "        query_start = sequence_output_s_[:,:1,:]\n",
    "        query_end = sequence_output_e_[:,:1,:]\n",
    "        return query_start, query_end\n",
    "    \n",
    "    def forward(self,\n",
    "            input_ids=None, attention_mask=None, token_type_ids=None,\n",
    "            input_ids_=None, attention_mask_=None, token_type_ids_=None,\n",
    "            start_positions=None, end_positions=None,\n",
    "            neg_input_ids=None, neg_attention_mask=None, neg_token_type_ids=None,\n",
    "            example_id=None,):\n",
    "        # Context-side\n",
    "        if input_ids is not None:\n",
    "            assert len(input_ids.size()) == 2\n",
    "            start, end = self.embed_phrase(input_ids, attention_mask, token_type_ids)\n",
    "\n",
    "            if neg_input_ids is not None:\n",
    "                neg_start, neg_end = self.embed_phrase(neg_input_ids, neg_attention_mask, neg_token_type_ids)\n",
    "\n",
    "            # Get filter logits\n",
    "            filter_output = start[:]\n",
    "            filter_start_logits, filter_end_logits = self.filter_linear(filter_output).chunk(2, dim=2)\n",
    "            filter_start_logits = filter_start_logits.squeeze(2)\n",
    "            filter_end_logits = filter_end_logits.squeeze(2)\n",
    "\n",
    "            if self.return_phrase:\n",
    "                return (start, end, filter_start_logits, filter_end_logits)\n",
    "\n",
    "        # Query-side\n",
    "        if input_ids_ is not None:\n",
    "            assert len(input_ids_.size()) == 2\n",
    "            query_start, query_end = self.embed_query(input_ids_, attention_mask_, token_type_ids_)\n",
    "\n",
    "            if self.return_query:\n",
    "                return (query_start, query_end)\n",
    "        \n",
    "        # Get dense logits\n",
    "        start_logits = start.matmul(query_start.transpose(1, 2)).squeeze(-1)\n",
    "        end_logits = end.matmul(query_end.transpose(1, 2)).squeeze(-1)\n",
    "        dense_logits = start_logits.unsqueeze(2) + end_logits.unsqueeze(1)\n",
    "\n",
    "        if start_positions is not None and end_positions is not None:\n",
    "            # If we are on multi-GPU, split add a dimension\n",
    "            if len(start_positions.size()) > 1:\n",
    "                start_positions = start_positions.squeeze(-1)\n",
    "            if len(end_positions.size()) > 1:\n",
    "                end_positions = end_positions.squeeze(-1)\n",
    "\n",
    "            ignored_index = start_logits.size(1)\n",
    "            start_positions.clamp_(0, ignored_index)\n",
    "            end_positions.clamp_(0, ignored_index)\n",
    "\n",
    "            # 1) Single-passage loss\n",
    "            loss_fct = CrossEntropyLoss(ignore_index=ignored_index)\n",
    "            start_loss = loss_fct(dense_logits.mean(2), start_positions)\n",
    "            end_loss = loss_fct(dense_logits.mean(1), end_positions)\n",
    "            single_loss = (start_loss + end_loss) / 2\n",
    "            total_loss = single_loss\n",
    "\n",
    "            return (total_loss,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at princeton-nlp/densephrases-multi and are newly initialized: ['encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.2.intermediate.dense.weight', 'pooler.dense.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.4.output.dense.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.11.output.dense.weight', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.6.output.dense.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.9.output.dense.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.4.attention.self.key.bias', 'embeddings.token_type_embeddings.weight', 'encoder.layer.7.attention.self.key.bias', 'embeddings.position_embeddings.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.2.output.dense.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.8.output.dense.bias', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.1.output.dense.bias', 'embeddings.LayerNorm.bias', 'encoder.layer.0.attention.self.key.weight', 'embeddings.LayerNorm.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.1.attention.self.query.weight', 'pooler.dense.weight', 'encoder.layer.2.output.dense.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.8.output.dense.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.5.output.dense.bias', 'encoder.layer.4.output.dense.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.9.output.dense.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.5.output.dense.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'embeddings.word_embeddings.weight', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.8.attention.self.key.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model=Encoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, 128)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preprocessing the datasets.\n",
    "# Preprocessing is slighlty different for training and evaluation.\n",
    "column_names = dataset[\"train\"].column_names\n",
    "\n",
    "question_column_name = \"question\" if \"question\" in column_names else column_names[0]\n",
    "context_column_name = \"context\" if \"context\" in column_names else column_names[1]\n",
    "answer_column_name = \"answers\" if \"answers\" in column_names else column_names[2]\n",
    "\n",
    "# Padding side determines if we do (question|context) or (context|question).\n",
    "pad_on_right = tokenizer.padding_side == \"right\"\n",
    "\n",
    "# max_seq_length = tokenizer.model_max_length\n",
    "max_seq_length = 128\n",
    "max_query_length = 25#50\n",
    "pad_on_right,max_seq_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training preprocessing\n",
    "def prepare_train_features(examples):\n",
    "    # Some of the questions have lots of whitespace on the left, which is not useful and will make the\n",
    "    # truncation of the context fail (the tokenized question will take a lots of space). So we remove that\n",
    "    # left whitespace\n",
    "    examples[question_column_name] = [q.lstrip() for q in examples[question_column_name]]\n",
    "\n",
    "    # Tokenize our examples with truncation and maybe padding, but keep the overflows using a stride. This results\n",
    "    # in one example possible giving several features when a context is long, each of those features having a\n",
    "    # context that overlaps a bit the context of the previous feature.\n",
    "    \n",
    "    tokenized_examples = tokenizer(\n",
    "        examples[context_column_name],\n",
    "        truncation=\"only_first\",\n",
    "        max_length=max_seq_length,\n",
    "        # stride=128,\n",
    "        stride=64,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "\n",
    "    tokenized_questions = tokenizer(\n",
    "        examples[question_column_name],\n",
    "        truncation=\"only_first\",\n",
    "        max_length=max_query_length,\n",
    "        return_overflowing_tokens=False,\n",
    "        return_offsets_mapping=False,\n",
    "        padding=\"max_length\" \n",
    "    )\n",
    "\n",
    "    #bugfix: vocab tem 32000 tokens, mas embedding so tem 30522. Ajeitar!!!\n",
    "    # #if any input ids is bigger than tokenizer.vocab_size set it to 0\n",
    "    # for i in range(len(tokenized_questions[\"input_ids\"])):\n",
    "    #     for j in range(len(tokenized_questions[\"input_ids\"][i])):\n",
    "    #         if tokenized_questions[\"input_ids\"][i][j]>=tokenizer.vocab_size-3000:\n",
    "    #             tokenized_questions[\"input_ids\"][i][j]=0\n",
    "    # #if any input ids is bigger than tokenizer.vocab_size set it to 0\n",
    "    # for i in range(len(tokenized_examples[\"input_ids\"])):\n",
    "    #     for j in range(len(tokenized_examples[\"input_ids\"][i])):\n",
    "    #         if tokenized_examples[\"input_ids\"][i][j]>=tokenizer.vocab_size-3000:\n",
    "    #             tokenized_examples[\"input_ids\"][i][j]=0\n",
    "\n",
    "    # Since one example might give us several features if it has a long context, we need a map from a feature to\n",
    "    # its corresponding example. This key gives us just that.\n",
    "    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
    "    # The offset mappings will give us a map from token to character position in the original context. This will\n",
    "    # help us compute the start_positions and end_positions.\n",
    "    offset_mapping = tokenized_examples.pop(\"offset_mapping\")\n",
    "\n",
    "    \n",
    "\n",
    "    # Inflate questions based on sample_mapping\n",
    "    tokenized_examples['input_ids_'] = [tokenized_questions['input_ids'][i] for i in sample_mapping]\n",
    "    tokenized_examples['token_type_ids_'] = [tokenized_questions['token_type_ids'][i] for i in sample_mapping]\n",
    "    tokenized_examples['attention_mask_'] = [tokenized_questions['attention_mask'][i] for i in sample_mapping]\n",
    "\n",
    "    # Let's label those examples!\n",
    "    tokenized_examples[\"start_positions\"] = []\n",
    "    tokenized_examples[\"end_positions\"] = []\n",
    "\n",
    "    \n",
    "\n",
    "    for i, offsets in enumerate(offset_mapping):\n",
    "        # We will label impossible answers with the index of the CLS token.\n",
    "        input_ids = tokenized_examples[\"input_ids\"][i]\n",
    "        # cls_token_id = tokenizer.cls_token_id\n",
    "        # cls_index = input_ids.index(cls_token_id)\n",
    "        cls_index=0\n",
    "\n",
    "        # Grab the sequence corresponding to that example (to know what is the context and what is the question).\n",
    "        sequence_ids = tokenized_examples.sequence_ids(i)\n",
    "        #context_index = 1 if pad_on_right and args.append_title else 0\n",
    "        context_index =  0\n",
    "\n",
    "        # One example can give several spans, this is the index of the example containing this span of text.\n",
    "        sample_index = sample_mapping[i]\n",
    "        answers = examples[answer_column_name][sample_index]\n",
    "        # If no answers are given, set the cls_index as answer.\n",
    "        if len(answers[\"answer_start\"]) == 0:\n",
    "            tokenized_examples[\"start_positions\"].append(cls_index)\n",
    "            tokenized_examples[\"end_positions\"].append(cls_index)\n",
    "        else:\n",
    "            # Start/end character index of the answer in the text.\n",
    "            start_char = answers[\"answer_start\"][0]\n",
    "            end_char = start_char + len(answers[\"text\"][0])\n",
    "\n",
    "            # Start token index of the current span in the text.\n",
    "            token_start_index = 0\n",
    "            while sequence_ids[token_start_index] != context_index:\n",
    "                token_start_index += 1\n",
    "\n",
    "            # End token index of the current span in the text.\n",
    "            token_end_index = len(input_ids) - 1\n",
    "            while sequence_ids[token_end_index] != context_index:\n",
    "                token_end_index -= 1\n",
    "\n",
    "            # Detect if the answer is out of the span (in which case this feature is labeled with the CLS index).\n",
    "            if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n",
    "                tokenized_examples[\"start_positions\"].append(cls_index)\n",
    "                tokenized_examples[\"end_positions\"].append(cls_index)\n",
    "            else:\n",
    "                # Otherwise move the token_start_index and token_end_index to the two ends of the answer.\n",
    "                # Note: we could go after the last offset if the answer is the last word (edge case).\n",
    "                while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n",
    "                    token_start_index += 1\n",
    "                tokenized_examples[\"start_positions\"].append(token_start_index - 1)\n",
    "                while offsets[token_end_index][1] >= end_char:\n",
    "                    token_end_index -= 1\n",
    "                tokenized_examples[\"end_positions\"].append(token_end_index + 1)\n",
    "\n",
    "\n",
    "\n",
    "    #bugfix: limit to 1000. Ajeitar!!!\n",
    "    \n",
    "    tokenized_examples[\"input_ids\"]=tokenized_examples[\"input_ids\"][:1000]\n",
    "    tokenized_examples[\"input_ids_\"]=tokenized_examples[\"input_ids_\"][:1000]\n",
    "    tokenized_examples[\"attention_mask\"]=tokenized_examples[\"attention_mask\"][:1000]\n",
    "    tokenized_examples[\"attention_mask_\"]=tokenized_examples[\"attention_mask_\"][:1000]\n",
    "    tokenized_examples[\"token_type_ids\"]=tokenized_examples[\"token_type_ids\"][:1000]\n",
    "    tokenized_examples[\"token_type_ids_\"]=tokenized_examples[\"token_type_ids_\"][:1000]\n",
    "    tokenized_examples[\"start_positions\"]=tokenized_examples[\"start_positions\"][:1000]\n",
    "    tokenized_examples[\"end_positions\"]=tokenized_examples[\"end_positions\"][:1000]\n",
    "    return tokenized_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d59c227abf474e6190000dcaf38921e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on train dataset:   0%|          | 0/4000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_dataset = dataset[\"train\"]\n",
    "train_dataset = train_dataset.map(\n",
    "                prepare_train_features,\n",
    "                batched=True,\n",
    "                num_proc=1,\n",
    "                #remove_columns=column_names,\n",
    "                #load_from_cache_file=not args.overwrite_cache,\n",
    "                desc=\"Running tokenizer on train dataset\",\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import default_data_collator\n",
    "from transformers import DataCollatorWithPadding,EvalPrediction\n",
    "from datasets import load_metric\n",
    "from utils.utils_qa import postprocess_qa_predictions\n",
    "\n",
    "# Post-processing:\n",
    "def post_processing_function(examples, features, predictions, stage=\"eval\", filter_threshold=-1e5):\n",
    "    # Post-processing: we match the start logits and end logits to answers in the original context.\n",
    "    predictions, save_rate = postprocess_qa_predictions(\n",
    "        examples=examples,\n",
    "        features=features,\n",
    "        predictions=predictions,\n",
    "        version_2_with_negative=args.version_2_with_negative,\n",
    "        n_best_size=args.n_best_size,\n",
    "        max_answer_length=args.max_answer_length,\n",
    "        null_score_diff_threshold=args.null_score_diff_threshold,\n",
    "        output_dir=args.output_dir,\n",
    "        log_level=0,\n",
    "        prefix=stage,\n",
    "        filter_threshold=filter_threshold,\n",
    "    )\n",
    "    # Format the result to the format the metric expects.\n",
    "    if args.version_2_with_negative:\n",
    "        formatted_predictions = [\n",
    "            {\"id\": k, \"prediction_text\": v, \"no_answer_probability\": 0.0} for k, v in predictions.items()\n",
    "        ]\n",
    "    else:\n",
    "        formatted_predictions = [{\"id\": k, \"prediction_text\": v} for k, v in predictions.items()]\n",
    "\n",
    "    references = [{\"id\": ex[\"id\"], \"answers\": ex[answer_column_name]} for ex in examples]\n",
    "    return EvalPrediction(predictions=formatted_predictions, label_ids=references), save_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_380849/1562132463.py:5: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = load_metric(\"squad\")\n",
      "/home/miu/miniconda3/envs/torch/lib/python3.10/site-packages/datasets/load.py:752: FutureWarning: The repository for squad contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.16.0/metrics/squad/squad.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
     ]
    }
   ],
   "source": [
    "from utils.trainer_qa import QuestionAnsweringTrainer\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "pad_to_max_length=True\n",
    "metric = load_metric(\"squad\")\n",
    "# Data collator\n",
    "# We have already padded to max length if the corresponding flag is True, otherwise we need to pad in the data\n",
    "# collator.\n",
    "data_collator = (\n",
    "    default_data_collator\n",
    "    # if args.pad_to_max_length\n",
    "    # else DataCollatorWithPadding(tokenizer, pad_to_multiple_of=8 if args.fp16 else None)\n",
    "    if pad_to_max_length\n",
    "    else DataCollatorWithPadding(tokenizer, pad_to_multiple_of=8 if args.fp16 else None)\n",
    ")\n",
    "\n",
    "def compute_metrics(p: EvalPrediction):\n",
    "    return metric.compute(predictions=p.predictions, references=p.label_ids)\n",
    "\n",
    "args = TrainingArguments(\n",
    "    \"bert-finetuned-squad\",\n",
    "    evaluation_strategy=\"no\",\n",
    "    save_strategy=\"no\",\n",
    "    # use_cpu=True,\n",
    "    # learning_rate=2e-5,\n",
    "    # learning_rate=4e-5,\n",
    "    learning_rate=3e-5,\n",
    "    num_train_epochs=5,\n",
    "    warmup_steps=0.1,\n",
    "    # lr_scheduler_type=\"cosine\",\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    weight_decay=0.01,\n",
    "    fp16=True,\n",
    "    push_to_hub=False,\n",
    ")\n",
    "\n",
    "# Initialize our Trainer\n",
    "trainer = QuestionAnsweringTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    # args=None,\n",
    "    train_dataset=train_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    post_process_function=post_processing_function,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2500' max='2500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2500/2500 03:36, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>7.408100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>4.348500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>3.686800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>3.170000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>2.786900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_result = trainer.train(resume_from_checkpoint=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cramming original\n",
    "500\t7.016400\n",
    "1000\t4.341700\n",
    "1500\t3.777300\n",
    "2000\t3.374300\n",
    "2500\t3.108100\n",
    "\n",
    "#dataset treinado 10m\n",
    "# {'loss': 6.9726, 'learning_rate': 1.6016e-05, 'epoch': 1.0}\n",
    "# {'loss': 4.4086, 'learning_rate': 1.2016000000000002e-05, 'epoch': 2.0}\n",
    "# {'loss': 3.7937, 'learning_rate': 8.016e-06, 'epoch': 3.0}\n",
    "# {'loss': 3.3883, 'learning_rate': 4.016e-06, 'epoch': 4.0}\n",
    "# {'loss': 3.1216, 'learning_rate': 1.6e-08, 'epoch': 5.0}\n",
    "# {'train_runtime': 189.2476, 'train_samples_per_second': 105.682, 'train_steps_per_second': 13.21, 'train_loss': 4.336957421875, 'epoch': 5.0}\n",
    "\n",
    "#bert original\n",
    "#  20%|██        | 501/2500 [01:03<04:10,  7.97it/s]\n",
    "# {'loss': 4.8419, 'learning_rate': 1.6064e-05, 'epoch': 1.0}\n",
    "#  40%|████      | 1001/2500 [02:05<03:07,  7.99it/s]\n",
    "# {'loss': 0.999, 'learning_rate': 1.2064e-05, 'epoch': 2.0}\n",
    "#  60%|██████    | 1501/2500 [03:08<02:05,  7.96it/s]\n",
    "# {'loss': 0.4677, 'learning_rate': 8.064000000000001e-06, 'epoch': 3.0}\n",
    "#  80%|████████  | 2001/2500 [04:08<01:00,  8.26it/s]\n",
    "# {'loss': 0.2472, 'learning_rate': 4.064e-06, 'epoch': 4.0}\n",
    "# 100%|██████████| 2500/2500 [05:08<00:00,  8.11it/s]\n",
    "# {'loss': 0.1275, 'learning_rate': 6.4e-08, 'epoch': 5.0}\n",
    "# {'train_runtime': 308.1526, 'train_samples_per_second': 64.903, 'train_steps_per_second': 8.113, 'train_loss': 1.3366608764648438, 'epoch': 5.0}\n",
    "\n",
    "\n",
    "# 500\t9.653200\n",
    "# 1000\t5.496600\n",
    "# 1500\t4.896600\n",
    "# 2000\t4.454100\n",
    "# 2500\t4.119600"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create phrase vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving 18829 (context, question, answer) triples.\n",
      "Writing to /home/miu/Projects/NLP/PRO-ConvQA/wikidump/wiki-dev/0000_hf.json\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from utils.utils_dp import convert_squad_to_hf\n",
    "import logging\n",
    "\n",
    "# Make one log on every process with the configuration for debugging.\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
    "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "    level=logging.INFO,\n",
    ")\n",
    "\n",
    "data_files = {}\n",
    "data_files[\"test\"] = convert_squad_to_hf('/home/miu/Projects/NLP/PRO-ConvQA/wikidump/wiki-dev/0000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/miu/Projects/NLP/PRO-ConvQA/wikidump/wiki-dev/0000_hf.json'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_files[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b6aac5a35b642d290d02c14e73a4966",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "extension = data_files[\"test\"].split(\".\")[-1]\n",
    "raw_datasets = load_dataset(extension, data_files=data_files, field=\"data\")\n",
    "\n",
    "column_names = raw_datasets[\"test\"].column_names\n",
    "context_column_name = \"context\" if \"context\" in column_names else column_names[1]\n",
    "\n",
    "# Padding side determines if we do (question|context) or (context|question).\n",
    "pad_on_right = tokenizer.padding_side == \"right\"\n",
    "# max_seq_length = 512\n",
    "max_seq_length = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'doc_idx': 0,\n",
       " 'par_idx': 0,\n",
       " 'title': 'Percy Fawcett',\n",
       " 'context': ' Lieutenant Colonel Percy Harrison Fawcett (18 August 1867during or after 1925) was a British geographer, artillery officer, cartographer, archaeologist, and explorer of South America. Fawcett disappeared in 1925 (along with his eldest son, Jack, and one of Jack\\'s friends, Raleigh Rimmell) during an expedition to find \"Z\"—his name for an ancient lost city which he and others believed existed in the jungles of Brazil. Percy Fawcett was born on 18 August 1867 in Torquay, Devon, England, to Edward Boyd Fawcett and Myra Elizabeth (née MacDougall). Fawcett received his early education at Newton Abbot Proprietary College along with Bertram Fletcher Robinson. Fawcett\\'s India-born father was a Fellow of the Royal Geographical Society (RGS); his elder brother Edward Douglas Fawcett (1866–1960) was a mountain climber, an Eastern occultist, and the author of philosophical books and popular adventure novels.'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets['test'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "offset = 0\n",
    "\n",
    "# Validation preprocessing\n",
    "def prepare_validation_features(examples, indexes):\n",
    "    # Tokenize our examples with truncation and maybe padding, but keep the overflows using a stride. This results\n",
    "    # in one example possible giving several features when a context is long, each of those features having a\n",
    "    # context that overlaps a bit the context of the previous feature.\n",
    "    tokenized_examples = tokenizer(\n",
    "        examples[context_column_name],\n",
    "        truncation=\"only_first\",\n",
    "        max_length=max_seq_length,\n",
    "        # stride=128,\n",
    "        stride=64,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\" ,\n",
    "    )\n",
    "\n",
    "    # Since one example might give us several features if it has a long context, we need a map from a feature to\n",
    "    # its corresponding example. This key gives us just that.\n",
    "    sample_mapping = tokenized_examples[\"overflow_to_sample_mapping\"]\n",
    "    \n",
    "    # Inflate doc_idxs based on sample_mapping\n",
    "    tokenized_examples['doc_idx'] = [offset + examples['doc_idx'][i] for i in sample_mapping]\n",
    "\n",
    "    # This example_id indicates the index of an original paragraph (not question id)\n",
    "    tokenized_examples['example_id'] = [indexes[i] for i in sample_mapping]\n",
    "\n",
    "    for i in range(len(tokenized_examples[\"input_ids\"])):\n",
    "        # Grab the sequence corresponding to that example (to know what is the context and what is the question).\n",
    "        sequence_ids = tokenized_examples.sequence_ids(i)\n",
    "        context_index = 1\n",
    "\n",
    "        # Set to None the offset_mapping that are not part of the context so it's easy to determine if a token\n",
    "        # position is part of the context or not.\n",
    "        tokenized_examples[\"offset_mapping\"][i] = [\n",
    "            (o if sequence_ids[k] == context_index else None)\n",
    "            for k, o in enumerate(tokenized_examples[\"offset_mapping\"][i])\n",
    "        ]\n",
    "\n",
    "    return tokenized_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49a0604cf17b4eedaac00e1c66750ef3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on prediction dataset:   0%|          | 0/18829 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "examples = raw_datasets[\"test\"]\n",
    "accelerator = Accelerator(gradient_accumulation_steps=args.gradient_accumulation_steps)\n",
    "\n",
    "# Predict Feature Creation\n",
    "with accelerator.main_process_first():\n",
    "    dataset = examples.map(\n",
    "        prepare_validation_features,\n",
    "        batched=True,\n",
    "        num_proc=1,\n",
    "        with_indices=True,\n",
    "        remove_columns=column_names,\n",
    "        # load_from_cache_file=not args.overwrite_cache,\n",
    "        desc=\"Running tokenizer on prediction dataset\",\n",
    "    )\n",
    "\n",
    "# Data collator\n",
    "# We have already padded to max length if the corresponding flag is True, otherwise we need to pad in the data\n",
    "# collator.\n",
    "data_collator = (\n",
    "    default_data_collator\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "per_device_train_batch_size=8\n",
    "\n",
    "predict_dataset_for_model = dataset.remove_columns([ \"offset_mapping\",\"doc_idx\",\"overflow_to_sample_mapping\"])\n",
    "predict_dataloader = DataLoader(\n",
    "    predict_dataset_for_model, shuffle=True, collate_fn=data_collator, batch_size=per_device_train_batch_size,\n",
    ")\n",
    "\n",
    "model, predict_dataloader = accelerator.prepare(model, predict_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5882"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(predict_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5882 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 1000/5882 [00:10<00:51, 95.64it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([8, 128, 768]),\n",
       " torch.Size([8, 128, 768]),\n",
       " torch.Size([8, 128]),\n",
       " torch.Size([8, 128]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def to_numpy(tensor):\n",
    "    return tensor.detach().cpu().numpy()\n",
    "import tqdm\n",
    "c=0\n",
    "feature_id=0\n",
    "all_predictions=[]\n",
    "model.return_phrase=True\n",
    "# for step, inputs in enumerate(predict_dataloader):\n",
    "for inputs in tqdm.tqdm(predict_dataloader):\n",
    "    outputs=model(**inputs)\n",
    "\n",
    "    for item_idx, (start, end, filter_start, filter_end) in enumerate(zip(*outputs)):\n",
    "        a= {'example_id': inputs['example_id'][item_idx].item(),\n",
    "            'feature_id': 0, # dataloader should not be shuffled\n",
    "            'start': to_numpy(start),\n",
    "            'end': to_numpy(end),\n",
    "            'filter_start': to_numpy(filter_start),\n",
    "            'filter_end': to_numpy(filter_end),\n",
    "            'inputs': inputs['input_ids'][item_idx].tolist(),\n",
    "        }\n",
    "        all_predictions.append(a)\n",
    "    feature_id+=1\n",
    "    c+=1\n",
    "    if c>1000:\n",
    "        break\n",
    "outputs[0].shape,outputs[1].shape,outputs[2].shape,outputs[3].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8008, (128, 768))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_predictions),all_predictions[0]['start'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Search Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 128])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] hours ; the last 100 mg / kg are infused over the remaining 16 hours of the protocol. Intravenous acetylcysteine has the advantage of shortening hospital stay, increasing both doctor and patient convenience, and allowing administration of activated charcoal to reduce absorption of both the paracetamol and any co - ingested drugs without concerns about interference with oral acetylcysteine. Intravenous dosing varies with weight, specifically in children. For patients less than 20 kg, the loading dose is 150 mg / kg in 3 mL / kg diluent, administered over 60 minutes ; [SEP]'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(inputs['input_ids'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b62bb43a5b5473889fc528791ff23de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09038b9169d04c56a4a631da90b1c111",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on prediction dataset:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df=[]\n",
    "query = 'tell me about the historical spending on defense'\n",
    "\n",
    "for i in range(1):\n",
    "    df.append({'doc_idx':0, 'par_idx':0, 'title':'', 'context':query})\n",
    "p='/home/miu/Projects/NLPLab/DensePhrasesMinimal/query.json'\n",
    "with open(p, 'w') as f:\n",
    "        json.dump({'data': df}, f)\n",
    "r_d = load_dataset(extension, data_files=p, field=\"data\")\n",
    "r_d=r_d['train']\n",
    "# query_inputs = prepare_features(r_d['train'])\n",
    "with accelerator.main_process_first():\n",
    "    query_dataset = r_d.map(\n",
    "        prepare_validation_features,\n",
    "        batched=True,\n",
    "        num_proc=1,\n",
    "        with_indices=True,\n",
    "        desc=\"Running tokenizer on prediction dataset\",\n",
    "    )\n",
    "query_dataset = query_dataset.remove_columns([\"par_idx\", \"offset_mapping\",\"doc_idx\",\"overflow_to_sample_mapping\"])\n",
    "query_dataloader = DataLoader(\n",
    "    query_dataset, shuffle=True, collate_fn=data_collator, batch_size=1,\n",
    ")\n",
    "query_dataloader = accelerator.prepare(query_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.return_query=True\n",
    "model.return_phrase=False\n",
    "\n",
    "for step, query_inputs in enumerate(query_dataloader):\n",
    "    #print(query_inputs)\n",
    "    query_outputs=model(input_ids_=query_inputs['input_ids'],attention_mask_=query_inputs['attention_mask'])\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def search_mips(phrases_embs,query_emb):\n",
    "    print(phrases_embs.shape,query_emb.shape)\n",
    "    scores=[]\n",
    "    for i in range(phrases_embs.shape[0]):\n",
    "        batch=[]\n",
    "        for j in range(phrases_embs.shape[1]):\n",
    "            # if j%10==0:\n",
    "                #batch.append(phrases_embs[i][j])\n",
    "            s=np.dot(phrases_embs[i][j],query_emb)\n",
    "            scores.append((i,j,s))\n",
    "\n",
    "        #reduced_embs.append(batch)\n",
    "    scores=np.asarray(scores)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(768,) (768,)\n"
     ]
    }
   ],
   "source": [
    "q_start=query_outputs[0][0].cpu().detach().numpy()[0]\n",
    "q_end=query_outputs[1][0].cpu().detach().numpy()[0]\n",
    "print(q_start.shape,q_end.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8008, 128, 768)\n",
      "(8008, 128, 768) (768,)\n"
     ]
    }
   ],
   "source": [
    "# scores=search_mips(outputs[0].cpu().detach().numpy(),q_start)\n",
    "search_embs=np.asarray([all_predictions[i]['start'] for i in range(len(all_predictions))])\n",
    "# search_ids = np.asarray([all_predictions[i]['inputs'] for i in range(len(all_predictions))])\n",
    "print(search_embs.shape)\n",
    "scores=search_mips(search_embs,q_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(986880, array([7710.        ,    0.        ,  651.54821777]))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_score = np.argmax(scores[:,2])\n",
    "i=int(scores[best_score][0])\n",
    "j=int(scores[best_score][1])\n",
    "best_score,scores[best_score]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.decode(inputs['input_ids'][i][j:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"[CLS] that he could inherit a fortune by murdering his uncle and cousin. Stapleton had taken Sir Henry's old boot because the new, unworn boot lacked his scent. The hound had pursued Selden to his death because of the scent on Sir Henry's old clothes. Mrs Stapleton had disavowed her husband's plot, so he had imprisoned her to prevent her from interfering. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\""
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp=all_predictions[i]['inputs']\n",
    "#decode\n",
    "tokenizer.decode(inp[j:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2016 and 2017 defense spending budgets that \" [ reflect ] the priorities necessary for our force today and in the future to best serve and protect our nation in a rapidly changing security environment. \" in february 2018, the pentagon requested $ 686 billion for fy 2019. the john s. mccain national defense authorization act authorized department of defense appropriations for 2019 and established policies, but it did not contain the budget itself. on july 26, this bill passed in the house of representatives by 359 - 54. on august 1, the us senate passed it by 87 - 10. the bill was presented to president trump two days later. he signed it on august 13. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]'"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
