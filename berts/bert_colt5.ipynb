{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/miu/miniconda3/envs/torch/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "print(torch.__version__)\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "torch.cuda.is_available()\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Iterator\n",
    "import time\n",
    "\n",
    "import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "from tokenizers import BertWordPieceTokenizer, Regex, normalizers\n",
    "from tqdm import tqdm\n",
    "from transformers import (\n",
    "    BertConfig,\n",
    "    BertForMaskedLM,\n",
    "    BertTokenizerFast,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "import math\n",
    "from functools import reduce\n",
    "from collections import namedtuple\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "# from reformer_pytorch import ReformerLM\n",
    "# from x_transformers import TransformerWrapper, Decoder,Encoder\n",
    "from torch.utils.data import DataLoader\n",
    "import math \n",
    "from accelerate import Accelerator\n",
    "import torch.nn.functional as F\n",
    "# from einops import rearrange, pack, unpack\n",
    "import sys\n",
    "import os\n",
    "#get cur dir\n",
    "cur_dir = os.getcwd()\n",
    "if 'miu' in cur_dir:\n",
    "    sys.path.append('/home/miu/Projects/NLPLab/')\n",
    "elif 'kiki' in cur_dir:\n",
    "    sys.path.append('/home/kiki/dados_servidor/Servidor/NLPLab/')\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size:  2040\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Resolving data files: 100%|██████████| 53/53 [00:00<00:00, 91.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded dataset!!\n"
     ]
    }
   ],
   "source": [
    "model_training='hf'#'bert' , 'tokemonster\n",
    "# LIMIT_DATASET = 2016 * 4  # keep small for development, set to None for full dataset\n",
    "#(3090) Para 128 tokens(Bert) e 32000 vocab: 1000 = 6 seg, 10000 = 1min, 100000 = 10min, 1m = 100min, 10m = 16h, 100m = 160h ou 6.6 dias\n",
    "#(3090) Para 84 tokens(tokenmonster salva 35%) e 24000 vocab(Bert): 1000 = 4.3s, 10000 = 43s, 100000 = 7.1m, 1m = 71min, 10m = 11.8h, 100m = 118h ou 5 dias\n",
    "#(4090) Para 84 tokens(tokenmonster salva 35%) e 24000 vocab(Electra): 1000 = 3.5s, 10000 = 35s, 100000 = 5.9m, 1m = 59min, 10m = 9.8h, 100m = 98h ou 4 dias\n",
    "#(4090) Para 84 tokens(tokenmonster salva 35%) e 16000 vocab (Electra): 1000 = 2.1s, 10000 = 21s, 100000 = 3.53m, 1m = 35.3min, 10m = 5.8h, 100m = 58h ou 2.4 dias\n",
    "\n",
    "#com retnet. Confirmado que é O(n). Se lembrando que não existe mta vantagem além do custo crescer sequencialmente, já que o conteudo \n",
    "#(4090) Para 84 tokens(tokenmonster salva 35%) batch size 200 e 16000 vocab (Electra): 1000 = 2.1s, 10000 = 21s, 100000 = 3.53m, 1m = 35.3min, 10m = 5.8h, 100m = 58h ou 2.4 dias\n",
    "#(4090) Para 168 tokens(tokenmonster salva 35%) batch size 100 e 16000 vocab (Electra): 1000 = 3.7s, 10000 = 37.5s,\n",
    "#(4090) Para 1000 tokens(tokenmonster salva 35%) batch size 12 e 16000 vocab (Electra):            , 10000 = 4min32s,\n",
    "\n",
    "\n",
    "#HIPOTÉTICO(Se conseguisse representar um texto com menos tokens):\n",
    "#(4090) Para 24 tokens(tokenmonster salva 35%)(Electra): 1000 = , 10000 = , 100000 = 3.19m, 1m = 32min, 10m = 6.5h, 100m = 65h ou 2.7 dias\n",
    "#HIPOTÉTICO(Se diminuisse o vocabulario para 1024 tokens diferentes)\n",
    "#(4090) Para 84 tokens(tokenmonster salva 35%) e 1024 vocab: 1000 =1.6s , 10000 = 16.2s,\n",
    "\n",
    "#losses:\n",
    "#1m dataset, electra, tokenmonster: iteration:  7800 , total_loss:  26.277032788594564\n",
    "#10m dataset, 84 tokens(tokenmonster salva 35%) e 16000 vocab (Electra) iteration:  49980 , total_loss:  20.8733185450236. 339min total\n",
    "\n",
    "\n",
    "LIMIT_DATASET = 10_000\n",
    "RANDOM_SEED = 42\n",
    "NUM_TOKENIZER_TRAINING_ITEMS = 1_000_000  # I made this up, but it seems reasonable\n",
    "# VOCAB_SIZE = 32_768  # from Cramming\n",
    "# DEVICE_BATCH_SIZE = 100 # aprox 128, adjust to get near 100% gpu memory use\n",
    "# MODEL_MAX_SEQ_LEN = 128  # from Cramming\n",
    "\n",
    "VOCAB_SIZE = 32_768  # tokenmonster\n",
    "# VOCAB_SIZE = 1_024  # tokenmonster\n",
    "DEVICE_BATCH_SIZE = 12 # Token monster aguenta um batch size de (200-248)!! Geralmente melhora a qualidade do treino\n",
    "MODEL_MAX_SEQ_LEN = 512 # token_monster\n",
    "\n",
    "\n",
    "MASK_ID=4\n",
    "PAD_ID=0\n",
    "gradient_accumulation_steps = 2048 // DEVICE_BATCH_SIZE  # roughly based on Cramming\n",
    "batch_size = DEVICE_BATCH_SIZE * gradient_accumulation_steps\n",
    "print('batch_size: ',batch_size)\n",
    "RUN_DIR = Path(\"data\") / f\"run_{time.strftime('%Y%m%d-%H%M%S')}\"\n",
    "CHECKPOINT_DIR = RUN_DIR / \"training_checkpoints\"\n",
    "MODEL_DIR = RUN_DIR / \"model\"\n",
    "TOKENIZER_PATH = RUN_DIR / \"tokenizer.json\"\n",
    "TRAINER_HISTORY_PATH = RUN_DIR / \"trainer_history.json\"\n",
    "\n",
    "RUN_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "dataset = datasets.load_dataset(\n",
    "    \"sradc/chunked-shuffled-wikipedia20220301en-bookcorpusopen\",\n",
    "    split=f\"train[:{LIMIT_DATASET}]\" if LIMIT_DATASET else \"train\",\n",
    "    revision=\"0e6fada2dd43136e4a3f637da41de2e596aee674\",\n",
    ")\n",
    "print('loaded dataset!!')\n",
    "\n",
    "from utils.process_tokenizer import get_tokenizer\n",
    "# tokenizer,tokenized_dataset,norm,vocab=get_tokenizer(dataset,NUM_TOKENIZER_TRAINING_ITEMS,VOCAB_SIZE,TOKENIZER_PATH,'clm',MODEL_MAX_SEQ_LEN+2)\n",
    "tokenized_dataset,norm,vocab,tokenizer=get_tokenizer(dataset,NUM_TOKENIZER_TRAINING_ITEMS,VOCAB_SIZE,TOKENIZER_PATH,'mlm',MODEL_MAX_SEQ_LEN+2)\n",
    "# tokenized_dataset,norm,vocab=get_tokenizer(dataset,NUM_TOKENIZER_TRAINING_ITEMS,VOCAB_SIZE,TOKENIZER_PATH,'smlm',MODEL_MAX_SEQ_LEN+2)\n",
    "MASK_ID=vocab.tokenize(\"[MASK]\")[0]\n",
    "N_DOMAINS=3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PreTrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from x_transformers import TransformerWrapper, Decoder,Encoder\n",
    "\n",
    "\n",
    "#encoder decoder\n",
    "colt5_dict={\n",
    "            'encoder':{'is_colt5_encoder':True,'light_ff_mult':0.5,'heavy_ff_mult':4,'num_heavy_tokens':30,\n",
    "                   'light_dim_head':64,'light_heads':8,'light_window_size':128,\n",
    "                   'heavy_dim_head':64,'heavy_heads':8,'num_heavy_tokens_q':30,\n",
    "                   'num_heavy_tokens_kv':30},\n",
    "            }\n",
    "\n",
    "# model = TransformerWrapper(\n",
    "model = TransformerWrapper(\n",
    "    num_tokens = VOCAB_SIZE+8,\n",
    "    max_seq_len = MODEL_MAX_SEQ_LEN,\n",
    "    attn_layers = Encoder(\n",
    "        dim = int(768),\n",
    "        depth = 12,\n",
    "        heads =12,\n",
    "        attn_flash = True,\n",
    "        colt5_dict=colt5_dict['encoder'],\n",
    "        # rel_pos_bias = True\n",
    "    ),\n",
    "    emb_dim=128,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_ids are the indices corresponding to each token in the sentence.\n",
    "# attention_mask indicates whether a token should be attended to or not.\n",
    "# token_type_ids identifies which sequence a token belongs to when there is more than one sequence\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "        tokenized_dataset, shuffle=True, batch_size=DEVICE_BATCH_SIZE\n",
    "    )\n",
    "from utils.utils import get_optimizer_scheduler\n",
    "optimizer,lr_scheduler,max_train_steps = get_optimizer_scheduler(model,train_dataloader,gradient_accumulation_steps,learning_rate=5e-5,weight_decay=0, num_warmup_steps=0, max_train_steps=None,lr_scheduler_type='linear',num_train_epochs=1)\n",
    "\n",
    "accelerator = Accelerator()\n",
    "# Prepare everything with our `accelerator`.\n",
    "model, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(\n",
    "    model, optimizer, train_dataloader, lr_scheduler\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration:  0 , total_loss:  0.3496361414591471\n",
      "count_amostra: 12\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/miu/Projects/NLPLab/berts/bert_colt5.ipynb Cell 5\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/miu/Projects/NLPLab/berts/bert_colt5.ipynb#W4sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39m# print(loss)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/miu/Projects/NLPLab/berts/bert_colt5.ipynb#W4sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m total_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mfloat()\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy()\u001b[39m.\u001b[39mitem()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/miu/Projects/NLPLab/berts/bert_colt5.ipynb#W4sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m accelerator\u001b[39m.\u001b[39;49mbackward(loss)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/miu/Projects/NLPLab/berts/bert_colt5.ipynb#W4sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/miu/Projects/NLPLab/berts/bert_colt5.ipynb#W4sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m lr_scheduler\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.10/site-packages/accelerate/accelerator.py:1905\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   1903\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscaler\u001b[39m.\u001b[39mscale(loss)\u001b[39m.\u001b[39mbackward(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1904\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1905\u001b[0m     loss\u001b[39m.\u001b[39;49mbackward(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.10/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    493\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    494\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.10/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[39m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m     tensors,\n\u001b[1;32m    253\u001b[0m     grad_tensors_,\n\u001b[1;32m    254\u001b[0m     retain_graph,\n\u001b[1;32m    255\u001b[0m     create_graph,\n\u001b[1;32m    256\u001b[0m     inputs,\n\u001b[1;32m    257\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    258\u001b[0m     accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    259\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Only show the progress bar once on each machine.\n",
    "# progress_bar = tqdm(range(max_train_steps), disable=not accelerator.is_local_main_process)\n",
    "completed_steps = 0\n",
    "starting_epoch = 0\n",
    "\n",
    "# update the progress_bar if load from checkpoint\n",
    "total_loss=0\n",
    "# progress_bar.update(completed_steps)\n",
    "steps_log=30\n",
    "count_amostra=0\n",
    "num_train_epochs=1\n",
    "for epoch in range(starting_epoch, num_train_epochs):\n",
    "    model.train()\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "        # with accelerator.accumulate(model):\n",
    "       \n",
    "        logits=model(batch['input_ids']) \n",
    "        loss = F.cross_entropy(logits.transpose(1, 2),batch['input_ids'],ignore_index = PAD_ID)\n",
    "        count_amostra+=int(len(batch['input_ids']))\n",
    "        # print(loss)\n",
    "        total_loss += loss.detach().float().cpu().numpy().item()\n",
    "        accelerator.backward(loss)\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()       \n",
    "        \n",
    "        if step%steps_log==0:\n",
    "            print('iteration: ',step,', total_loss: ',total_loss/steps_log)\n",
    "            print('count_amostra:',count_amostra)\n",
    "            total_loss=0\n",
    "    # Checks if the accelerator has performed an optimization step behind the scenes\n",
    "    if accelerator.sync_gradients:\n",
    "        # progress_bar.update(1)\n",
    "        completed_steps += 1\n",
    "\n",
    "    if completed_steps >= max_train_steps:\n",
    "        break\n",
    "\n",
    "    model.eval()\n",
    "    losses = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "53s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#convert to huggingface Transformers\n",
    "\n",
    "from transformers import BertConfig, BertForMaskedLM, BertTokenizerFast\n",
    "\n",
    "config = BertConfig(\n",
    "    vocab_size=VOCAB_SIZE+8,\n",
    "    max_position_embeddings=MODEL_MAX_SEQ_LEN,\n",
    "    num_attention_heads=12,\n",
    "    num_hidden_layers=12,\n",
    "    type_vocab_size=2,\n",
    ")\n",
    "\n",
    "bert_model=BertForMaskedLM(config=config)\n",
    "bert_model.bert=model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model.save_pretrained('models/bert')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trainer hf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "        num_rows: 4000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"squad\", split=\"train[:5000]\")\n",
    "dataset = dataset.train_test_split(test_size=0.2)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = MODEL_MAX_SEQ_LEN#384\n",
    "stride = 128\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "def preprocess_validation_examples(examples):\n",
    "    questions = [q.strip() for q in examples[\"question\"]]\n",
    "    inputs = tokenizer(\n",
    "        questions,\n",
    "        examples[\"context\"],\n",
    "        max_length=max_length,\n",
    "        truncation=\"only_second\",\n",
    "        stride=stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    sample_map = inputs.pop(\"overflow_to_sample_mapping\")\n",
    "    example_ids = []\n",
    "\n",
    "    for i in range(len(inputs[\"input_ids\"])):\n",
    "        sample_idx = sample_map[i]\n",
    "        example_ids.append(examples[\"id\"][sample_idx])\n",
    "\n",
    "        sequence_ids = inputs.sequence_ids(i)\n",
    "        offset = inputs[\"offset_mapping\"][i]\n",
    "        inputs[\"offset_mapping\"][i] = [\n",
    "            o if sequence_ids[k] == 1 else None for k, o in enumerate(offset)\n",
    "        ]\n",
    "\n",
    "    inputs[\"example_id\"] = example_ids\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1000/1000 [00:00<00:00, 4972.96 examples/s]\n"
     ]
    }
   ],
   "source": [
    "validation_dataset = dataset[\"test\"].map(\n",
    "    preprocess_validation_examples,\n",
    "    batched=True,\n",
    "    remove_columns=dataset[\"test\"].column_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    questions = [q.strip() for q in examples[\"question\"]]\n",
    "    inputs = tokenizer(\n",
    "        questions,\n",
    "        examples[\"context\"],\n",
    "        max_length=tokenizer.model_max_length,\n",
    "        truncation=\"only_second\",\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    offset_mapping = inputs.pop(\"offset_mapping\")\n",
    "    answers = examples[\"answers\"]\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "\n",
    "    for i, offset in enumerate(offset_mapping):\n",
    "        answer = answers[i]\n",
    "        start_char = answer[\"answer_start\"][0]\n",
    "        end_char = answer[\"answer_start\"][0] + len(answer[\"text\"][0])\n",
    "        sequence_ids = inputs.sequence_ids(i)\n",
    "        # print('sequence_ids: ',sequence_ids)\n",
    "        # print('sequence_ids: ',len(sequence_ids))\n",
    "\n",
    "        # Find the start and end of the context\n",
    "        idx = 0\n",
    "        while sequence_ids[idx] != 1:\n",
    "            idx += 1\n",
    "        context_start = idx\n",
    "        while sequence_ids[idx] == 1 and idx < len(sequence_ids) - 1:\n",
    "            idx += 1\n",
    "        context_end = idx - 1\n",
    "\n",
    "        # If the answer is not fully inside the context, label it (0, 0)\n",
    "        if offset[context_start][0] > end_char or offset[context_end][1] < start_char:\n",
    "            start_positions.append(0)\n",
    "            end_positions.append(0)\n",
    "        else:\n",
    "            # Otherwise it's the start and end token positions\n",
    "            idx = context_start\n",
    "            while idx <= context_end and offset[idx][0] <= start_char:\n",
    "                idx += 1\n",
    "            start_positions.append(idx - 1)\n",
    "\n",
    "            idx = context_end\n",
    "            while idx >= context_start and offset[idx][1] >= end_char:\n",
    "                idx -= 1\n",
    "            end_positions.append(idx + 1)\n",
    "\n",
    "    inputs[\"start_positions\"] = start_positions\n",
    "    inputs[\"end_positions\"] = end_positions\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/4000 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 4000/4000 [00:02<00:00, 1393.92 examples/s]\n",
      "Map: 100%|██████████| 1000/1000 [00:00<00:00, 1404.71 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import DefaultDataCollator\n",
    "\n",
    "tokenized_dataset = dataset.map(preprocess_function, batched=True, remove_columns=dataset['train'].column_names,batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at /home/miu/Projects/NLPLab/berts/models/bert and are newly initialized: ['bert.encoder.layer.3.intermediate.dense.weight', 'bert.encoder.layer.9.attention.output.LayerNorm.bias', 'bert.encoder.layer.6.attention.output.dense.bias', 'bert.encoder.layer.1.attention.output.dense.weight', 'bert.encoder.layer.2.attention.self.value.bias', 'bert.encoder.layer.8.attention.output.LayerNorm.bias', 'bert.encoder.layer.10.attention.output.dense.weight', 'bert.encoder.layer.8.output.LayerNorm.weight', 'bert.encoder.layer.6.attention.output.dense.weight', 'bert.encoder.layer.2.attention.self.query.weight', 'bert.encoder.layer.4.intermediate.dense.weight', 'bert.encoder.layer.4.attention.output.LayerNorm.bias', 'bert.encoder.layer.1.output.LayerNorm.weight', 'bert.encoder.layer.9.attention.self.value.bias', 'bert.encoder.layer.5.attention.self.query.weight', 'qa_outputs.bias', 'bert.encoder.layer.8.attention.self.value.bias', 'bert.encoder.layer.0.output.LayerNorm.bias', 'bert.encoder.layer.1.output.dense.weight', 'bert.encoder.layer.8.attention.self.key.weight', 'bert.encoder.layer.11.output.dense.weight', 'bert.encoder.layer.2.attention.self.key.weight', 'bert.encoder.layer.9.attention.output.dense.bias', 'bert.embeddings.word_embeddings.weight', 'bert.encoder.layer.8.output.dense.weight', 'bert.encoder.layer.11.intermediate.dense.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.bias', 'bert.encoder.layer.5.attention.self.value.bias', 'bert.encoder.layer.5.output.LayerNorm.bias', 'bert.encoder.layer.10.output.LayerNorm.weight', 'bert.encoder.layer.3.output.LayerNorm.weight', 'bert.embeddings.token_type_embeddings.weight', 'bert.encoder.layer.11.attention.output.LayerNorm.weight', 'bert.encoder.layer.7.attention.self.key.weight', 'bert.encoder.layer.0.attention.self.key.weight', 'bert.encoder.layer.0.attention.self.query.bias', 'bert.encoder.layer.9.intermediate.dense.weight', 'bert.encoder.layer.2.attention.self.key.bias', 'bert.encoder.layer.10.output.dense.bias', 'bert.encoder.layer.3.attention.self.key.bias', 'bert.encoder.layer.6.attention.self.value.weight', 'bert.encoder.layer.3.attention.output.dense.bias', 'bert.encoder.layer.10.attention.output.LayerNorm.weight', 'bert.encoder.layer.7.output.LayerNorm.bias', 'bert.encoder.layer.5.attention.self.key.bias', 'bert.encoder.layer.5.output.dense.bias', 'bert.encoder.layer.1.output.dense.bias', 'bert.encoder.layer.9.attention.output.dense.weight', 'bert.encoder.layer.11.attention.self.query.bias', 'bert.encoder.layer.9.attention.self.key.weight', 'bert.encoder.layer.8.attention.self.key.bias', 'bert.encoder.layer.9.output.LayerNorm.bias', 'bert.encoder.layer.4.attention.self.value.weight', 'bert.encoder.layer.1.attention.output.dense.bias', 'bert.encoder.layer.3.output.dense.weight', 'bert.encoder.layer.8.attention.output.dense.weight', 'bert.encoder.layer.7.intermediate.dense.bias', 'bert.encoder.layer.0.attention.self.value.weight', 'bert.encoder.layer.8.attention.output.LayerNorm.weight', 'bert.encoder.layer.7.attention.self.value.weight', 'bert.encoder.layer.3.attention.self.query.weight', 'bert.encoder.layer.6.attention.self.key.weight', 'bert.encoder.layer.6.intermediate.dense.bias', 'bert.encoder.layer.10.attention.self.query.weight', 'bert.encoder.layer.3.attention.output.LayerNorm.bias', 'bert.encoder.layer.4.output.dense.bias', 'bert.encoder.layer.5.attention.output.LayerNorm.weight', 'bert.encoder.layer.8.attention.self.value.weight', 'bert.encoder.layer.9.output.dense.weight', 'bert.encoder.layer.8.output.dense.bias', 'bert.encoder.layer.0.attention.output.dense.bias', 'bert.encoder.layer.7.attention.self.query.weight', 'bert.encoder.layer.1.attention.self.key.weight', 'bert.encoder.layer.6.output.dense.bias', 'bert.encoder.layer.11.attention.self.value.weight', 'bert.encoder.layer.7.attention.output.dense.weight', 'bert.encoder.layer.0.output.dense.bias', 'bert.encoder.layer.10.attention.output.dense.bias', 'bert.encoder.layer.2.attention.self.query.bias', 'bert.encoder.layer.5.output.LayerNorm.weight', 'bert.encoder.layer.4.intermediate.dense.bias', 'bert.encoder.layer.11.attention.self.key.bias', 'bert.encoder.layer.1.attention.self.value.bias', 'bert.encoder.layer.2.attention.output.LayerNorm.weight', 'bert.encoder.layer.1.attention.self.key.bias', 'bert.encoder.layer.6.attention.self.key.bias', 'bert.encoder.layer.8.attention.self.query.bias', 'bert.encoder.layer.6.attention.self.query.weight', 'bert.encoder.layer.3.attention.self.value.bias', 'bert.encoder.layer.10.output.LayerNorm.bias', 'bert.encoder.layer.7.output.dense.weight', 'bert.encoder.layer.5.attention.output.dense.bias', 'bert.encoder.layer.3.attention.self.value.weight', 'bert.encoder.layer.9.output.dense.bias', 'bert.encoder.layer.10.attention.self.value.weight', 'bert.encoder.layer.6.intermediate.dense.weight', 'bert.encoder.layer.7.output.LayerNorm.weight', 'bert.encoder.layer.11.attention.self.query.weight', 'bert.encoder.layer.6.attention.self.query.bias', 'bert.encoder.layer.11.attention.output.dense.bias', 'bert.encoder.layer.4.attention.self.key.weight', 'bert.encoder.layer.7.attention.self.query.bias', 'bert.encoder.layer.8.intermediate.dense.bias', 'bert.encoder.layer.10.attention.self.key.weight', 'bert.encoder.layer.2.attention.output.dense.weight', 'bert.encoder.layer.2.output.dense.bias', 'bert.encoder.layer.5.output.dense.weight', 'bert.encoder.layer.4.attention.output.dense.bias', 'bert.encoder.layer.11.attention.self.key.weight', 'bert.encoder.layer.10.output.dense.weight', 'bert.encoder.layer.5.attention.output.dense.weight', 'bert.encoder.layer.10.attention.output.LayerNorm.bias', 'bert.encoder.layer.5.attention.self.query.bias', 'bert.encoder.layer.1.output.LayerNorm.bias', 'bert.encoder.layer.2.intermediate.dense.weight', 'bert.encoder.layer.0.attention.self.key.bias', 'bert.encoder.layer.2.intermediate.dense.bias', 'bert.encoder.layer.4.attention.self.query.bias', 'bert.encoder.layer.7.intermediate.dense.weight', 'bert.encoder.layer.10.intermediate.dense.weight', 'bert.encoder.layer.1.attention.self.value.weight', 'bert.encoder.layer.7.attention.output.LayerNorm.weight', 'bert.encoder.layer.3.intermediate.dense.bias', 'bert.encoder.layer.11.attention.self.value.bias', 'bert.encoder.layer.4.attention.self.query.weight', 'bert.encoder.layer.6.output.LayerNorm.weight', 'bert.encoder.layer.3.attention.self.key.weight', 'bert.encoder.layer.9.attention.self.query.weight', 'bert.encoder.layer.0.attention.self.value.bias', 'bert.encoder.layer.3.attention.output.dense.weight', 'bert.encoder.layer.0.output.LayerNorm.weight', 'bert.encoder.layer.7.attention.self.value.bias', 'bert.encoder.layer.3.attention.self.query.bias', 'bert.encoder.layer.1.intermediate.dense.bias', 'bert.encoder.layer.9.attention.self.key.bias', 'bert.encoder.layer.5.attention.self.key.weight', 'bert.encoder.layer.4.attention.output.LayerNorm.weight', 'bert.encoder.layer.1.intermediate.dense.weight', 'bert.encoder.layer.2.attention.output.LayerNorm.bias', 'bert.encoder.layer.8.output.LayerNorm.bias', 'bert.encoder.layer.11.output.LayerNorm.weight', 'qa_outputs.weight', 'bert.encoder.layer.11.output.dense.bias', 'bert.encoder.layer.4.output.LayerNorm.weight', 'bert.encoder.layer.7.attention.self.key.bias', 'bert.encoder.layer.6.output.dense.weight', 'bert.encoder.layer.2.output.LayerNorm.bias', 'bert.encoder.layer.7.attention.output.dense.bias', 'bert.encoder.layer.0.intermediate.dense.bias', 'bert.encoder.layer.11.attention.output.dense.weight', 'bert.encoder.layer.11.intermediate.dense.weight', 'bert.encoder.layer.0.attention.output.dense.weight', 'bert.embeddings.LayerNorm.weight', 'bert.encoder.layer.10.attention.self.key.bias', 'bert.encoder.layer.5.attention.output.LayerNorm.bias', 'bert.encoder.layer.5.intermediate.dense.weight', 'bert.embeddings.position_embeddings.weight', 'bert.encoder.layer.0.intermediate.dense.weight', 'bert.encoder.layer.4.output.LayerNorm.bias', 'bert.encoder.layer.8.intermediate.dense.weight', 'bert.encoder.layer.9.attention.self.query.bias', 'bert.encoder.layer.2.attention.output.dense.bias', 'bert.encoder.layer.3.attention.output.LayerNorm.weight', 'bert.encoder.layer.10.attention.self.value.bias', 'bert.encoder.layer.1.attention.self.query.weight', 'bert.encoder.layer.1.attention.self.query.bias', 'bert.encoder.layer.5.intermediate.dense.bias', 'bert.encoder.layer.4.output.dense.weight', 'bert.encoder.layer.6.attention.self.value.bias', 'bert.encoder.layer.3.output.dense.bias', 'bert.encoder.layer.9.attention.output.LayerNorm.weight', 'bert.encoder.layer.7.attention.output.LayerNorm.bias', 'bert.encoder.layer.1.attention.output.LayerNorm.bias', 'bert.encoder.layer.4.attention.self.value.bias', 'bert.encoder.layer.6.attention.output.LayerNorm.weight', 'bert.encoder.layer.9.intermediate.dense.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.weight', 'bert.encoder.layer.10.intermediate.dense.bias', 'bert.embeddings.LayerNorm.bias', 'bert.encoder.layer.1.attention.output.LayerNorm.weight', 'bert.encoder.layer.6.attention.output.LayerNorm.bias', 'bert.encoder.layer.6.output.LayerNorm.bias', 'bert.encoder.layer.2.output.dense.weight', 'bert.encoder.layer.8.attention.self.query.weight', 'bert.encoder.layer.11.output.LayerNorm.bias', 'bert.encoder.layer.2.attention.self.value.weight', 'bert.encoder.layer.7.output.dense.bias', 'bert.encoder.layer.11.attention.output.LayerNorm.bias', 'bert.encoder.layer.4.attention.self.key.bias', 'bert.encoder.layer.10.attention.self.query.bias', 'bert.encoder.layer.0.attention.self.query.weight', 'bert.encoder.layer.9.attention.self.value.weight', 'bert.encoder.layer.3.output.LayerNorm.bias', 'bert.encoder.layer.4.attention.output.dense.weight', 'bert.encoder.layer.8.attention.output.dense.bias', 'bert.encoder.layer.9.output.LayerNorm.weight', 'bert.encoder.layer.5.attention.self.value.weight', 'bert.encoder.layer.2.output.LayerNorm.weight', 'bert.encoder.layer.0.output.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "from transformers import TrainingArguments\n",
    "from transformers import AutoModelForQuestionAnswering, AutoTokenizer\n",
    "\n",
    "#BertForMaskedLM\n",
    "from transformers import BertForMaskedLM\n",
    "\n",
    "args = TrainingArguments(\n",
    "    \"bert-finetuned-squad\",\n",
    "    evaluation_strategy=\"no\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    fp16=True,\n",
    "    push_to_hub=False,\n",
    ")\n",
    "# bert_model = BertForMaskedLM.from_pretrained(pretrained_model_name_or_path='bert-base-uncased')\n",
    "# bert_model = AutoModelForQuestionAnswering.from_pretrained(\"bert-base-uncased\")\n",
    "# bert_model = AutoModelForQuestionAnswering.from_pretrained(\"/home/kiki/dados_servidor/Servidor/NLPLab/berts/models/bert_spanbert\")\n",
    "if 'miu' in cur_dir:\n",
    "    bert_model = AutoModelForQuestionAnswering.from_pretrained(\"/home/miu/Projects/NLPLab/berts/models/bert\")\n",
    "else:\n",
    "    bert_model = AutoModelForQuestionAnswering.from_pretrained(\"/home/kiki/dados_servidor/Servidor/NLPLab/berts/models/bert\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mabcp4\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/miu/Projects/NLPLab/berts/wandb/run-20231224_114640-25o8xhra</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/abcp4/huggingface/runs/25o8xhra' target=\"_blank\">youthful-firebrand-172</a></strong> to <a href='https://wandb.ai/abcp4/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/abcp4/huggingface' target=\"_blank\">https://wandb.ai/abcp4/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/abcp4/huggingface/runs/25o8xhra' target=\"_blank\">https://wandb.ai/abcp4/huggingface/runs/25o8xhra</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2500 [00:00<?, ?it/s]You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      " 20%|██        | 500/2500 [00:39<02:38, 12.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.7581, 'learning_rate': 1.6000000000000003e-05, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 1000/2500 [01:20<01:57, 12.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.0755, 'learning_rate': 1.2e-05, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 1500/2500 [02:00<01:19, 12.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.7534, 'learning_rate': 8.000000000000001e-06, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 2000/2500 [02:40<00:39, 12.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.5015, 'learning_rate': 4.000000000000001e-06, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2500/2500 [03:21<00:00, 12.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.3212, 'learning_rate': 8e-09, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2500/2500 [03:22<00:00, 12.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 206.7869, 'train_samples_per_second': 96.718, 'train_steps_per_second': 12.09, 'train_loss': 3.8819138671875, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2500, training_loss=3.8819138671875, metrics={'train_runtime': 206.7869, 'train_samples_per_second': 96.718, 'train_steps_per_second': 12.09, 'train_loss': 3.8819138671875, 'epoch': 5.0})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=bert_model,\n",
    "    args=args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import collections\n",
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"squad\")\n",
    "n_best = 20\n",
    "max_answer_length = 30\n",
    "predicted_answers = []\n",
    "\n",
    "def compute_metrics(start_logits, end_logits, features, examples):\n",
    "    example_to_features = collections.defaultdict(list)\n",
    "    for idx, feature in enumerate(features):\n",
    "        example_to_features[feature[\"example_id\"]].append(idx)\n",
    "\n",
    "    predicted_answers = []\n",
    "    for example in tqdm(examples):\n",
    "        example_id = example[\"id\"]\n",
    "        context = example[\"context\"]\n",
    "        answers = []\n",
    "\n",
    "        # Loop through all features associated with that example\n",
    "        for feature_index in example_to_features[example_id]:\n",
    "            start_logit = start_logits[feature_index]\n",
    "            end_logit = end_logits[feature_index]\n",
    "            offsets = features[feature_index][\"offset_mapping\"]\n",
    "\n",
    "            start_indexes = np.argsort(start_logit)[-1 : -n_best - 1 : -1].tolist()\n",
    "            end_indexes = np.argsort(end_logit)[-1 : -n_best - 1 : -1].tolist()\n",
    "            for start_index in start_indexes:\n",
    "                for end_index in end_indexes:\n",
    "                    # Skip answers that are not fully in the context\n",
    "                    if offsets[start_index] is None or offsets[end_index] is None:\n",
    "                        continue\n",
    "                    # Skip answers with a length that is either < 0 or > max_answer_length\n",
    "                    if (\n",
    "                        end_index < start_index\n",
    "                        or end_index - start_index + 1 > max_answer_length\n",
    "                    ):\n",
    "                        continue\n",
    "\n",
    "                    answer = {\n",
    "                        \"text\": context[offsets[start_index][0] : offsets[end_index][1]],\n",
    "                        \"logit_score\": start_logit[start_index] + end_logit[end_index],\n",
    "                    }\n",
    "                    answers.append(answer)\n",
    "\n",
    "        # Select the answer with the best score\n",
    "        if len(answers) > 0:\n",
    "            best_answer = max(answers, key=lambda x: x[\"logit_score\"])\n",
    "            predicted_answers.append(\n",
    "                {\"id\": example_id, \"prediction_text\": best_answer[\"text\"]}\n",
    "            )\n",
    "        else:\n",
    "            predicted_answers.append({\"id\": example_id, \"prediction_text\": \"\"})\n",
    "\n",
    "    theoretical_answers = [{\"id\": ex[\"id\"], \"answers\": ex[\"answers\"]} for ex in examples]\n",
    "    return metric.compute(predictions=predicted_answers, references=theoretical_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 126/126 [00:02<00:00, 44.42it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 1607.05it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'exact_match': 3.9, 'f1': 8.451318538500729}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import evaluate\n",
    "\n",
    "predictions, _, _ = trainer.predict(validation_dataset)\n",
    "start_logits, end_logits = predictions\n",
    "compute_metrics(start_logits, end_logits, validation_dataset, dataset[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 126/126 [00:02<00:00, 44.45it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 1573.49it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'exact_match': 4.2, 'f1': 8.794485888565596}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#100k 512 tokens\n",
    "{'exact_match': 4.2, 'f1': 8.794485888565596}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SEQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hidden layer extractor class, for magically adding adapter to language model to be pretrained\n",
    "class HiddenLayerExtractor(nn.Module):\n",
    "    def __init__(self, net, layer = -2):\n",
    "        super().__init__()\n",
    "        self.net = net\n",
    "        self.layer = layer\n",
    "\n",
    "        self.hidden = None\n",
    "        self.hook_registered = False\n",
    "\n",
    "    def _find_layer(self):\n",
    "        if type(self.layer) == str:\n",
    "            modules = dict([*self.net.named_modules()])\n",
    "            return modules.get(self.layer, None)\n",
    "        elif type(self.layer) == int:\n",
    "            children = [*self.net.children()]\n",
    "            return children[self.layer]\n",
    "        return None\n",
    "\n",
    "    def _hook(self, _, __, output):\n",
    "        self.hidden = output\n",
    "\n",
    "    def _register_hook(self):\n",
    "        layer = self._find_layer()\n",
    "        assert layer is not None, f'hidden layer ({self.layer}) not found'\n",
    "        handle = layer.register_forward_hook(self._hook)\n",
    "        self.hook_registered = True\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.layer == -1:\n",
    "            return self.net(x)\n",
    "\n",
    "        if not self.hook_registered:\n",
    "            self._register_hook()\n",
    "\n",
    "        _ = self.net(x)\n",
    "        hidden = self.hidden\n",
    "        self.hidden = None\n",
    "        assert hidden is not None, f'hidden layer {self.layer} never emitted an output'\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_extractor = HiddenLayerExtractor(model,-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "import torch\n",
    "import os\n",
    "from torch.utils.data import  DataLoader\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "class Classifier(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Classifier, self).__init__()\n",
    "        # self.l1 = model._modules['model']\n",
    "        self.l1 = model_extractor\n",
    "        self.l2 = torch.nn.Dropout(0.3)\n",
    "        self.fl = torch.nn.Flatten()\n",
    "        self.l3 = torch.nn.Linear(768*(MODEL_MAX_SEQ_LEN-2), 3)\n",
    "    \n",
    "    def forward(self, ids):\n",
    "        output= self.l1(ids)\n",
    "        # print('l1:',output.shape)\n",
    "        output = self.l2(output)\n",
    "        # print('l2:',output.shape)\n",
    "        output = self.fl(output)\n",
    "        # print('fl:',output.shape)\n",
    "        output = self.l3(output)\n",
    "        # print('l3:',output.shape)\n",
    "        return output\n",
    "\n",
    "classifier_model = Classifier()\n",
    "classifier_model=classifier_model.to('cuda')\n",
    "optimizer = torch.optim.Adam(params =  classifier_model.parameters(), lr=2e-5,weight_decay=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "task='mrpc'\n",
    "dataset = load_dataset('glue', task, split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class TokenizedDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "        self.ls=[]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        s=norm.normalize_str(self.dataset[\"sentence1\"][i])+'[SEP]'+norm.normalize_str(self.dataset[\"sentence2\"][i])\n",
    "        tokens = vocab.tokenize(s).tolist()\n",
    "        \n",
    "        self.ls.append(i)\n",
    "        \n",
    "        #trucate\n",
    "        tokens=tokens[:MODEL_MAX_SEQ_LEN - 2]\n",
    "        l=len(tokens)\n",
    "        for j in range(l,MODEL_MAX_SEQ_LEN - 2):\n",
    "            tokens.append(0)\n",
    "        tokens=torch.Tensor(tokens).long()\n",
    "\n",
    "        label=self.dataset[\"label\"][i]\n",
    "        \n",
    "        d={'input_ids':tokens,'label':label}\n",
    "        return d\n",
    "\n",
    "\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "tokenized_dataset = TokenizedDataset(dataset)\n",
    "# train_dataloader=CustomDataloader(tokenized_dataset)\n",
    "train_dataloader = DataLoader(\n",
    "        tokenized_dataset, batch_size=DEVICE_BATCH_SIZE\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "\n",
    "classifier_model.train()\n",
    "\n",
    "def loss_fn(outputs, targets):\n",
    "    loss=torch.nn.CrossEntropyLoss()(outputs, targets)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss:  0.030146934986114503\n",
      "Epoch: 1, Loss:  0.5000924682617187\n",
      "Epoch: 2, Loss:  0.5059760808944702\n",
      "Epoch: 3, Loss:  0.4948312652111053\n",
      "Epoch: 4, Loss:  0.48995151281356814\n"
     ]
    }
   ],
   "source": [
    "#FINETUNING\n",
    "\n",
    "epochs=5\n",
    "running_loss=0\n",
    "for i in range(epochs):\n",
    "    for _,data in enumerate(train_dataloader):\n",
    "        ids = data['input_ids'].to('cuda', dtype = torch.long)\n",
    "        targets = data['label'].to('cuda', dtype = torch.long)\n",
    "\n",
    "        outputs = classifier_model(ids)\n",
    "        \n",
    "        loss = loss_fn(outputs,targets)\n",
    "        # loss=outputs.loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        running_loss+=loss.item()\n",
    "        if _%50==0:\n",
    "            print(f'Epoch: {i}, Loss:  {running_loss/50}')\n",
    "            running_loss=0\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EVALUATE\n",
    "dataset = load_dataset('glue', task, split='validation')\n",
    "tokenized_dataset = TokenizedDataset(dataset)\n",
    "\n",
    "eval_dataloader = DataLoader(\n",
    "        tokenized_dataset, batch_size=DEVICE_BATCH_SIZE\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "predictions=[]\n",
    "labels=[]\n",
    "c=0\n",
    "for _,data in enumerate(eval_dataloader):\n",
    "    ids = data['input_ids'].to('cuda', dtype = torch.long)\n",
    "    targets = data['label'].to('cuda', dtype = torch.long)\n",
    "    \n",
    "    outputs = classifier_model(ids)\n",
    "    \n",
    "    outputs=torch.argmax(outputs, dim=1).cpu()\n",
    "    targets=targets.cpu()\n",
    "    \n",
    "    outputs=[int(o.item()) for o in outputs]\n",
    "    targets=[int(o.item()) for o in targets]\n",
    "\n",
    "    predictions+=outputs\n",
    "    labels+=targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.07      0.12       129\n",
      "           1       0.69      0.98      0.81       279\n",
      "\n",
      "    accuracy                           0.69       408\n",
      "   macro avg       0.65      0.52      0.47       408\n",
      "weighted avg       0.66      0.69      0.60       408\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(labels, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bert 3 layers  10000 dataset\n",
    "#               precision    recall  f1-score   support\n",
    "\n",
    "#            0       0.61      0.09      0.15       129\n",
    "#            1       0.70      0.97      0.81       279\n",
    "\n",
    "#     accuracy                           0.69       408\n",
    "#    macro avg       0.65      0.53      0.48       408\n",
    "# weighted avg       0.67      0.69      0.60       408"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DensePhrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "        num_rows: 4000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"squad\", split=\"train[:5000]\")\n",
    "dataset = dataset.train_test_split(test_size=0.2)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load model and tokenizer\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel,AutoModelForQuestionAnswering\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "#model = AutoModelForQuestionAnswering.from_pretrained(\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedModel\n",
    "from transformers import AutoConfig\n",
    "import copy\n",
    "from torch.nn import CrossEntropyLoss\n",
    "# Encoders: three LMs\n",
    "config = AutoConfig.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "class Encoder(PreTrainedModel):\n",
    "    def __init__(self,return_phrase=False,\n",
    "                 return_query=False):\n",
    "        super().__init__(config)\n",
    "        self.phrase_encoder = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
    "        self.query_start_encoder = copy.deepcopy(self.phrase_encoder)\n",
    "        self.query_end_encoder = copy.deepcopy(self.phrase_encoder)\n",
    "        # Additional parameters\n",
    "        self.filter_linear = nn.Linear(config.hidden_size, 2)\n",
    "        self.return_phrase = return_phrase\n",
    "        self.return_query = return_query\n",
    "\n",
    "    def embed_phrase(self, input_ids, attention_mask, token_type_ids):\n",
    "        \n",
    "        \"\"\" Get phrase embeddings (token-wise) \"\"\"\n",
    "        outputs = self.phrase_encoder(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "        )\n",
    "        return outputs[0], outputs[0]\n",
    "\n",
    "    def embed_query(self, input_ids_, attention_mask_, token_type_ids_):\n",
    "        \"\"\" Get query start/end embeddings \"\"\"\n",
    "        # print('input_ids_.shape: ',input_ids_.shape)\n",
    "        # Two LM based query reps\n",
    "        outputs_s_ = self.query_start_encoder(\n",
    "            input_ids_,\n",
    "            attention_mask=attention_mask_,\n",
    "            token_type_ids=token_type_ids_,\n",
    "        )\n",
    "        outputs_e_ = self.query_end_encoder(\n",
    "            input_ids_,\n",
    "            attention_mask=attention_mask_,\n",
    "            token_type_ids=token_type_ids_,\n",
    "        )\n",
    "        sequence_output_s_ = outputs_s_[0]\n",
    "        sequence_output_e_ = outputs_e_[0]\n",
    "        query_start = sequence_output_s_[:,:1,:]\n",
    "        query_end = sequence_output_e_[:,:1,:]\n",
    "        return query_start, query_end\n",
    "    \n",
    "    def forward(self,\n",
    "            input_ids=None, attention_mask=None, token_type_ids=None,\n",
    "            input_ids_=None, attention_mask_=None, token_type_ids_=None,\n",
    "            start_positions=None, end_positions=None,\n",
    "            neg_input_ids=None, neg_attention_mask=None, neg_token_type_ids=None,\n",
    "            example_id=None,):\n",
    "        # Context-side\n",
    "        if input_ids is not None:\n",
    "            assert len(input_ids.size()) == 2\n",
    "            start, end = self.embed_phrase(input_ids, attention_mask, token_type_ids)\n",
    "\n",
    "            if neg_input_ids is not None:\n",
    "                neg_start, neg_end = self.embed_phrase(neg_input_ids, neg_attention_mask, neg_token_type_ids)\n",
    "\n",
    "            # Get filter logits\n",
    "            filter_output = start[:]\n",
    "            filter_start_logits, filter_end_logits = self.filter_linear(filter_output).chunk(2, dim=2)\n",
    "            filter_start_logits = filter_start_logits.squeeze(2)\n",
    "            filter_end_logits = filter_end_logits.squeeze(2)\n",
    "\n",
    "            if self.return_phrase:\n",
    "                return (start, end, filter_start_logits, filter_end_logits)\n",
    "\n",
    "        # Query-side\n",
    "        if input_ids_ is not None:\n",
    "            assert len(input_ids_.size()) == 2\n",
    "            query_start, query_end = self.embed_query(input_ids_, attention_mask_, token_type_ids_)\n",
    "\n",
    "            if self.return_query:\n",
    "                return (query_start, query_end)\n",
    "        \n",
    "        # Get dense logits\n",
    "        start_logits = start.matmul(query_start.transpose(1, 2)).squeeze(-1)\n",
    "        end_logits = end.matmul(query_end.transpose(1, 2)).squeeze(-1)\n",
    "        dense_logits = start_logits.unsqueeze(2) + end_logits.unsqueeze(1)\n",
    "\n",
    "        if start_positions is not None and end_positions is not None:\n",
    "            # If we are on multi-GPU, split add a dimension\n",
    "            if len(start_positions.size()) > 1:\n",
    "                start_positions = start_positions.squeeze(-1)\n",
    "            if len(end_positions.size()) > 1:\n",
    "                end_positions = end_positions.squeeze(-1)\n",
    "\n",
    "            ignored_index = start_logits.size(1)\n",
    "            start_positions.clamp_(0, ignored_index)\n",
    "            end_positions.clamp_(0, ignored_index)\n",
    "\n",
    "            # 1) Single-passage loss\n",
    "            loss_fct = CrossEntropyLoss(ignore_index=ignored_index)\n",
    "            start_loss = loss_fct(dense_logits.mean(2), start_positions)\n",
    "            end_loss = loss_fct(dense_logits.mean(1), end_positions)\n",
    "            single_loss = (start_loss + end_loss) / 2\n",
    "            total_loss = single_loss\n",
    "\n",
    "            return (total_loss,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Encoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, 512)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preprocessing the datasets.\n",
    "# Preprocessing is slighlty different for training and evaluation.\n",
    "column_names = dataset[\"train\"].column_names\n",
    "\n",
    "question_column_name = \"question\" if \"question\" in column_names else column_names[0]\n",
    "context_column_name = \"context\" if \"context\" in column_names else column_names[1]\n",
    "answer_column_name = \"answers\" if \"answers\" in column_names else column_names[2]\n",
    "\n",
    "# Padding side determines if we do (question|context) or (context|question).\n",
    "pad_on_right = tokenizer.padding_side == \"right\"\n",
    "\n",
    "max_seq_length = tokenizer.model_max_length\n",
    "max_query_length = 50#50\n",
    "pad_on_right,max_seq_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training preprocessing\n",
    "def prepare_train_features(examples):\n",
    "    # Some of the questions have lots of whitespace on the left, which is not useful and will make the\n",
    "    # truncation of the context fail (the tokenized question will take a lots of space). So we remove that\n",
    "    # left whitespace\n",
    "    examples[question_column_name] = [q.lstrip() for q in examples[question_column_name]]\n",
    "\n",
    "    # Tokenize our examples with truncation and maybe padding, but keep the overflows using a stride. This results\n",
    "    # in one example possible giving several features when a context is long, each of those features having a\n",
    "    # context that overlaps a bit the context of the previous feature.\n",
    "    \n",
    "    tokenized_examples = tokenizer(\n",
    "        examples[context_column_name],\n",
    "        truncation=\"only_first\",\n",
    "        max_length=max_seq_length,\n",
    "        stride=128,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "\n",
    "    tokenized_questions = tokenizer(\n",
    "        examples[question_column_name],\n",
    "        truncation=\"only_first\",\n",
    "        max_length=max_query_length,\n",
    "        return_overflowing_tokens=False,\n",
    "        return_offsets_mapping=False,\n",
    "        padding=\"max_length\" \n",
    "    )\n",
    "\n",
    "    # Since one example might give us several features if it has a long context, we need a map from a feature to\n",
    "    # its corresponding example. This key gives us just that.\n",
    "    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
    "    # The offset mappings will give us a map from token to character position in the original context. This will\n",
    "    # help us compute the start_positions and end_positions.\n",
    "    offset_mapping = tokenized_examples.pop(\"offset_mapping\")\n",
    "\n",
    "    # Inflate questions based on sample_mapping\n",
    "    tokenized_examples['input_ids_'] = [tokenized_questions['input_ids'][i] for i in sample_mapping]\n",
    "    tokenized_examples['token_type_ids_'] = [tokenized_questions['token_type_ids'][i] for i in sample_mapping]\n",
    "    tokenized_examples['attention_mask_'] = [tokenized_questions['attention_mask'][i] for i in sample_mapping]\n",
    "\n",
    "    # Let's label those examples!\n",
    "    tokenized_examples[\"start_positions\"] = []\n",
    "    tokenized_examples[\"end_positions\"] = []\n",
    "\n",
    "    for i, offsets in enumerate(offset_mapping):\n",
    "        # We will label impossible answers with the index of the CLS token.\n",
    "        input_ids = tokenized_examples[\"input_ids\"][i]\n",
    "        cls_index = input_ids.index(tokenizer.cls_token_id)\n",
    "\n",
    "        # Grab the sequence corresponding to that example (to know what is the context and what is the question).\n",
    "        sequence_ids = tokenized_examples.sequence_ids(i)\n",
    "        #context_index = 1 if pad_on_right and args.append_title else 0\n",
    "        context_index =  0\n",
    "\n",
    "        # One example can give several spans, this is the index of the example containing this span of text.\n",
    "        sample_index = sample_mapping[i]\n",
    "        answers = examples[answer_column_name][sample_index]\n",
    "        # If no answers are given, set the cls_index as answer.\n",
    "        if len(answers[\"answer_start\"]) == 0:\n",
    "            tokenized_examples[\"start_positions\"].append(cls_index)\n",
    "            tokenized_examples[\"end_positions\"].append(cls_index)\n",
    "        else:\n",
    "            # Start/end character index of the answer in the text.\n",
    "            start_char = answers[\"answer_start\"][0]\n",
    "            end_char = start_char + len(answers[\"text\"][0])\n",
    "\n",
    "            # Start token index of the current span in the text.\n",
    "            token_start_index = 0\n",
    "            while sequence_ids[token_start_index] != context_index:\n",
    "                token_start_index += 1\n",
    "\n",
    "            # End token index of the current span in the text.\n",
    "            token_end_index = len(input_ids) - 1\n",
    "            while sequence_ids[token_end_index] != context_index:\n",
    "                token_end_index -= 1\n",
    "\n",
    "            # Detect if the answer is out of the span (in which case this feature is labeled with the CLS index).\n",
    "            if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n",
    "                tokenized_examples[\"start_positions\"].append(cls_index)\n",
    "                tokenized_examples[\"end_positions\"].append(cls_index)\n",
    "            else:\n",
    "                # Otherwise move the token_start_index and token_end_index to the two ends of the answer.\n",
    "                # Note: we could go after the last offset if the answer is the last word (edge case).\n",
    "                while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n",
    "                    token_start_index += 1\n",
    "                tokenized_examples[\"start_positions\"].append(token_start_index - 1)\n",
    "                while offsets[token_end_index][1] >= end_char:\n",
    "                    token_end_index -= 1\n",
    "                tokenized_examples[\"end_positions\"].append(token_end_index + 1)\n",
    "\n",
    "    #bugfix: limit to 1000. Ajeitar!!!\n",
    "    tokenized_examples[\"input_ids\"]=tokenized_examples[\"input_ids\"][:1000]\n",
    "    tokenized_examples[\"input_ids_\"]=tokenized_examples[\"input_ids_\"][:1000]\n",
    "    tokenized_examples[\"attention_mask\"]=tokenized_examples[\"attention_mask\"][:1000]\n",
    "    tokenized_examples[\"attention_mask_\"]=tokenized_examples[\"attention_mask_\"][:1000]\n",
    "    tokenized_examples[\"token_type_ids\"]=tokenized_examples[\"token_type_ids\"][:1000]\n",
    "    tokenized_examples[\"token_type_ids_\"]=tokenized_examples[\"token_type_ids_\"][:1000]\n",
    "    tokenized_examples[\"start_positions\"]=tokenized_examples[\"start_positions\"][:1000]\n",
    "    tokenized_examples[\"end_positions\"]=tokenized_examples[\"end_positions\"][:1000]\n",
    "    return tokenized_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running tokenizer on train dataset:   0%|          | 0/4000 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running tokenizer on train dataset: 100%|██████████| 4000/4000 [00:00<00:00, 4517.06 examples/s]\n"
     ]
    }
   ],
   "source": [
    "train_dataset = dataset[\"train\"]\n",
    "train_dataset = train_dataset.map(\n",
    "                prepare_train_features,\n",
    "                batched=True,\n",
    "                num_proc=1,\n",
    "                #remove_columns=column_names,\n",
    "                #load_from_cache_file=not args.overwrite_cache,\n",
    "                desc=\"Running tokenizer on train dataset\",\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import default_data_collator\n",
    "from transformers import DataCollatorWithPadding,EvalPrediction\n",
    "from datasets import load_metric\n",
    "from utils.utils_qa import postprocess_qa_predictions\n",
    "\n",
    "# Post-processing:\n",
    "def post_processing_function(examples, features, predictions, stage=\"eval\", filter_threshold=-1e5):\n",
    "    # Post-processing: we match the start logits and end logits to answers in the original context.\n",
    "    predictions, save_rate = postprocess_qa_predictions(\n",
    "        examples=examples,\n",
    "        features=features,\n",
    "        predictions=predictions,\n",
    "        version_2_with_negative=args.version_2_with_negative,\n",
    "        n_best_size=args.n_best_size,\n",
    "        max_answer_length=args.max_answer_length,\n",
    "        null_score_diff_threshold=args.null_score_diff_threshold,\n",
    "        output_dir=args.output_dir,\n",
    "        log_level=0,\n",
    "        prefix=stage,\n",
    "        filter_threshold=filter_threshold,\n",
    "    )\n",
    "    # Format the result to the format the metric expects.\n",
    "    if args.version_2_with_negative:\n",
    "        formatted_predictions = [\n",
    "            {\"id\": k, \"prediction_text\": v, \"no_answer_probability\": 0.0} for k, v in predictions.items()\n",
    "        ]\n",
    "    else:\n",
    "        formatted_predictions = [{\"id\": k, \"prediction_text\": v} for k, v in predictions.items()]\n",
    "\n",
    "    references = [{\"id\": ex[\"id\"], \"answers\": ex[answer_column_name]} for ex in examples]\n",
    "    return EvalPrediction(predictions=formatted_predictions, label_ids=references), save_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2503/3832162653.py:5: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = load_metric(\"squad\")\n",
      "/home/miu/miniconda3/envs/torch/lib/python3.10/site-packages/datasets/load.py:752: FutureWarning: The repository for squad contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.16.0/metrics/squad/squad.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
     ]
    }
   ],
   "source": [
    "from utils.trainer_qa import QuestionAnsweringTrainer\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "pad_to_max_length=True\n",
    "metric = load_metric(\"squad\")\n",
    "# Data collator\n",
    "# We have already padded to max length if the corresponding flag is True, otherwise we need to pad in the data\n",
    "# collator.\n",
    "data_collator = (\n",
    "    default_data_collator\n",
    "    # if args.pad_to_max_length\n",
    "    # else DataCollatorWithPadding(tokenizer, pad_to_multiple_of=8 if args.fp16 else None)\n",
    "    if pad_to_max_length\n",
    "    else DataCollatorWithPadding(tokenizer, pad_to_multiple_of=8 if args.fp16 else None)\n",
    ")\n",
    "\n",
    "def compute_metrics(p: EvalPrediction):\n",
    "    return metric.compute(predictions=p.predictions, references=p.label_ids)\n",
    "\n",
    "args = TrainingArguments(\n",
    "    \"bert-finetuned-squad\",\n",
    "    evaluation_strategy=\"no\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    fp16=True,\n",
    "    push_to_hub=False,\n",
    ")\n",
    "\n",
    "# Initialize our Trainer\n",
    "trainer = QuestionAnsweringTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    # args=None,\n",
    "    train_dataset=train_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    post_process_function=post_processing_function,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 500/2500 [01:02<04:11,  7.95it/s]Checkpoint destination directory bert-finetuned-squad/checkpoint-500 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.7711, 'learning_rate': 1.6064e-05, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 1000/2500 [02:08<02:58,  8.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0071, 'learning_rate': 1.2064e-05, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 1500/2500 [03:13<02:08,  7.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4843, 'learning_rate': 8.064000000000001e-06, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 2000/2500 [04:19<01:02,  8.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2555, 'learning_rate': 4.064e-06, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2500/2500 [05:25<00:00,  7.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1397, 'learning_rate': 6.4e-08, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2500/2500 [05:28<00:00,  7.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 328.3635, 'train_samples_per_second': 60.908, 'train_steps_per_second': 7.614, 'train_loss': 1.3315405059814454, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_result = trainer.train(resume_from_checkpoint=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create phrase vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving 18829 (context, question, answer) triples.\n",
      "Writing to /home/miu/Projects/NLP/PRO-ConvQA/wikidump/wiki-dev/0000_hf.json\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from utils.utils_dp import convert_squad_to_hf\n",
    "import logging\n",
    "\n",
    "# Make one log on every process with the configuration for debugging.\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
    "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "    level=logging.INFO,\n",
    ")\n",
    "\n",
    "data_files = {}\n",
    "data_files[\"test\"] = convert_squad_to_hf('/home/miu/Projects/NLP/PRO-ConvQA/wikidump/wiki-dev/0000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating test split: 18829 examples [00:00, 264724.78 examples/s]\n"
     ]
    }
   ],
   "source": [
    "extension = data_files[\"test\"].split(\".\")[-1]\n",
    "raw_datasets = load_dataset(extension, data_files=data_files, field=\"data\")\n",
    "\n",
    "column_names = raw_datasets[\"test\"].column_names\n",
    "context_column_name = \"context\" if \"context\" in column_names else column_names[1]\n",
    "\n",
    "# Padding side determines if we do (question|context) or (context|question).\n",
    "pad_on_right = tokenizer.padding_side == \"right\"\n",
    "max_seq_length = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "offset = 0\n",
    "\n",
    "# Validation preprocessing\n",
    "def prepare_validation_features(examples, indexes):\n",
    "    # Tokenize our examples with truncation and maybe padding, but keep the overflows using a stride. This results\n",
    "    # in one example possible giving several features when a context is long, each of those features having a\n",
    "    # context that overlaps a bit the context of the previous feature.\n",
    "    tokenized_examples = tokenizer(\n",
    "        examples[context_column_name],\n",
    "        truncation=\"only_first\",\n",
    "        max_length=max_seq_length,\n",
    "        stride=128,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\" ,\n",
    "    )\n",
    "\n",
    "    # Since one example might give us several features if it has a long context, we need a map from a feature to\n",
    "    # its corresponding example. This key gives us just that.\n",
    "    sample_mapping = tokenized_examples[\"overflow_to_sample_mapping\"]\n",
    "    \n",
    "    # Inflate doc_idxs based on sample_mapping\n",
    "    tokenized_examples['doc_idx'] = [offset + examples['doc_idx'][i] for i in sample_mapping]\n",
    "\n",
    "    # This example_id indicates the index of an original paragraph (not question id)\n",
    "    tokenized_examples['example_id'] = [indexes[i] for i in sample_mapping]\n",
    "\n",
    "    for i in range(len(tokenized_examples[\"input_ids\"])):\n",
    "        # Grab the sequence corresponding to that example (to know what is the context and what is the question).\n",
    "        sequence_ids = tokenized_examples.sequence_ids(i)\n",
    "        context_index = 1\n",
    "\n",
    "        # Set to None the offset_mapping that are not part of the context so it's easy to determine if a token\n",
    "        # position is part of the context or not.\n",
    "        tokenized_examples[\"offset_mapping\"][i] = [\n",
    "            (o if sequence_ids[k] == context_index else None)\n",
    "            for k, o in enumerate(tokenized_examples[\"offset_mapping\"][i])\n",
    "        ]\n",
    "\n",
    "    return tokenized_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running tokenizer on prediction dataset:   0%|          | 0/18829 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running tokenizer on prediction dataset: 100%|██████████| 18829/18829 [00:03<00:00, 5876.06 examples/s]\n"
     ]
    }
   ],
   "source": [
    "examples = raw_datasets[\"test\"]\n",
    "accelerator = Accelerator(gradient_accumulation_steps=args.gradient_accumulation_steps)\n",
    "\n",
    "# Predict Feature Creation\n",
    "with accelerator.main_process_first():\n",
    "    dataset = examples.map(\n",
    "        prepare_validation_features,\n",
    "        batched=True,\n",
    "        num_proc=1,\n",
    "        with_indices=True,\n",
    "        remove_columns=column_names,\n",
    "        # load_from_cache_file=not args.overwrite_cache,\n",
    "        desc=\"Running tokenizer on prediction dataset\",\n",
    "    )\n",
    "\n",
    "# Data collator\n",
    "# We have already padded to max length if the corresponding flag is True, otherwise we need to pad in the data\n",
    "# collator.\n",
    "data_collator = (\n",
    "    default_data_collator\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "per_device_train_batch_size=8\n",
    "\n",
    "predict_dataset_for_model = dataset.remove_columns([ \"offset_mapping\",\"doc_idx\",\"overflow_to_sample_mapping\"])\n",
    "predict_dataloader = DataLoader(\n",
    "    predict_dataset_for_model, shuffle=True, collate_fn=data_collator, batch_size=per_device_train_batch_size,\n",
    ")\n",
    "\n",
    "model, predict_dataloader = accelerator.prepare(model, predict_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2362"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(predict_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([8, 512, 768]),\n",
       " torch.Size([8, 512, 768]),\n",
       " torch.Size([8, 512]),\n",
       " torch.Size([8, 512]))"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def to_numpy(tensor):\n",
    "    return tensor.detach().cpu().numpy()\n",
    "\n",
    "c=0\n",
    "feature_id=0\n",
    "all_predictions=[]\n",
    "model.return_phrase=True\n",
    "for step, inputs in enumerate(predict_dataloader):\n",
    "    outputs=model(**inputs)\n",
    "\n",
    "    for item_idx, (start, end, filter_start, filter_end) in enumerate(zip(*outputs)):\n",
    "        a= {'example_id': inputs['example_id'][item_idx].item(),\n",
    "            'feature_id': 0, # dataloader should not be shuffled\n",
    "            'start': to_numpy(start),\n",
    "            'end': to_numpy(end),\n",
    "            'filter_start': to_numpy(filter_start),\n",
    "            'filter_end': to_numpy(filter_end),\n",
    "            'inputs': inputs['input_ids'][item_idx].tolist(),\n",
    "        }\n",
    "        all_predictions.append(a)\n",
    "    feature_id+=1\n",
    "    c+=1\n",
    "    if c>100:\n",
    "        break\n",
    "outputs[0].shape,outputs[1].shape,outputs[2].shape,outputs[3].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(808, (512, 768))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_predictions),all_predictions[0]['start'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Search Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 512])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] today, mail is delivered once a day on - site to most private homes and businesses. the usps still distinguishes between city delivery ( where carriers generally walk and deliver to mailboxes hung on exterior walls or porches, or to commercial reception areas ) and rural delivery ( where carriers generally drive ). with \" curbside delivery \", mailboxes are at the ends of driveways, on the nearest convenient road. \" central point delivery \" is used in some locations, where several nearby residences share a \" cluster \" of individual mailboxes in a single housing. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(inputs['input_ids'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 1 examples [00:00, 925.89 examples/s]\n",
      "Running tokenizer on prediction dataset: 100%|██████████| 1/1 [00:00<00:00, 342.25 examples/s]\n"
     ]
    }
   ],
   "source": [
    "df=[]\n",
    "query = 'tell me about the historical spending on defense'\n",
    "\n",
    "for i in range(1):\n",
    "    df.append({'doc_idx':0, 'par_idx':0, 'title':'', 'context':query})\n",
    "p='/home/miu/Projects/NLPLab/DensePhrasesMinimal/query.json'\n",
    "with open(p, 'w') as f:\n",
    "        json.dump({'data': df}, f)\n",
    "r_d = load_dataset(extension, data_files=p, field=\"data\")\n",
    "r_d=r_d['train']\n",
    "# query_inputs = prepare_features(r_d['train'])\n",
    "with accelerator.main_process_first():\n",
    "    query_dataset = r_d.map(\n",
    "        prepare_validation_features,\n",
    "        batched=True,\n",
    "        num_proc=1,\n",
    "        with_indices=True,\n",
    "        desc=\"Running tokenizer on prediction dataset\",\n",
    "    )\n",
    "query_dataset = query_dataset.remove_columns([\"par_idx\", \"offset_mapping\",\"doc_idx\",\"overflow_to_sample_mapping\"])\n",
    "query_dataloader = DataLoader(\n",
    "    query_dataset, shuffle=True, collate_fn=data_collator, batch_size=1,\n",
    ")\n",
    "query_dataloader = accelerator.prepare(query_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.return_query=True\n",
    "model.return_phrase=False\n",
    "\n",
    "for step, query_inputs in enumerate(query_dataloader):\n",
    "    #print(query_inputs)\n",
    "    query_outputs=model(input_ids_=query_inputs['input_ids'],attention_mask_=query_inputs['attention_mask'])\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def search_mips(phrases_embs,query_emb):\n",
    "    print(phrases_embs.shape,query_emb.shape)\n",
    "    scores=[]\n",
    "    for i in range(phrases_embs.shape[0]):\n",
    "        batch=[]\n",
    "        for j in range(phrases_embs.shape[1]):\n",
    "            # if j%10==0:\n",
    "                #batch.append(phrases_embs[i][j])\n",
    "            s=np.dot(phrases_embs[i][j],query_emb)\n",
    "            scores.append((i,j,s))\n",
    "\n",
    "        #reduced_embs.append(batch)\n",
    "    scores=np.asarray(scores)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(768,) (768,)\n"
     ]
    }
   ],
   "source": [
    "q_start=query_outputs[0][0].cpu().detach().numpy()[0]\n",
    "q_end=query_outputs[1][0].cpu().detach().numpy()[0]\n",
    "print(q_start.shape,q_end.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(808, 512, 768)\n",
      "(808, 512, 768) (768,)\n"
     ]
    }
   ],
   "source": [
    "# scores=search_mips(outputs[0].cpu().detach().numpy(),q_start)\n",
    "search_embs=np.asarray([all_predictions[i]['start'] for i in range(len(all_predictions))])\n",
    "# search_ids = np.asarray([all_predictions[i]['inputs'] for i in range(len(all_predictions))])\n",
    "print(search_embs.shape)\n",
    "scores=search_mips(search_embs,q_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(114194, array([223.        ,  18.        ,  61.62268829]))"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_score = np.argmax(scores[:,2])\n",
    "i=int(scores[best_score][0])\n",
    "j=int(scores[best_score][1])\n",
    "best_score,scores[best_score]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.decode(inputs['input_ids'][i][j:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'louisiana purchase legislative timeline - - 1803 - 1804. france turned over new orleans, the historic colonial capital, on december 20, 1803, at the cabildo, with a flag - raising ceremony in the plaza de armas, now jackson square. just three weeks earlier, on november 30, 1803, spanish officials had formally conveyed the colonial lands and their administration to france. on march 9 and 10, 1804, another ceremony, commemorated as three flags day, was conducted in st. louis, to transfer ownership of upper louisiana from spain to the french first republic, and then from france to the united states. from march 10 to september 30, 1804, upper louisiana was supervised as a military district, under commandant amos stoddard. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]'"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp=all_predictions[i]['inputs']\n",
    "#decode\n",
    "tokenizer.decode(inp[j:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
