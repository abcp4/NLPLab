{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/miu/miniconda3/envs/torch/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "print(torch.__version__)\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "torch.cuda.is_available()\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Iterator\n",
    "import time\n",
    "\n",
    "import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "from tokenizers import BertWordPieceTokenizer, Regex, normalizers\n",
    "from tqdm import tqdm\n",
    "from transformers import (\n",
    "    BertConfig,\n",
    "    BertForMaskedLM,\n",
    "    BertTokenizerFast,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "import math\n",
    "from functools import reduce\n",
    "from collections import namedtuple\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "# from reformer_pytorch import ReformerLM\n",
    "# from x_transformers import TransformerWrapper, Decoder,Encoder\n",
    "from torch.utils.data import DataLoader\n",
    "import math \n",
    "from accelerate import Accelerator\n",
    "import torch.nn.functional as F\n",
    "# from einops import rearrange, pack, unpack\n",
    "import sys\n",
    "import os\n",
    "#get cur dir\n",
    "cur_dir = os.getcwd()\n",
    "if 'miu' in cur_dir:\n",
    "    sys.path.append('/home/miu/Projects/NLPLab/')\n",
    "elif 'kiki' in cur_dir:\n",
    "    sys.path.append('/home/kiki/dados_servidor/Servidor/NLPLab/')\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size:  2040\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Resolving data files: 100%|██████████| 53/53 [00:00<00:00, 84.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded dataset!!\n"
     ]
    }
   ],
   "source": [
    "model_training='hf'#'bert' , 'tokemonster\n",
    "# LIMIT_DATASET = 2016 * 4  # keep small for development, set to None for full dataset\n",
    "#(3090) Para 128 tokens(Bert) e 32000 vocab: 1000 = 6 seg, 10000 = 1min, 100000 = 10min, 1m = 100min, 10m = 16h, 100m = 160h ou 6.6 dias\n",
    "#(3090) Para 84 tokens(tokenmonster salva 35%) e 24000 vocab(Bert): 1000 = 4.3s, 10000 = 43s, 100000 = 7.1m, 1m = 71min, 10m = 11.8h, 100m = 118h ou 5 dias\n",
    "#(4090) Para 84 tokens(tokenmonster salva 35%) e 24000 vocab(Electra): 1000 = 3.5s, 10000 = 35s, 100000 = 5.9m, 1m = 59min, 10m = 9.8h, 100m = 98h ou 4 dias\n",
    "#(4090) Para 84 tokens(tokenmonster salva 35%) e 16000 vocab (Electra): 1000 = 2.1s, 10000 = 21s, 100000 = 3.53m, 1m = 35.3min, 10m = 5.8h, 100m = 58h ou 2.4 dias\n",
    "\n",
    "#com retnet. Confirmado que é O(n). Se lembrando que não existe mta vantagem além do custo crescer sequencialmente, já que o conteudo \n",
    "#(4090) Para 84 tokens(tokenmonster salva 35%) batch size 200 e 16000 vocab (Electra): 1000 = 2.1s, 10000 = 21s, 100000 = 3.53m, 1m = 35.3min, 10m = 5.8h, 100m = 58h ou 2.4 dias\n",
    "#(4090) Para 168 tokens(tokenmonster salva 35%) batch size 100 e 16000 vocab (Electra): 1000 = 3.7s, 10000 = 37.5s,\n",
    "#(4090) Para 1000 tokens(tokenmonster salva 35%) batch size 12 e 16000 vocab (Electra):            , 10000 = 4min32s,\n",
    "\n",
    "\n",
    "#HIPOTÉTICO(Se conseguisse representar um texto com menos tokens):\n",
    "#(4090) Para 24 tokens(tokenmonster salva 35%)(Electra): 1000 = , 10000 = , 100000 = 3.19m, 1m = 32min, 10m = 6.5h, 100m = 65h ou 2.7 dias\n",
    "#HIPOTÉTICO(Se diminuisse o vocabulario para 1024 tokens diferentes)\n",
    "#(4090) Para 84 tokens(tokenmonster salva 35%) e 1024 vocab: 1000 =1.6s , 10000 = 16.2s,\n",
    "\n",
    "#losses:\n",
    "#1m dataset, electra, tokenmonster: iteration:  7800 , total_loss:  26.277032788594564\n",
    "#10m dataset, 84 tokens(tokenmonster salva 35%) e 16000 vocab (Electra) iteration:  49980 , total_loss:  20.8733185450236. 339min total\n",
    "\n",
    "\n",
    "LIMIT_DATASET = 10_000\n",
    "RANDOM_SEED = 42\n",
    "NUM_TOKENIZER_TRAINING_ITEMS = 1_000_000  # I made this up, but it seems reasonable\n",
    "# VOCAB_SIZE = 32_768  # from Cramming\n",
    "# DEVICE_BATCH_SIZE = 100 # aprox 128, adjust to get near 100% gpu memory use\n",
    "# MODEL_MAX_SEQ_LEN = 128  # from Cramming\n",
    "\n",
    "VOCAB_SIZE = 32_768  # tokenmonster\n",
    "# VOCAB_SIZE = 1_024  # tokenmonster\n",
    "DEVICE_BATCH_SIZE = 12 # Token monster aguenta um batch size de (200-248)!! Geralmente melhora a qualidade do treino\n",
    "MODEL_MAX_SEQ_LEN = 512 # token_monster\n",
    "\n",
    "\n",
    "MASK_ID=4\n",
    "PAD_ID=0\n",
    "gradient_accumulation_steps = 2048 // DEVICE_BATCH_SIZE  # roughly based on Cramming\n",
    "batch_size = DEVICE_BATCH_SIZE * gradient_accumulation_steps\n",
    "print('batch_size: ',batch_size)\n",
    "RUN_DIR = Path(\"data\") / f\"run_{time.strftime('%Y%m%d-%H%M%S')}\"\n",
    "CHECKPOINT_DIR = RUN_DIR / \"training_checkpoints\"\n",
    "MODEL_DIR = RUN_DIR / \"model\"\n",
    "TOKENIZER_PATH = RUN_DIR / \"tokenizer.json\"\n",
    "TRAINER_HISTORY_PATH = RUN_DIR / \"trainer_history.json\"\n",
    "\n",
    "RUN_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "dataset = datasets.load_dataset(\n",
    "    \"sradc/chunked-shuffled-wikipedia20220301en-bookcorpusopen\",\n",
    "    split=f\"train[:{LIMIT_DATASET}]\" if LIMIT_DATASET else \"train\",\n",
    "    revision=\"0e6fada2dd43136e4a3f637da41de2e596aee674\",\n",
    ")\n",
    "print('loaded dataset!!')\n",
    "\n",
    "from utils.process_tokenizer import get_tokenizer\n",
    "# tokenizer,tokenized_dataset,norm,vocab=get_tokenizer(dataset,NUM_TOKENIZER_TRAINING_ITEMS,VOCAB_SIZE,TOKENIZER_PATH,'clm',MODEL_MAX_SEQ_LEN+2)\n",
    "tokenized_dataset,norm,vocab,tokenizer=get_tokenizer(dataset,NUM_TOKENIZER_TRAINING_ITEMS,VOCAB_SIZE,TOKENIZER_PATH,'mlm',MODEL_MAX_SEQ_LEN+2)\n",
    "# tokenized_dataset,norm,vocab=get_tokenizer(dataset,NUM_TOKENIZER_TRAINING_ITEMS,VOCAB_SIZE,TOKENIZER_PATH,'smlm',MODEL_MAX_SEQ_LEN+2)\n",
    "MASK_ID=vocab.tokenize(\"[MASK]\")[0]\n",
    "N_DOMAINS=3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from x_transformers import TransformerWrapper, Decoder,Encoder\n",
    "\n",
    "\n",
    "#encoder decoder\n",
    "colt5_dict={\n",
    "            'encoder':{'is_colt5_encoder':True,'light_ff_mult':0.5,'heavy_ff_mult':4,'num_heavy_tokens':30,\n",
    "                   'light_dim_head':64,'light_heads':8,'light_window_size':128,\n",
    "                   'heavy_dim_head':64,'heavy_heads':8,'num_heavy_tokens_q':30,\n",
    "                   'num_heavy_tokens_kv':30},\n",
    "            }\n",
    "\n",
    "# model = TransformerWrapper(\n",
    "model = TransformerWrapper(\n",
    "    num_tokens = VOCAB_SIZE+8,\n",
    "    max_seq_len = MODEL_MAX_SEQ_LEN,\n",
    "    attn_layers = Encoder(\n",
    "        dim = int(768),\n",
    "        depth = 12,\n",
    "        heads =12,\n",
    "        attn_flash = True,\n",
    "        colt5_dict=colt5_dict['encoder'],\n",
    "        # rel_pos_bias = True\n",
    "    ),\n",
    "    emb_dim=128,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_ids are the indices corresponding to each token in the sentence.\n",
    "# attention_mask indicates whether a token should be attended to or not.\n",
    "# token_type_ids identifies which sequence a token belongs to when there is more than one sequence\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "        tokenized_dataset, shuffle=True, batch_size=DEVICE_BATCH_SIZE\n",
    "    )\n",
    "from utils.utils import get_optimizer_scheduler\n",
    "optimizer,lr_scheduler,max_train_steps = get_optimizer_scheduler(model,train_dataloader,gradient_accumulation_steps,learning_rate=5e-5,weight_decay=0, num_warmup_steps=0, max_train_steps=None,lr_scheduler_type='linear',num_train_epochs=1)\n",
    "\n",
    "accelerator = Accelerator()\n",
    "# Prepare everything with our `accelerator`.\n",
    "model, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(\n",
    "    model, optimizer, train_dataloader, lr_scheduler\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration:  0 , total_loss:  0.3496361414591471\n",
      "count_amostra: 12\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/miu/Projects/NLPLab/berts/bert_colt5.ipynb Cell 5\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/miu/Projects/NLPLab/berts/bert_colt5.ipynb#W4sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39m# print(loss)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/miu/Projects/NLPLab/berts/bert_colt5.ipynb#W4sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m total_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mfloat()\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy()\u001b[39m.\u001b[39mitem()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/miu/Projects/NLPLab/berts/bert_colt5.ipynb#W4sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m accelerator\u001b[39m.\u001b[39;49mbackward(loss)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/miu/Projects/NLPLab/berts/bert_colt5.ipynb#W4sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/miu/Projects/NLPLab/berts/bert_colt5.ipynb#W4sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m lr_scheduler\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.10/site-packages/accelerate/accelerator.py:1905\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   1903\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscaler\u001b[39m.\u001b[39mscale(loss)\u001b[39m.\u001b[39mbackward(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1904\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1905\u001b[0m     loss\u001b[39m.\u001b[39;49mbackward(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.10/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    493\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    494\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.10/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[39m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m     tensors,\n\u001b[1;32m    253\u001b[0m     grad_tensors_,\n\u001b[1;32m    254\u001b[0m     retain_graph,\n\u001b[1;32m    255\u001b[0m     create_graph,\n\u001b[1;32m    256\u001b[0m     inputs,\n\u001b[1;32m    257\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    258\u001b[0m     accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    259\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Only show the progress bar once on each machine.\n",
    "# progress_bar = tqdm(range(max_train_steps), disable=not accelerator.is_local_main_process)\n",
    "completed_steps = 0\n",
    "starting_epoch = 0\n",
    "\n",
    "# update the progress_bar if load from checkpoint\n",
    "total_loss=0\n",
    "# progress_bar.update(completed_steps)\n",
    "steps_log=30\n",
    "count_amostra=0\n",
    "num_train_epochs=1\n",
    "for epoch in range(starting_epoch, num_train_epochs):\n",
    "    model.train()\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "        # with accelerator.accumulate(model):\n",
    "       \n",
    "        logits=model(batch['input_ids']) \n",
    "        loss = F.cross_entropy(logits.transpose(1, 2),batch['input_ids'],ignore_index = PAD_ID)\n",
    "        count_amostra+=int(len(batch['input_ids']))\n",
    "        # print(loss)\n",
    "        total_loss += loss.detach().float().cpu().numpy().item()\n",
    "        accelerator.backward(loss)\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()       \n",
    "        \n",
    "        if step%steps_log==0:\n",
    "            print('iteration: ',step,', total_loss: ',total_loss/steps_log)\n",
    "            print('count_amostra:',count_amostra)\n",
    "            total_loss=0\n",
    "    # Checks if the accelerator has performed an optimization step behind the scenes\n",
    "    if accelerator.sync_gradients:\n",
    "        # progress_bar.update(1)\n",
    "        completed_steps += 1\n",
    "\n",
    "    if completed_steps >= max_train_steps:\n",
    "        break\n",
    "\n",
    "    model.eval()\n",
    "    losses = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "53s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#convert to huggingface Transformers\n",
    "\n",
    "from transformers import BertConfig, BertForMaskedLM, BertTokenizerFast\n",
    "\n",
    "config = BertConfig(\n",
    "    vocab_size=VOCAB_SIZE+8,\n",
    "    max_position_embeddings=MODEL_MAX_SEQ_LEN,\n",
    "    num_attention_heads=12,\n",
    "    num_hidden_layers=12,\n",
    "    type_vocab_size=2,\n",
    ")\n",
    "\n",
    "bert_model=BertForMaskedLM(config=config)\n",
    "bert_model.bert=model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model.save_pretrained('models/bert')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trainer hf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "        num_rows: 4000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"squad\", split=\"train[:5000]\")\n",
    "dataset = dataset.train_test_split(test_size=0.2)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = MODEL_MAX_SEQ_LEN#384\n",
    "stride = 128\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "def preprocess_validation_examples(examples):\n",
    "    questions = [q.strip() for q in examples[\"question\"]]\n",
    "    inputs = tokenizer(\n",
    "        questions,\n",
    "        examples[\"context\"],\n",
    "        max_length=max_length,\n",
    "        truncation=\"only_second\",\n",
    "        stride=stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    sample_map = inputs.pop(\"overflow_to_sample_mapping\")\n",
    "    example_ids = []\n",
    "\n",
    "    for i in range(len(inputs[\"input_ids\"])):\n",
    "        sample_idx = sample_map[i]\n",
    "        example_ids.append(examples[\"id\"][sample_idx])\n",
    "\n",
    "        sequence_ids = inputs.sequence_ids(i)\n",
    "        offset = inputs[\"offset_mapping\"][i]\n",
    "        inputs[\"offset_mapping\"][i] = [\n",
    "            o if sequence_ids[k] == 1 else None for k, o in enumerate(offset)\n",
    "        ]\n",
    "\n",
    "    inputs[\"example_id\"] = example_ids\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1000/1000 [00:00<00:00, 4972.96 examples/s]\n"
     ]
    }
   ],
   "source": [
    "validation_dataset = dataset[\"test\"].map(\n",
    "    preprocess_validation_examples,\n",
    "    batched=True,\n",
    "    remove_columns=dataset[\"test\"].column_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    questions = [q.strip() for q in examples[\"question\"]]\n",
    "    inputs = tokenizer(\n",
    "        questions,\n",
    "        examples[\"context\"],\n",
    "        max_length=tokenizer.model_max_length,\n",
    "        truncation=\"only_second\",\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    offset_mapping = inputs.pop(\"offset_mapping\")\n",
    "    answers = examples[\"answers\"]\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "\n",
    "    for i, offset in enumerate(offset_mapping):\n",
    "        answer = answers[i]\n",
    "        start_char = answer[\"answer_start\"][0]\n",
    "        end_char = answer[\"answer_start\"][0] + len(answer[\"text\"][0])\n",
    "        sequence_ids = inputs.sequence_ids(i)\n",
    "        # print('sequence_ids: ',sequence_ids)\n",
    "        # print('sequence_ids: ',len(sequence_ids))\n",
    "\n",
    "        # Find the start and end of the context\n",
    "        idx = 0\n",
    "        while sequence_ids[idx] != 1:\n",
    "            idx += 1\n",
    "        context_start = idx\n",
    "        while sequence_ids[idx] == 1 and idx < len(sequence_ids) - 1:\n",
    "            idx += 1\n",
    "        context_end = idx - 1\n",
    "\n",
    "        # If the answer is not fully inside the context, label it (0, 0)\n",
    "        if offset[context_start][0] > end_char or offset[context_end][1] < start_char:\n",
    "            start_positions.append(0)\n",
    "            end_positions.append(0)\n",
    "        else:\n",
    "            # Otherwise it's the start and end token positions\n",
    "            idx = context_start\n",
    "            while idx <= context_end and offset[idx][0] <= start_char:\n",
    "                idx += 1\n",
    "            start_positions.append(idx - 1)\n",
    "\n",
    "            idx = context_end\n",
    "            while idx >= context_start and offset[idx][1] >= end_char:\n",
    "                idx -= 1\n",
    "            end_positions.append(idx + 1)\n",
    "\n",
    "    inputs[\"start_positions\"] = start_positions\n",
    "    inputs[\"end_positions\"] = end_positions\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/4000 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 4000/4000 [00:02<00:00, 1393.92 examples/s]\n",
      "Map: 100%|██████████| 1000/1000 [00:00<00:00, 1404.71 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import DefaultDataCollator\n",
    "\n",
    "tokenized_dataset = dataset.map(preprocess_function, batched=True, remove_columns=dataset['train'].column_names,batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at /home/miu/Projects/NLPLab/berts/models/bert and are newly initialized: ['bert.encoder.layer.3.intermediate.dense.weight', 'bert.encoder.layer.9.attention.output.LayerNorm.bias', 'bert.encoder.layer.6.attention.output.dense.bias', 'bert.encoder.layer.1.attention.output.dense.weight', 'bert.encoder.layer.2.attention.self.value.bias', 'bert.encoder.layer.8.attention.output.LayerNorm.bias', 'bert.encoder.layer.10.attention.output.dense.weight', 'bert.encoder.layer.8.output.LayerNorm.weight', 'bert.encoder.layer.6.attention.output.dense.weight', 'bert.encoder.layer.2.attention.self.query.weight', 'bert.encoder.layer.4.intermediate.dense.weight', 'bert.encoder.layer.4.attention.output.LayerNorm.bias', 'bert.encoder.layer.1.output.LayerNorm.weight', 'bert.encoder.layer.9.attention.self.value.bias', 'bert.encoder.layer.5.attention.self.query.weight', 'qa_outputs.bias', 'bert.encoder.layer.8.attention.self.value.bias', 'bert.encoder.layer.0.output.LayerNorm.bias', 'bert.encoder.layer.1.output.dense.weight', 'bert.encoder.layer.8.attention.self.key.weight', 'bert.encoder.layer.11.output.dense.weight', 'bert.encoder.layer.2.attention.self.key.weight', 'bert.encoder.layer.9.attention.output.dense.bias', 'bert.embeddings.word_embeddings.weight', 'bert.encoder.layer.8.output.dense.weight', 'bert.encoder.layer.11.intermediate.dense.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.bias', 'bert.encoder.layer.5.attention.self.value.bias', 'bert.encoder.layer.5.output.LayerNorm.bias', 'bert.encoder.layer.10.output.LayerNorm.weight', 'bert.encoder.layer.3.output.LayerNorm.weight', 'bert.embeddings.token_type_embeddings.weight', 'bert.encoder.layer.11.attention.output.LayerNorm.weight', 'bert.encoder.layer.7.attention.self.key.weight', 'bert.encoder.layer.0.attention.self.key.weight', 'bert.encoder.layer.0.attention.self.query.bias', 'bert.encoder.layer.9.intermediate.dense.weight', 'bert.encoder.layer.2.attention.self.key.bias', 'bert.encoder.layer.10.output.dense.bias', 'bert.encoder.layer.3.attention.self.key.bias', 'bert.encoder.layer.6.attention.self.value.weight', 'bert.encoder.layer.3.attention.output.dense.bias', 'bert.encoder.layer.10.attention.output.LayerNorm.weight', 'bert.encoder.layer.7.output.LayerNorm.bias', 'bert.encoder.layer.5.attention.self.key.bias', 'bert.encoder.layer.5.output.dense.bias', 'bert.encoder.layer.1.output.dense.bias', 'bert.encoder.layer.9.attention.output.dense.weight', 'bert.encoder.layer.11.attention.self.query.bias', 'bert.encoder.layer.9.attention.self.key.weight', 'bert.encoder.layer.8.attention.self.key.bias', 'bert.encoder.layer.9.output.LayerNorm.bias', 'bert.encoder.layer.4.attention.self.value.weight', 'bert.encoder.layer.1.attention.output.dense.bias', 'bert.encoder.layer.3.output.dense.weight', 'bert.encoder.layer.8.attention.output.dense.weight', 'bert.encoder.layer.7.intermediate.dense.bias', 'bert.encoder.layer.0.attention.self.value.weight', 'bert.encoder.layer.8.attention.output.LayerNorm.weight', 'bert.encoder.layer.7.attention.self.value.weight', 'bert.encoder.layer.3.attention.self.query.weight', 'bert.encoder.layer.6.attention.self.key.weight', 'bert.encoder.layer.6.intermediate.dense.bias', 'bert.encoder.layer.10.attention.self.query.weight', 'bert.encoder.layer.3.attention.output.LayerNorm.bias', 'bert.encoder.layer.4.output.dense.bias', 'bert.encoder.layer.5.attention.output.LayerNorm.weight', 'bert.encoder.layer.8.attention.self.value.weight', 'bert.encoder.layer.9.output.dense.weight', 'bert.encoder.layer.8.output.dense.bias', 'bert.encoder.layer.0.attention.output.dense.bias', 'bert.encoder.layer.7.attention.self.query.weight', 'bert.encoder.layer.1.attention.self.key.weight', 'bert.encoder.layer.6.output.dense.bias', 'bert.encoder.layer.11.attention.self.value.weight', 'bert.encoder.layer.7.attention.output.dense.weight', 'bert.encoder.layer.0.output.dense.bias', 'bert.encoder.layer.10.attention.output.dense.bias', 'bert.encoder.layer.2.attention.self.query.bias', 'bert.encoder.layer.5.output.LayerNorm.weight', 'bert.encoder.layer.4.intermediate.dense.bias', 'bert.encoder.layer.11.attention.self.key.bias', 'bert.encoder.layer.1.attention.self.value.bias', 'bert.encoder.layer.2.attention.output.LayerNorm.weight', 'bert.encoder.layer.1.attention.self.key.bias', 'bert.encoder.layer.6.attention.self.key.bias', 'bert.encoder.layer.8.attention.self.query.bias', 'bert.encoder.layer.6.attention.self.query.weight', 'bert.encoder.layer.3.attention.self.value.bias', 'bert.encoder.layer.10.output.LayerNorm.bias', 'bert.encoder.layer.7.output.dense.weight', 'bert.encoder.layer.5.attention.output.dense.bias', 'bert.encoder.layer.3.attention.self.value.weight', 'bert.encoder.layer.9.output.dense.bias', 'bert.encoder.layer.10.attention.self.value.weight', 'bert.encoder.layer.6.intermediate.dense.weight', 'bert.encoder.layer.7.output.LayerNorm.weight', 'bert.encoder.layer.11.attention.self.query.weight', 'bert.encoder.layer.6.attention.self.query.bias', 'bert.encoder.layer.11.attention.output.dense.bias', 'bert.encoder.layer.4.attention.self.key.weight', 'bert.encoder.layer.7.attention.self.query.bias', 'bert.encoder.layer.8.intermediate.dense.bias', 'bert.encoder.layer.10.attention.self.key.weight', 'bert.encoder.layer.2.attention.output.dense.weight', 'bert.encoder.layer.2.output.dense.bias', 'bert.encoder.layer.5.output.dense.weight', 'bert.encoder.layer.4.attention.output.dense.bias', 'bert.encoder.layer.11.attention.self.key.weight', 'bert.encoder.layer.10.output.dense.weight', 'bert.encoder.layer.5.attention.output.dense.weight', 'bert.encoder.layer.10.attention.output.LayerNorm.bias', 'bert.encoder.layer.5.attention.self.query.bias', 'bert.encoder.layer.1.output.LayerNorm.bias', 'bert.encoder.layer.2.intermediate.dense.weight', 'bert.encoder.layer.0.attention.self.key.bias', 'bert.encoder.layer.2.intermediate.dense.bias', 'bert.encoder.layer.4.attention.self.query.bias', 'bert.encoder.layer.7.intermediate.dense.weight', 'bert.encoder.layer.10.intermediate.dense.weight', 'bert.encoder.layer.1.attention.self.value.weight', 'bert.encoder.layer.7.attention.output.LayerNorm.weight', 'bert.encoder.layer.3.intermediate.dense.bias', 'bert.encoder.layer.11.attention.self.value.bias', 'bert.encoder.layer.4.attention.self.query.weight', 'bert.encoder.layer.6.output.LayerNorm.weight', 'bert.encoder.layer.3.attention.self.key.weight', 'bert.encoder.layer.9.attention.self.query.weight', 'bert.encoder.layer.0.attention.self.value.bias', 'bert.encoder.layer.3.attention.output.dense.weight', 'bert.encoder.layer.0.output.LayerNorm.weight', 'bert.encoder.layer.7.attention.self.value.bias', 'bert.encoder.layer.3.attention.self.query.bias', 'bert.encoder.layer.1.intermediate.dense.bias', 'bert.encoder.layer.9.attention.self.key.bias', 'bert.encoder.layer.5.attention.self.key.weight', 'bert.encoder.layer.4.attention.output.LayerNorm.weight', 'bert.encoder.layer.1.intermediate.dense.weight', 'bert.encoder.layer.2.attention.output.LayerNorm.bias', 'bert.encoder.layer.8.output.LayerNorm.bias', 'bert.encoder.layer.11.output.LayerNorm.weight', 'qa_outputs.weight', 'bert.encoder.layer.11.output.dense.bias', 'bert.encoder.layer.4.output.LayerNorm.weight', 'bert.encoder.layer.7.attention.self.key.bias', 'bert.encoder.layer.6.output.dense.weight', 'bert.encoder.layer.2.output.LayerNorm.bias', 'bert.encoder.layer.7.attention.output.dense.bias', 'bert.encoder.layer.0.intermediate.dense.bias', 'bert.encoder.layer.11.attention.output.dense.weight', 'bert.encoder.layer.11.intermediate.dense.weight', 'bert.encoder.layer.0.attention.output.dense.weight', 'bert.embeddings.LayerNorm.weight', 'bert.encoder.layer.10.attention.self.key.bias', 'bert.encoder.layer.5.attention.output.LayerNorm.bias', 'bert.encoder.layer.5.intermediate.dense.weight', 'bert.embeddings.position_embeddings.weight', 'bert.encoder.layer.0.intermediate.dense.weight', 'bert.encoder.layer.4.output.LayerNorm.bias', 'bert.encoder.layer.8.intermediate.dense.weight', 'bert.encoder.layer.9.attention.self.query.bias', 'bert.encoder.layer.2.attention.output.dense.bias', 'bert.encoder.layer.3.attention.output.LayerNorm.weight', 'bert.encoder.layer.10.attention.self.value.bias', 'bert.encoder.layer.1.attention.self.query.weight', 'bert.encoder.layer.1.attention.self.query.bias', 'bert.encoder.layer.5.intermediate.dense.bias', 'bert.encoder.layer.4.output.dense.weight', 'bert.encoder.layer.6.attention.self.value.bias', 'bert.encoder.layer.3.output.dense.bias', 'bert.encoder.layer.9.attention.output.LayerNorm.weight', 'bert.encoder.layer.7.attention.output.LayerNorm.bias', 'bert.encoder.layer.1.attention.output.LayerNorm.bias', 'bert.encoder.layer.4.attention.self.value.bias', 'bert.encoder.layer.6.attention.output.LayerNorm.weight', 'bert.encoder.layer.9.intermediate.dense.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.weight', 'bert.encoder.layer.10.intermediate.dense.bias', 'bert.embeddings.LayerNorm.bias', 'bert.encoder.layer.1.attention.output.LayerNorm.weight', 'bert.encoder.layer.6.attention.output.LayerNorm.bias', 'bert.encoder.layer.6.output.LayerNorm.bias', 'bert.encoder.layer.2.output.dense.weight', 'bert.encoder.layer.8.attention.self.query.weight', 'bert.encoder.layer.11.output.LayerNorm.bias', 'bert.encoder.layer.2.attention.self.value.weight', 'bert.encoder.layer.7.output.dense.bias', 'bert.encoder.layer.11.attention.output.LayerNorm.bias', 'bert.encoder.layer.4.attention.self.key.bias', 'bert.encoder.layer.10.attention.self.query.bias', 'bert.encoder.layer.0.attention.self.query.weight', 'bert.encoder.layer.9.attention.self.value.weight', 'bert.encoder.layer.3.output.LayerNorm.bias', 'bert.encoder.layer.4.attention.output.dense.weight', 'bert.encoder.layer.8.attention.output.dense.bias', 'bert.encoder.layer.9.output.LayerNorm.weight', 'bert.encoder.layer.5.attention.self.value.weight', 'bert.encoder.layer.2.output.LayerNorm.weight', 'bert.encoder.layer.0.output.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "from transformers import TrainingArguments\n",
    "from transformers import AutoModelForQuestionAnswering, AutoTokenizer\n",
    "\n",
    "#BertForMaskedLM\n",
    "from transformers import BertForMaskedLM\n",
    "\n",
    "args = TrainingArguments(\n",
    "    \"bert-finetuned-squad\",\n",
    "    evaluation_strategy=\"no\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    fp16=True,\n",
    "    push_to_hub=False,\n",
    ")\n",
    "# bert_model = BertForMaskedLM.from_pretrained(pretrained_model_name_or_path='bert-base-uncased')\n",
    "# bert_model = AutoModelForQuestionAnswering.from_pretrained(\"bert-base-uncased\")\n",
    "# bert_model = AutoModelForQuestionAnswering.from_pretrained(\"/home/kiki/dados_servidor/Servidor/NLPLab/berts/models/bert_spanbert\")\n",
    "if 'miu' in cur_dir:\n",
    "    bert_model = AutoModelForQuestionAnswering.from_pretrained(\"/home/miu/Projects/NLPLab/berts/models/bert\")\n",
    "else:\n",
    "    bert_model = AutoModelForQuestionAnswering.from_pretrained(\"/home/kiki/dados_servidor/Servidor/NLPLab/berts/models/bert\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mabcp4\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/miu/Projects/NLPLab/berts/wandb/run-20231224_114640-25o8xhra</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/abcp4/huggingface/runs/25o8xhra' target=\"_blank\">youthful-firebrand-172</a></strong> to <a href='https://wandb.ai/abcp4/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/abcp4/huggingface' target=\"_blank\">https://wandb.ai/abcp4/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/abcp4/huggingface/runs/25o8xhra' target=\"_blank\">https://wandb.ai/abcp4/huggingface/runs/25o8xhra</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2500 [00:00<?, ?it/s]You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      " 20%|██        | 500/2500 [00:39<02:38, 12.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.7581, 'learning_rate': 1.6000000000000003e-05, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 1000/2500 [01:20<01:57, 12.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.0755, 'learning_rate': 1.2e-05, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 1500/2500 [02:00<01:19, 12.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.7534, 'learning_rate': 8.000000000000001e-06, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 2000/2500 [02:40<00:39, 12.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.5015, 'learning_rate': 4.000000000000001e-06, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2500/2500 [03:21<00:00, 12.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.3212, 'learning_rate': 8e-09, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2500/2500 [03:22<00:00, 12.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 206.7869, 'train_samples_per_second': 96.718, 'train_steps_per_second': 12.09, 'train_loss': 3.8819138671875, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2500, training_loss=3.8819138671875, metrics={'train_runtime': 206.7869, 'train_samples_per_second': 96.718, 'train_steps_per_second': 12.09, 'train_loss': 3.8819138671875, 'epoch': 5.0})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=bert_model,\n",
    "    args=args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import collections\n",
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"squad\")\n",
    "n_best = 20\n",
    "max_answer_length = 30\n",
    "predicted_answers = []\n",
    "\n",
    "def compute_metrics(start_logits, end_logits, features, examples):\n",
    "    example_to_features = collections.defaultdict(list)\n",
    "    for idx, feature in enumerate(features):\n",
    "        example_to_features[feature[\"example_id\"]].append(idx)\n",
    "\n",
    "    predicted_answers = []\n",
    "    for example in tqdm(examples):\n",
    "        example_id = example[\"id\"]\n",
    "        context = example[\"context\"]\n",
    "        answers = []\n",
    "\n",
    "        # Loop through all features associated with that example\n",
    "        for feature_index in example_to_features[example_id]:\n",
    "            start_logit = start_logits[feature_index]\n",
    "            end_logit = end_logits[feature_index]\n",
    "            offsets = features[feature_index][\"offset_mapping\"]\n",
    "\n",
    "            start_indexes = np.argsort(start_logit)[-1 : -n_best - 1 : -1].tolist()\n",
    "            end_indexes = np.argsort(end_logit)[-1 : -n_best - 1 : -1].tolist()\n",
    "            for start_index in start_indexes:\n",
    "                for end_index in end_indexes:\n",
    "                    # Skip answers that are not fully in the context\n",
    "                    if offsets[start_index] is None or offsets[end_index] is None:\n",
    "                        continue\n",
    "                    # Skip answers with a length that is either < 0 or > max_answer_length\n",
    "                    if (\n",
    "                        end_index < start_index\n",
    "                        or end_index - start_index + 1 > max_answer_length\n",
    "                    ):\n",
    "                        continue\n",
    "\n",
    "                    answer = {\n",
    "                        \"text\": context[offsets[start_index][0] : offsets[end_index][1]],\n",
    "                        \"logit_score\": start_logit[start_index] + end_logit[end_index],\n",
    "                    }\n",
    "                    answers.append(answer)\n",
    "\n",
    "        # Select the answer with the best score\n",
    "        if len(answers) > 0:\n",
    "            best_answer = max(answers, key=lambda x: x[\"logit_score\"])\n",
    "            predicted_answers.append(\n",
    "                {\"id\": example_id, \"prediction_text\": best_answer[\"text\"]}\n",
    "            )\n",
    "        else:\n",
    "            predicted_answers.append({\"id\": example_id, \"prediction_text\": \"\"})\n",
    "\n",
    "    theoretical_answers = [{\"id\": ex[\"id\"], \"answers\": ex[\"answers\"]} for ex in examples]\n",
    "    return metric.compute(predictions=predicted_answers, references=theoretical_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 126/126 [00:02<00:00, 44.42it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 1607.05it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'exact_match': 3.9, 'f1': 8.451318538500729}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import evaluate\n",
    "\n",
    "predictions, _, _ = trainer.predict(validation_dataset)\n",
    "start_logits, end_logits = predictions\n",
    "compute_metrics(start_logits, end_logits, validation_dataset, dataset[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 126/126 [00:02<00:00, 44.45it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 1573.49it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'exact_match': 4.2, 'f1': 8.794485888565596}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#100k 512 tokens\n",
    "{'exact_match': 4.2, 'f1': 8.794485888565596}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SEQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hidden layer extractor class, for magically adding adapter to language model to be pretrained\n",
    "class HiddenLayerExtractor(nn.Module):\n",
    "    def __init__(self, net, layer = -2):\n",
    "        super().__init__()\n",
    "        self.net = net\n",
    "        self.layer = layer\n",
    "\n",
    "        self.hidden = None\n",
    "        self.hook_registered = False\n",
    "\n",
    "    def _find_layer(self):\n",
    "        if type(self.layer) == str:\n",
    "            modules = dict([*self.net.named_modules()])\n",
    "            return modules.get(self.layer, None)\n",
    "        elif type(self.layer) == int:\n",
    "            children = [*self.net.children()]\n",
    "            return children[self.layer]\n",
    "        return None\n",
    "\n",
    "    def _hook(self, _, __, output):\n",
    "        self.hidden = output\n",
    "\n",
    "    def _register_hook(self):\n",
    "        layer = self._find_layer()\n",
    "        assert layer is not None, f'hidden layer ({self.layer}) not found'\n",
    "        handle = layer.register_forward_hook(self._hook)\n",
    "        self.hook_registered = True\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.layer == -1:\n",
    "            return self.net(x)\n",
    "\n",
    "        if not self.hook_registered:\n",
    "            self._register_hook()\n",
    "\n",
    "        _ = self.net(x)\n",
    "        hidden = self.hidden\n",
    "        self.hidden = None\n",
    "        assert hidden is not None, f'hidden layer {self.layer} never emitted an output'\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_extractor = HiddenLayerExtractor(model,-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "import torch\n",
    "import os\n",
    "from torch.utils.data import  DataLoader\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "class Classifier(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Classifier, self).__init__()\n",
    "        # self.l1 = model._modules['model']\n",
    "        self.l1 = model_extractor\n",
    "        self.l2 = torch.nn.Dropout(0.3)\n",
    "        self.fl = torch.nn.Flatten()\n",
    "        self.l3 = torch.nn.Linear(768*(MODEL_MAX_SEQ_LEN-2), 3)\n",
    "    \n",
    "    def forward(self, ids):\n",
    "        output= self.l1(ids)\n",
    "        # print('l1:',output.shape)\n",
    "        output = self.l2(output)\n",
    "        # print('l2:',output.shape)\n",
    "        output = self.fl(output)\n",
    "        # print('fl:',output.shape)\n",
    "        output = self.l3(output)\n",
    "        # print('l3:',output.shape)\n",
    "        return output\n",
    "\n",
    "classifier_model = Classifier()\n",
    "classifier_model=classifier_model.to('cuda')\n",
    "optimizer = torch.optim.Adam(params =  classifier_model.parameters(), lr=2e-5,weight_decay=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "task='mrpc'\n",
    "dataset = load_dataset('glue', task, split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class TokenizedDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "        self.ls=[]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        s=norm.normalize_str(self.dataset[\"sentence1\"][i])+'[SEP]'+norm.normalize_str(self.dataset[\"sentence2\"][i])\n",
    "        tokens = vocab.tokenize(s).tolist()\n",
    "        \n",
    "        self.ls.append(i)\n",
    "        \n",
    "        #trucate\n",
    "        tokens=tokens[:MODEL_MAX_SEQ_LEN - 2]\n",
    "        l=len(tokens)\n",
    "        for j in range(l,MODEL_MAX_SEQ_LEN - 2):\n",
    "            tokens.append(0)\n",
    "        tokens=torch.Tensor(tokens).long()\n",
    "\n",
    "        label=self.dataset[\"label\"][i]\n",
    "        \n",
    "        d={'input_ids':tokens,'label':label}\n",
    "        return d\n",
    "\n",
    "\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "tokenized_dataset = TokenizedDataset(dataset)\n",
    "# train_dataloader=CustomDataloader(tokenized_dataset)\n",
    "train_dataloader = DataLoader(\n",
    "        tokenized_dataset, batch_size=DEVICE_BATCH_SIZE\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "\n",
    "classifier_model.train()\n",
    "\n",
    "def loss_fn(outputs, targets):\n",
    "    loss=torch.nn.CrossEntropyLoss()(outputs, targets)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss:  0.030146934986114503\n",
      "Epoch: 1, Loss:  0.5000924682617187\n",
      "Epoch: 2, Loss:  0.5059760808944702\n",
      "Epoch: 3, Loss:  0.4948312652111053\n",
      "Epoch: 4, Loss:  0.48995151281356814\n"
     ]
    }
   ],
   "source": [
    "#FINETUNING\n",
    "\n",
    "epochs=5\n",
    "running_loss=0\n",
    "for i in range(epochs):\n",
    "    for _,data in enumerate(train_dataloader):\n",
    "        ids = data['input_ids'].to('cuda', dtype = torch.long)\n",
    "        targets = data['label'].to('cuda', dtype = torch.long)\n",
    "\n",
    "        outputs = classifier_model(ids)\n",
    "        \n",
    "        loss = loss_fn(outputs,targets)\n",
    "        # loss=outputs.loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        running_loss+=loss.item()\n",
    "        if _%50==0:\n",
    "            print(f'Epoch: {i}, Loss:  {running_loss/50}')\n",
    "            running_loss=0\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EVALUATE\n",
    "dataset = load_dataset('glue', task, split='validation')\n",
    "tokenized_dataset = TokenizedDataset(dataset)\n",
    "\n",
    "eval_dataloader = DataLoader(\n",
    "        tokenized_dataset, batch_size=DEVICE_BATCH_SIZE\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "predictions=[]\n",
    "labels=[]\n",
    "c=0\n",
    "for _,data in enumerate(eval_dataloader):\n",
    "    ids = data['input_ids'].to('cuda', dtype = torch.long)\n",
    "    targets = data['label'].to('cuda', dtype = torch.long)\n",
    "    \n",
    "    outputs = classifier_model(ids)\n",
    "    \n",
    "    outputs=torch.argmax(outputs, dim=1).cpu()\n",
    "    targets=targets.cpu()\n",
    "    \n",
    "    outputs=[int(o.item()) for o in outputs]\n",
    "    targets=[int(o.item()) for o in targets]\n",
    "\n",
    "    predictions+=outputs\n",
    "    labels+=targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.07      0.12       129\n",
      "           1       0.69      0.98      0.81       279\n",
      "\n",
      "    accuracy                           0.69       408\n",
      "   macro avg       0.65      0.52      0.47       408\n",
      "weighted avg       0.66      0.69      0.60       408\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(labels, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bert 3 layers  10000 dataset\n",
    "#               precision    recall  f1-score   support\n",
    "\n",
    "#            0       0.61      0.09      0.15       129\n",
    "#            1       0.70      0.97      0.81       279\n",
    "\n",
    "#     accuracy                           0.69       408\n",
    "#    macro avg       0.65      0.53      0.48       408\n",
    "# weighted avg       0.67      0.69      0.60       408"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DensePhrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "        num_rows: 4000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"squad\", split=\"train[:5000]\")\n",
    "dataset = dataset.train_test_split(test_size=0.2)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load model and tokenizer\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel,AutoModelForQuestionAnswering\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "#model = AutoModelForQuestionAnswering.from_pretrained(\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedModel\n",
    "from transformers import AutoConfig\n",
    "import copy\n",
    "from torch.nn import CrossEntropyLoss\n",
    "# Encoders: three LMs\n",
    "config = AutoConfig.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "class Encoder(PreTrainedModel):\n",
    "    def __init__(self):\n",
    "        super().__init__(config)\n",
    "        self.phrase_encoder = AutoModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "        self.query_start_encoder = copy.deepcopy(self.phrase_encoder)\n",
    "        self.query_end_encoder = copy.deepcopy(self.phrase_encoder)\n",
    "    def embed_phrase(self, input_ids, attention_mask, token_type_ids):\n",
    "        \n",
    "        \"\"\" Get phrase embeddings (token-wise) \"\"\"\n",
    "        outputs = self.phrase_encoder(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "        )\n",
    "        return outputs[0], outputs[0]\n",
    "\n",
    "    def embed_query(self, input_ids_, attention_mask_, token_type_ids_):\n",
    "        \"\"\" Get query start/end embeddings \"\"\"\n",
    "        # Two LM based query reps\n",
    "        outputs_s_ = self.query_start_encoder(\n",
    "            input_ids_,\n",
    "            attention_mask=attention_mask_,\n",
    "            token_type_ids=token_type_ids_,\n",
    "        )\n",
    "        outputs_e_ = self.query_end_encoder(\n",
    "            input_ids_,\n",
    "            attention_mask=attention_mask_,\n",
    "            token_type_ids=token_type_ids_,\n",
    "        )\n",
    "        sequence_output_s_ = outputs_s_[0]\n",
    "        sequence_output_e_ = outputs_e_[0]\n",
    "        query_start = sequence_output_s_[:,:1,:]\n",
    "        query_end = sequence_output_e_[:,:1,:]\n",
    "        return query_start, query_end\n",
    "    \n",
    "    def forward(self,\n",
    "            input_ids=None, attention_mask=None, token_type_ids=None,\n",
    "            input_ids_=None, attention_mask_=None, token_type_ids_=None,\n",
    "            start_positions=None, end_positions=None,\n",
    "            neg_input_ids=None, neg_attention_mask=None, neg_token_type_ids=None,\n",
    "            example_id=None,):\n",
    "        # Context-side\n",
    "        if input_ids is not None:\n",
    "            assert len(input_ids.size()) == 2\n",
    "            start, end = self.embed_phrase(input_ids, attention_mask, token_type_ids)\n",
    "\n",
    "            if neg_input_ids is not None:\n",
    "                neg_start, neg_end = self.embed_phrase(neg_input_ids, neg_attention_mask, neg_token_type_ids)\n",
    "\n",
    "            # Get filter logits\n",
    "            filter_output = start[:]\n",
    "            filter_start_logits, filter_end_logits = self.filter_linear(filter_output).chunk(2, dim=2)\n",
    "            filter_start_logits = filter_start_logits.squeeze(2)\n",
    "            filter_end_logits = filter_end_logits.squeeze(2)\n",
    "\n",
    "            if self.return_phrase:\n",
    "                return (start, end, filter_start_logits, filter_end_logits)\n",
    "\n",
    "        # Query-side\n",
    "        if input_ids_ is not None:\n",
    "            assert len(input_ids_.size()) == 2\n",
    "            query_start, query_end = self.embed_query(input_ids_, attention_mask_, token_type_ids_)\n",
    "\n",
    "            if self.return_query:\n",
    "                return (query_start, query_end)\n",
    "        \n",
    "        # Get dense logits\n",
    "        start_logits = start.matmul(query_start.transpose(1, 2)).squeeze(-1)\n",
    "        end_logits = end.matmul(query_end.transpose(1, 2)).squeeze(-1)\n",
    "        dense_logits = start_logits.unsqueeze(2) + end_logits.unsqueeze(1)\n",
    "\n",
    "        if start_positions is not None and end_positions is not None:\n",
    "            # If we are on multi-GPU, split add a dimension\n",
    "            if len(start_positions.size()) > 1:\n",
    "                start_positions = start_positions.squeeze(-1)\n",
    "            if len(end_positions.size()) > 1:\n",
    "                end_positions = end_positions.squeeze(-1)\n",
    "\n",
    "            ignored_index = start_logits.size(1)\n",
    "            start_positions.clamp_(0, ignored_index)\n",
    "            end_positions.clamp_(0, ignored_index)\n",
    "\n",
    "            # 1) Single-passage loss\n",
    "            loss_fct = CrossEntropyLoss(ignore_index=ignored_index)\n",
    "            start_loss = loss_fct(dense_logits.mean(2), start_positions)\n",
    "            end_loss = loss_fct(dense_logits.mean(1), end_positions)\n",
    "            single_loss = (start_loss + end_loss) / 2\n",
    "            total_loss = single_loss\n",
    "\n",
    "            return (total_loss,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Encoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preprocessing the datasets.\n",
    "# Preprocessing is slighlty different for training and evaluation.\n",
    "column_names = dataset[\"train\"].column_names\n",
    "\n",
    "question_column_name = \"question\" if \"question\" in column_names else column_names[0]\n",
    "context_column_name = \"context\" if \"context\" in column_names else column_names[1]\n",
    "answer_column_name = \"answers\" if \"answers\" in column_names else column_names[2]\n",
    "\n",
    "# Padding side determines if we do (question|context) or (context|question).\n",
    "pad_on_right = tokenizer.padding_side == \"right\"\n",
    "\n",
    "max_seq_length = tokenizer.model_max_length\n",
    "max_query_length = 50\n",
    "pad_on_right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training preprocessing\n",
    "def prepare_train_features(examples):\n",
    "    # Some of the questions have lots of whitespace on the left, which is not useful and will make the\n",
    "    # truncation of the context fail (the tokenized question will take a lots of space). So we remove that\n",
    "    # left whitespace\n",
    "    examples[question_column_name] = [q.lstrip() for q in examples[question_column_name]]\n",
    "\n",
    "    # Tokenize our examples with truncation and maybe padding, but keep the overflows using a stride. This results\n",
    "    # in one example possible giving several features when a context is long, each of those features having a\n",
    "    # context that overlaps a bit the context of the previous feature.\n",
    "    \n",
    "    tokenized_examples = tokenizer(\n",
    "        examples[context_column_name],\n",
    "        truncation=\"only_first\",\n",
    "        max_length=max_seq_length,\n",
    "        stride=128,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "\n",
    "    tokenized_questions = tokenizer(\n",
    "        examples[question_column_name],\n",
    "        truncation=\"only_first\",\n",
    "        max_length=max_query_length,\n",
    "        return_overflowing_tokens=False,\n",
    "        return_offsets_mapping=False,\n",
    "        padding=\"max_length\" \n",
    "    )\n",
    "\n",
    "    # Since one example might give us several features if it has a long context, we need a map from a feature to\n",
    "    # its corresponding example. This key gives us just that.\n",
    "    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
    "    # The offset mappings will give us a map from token to character position in the original context. This will\n",
    "    # help us compute the start_positions and end_positions.\n",
    "    offset_mapping = tokenized_examples.pop(\"offset_mapping\")\n",
    "\n",
    "    # Inflate questions based on sample_mapping\n",
    "    tokenized_examples['input_ids_'] = [tokenized_questions['input_ids'][i] for i in sample_mapping]\n",
    "    #tokenized_examples['token_type_ids_'] = [tokenized_questions['token_type_ids'][i] for i in sample_mapping]\n",
    "    #tokenized_examples['attention_mask_'] = [tokenized_questions['attention_mask'][i] for i in sample_mapping]\n",
    "\n",
    "    # Let's label those examples!\n",
    "    tokenized_examples[\"start_positions\"] = []\n",
    "    tokenized_examples[\"end_positions\"] = []\n",
    "\n",
    "    for i, offsets in enumerate(offset_mapping):\n",
    "        # We will label impossible answers with the index of the CLS token.\n",
    "        input_ids = tokenized_examples[\"input_ids\"][i]\n",
    "        cls_index = input_ids.index(tokenizer.cls_token_id)\n",
    "\n",
    "        # Grab the sequence corresponding to that example (to know what is the context and what is the question).\n",
    "        sequence_ids = tokenized_examples.sequence_ids(i)\n",
    "        #context_index = 1 if pad_on_right and args.append_title else 0\n",
    "        context_index =  0\n",
    "\n",
    "        # One example can give several spans, this is the index of the example containing this span of text.\n",
    "        sample_index = sample_mapping[i]\n",
    "        answers = examples[answer_column_name][sample_index]\n",
    "        # If no answers are given, set the cls_index as answer.\n",
    "        if len(answers[\"answer_start\"]) == 0:\n",
    "            tokenized_examples[\"start_positions\"].append(cls_index)\n",
    "            tokenized_examples[\"end_positions\"].append(cls_index)\n",
    "        else:\n",
    "            # Start/end character index of the answer in the text.\n",
    "            start_char = answers[\"answer_start\"][0]\n",
    "            end_char = start_char + len(answers[\"text\"][0])\n",
    "\n",
    "            # Start token index of the current span in the text.\n",
    "            token_start_index = 0\n",
    "            while sequence_ids[token_start_index] != context_index:\n",
    "                token_start_index += 1\n",
    "\n",
    "            # End token index of the current span in the text.\n",
    "            token_end_index = len(input_ids) - 1\n",
    "            while sequence_ids[token_end_index] != context_index:\n",
    "                token_end_index -= 1\n",
    "\n",
    "            # Detect if the answer is out of the span (in which case this feature is labeled with the CLS index).\n",
    "            if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n",
    "                tokenized_examples[\"start_positions\"].append(cls_index)\n",
    "                tokenized_examples[\"end_positions\"].append(cls_index)\n",
    "            else:\n",
    "                # Otherwise move the token_start_index and token_end_index to the two ends of the answer.\n",
    "                # Note: we could go after the last offset if the answer is the last word (edge case).\n",
    "                while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n",
    "                    token_start_index += 1\n",
    "                tokenized_examples[\"start_positions\"].append(token_start_index - 1)\n",
    "                while offsets[token_end_index][1] >= end_char:\n",
    "                    token_end_index -= 1\n",
    "                tokenized_examples[\"end_positions\"].append(token_end_index + 1)\n",
    "\n",
    "    #bugfix: limit to 1000. Ajeitar!!!\n",
    "    tokenized_examples[\"input_ids\"]=tokenized_examples[\"input_ids\"][:1000]\n",
    "    tokenized_examples[\"input_ids_\"]=tokenized_examples[\"input_ids\"][:1000]\n",
    "    tokenized_examples[\"attention_mask\"]=tokenized_examples[\"attention_mask\"][:1000]\n",
    "    # tokenized_examples[\"token_type_ids\"]=tokenized_examples[\"token_type_ids\"][:1000]\n",
    "    tokenized_examples[\"start_positions\"]=tokenized_examples[\"start_positions\"][:1000]\n",
    "    tokenized_examples[\"end_positions\"]=tokenized_examples[\"end_positions\"][:1000]\n",
    "    return tokenized_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running tokenizer on train dataset:   0%|          | 0/4000 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running tokenizer on train dataset: 100%|██████████| 4000/4000 [00:00<00:00, 5132.90 examples/s]\n"
     ]
    }
   ],
   "source": [
    "train_dataset = dataset[\"train\"]\n",
    "train_dataset = train_dataset.map(\n",
    "                prepare_train_features,\n",
    "                batched=True,\n",
    "                num_proc=1,\n",
    "                #remove_columns=column_names,\n",
    "                #load_from_cache_file=not args.overwrite_cache,\n",
    "                desc=\"Running tokenizer on train dataset\",\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import default_data_collator\n",
    "from transformers import DataCollatorWithPadding,EvalPrediction\n",
    "from datasets import load_metric\n",
    "from utils.utils_qa import postprocess_qa_predictions\n",
    "\n",
    "# Post-processing:\n",
    "def post_processing_function(examples, features, predictions, stage=\"eval\", filter_threshold=-1e5):\n",
    "    # Post-processing: we match the start logits and end logits to answers in the original context.\n",
    "    predictions, save_rate = postprocess_qa_predictions(\n",
    "        examples=examples,\n",
    "        features=features,\n",
    "        predictions=predictions,\n",
    "        version_2_with_negative=args.version_2_with_negative,\n",
    "        n_best_size=args.n_best_size,\n",
    "        max_answer_length=args.max_answer_length,\n",
    "        null_score_diff_threshold=args.null_score_diff_threshold,\n",
    "        output_dir=args.output_dir,\n",
    "        log_level=0,\n",
    "        prefix=stage,\n",
    "        filter_threshold=filter_threshold,\n",
    "    )\n",
    "    # Format the result to the format the metric expects.\n",
    "    if args.version_2_with_negative:\n",
    "        formatted_predictions = [\n",
    "            {\"id\": k, \"prediction_text\": v, \"no_answer_probability\": 0.0} for k, v in predictions.items()\n",
    "        ]\n",
    "    else:\n",
    "        formatted_predictions = [{\"id\": k, \"prediction_text\": v} for k, v in predictions.items()]\n",
    "\n",
    "    references = [{\"id\": ex[\"id\"], \"answers\": ex[answer_column_name]} for ex in examples]\n",
    "    return EvalPrediction(predictions=formatted_predictions, label_ids=references), save_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/miu/miniconda3/envs/torch/lib/python3.10/site-packages/datasets/load.py:752: FutureWarning: The repository for squad contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.16.0/metrics/squad/squad.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
     ]
    }
   ],
   "source": [
    "from utils.trainer_qa import QuestionAnsweringTrainer\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "pad_to_max_length=True\n",
    "metric = load_metric(\"squad\")\n",
    "# Data collator\n",
    "# We have already padded to max length if the corresponding flag is True, otherwise we need to pad in the data\n",
    "# collator.\n",
    "data_collator = (\n",
    "    default_data_collator\n",
    "    # if args.pad_to_max_length\n",
    "    # else DataCollatorWithPadding(tokenizer, pad_to_multiple_of=8 if args.fp16 else None)\n",
    "    if pad_to_max_length\n",
    "    else DataCollatorWithPadding(tokenizer, pad_to_multiple_of=8 if args.fp16 else None)\n",
    ")\n",
    "\n",
    "def compute_metrics(p: EvalPrediction):\n",
    "    return metric.compute(predictions=p.predictions, references=p.label_ids)\n",
    "\n",
    "args = TrainingArguments(\n",
    "    \"bert-finetuned-squad\",\n",
    "    evaluation_strategy=\"no\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    fp16=True,\n",
    "    push_to_hub=False,\n",
    ")\n",
    "\n",
    "# Initialize our Trainer\n",
    "trainer = QuestionAnsweringTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    # args=None,\n",
    "    train_dataset=train_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    post_process_function=post_processing_function,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2500 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "DistilBertModel.forward() got an unexpected keyword argument 'token_type_ids'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/miu/Projects/NLPLab/berts/bert_colt5.ipynb Cell 46\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/miu/Projects/NLPLab/berts/bert_colt5.ipynb#X63sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m train_result \u001b[39m=\u001b[39m trainer\u001b[39m.\u001b[39;49mtrain(resume_from_checkpoint\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.10/site-packages/transformers/trainer.py:1537\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1535\u001b[0m         hf_hub_utils\u001b[39m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1536\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1537\u001b[0m     \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   1538\u001b[0m         args\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m   1539\u001b[0m         resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[1;32m   1540\u001b[0m         trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[1;32m   1541\u001b[0m         ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[1;32m   1542\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.10/site-packages/transformers/trainer.py:1854\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1851\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_handler\u001b[39m.\u001b[39mon_step_begin(args, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol)\n\u001b[1;32m   1853\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maccelerator\u001b[39m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 1854\u001b[0m     tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining_step(model, inputs)\n\u001b[1;32m   1856\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m   1857\u001b[0m     args\u001b[39m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1858\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1859\u001b[0m     \u001b[39mand\u001b[39;00m (torch\u001b[39m.\u001b[39misnan(tr_loss_step) \u001b[39mor\u001b[39;00m torch\u001b[39m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1860\u001b[0m ):\n\u001b[1;32m   1861\u001b[0m     \u001b[39m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1862\u001b[0m     tr_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m tr_loss \u001b[39m/\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mglobal_step \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.10/site-packages/transformers/trainer.py:2735\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2732\u001b[0m     \u001b[39mreturn\u001b[39;00m loss_mb\u001b[39m.\u001b[39mreduce_mean()\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m   2734\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 2735\u001b[0m     loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_loss(model, inputs)\n\u001b[1;32m   2737\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mn_gpu \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   2738\u001b[0m     loss \u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mmean()  \u001b[39m# mean() to average on multi-gpu parallel training\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.10/site-packages/transformers/trainer.py:2758\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   2756\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   2757\u001b[0m     labels \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 2758\u001b[0m outputs \u001b[39m=\u001b[39m model(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49minputs)\n\u001b[1;32m   2759\u001b[0m \u001b[39m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   2760\u001b[0m \u001b[39m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   2761\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mpast_index \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.10/site-packages/accelerate/utils/operations.py:680\u001b[0m, in \u001b[0;36mconvert_outputs_to_fp32.<locals>.forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    679\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 680\u001b[0m     \u001b[39mreturn\u001b[39;00m model_forward(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.10/site-packages/accelerate/utils/operations.py:668\u001b[0m, in \u001b[0;36mConvertOutputsToFp32.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    667\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 668\u001b[0m     \u001b[39mreturn\u001b[39;00m convert_to_fp32(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel_forward(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs))\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.10/site-packages/torch/amp/autocast_mode.py:16\u001b[0m, in \u001b[0;36mautocast_decorator.<locals>.decorate_autocast\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m     14\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_autocast\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     15\u001b[0m     \u001b[39mwith\u001b[39;00m autocast_instance:\n\u001b[0;32m---> 16\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[1;32m/home/miu/Projects/NLPLab/berts/bert_colt5.ipynb Cell 46\u001b[0m line \u001b[0;36m5\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/miu/Projects/NLPLab/berts/bert_colt5.ipynb#X63sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m \u001b[39mif\u001b[39;00m input_ids \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/miu/Projects/NLPLab/berts/bert_colt5.ipynb#X63sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mlen\u001b[39m(input_ids\u001b[39m.\u001b[39msize()) \u001b[39m==\u001b[39m \u001b[39m2\u001b[39m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/miu/Projects/NLPLab/berts/bert_colt5.ipynb#X63sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m     start, end \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membed_phrase(input_ids, attention_mask, token_type_ids)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/miu/Projects/NLPLab/berts/bert_colt5.ipynb#X63sZmlsZQ%3D%3D?line=53'>54</a>\u001b[0m     \u001b[39mif\u001b[39;00m neg_input_ids \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/miu/Projects/NLPLab/berts/bert_colt5.ipynb#X63sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m         neg_start, neg_end \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed_phrase(neg_input_ids, neg_attention_mask, neg_token_type_ids)\n",
      "\u001b[1;32m/home/miu/Projects/NLPLab/berts/bert_colt5.ipynb Cell 46\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/miu/Projects/NLPLab/berts/bert_colt5.ipynb#X63sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39membed_phrase\u001b[39m(\u001b[39mself\u001b[39m, input_ids, attention_mask, token_type_ids):\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/miu/Projects/NLPLab/berts/bert_colt5.ipynb#X63sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\" Get phrase embeddings (token-wise) \"\"\"\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/miu/Projects/NLPLab/berts/bert_colt5.ipynb#X63sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mphrase_encoder(\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/miu/Projects/NLPLab/berts/bert_colt5.ipynb#X63sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m         input_ids,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/miu/Projects/NLPLab/berts/bert_colt5.ipynb#X63sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/miu/Projects/NLPLab/berts/bert_colt5.ipynb#X63sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m         token_type_ids\u001b[39m=\u001b[39;49mtoken_type_ids,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/miu/Projects/NLPLab/berts/bert_colt5.ipynb#X63sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m     )\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/miu/Projects/NLPLab/berts/bert_colt5.ipynb#X63sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m outputs[\u001b[39m0\u001b[39m], outputs[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: DistilBertModel.forward() got an unexpected keyword argument 'token_type_ids'"
     ]
    }
   ],
   "source": [
    "train_result = trainer.train(resume_from_checkpoint=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
